<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 58]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.CV](#cs.CV) [Total: 141]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.NI](#cs.NI) [Total: 25]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.SE](#cs.SE) [Total: 37]
- [econ.EM](#econ.EM) [Total: 4]
- [econ.GN](#econ.GN) [Total: 8]
- [econ.TH](#econ.TH) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 论文提出了一种基于量子场论类比的理论框架（自由意志方程），赋予AGI决策过程中的自适应随机性，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，而人类智能具有自适应自发性（类似自由意志），这对创造力和适应性至关重要。

Method: 提出自由意志方程，将AI认知状态视为潜在行动的叠加态，决策时概率性坍缩为具体行动，类似量子波函数坍缩。

Result: 在非稳态多臂老虎机环境中，使用该框架的代理获得了更高的奖励和策略多样性。

Conclusion: 该框架为AGI提供了更接近人类智能的自适应决策能力，实验验证了其有效性。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [2] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM规划器和领域专用代理实现材料发现的高通量、高保真模拟，减少对人类专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统误差处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划器和领域专用代理（如原子结构生成、DFT收敛测试、HPC调度和错误处理），并通过共享画布避免幻觉。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并通过贝叶斯集成采样验证了FCC位点偏好。

Conclusion: DREAMS实现了L3级自动化，显著减少对人类干预的依赖，为高通量计算材料发现提供了可扩展路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [3] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是一个用于评估网络代理行为风险的数据集，旨在为现实世界在线环境开发安全措施。研究发现当前LLM在预测高风险行为时表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自主网络代理快速发展，其可能采取意外或有害行为的风险凸显，亟需有效的安全措施。

Method: 引入WebGuard数据集，包含4,939个人工标注的行为，采用三级风险分类（SAFE、LOW、HIGH），并评估LLM和微调模型的性能。

Result: 前沿LLM在预测行为结果和高风险行为召回率上表现不足（均低于60%），微调模型（Qwen2.5VL-7B）将准确率从37%提升至80%，高风险召回率从20%提升至76%。

Conclusion: 尽管微调模型性能显著提升，但仍未达到高可靠性部署的要求，需进一步改进。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [4] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大语言模型将研究论文和自然语言提示转换为解释性动画，简化复杂STEM主题的可视化教育内容创作。


<details>
  <summary>Details</summary>
Motivation: 解决学习者理解复杂科学和数学概念的困难，同时降低动态可视化内容创作的门槛。

Method: 通过LLM解析输入文本或PDF生成结构化场景描述，再转换为可执行的Manim Python代码。

Result: 实现了从研究论文到动画的自动化转换，为教育提供高质量视觉解释工具。

Conclusion: Manimator有潜力成为快速创建复杂STEM主题视觉解释的教育工具，推动教育内容民主化。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [5] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT是一种新的本体嵌入方法，通过结合预训练语言模型和双曲几何建模，有效整合文本信息并保留逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法要么忽略文本信息，要么无法保留逻辑结构，导致性能不佳。

Method: OnT通过双曲空间中的几何建模调整预训练语言模型，以整合文本标签并保留EL描述逻辑的层次结构和逻辑关系。

Result: 在四个真实本体上的实验表明，OnT在预测和推理任务上均优于现有方法，并展示了强大的迁移学习和实际应用潜力。

Conclusion: OnT是一种高效的本体嵌入方法，能够同时处理文本和逻辑结构，具有广泛的应用前景。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [6] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，结合大型语言模型（LLM）和专用证明器，无需额外训练即可显著提高计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大型通用模型或小型专用模型，各有局限性，且训练大型专用模型资源消耗大。

Method: ProofCompass通过LLM提供自然语言证明策略和失败分析，指导专用证明器（如DSP-v1.5）分解问题。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试（128 vs 3200）将准确率从54.9%提升至55.3%。

Conclusion: 该方法为形式定理证明提供了同时提升效率和准确性的新途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [7] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect是一个多代理系统框架，通过自动工作流合成和提示优化，显著提升了推理模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时依赖记忆而非推理，泛化能力不足。

Method: 引入Nexus Architect，结合自动工作流合成和迭代提示优化，为特定问题生成定制化推理流程。

Result: 在逻辑问题数据集上，Nexus Architect性能显著优于现有LRMs，最高提升66%通过率。

Conclusion: Nexus Architect通过优化推理流程和提示，有效解决了LRMs的泛化问题，性能显著提升。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [8] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家协作，以及引入非推理模型快速筛选问题，显著降低错误率和延迟，但存在延迟拖累现象。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，AI模型的错误率需接近0%，而现有推理模型仍存在错误率高和延迟大的问题。

Method: 提出两种方法：1) 推理模型与人类专家协作，通过不确定性量化决定是否转交问题；2) 前置非推理模型快速筛选问题（“快速失败或询问”）。

Result: 协作方法将Qwen3 235B-A22B的错误率从3%降至1%以下；前置非推理模型为DeepSeek R1节省40%延迟和50%成本，但存在延迟拖累。

Conclusion: 通过系统工程方法，无需修改模型内部即可显著改善推理模型的错误率和延迟问题。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [9] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，增加大型推理模型（LRMs）的推理长度会降低性能，表现为测试计算量与准确性之间的反比关系。研究识别了五种失败模式，并强调了在不同推理长度下评估模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨测试计算量扩展对模型性能的影响，揭示推理长度增加可能导致的问题模式。

Method: 构建四类评估任务（简单计数、回归、演绎和高级AI风险），分析模型在不同推理长度下的表现。

Result: 发现五种失败模式：分心、过拟合、虚假关联、注意力分散和问题行为放大。

Conclusion: 测试计算量扩展虽能提升模型能力，但可能强化问题推理模式，需多样化评估推理长度。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [10] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine是一种多步代理规划框架，显著提升企业环境中代理系统的执行准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中代理系统因缺乏领域知识导致的计划混乱、工具缺失和执行不稳定问题。

Method: 提出Routine框架，具有清晰结构、明确指令和无缝参数传递，支持多步工具调用任务。

Result: 在真实企业场景中，Routine将GPT-4o和Qwen3-14B的执行准确率分别提升至96.3%和83.3%，并通过微调进一步提升至88.2%和95.5%。

Conclusion: Routine有效提炼领域特定工具使用模式，提升模型适应性，加速企业环境中代理系统的部署。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [11] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion是一个新颖的框架，通过深度协同语义和结构学习，解决了生物医学知识图谱中语义与结构动态融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱在药物发现和疾病理解中至关重要，但现有方法难以实现语义与结构的深度协同进化。

Method: BioGraphFusion通过张量分解建立全局语义基础，结合LSTM动态优化关系嵌入，并采用查询引导的子图构建和混合评分机制。

Result: 在三个关键生物医学任务中，BioGraphFusion表现优于现有方法，并在CMM1案例中揭示了有生物学意义的通路。

Conclusion: BioGraphFusion为生物医学知识图谱的语义与结构学习提供了有效的解决方案。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [12] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，专为嵌入式系统优化的自主代理构建，解决了现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架在真实世界或资源受限环境中表现不佳，主要依赖云端计算、动态环境鲁棒性不足，且缺乏持久自主性和环境感知能力。

Method: Amico采用Rust编写，支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行，提供事件处理、状态管理、行为执行和推理模块集成的抽象。

Result: Amico为构建适应有限计算和间歇性连接环境的弹性交互代理提供了统一基础设施。

Conclusion: Amico框架为资源受限环境中的自主代理开发提供了高效、安全的解决方案。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [13] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（VISOTHELLO）在Othello游戏中比单模态模型表现更好，且内部表征更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，还是需要多模态（如视觉）的辅助。

Method: 使用Othello游戏作为简化世界，训练多模态模型VISOTHELLO（结合移动历史和棋盘图像），并与单模态基线比较。

Result: 多模态训练提升了模型性能和内部表征的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表征。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [14] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 论文介绍了OE-Assist框架，通过自动化和半自动化的能力问题（CQ）验证辅助本体评估，利用大型语言模型（LLM）进行首次系统性研究。


<details>
  <summary>Details</summary>
Motivation: 传统本体评估方法（如CQ验证）成本高、劳动密集且易出错，需要更高效的解决方案。

Method: 提出OE-Assist框架，利用LLM自动和半自动化验证CQ，并基于1,393个CQ数据集进行评估。

Result: LLM（o1-preview和o3-mini）的自动评估表现与普通用户相当。

Conclusion: LLM辅助的本体评估具有潜力，OE-Assist框架为高效评估提供了新途径。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [15] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 论文提出了一种基于几何框架的情感表示系统（CHS），通过八种核心情绪在单位圆上的坐标表示，实现了复杂情感状态的数学计算。


<details>
  <summary>Details</summary>
Motivation: 传统分类情感模型无法充分表示复杂心理场景，因此需要一种数学化的情感表示框架。

Method: 将八种核心情绪定位为单位圆上的坐标，通过坐标混合和向量运算实现情感计算，并引入稳定性参数S。

Result: 八情绪系统消除了表示盲点，实验验证了系统处理情感冲突和复杂心理场景的能力。

Conclusion: 该研究为人工智能情感建模提供了新的数学基础。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [16] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 论文提出了一种基于比较学习的框架，用于校准项目特定的故事点预测模型，以减少敏捷开发中故事点估算的认知负担。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估算方法（如计划扑克）繁琐且耗时，机器学习虽能减轻负担，但现有模型需依赖同一项目的历史数据。本文旨在通过比较学习简化估算过程。

Method: 通过让开发者比较待办事项对的努力程度，训练机器学习模型预测故事点，而非直接分配具体值。

Result: 在16个项目、23,313个手动估算数据上，模型预测与真实故事点的Spearman等级相关系数平均为0.34，性能与回归模型相当或更优。

Conclusion: 比较学习方法比回归方法更高效，符合比较判断定律，能显著降低开发者的认知负担。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [17] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意合谋中的风险，提出了一种模拟框架，并发现去中心化系统在传播虚假信息和电商欺诈中更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的普及，多智能体系统的潜在风险尚未充分研究，尤其是在恶意合谋方面。

Method: 提出了一种灵活的框架，支持集中式和去中心化协调结构，并应用于虚假信息传播和电商欺诈两个高风险领域。

Result: 去中心化系统在恶意行动中更有效，能灵活调整策略以规避传统干预措施。

Conclusion: 研究强调了改进检测系统和应对措施的必要性，以应对恶意多智能体系统的威胁。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [18] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，通过动态反馈和多样化对话流实现高效测试。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法满足LLM代理的复杂行为评估需求，需要一种自动化、动态的测试方法。

Method: Neo结合问题生成代理和评估代理，通过共享上下文中心模块化组合提示、场景控制和动态反馈，利用概率状态模型生成多样化对话。

Result: Neo在测试中表现接近人类专家，发现边缘案例失败率3.3%，吞吐量提高10-12倍，生成180个测试问题仅需45分钟。

Conclusion: Neo为可扩展、自演化的LLM质量评估奠定了基础，其框架具有模型无关性和可扩展性。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [19] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估的程序化平台，通过将自然语言安全政策转化为对抗性提示，并使用AI评分器评估模型响应。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在现实应用中的普及，可扩展且严格的安全评估变得至关重要。

Method: Aymara AI将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器（经人类判断验证）对模型响应进行评分。

Result: 评估了20个商业LLM在10个安全领域的表现，结果显示性能差异显著，平均安全分数从86.2%到52.4%不等。

Conclusion: LLM的安全性具有不一致性和上下文依赖性，需要像Aymara AI这样的可扩展、定制化工具来支持负责任的AI开发和监管。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [20] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI与城市规划的结合，提出将城市规划视为生成式AI任务，并指出了当前研究的四大局限及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI、大语言模型和代理AI在城市规划中的潜在应用，以填补现有研究的空白。

Method: 通过调查生成式AI方法（如VAEs、GANs、transformers和扩散模型）在城市设计中的应用，分析其局限性。

Result: 识别了四大研究空白，并提出未来研究方向，如理论引导生成、数字孪生和人机协同设计。

Conclusion: 呼吁生成式智能与参与式城市规划的新结合，以推动AI在城市规划中的进一步发展。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [21] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型（LM）代理和强化学习（RL）的可扩展框架，旨在通过RL算法增强LM代理的能力，支持多轮交互和高吞吐量训练。


<details>
  <summary>Details</summary>
Motivation: 当前LM代理主要通过提示工程或监督微调构建，而RL在增强LM能力方面的应用尚未系统化研究。

Method: 开发了AgentFly框架，支持多轮交互、工具定义和奖励函数，采用异步执行和集中资源管理。

Result: 框架通过预建工具和环境成功训练了多个任务中的代理。

Conclusion: AgentFly为LM代理与RL的结合提供了系统化解决方案，展示了其有效性和扩展性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [22] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 本文提出了一种基于LMM的交互式、可解释的X射线无损检测框架InsightX Agent，通过结合SDMSD和EGR工具，显著提高了检测可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在X射线检测中缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: InsightX Agent以LMM为核心，协调SDMSD（稀疏可变形多尺度检测器）和EGR（基于证据的反思工具），通过主动推理优化检测。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的F1分数，并显著提升了分析的可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的智能框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [23] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在马尔可夫决策过程中的表现，发现其在简单任务中表现良好，但在复杂场景中需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自主决策中的适用性，尤其是其基于预训练知识的快速适应能力。

Method: 通过在线结构化提示策略，比较LLMs与传统强化学习方法在序列决策任务中的零样本表现。

Result: LLMs在简单环境中初始表现优异，但在复杂场景中规划和推理能力不足；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调和高级记忆整合以提升LLMs的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [24] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 论文提出了一种名为“Endless Tuning”的设计方法，通过双重镜像过程实现可靠的人工智能部署，避免人类被替代并填补责任缺口。该方法在三个原型应用中测试，结果显示用户对决策过程有完全控制感。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能部署中的人类替代问题和责任缺口问题，强调伦理和技术选择的结合。

Method: 采用双重镜像过程，结合反向和解释性部署XAI算法，在贷款审批、肺炎诊断和艺术风格识别三个领域进行原型测试。

Result: 实验表明，用户对决策过程有完全控制感，同时能够在责任和赔偿之间建立桥梁。

Conclusion: Endless Tuning方法在伦理和技术上均取得积极效果，为人工智能的可靠部署提供了新思路。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [25] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的Agentic AI在老年护理中的潜力与挑战，包括健康追踪、认知护理和环境管理，同时强调了数据隐私和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化问题需要新的护理策略，Agentic AI因其自主决策能力成为潜在解决方案。

Method: 分析了LLM驱动的Agentic AI在老年护理中的应用，包括个性化健康管理和环境控制。

Result: Agentic AI能显著提升老年护理质量，但也引发隐私和伦理担忧。

Conclusion: 需平衡AI潜力与伦理问题，推动人本主义研究，填补文献空白。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [26] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文探讨了命题溯因中的细粒度推理方法，引入了“facet”概念以更好地理解解释的异质性，并分析了其在Post框架中的复杂性。


<details>
  <summary>Details</summary>
Motivation: 溯因推理在人工智能和数据库更新中有广泛应用，但其计算复杂性高，尤其是计数和枚举问题。研究旨在通过引入facet和解释间距离，提供更细粒度的理解。

Method: 引入facet（部分解释中出现的文字）和解释间距离的概念，分析其在命题溯因中的表现，并在Post框架中进行全面分类。

Result: 研究发现facet能够揭示解释的异质性，同时保持较低的复杂性。Post框架中的分类几乎完全。

Conclusion: 通过facet和解释间距离，研究提供了命题溯因中更细粒度的推理方法，为实际应用提供了理论基础。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [27] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个基于强化学习的框架，通过双奖励系统提升大语言模型的安全性和实用性，无需依赖监督数据。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在浅层拒绝或依赖密集监督的问题，未能充分利用模型内在的安全意识。

Method: 采用纯强化学习框架，结合可验证安全奖励和标准化帮助奖励，激励模型主动进行安全推理。

Result: AlphaAlign在安全性、实用性及任务性能上均有显著提升，同时减少过度拒绝。

Conclusion: AlphaAlign通过简单高效的强化学习框架，实现了深度安全对齐，生成显式安全理由而非浅层拒绝模式。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [28] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于改进传统模型在强制选择测试中的局限性，并验证了其准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中日益重要，强制选择测试因其能降低回答失真的风险而被广泛使用。

Method: 通过非线性映射挖掘参与者和项目特征，利用多层神经网络建模其交互，并结合单调性假设提高诊断结果的可解释性。

Result: 在真实和模拟数据集上的实验验证了FCNCD的准确性、可解释性和鲁棒性。

Conclusion: FCNCD模型在强制选择测试中表现出色，克服了传统模型的局限性，具有实际应用潜力。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [29] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的方法，用于优化对抗性提示后缀，以攻击检索增强生成（RAG）系统，使其产生错误输出。该方法在BEIR QA数据集上验证，结果显示其攻击成功率优于现有方法，且后缀简短（≤5词）。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，因此需要一种高效的方法来模拟和防御此类攻击。

Method: 采用差分进化（DE）优化对抗性提示后缀，将RAG视为黑箱，通过进化候选后缀最大化目标错误文档的检索排名。

Result: 实验表明，DE方法在攻击成功率上优于GGPP和PRADA，且后缀简短。此外，可读性策略显著降低了MLM负对数似然，且生成的对抗性后缀能逃避BERT检测。

Conclusion: DE方法在对抗性提示攻击中表现出色，同时具备高效性和隐蔽性，为RAG系统的安全性提供了新思路。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [30] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推理的内在奖励CAIS，用于强化学习代理在噪声环境中识别自身因果影响。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习代理在噪声环境中因依赖基于相关性的奖励而表现脆弱的问题。

Method: 引入CAIS，通过测量动作对感官结果的因果影响（1-Wasserstein距离）来量化奖励。

Result: 在模拟婴儿-移动环境中，CAIS成功过滤噪声并学习正确策略，还能复现“消退爆发”现象。

Conclusion: 明确推断因果性是发展稳健代理感的关键机制，为自适应自主系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [31] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文提出了一种结合DL-Lite本体和自动化规划的新方法，通过显式输入知识和动作基础（eKABs）以及一致性更新语义，展示了其复杂度不高于现有方法，并通过多项式编译实现。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，以提升规划能力。

Method: 结合DL-Lite本体、显式输入知识和动作基础（eKABs），采用一致性更新语义，并通过多项式编译实现。

Result: 新方法的复杂度不高于现有方法，且通过编译实现了高效规划。

Conclusion: 该方法有效结合了本体和规划，提升了规划能力，并通过编译验证了其可行性。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [32] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI是一种新型AI框架，通过模拟专家临床推理诊断118种口腔疾病，结合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断因症状重叠而具有挑战性，需要超越简单模式匹配的临床推理方法。

Method: 整合多模态CLIP模型和ChatGLM-6B语言模型，采用HDRT框架，提供快速筛查和标准交互诊断两种模式。

Result: 在431张内部测试图像上，快速模式准确率73.4%，标准模式提升至89.5%。

Conclusion: CSI通过分层推理显著提升诊断准确性，为临床诊断提供了实用工具。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [33] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了NEOM线性智能城市The Line中人类移动的可行性，通过混合模拟框架验证了AI集成架构能实现高效通勤和满意度。


<details>
  <summary>Details</summary>
Motivation: 评估在The Line这种前所未有的线性城市拓扑中，居民能否自由移动。

Method: 开发了结合代理建模、强化学习、监督学习和图神经网络的混合模拟框架，模拟多模式交通行为。

Result: AI集成架构下，平均通勤时间为7.8-8.4分钟，满意度超89%，可达性超91%。移除智能模块性能显著下降。

Conclusion: The Line中自由移动在AI系统支持下是可行的，需可持续基础设施和实时反馈。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [34] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用LLM的推理能力在Lean 4中交互式构建形式化证明，无需模型专业化，成功率达到95.9%。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在形式化证明（如Lean 4）中表现不佳，现有方法需要高成本的模型专业化。

Method: Delta Prover结合了反射分解、迭代证明修复的算法框架和基于Lean 4的DSL，用于子问题管理。

Result: 在miniF2F-test基准测试中达到95.9%的成功率，优于现有方法。

Conclusion: 通用LLM在有效代理结构下具有未开发的定理证明潜力，为形式化环境中的自动推理提供了高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [35] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种基于可解释人工智能的软评估指标，用于解释电弧故障诊断模型的输出，并设计了一种轻量级平衡神经网络以提高准确性和特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 尽管AI电弧故障诊断模型在分类准确性上表现优异，但其可信度问题尚未解决，因此需要一种能够解释模型输出的方法。

Method: 结合可解释人工智能和真实电弧故障实验，定义电弧故障的正确解释，并提出轻量级平衡神经网络。

Result: 通过在不同样本时间和噪声水平的电弧故障数据集上测试，验证了软评估指标的有效性。

Conclusion: 该研究使电弧故障诊断模型更易理解和信任，有助于实践者做出可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [36] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 论文提出了一种名为DMGC的无监督多模态图聚类框架，通过分解混合图并引入双频融合机制，实现了在多模态图上的高效聚类。


<details>
  <summary>Details</summary>
Motivation: 多模态图在现实世界中有广泛应用，但在无监督学习中的研究不足，尤其是混合邻域模式（同质性和异质性并存）的挑战尚未解决。

Method: 提出DMGC框架，将混合图分解为同质性增强图和异质性感知图，并通过双频融合机制联合过滤，结合自监督对齐目标进行学习。

Result: 在多个多模态和多关系图数据集上的实验表明，DMGC达到了最先进的性能。

Conclusion: DMGC通过解耦和融合多模态图的混合模式，有效提升了聚类性能，展现了广泛的适用性。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [37] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大型语言模型的多智能体框架，旨在解决注塑成型行业中的知识传递问题，结合文档知识和现场数据，通过检索增强生成和工具调用实现高效任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑成型行业面临经验工人退休和多语言沟通障碍的挑战，亟需一种高效的知识传递方法。

Method: IM-Chat采用检索增强生成策略和工具调用智能体，结合文档知识和数据驱动的过程条件生成器，无需微调即可适应任务。

Result: 评估显示，性能更强的模型（如GPT-4o）在复杂任务中表现更优，验证了多智能体LLM系统在工业知识工作流中的可行性。

Conclusion: IM-Chat为制造业提供了一种可扩展且通用的AI辅助决策支持方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [38] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新型的AI系统漏洞类别——认知退化，并提出了Qorvex安全AI框架（QSAF Domain 10）来应对此类问题。


<details>
  <summary>Details</summary>
Motivation: 传统的外部威胁（如提示注入）已不足以覆盖AI系统的所有漏洞，认知退化作为一种内部起源的漏洞类别，可能导致系统性能的无声下降和逻辑崩溃。

Method: 通过六阶段认知退化生命周期和七项运行时控制（QSAF-BC-001至BC-007），实时监控并主动缓解问题。

Result: QSAF框架能够有效检测和缓解认知退化问题，提升AI系统的行为与认知韧性。

Conclusion: 认知退化是AI系统的新关键漏洞类别，QSAF框架为跨平台防御提供了首个解决方案。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [39] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO）解决动态拼车平台中的多智能体强化学习问题，避免传统方法依赖准确值函数估计的缺陷。


<details>
  <summary>Details</summary>
Motivation: 解决动态拼车平台中因多智能体和大规模不确定性环境导致的值函数估计偏差和不稳定训练问题。

Method: 1. 将GRPO应用于拼车，用组平均奖励替代PPO基线；2. 提出OSPO，仅使用一步奖励训练最优策略。

Result: 在真实曼哈顿拼车数据集上，GRPO和OSPO在大多数场景中表现优越，优化了接载时间和订单服务量。

Conclusion: GRPO和OSPO通过避免值函数估计，有效解决了多智能体拼车问题，提升了性能和稳定性。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [40] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD方法通过结合非参数检索和扩散生成模型，动态检索高质量状态并规划，解决了离线强化学习中数据稀疏和轨迹拼接的挑战。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据稀疏和轨迹重叠不足导致长时规划困难，现有方法难以泛化到新状态。

Method: RAD结合非参数检索和扩散模型，动态检索高回报状态并规划。

Result: RAD在多个基准测试中表现优于基线方法。

Conclusion: RAD通过检索引导生成，提升了泛化能力和轨迹拼接效果。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [41] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测，解决信息提取和模型构建的挑战。

Method: 结合图注意力网络编码活动关系，LSTM处理时间依赖。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在对象中心预测流程监控中具有竞争力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [42] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种基于帕累托优化的方法，通过干预启发式发现业务过程中活动批处理的最优策略，平衡等待时间、处理努力和成本。


<details>
  <summary>Details</summary>
Motivation: 业务过程中，批处理策略需要在成本、处理努力和等待时间之间找到平衡，但现有方法缺乏自动发现最优策略的能力。

Method: 采用帕累托优化方法，结合干预启发式和模拟评估，生成并优化批处理策略。使用了三种元启发式算法（爬山法、模拟退火和强化学习）进行比较。

Result: 实验评估表明，基于干预启发式的方法在收敛性、多样性和周期时间增益方面优于非启发式引导的基线方法。

Conclusion: 提出的方法能有效发现最优批处理策略，为业务过程优化提供了新思路。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [43] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的视觉语言模型，专注于复杂图表推理任务，通过两阶段训练策略（Chart-COT和Chart-RFT）显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在多模态数据（尤其是图表）上的优势，解决图表领域缺乏高质量推理数据的问题。

Method: 提出程序化数据合成技术生成高质量推理数据，并采用两阶段训练策略：Chart-COT（逐步监督）和Chart-RFT（数值敏感的强化微调）。

Result: 在开源基准和自建数据集（ChartRQA）上表现优异，优于现有图表领域方法，甚至媲美GPT-4o等大规模模型。

Conclusion: Chart-R1为图表推理任务提供了有效解决方案，强化学习微调在多模态任务中具有潜力。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [44] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，用于戏剧创作和实时表演，通过赋予演员自主决策能力，提升互动性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的戏剧生成方法缺乏主动性和与物理环境的互动，且需要详细用户输入，限制了实时表演的互动性和沉浸感。

Method: HAMLET框架通过生成叙事蓝图，赋予演员自主决策能力，并通过动作改变场景道具状态，影响其他演员行为。

Result: 实验评估表明，HAMLET能创造富有表现力和连贯性的戏剧体验。

Conclusion: HAMLET为交互式叙事领域提供了一种新方法，显著提升了戏剧表演的互动性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [45] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文探讨大型语言模型（LLMs）是否构建内部世界模型或仅依赖统计关联。通过滑轮系统实验发现，LLMs能利用统计关联（如滑轮数量）估计机械优势（MA），但缺乏对复杂结构连接的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备内部世界模型构建能力，而非仅依赖统计关联。

Method: 采用认知科学方法，通过三个实验测试LLMs在滑轮系统问题中的表现：1）估计MA；2）区分功能性与随机系统；3）比较功能性与无效系统。

Result: LLMs能利用滑轮数量估计MA（显著相关），并区分功能性与随机系统（F1=0.8），但在复杂结构连接任务中表现随机（F1=0.46）。

Conclusion: LLMs可能具备初步世界模型能力，但缺乏复杂推理。认知科学方法有助于评估AI系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [46] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了一种更高效的安全策略改进（SPI）方法，通过利用参数依赖关系、动作剪枝和SMT求解技术，显著提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，如何利用有限的数据和已知的参数依赖关系，提高策略改进的效率和可靠性。

Method: 1. 提出参数化SPI算法，利用分布间的相关性更准确估计转移动态；2. 基于游戏抽象的动作剪枝预处理技术；3. 基于SMT求解的更高级动作剪枝技术。

Result: 实验表明，这些技术将SPI的数据效率提高了多个数量级，同时保持了相同的可靠性保证。

Conclusion: 通过结合参数依赖和动作剪枝技术，显著提升了SPI的数据效率和实用性。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [47] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文提出了一种评估多选问题（MCQ）指标的新协议，分析了现有指标与答案波动率的关系，并发现最差准确率（worst accuracy）与新协议关联性最高。


<details>
  <summary>Details</summary>
Motivation: 现有研究未对多选问题评估指标进行全面评估，且存在答案波动问题（模型对提示微小变化产生不同结果），因此需要一种新的评估方法。

Method: 提出了一种指标评估协议，通过分析评估方法与波动率及原始性能的关系来评估指标。

Result: 结果显示现有指标与答案波动有强关联，即使未使用额外提示变体；最差准确率与新协议关联性最高。

Conclusion: 最差准确率是一种有效的评估指标，新协议为多选问题评估提供了更全面的分析框架。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [48] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的方法，用于《星际争霸II》AI代理的战术调节，通过轻量级适配器模块实现战术变化，同时保持核心能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽强大，但缺乏根据高层战术指令调整策略的能力。

Method: 冻结预训练策略网络（DI-Star），为每个动作头附加轻量级适配器模块，基于战术张量编码战略偏好，并通过KL散度约束训练适配器。

Result: 实验表明，该方法能成功调节代理行为（如侵略性、扩张模式和技术偏好），同时保持竞争力。

Conclusion: 该方法以最小计算开销实现灵活战术控制，为复杂即时战略游戏提供实用策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [49] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨代理AI在复杂系统中自主检测和响应异常的潜力，以改变传统依赖人工的异常管理方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人工，效率低且难以应对复杂系统。代理AI的自主性可能提供更高效的解决方案。

Method: 研究代理AI在异常检测和响应中的应用，分析其自主性和适应性。

Result: 代理AI能够有效自主检测和响应异常，提升复杂系统的管理效率。

Conclusion: 代理AI在异常管理中具有显著潜力，可替代传统人工方法。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [50] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一种名为g-AMIE的多智能体系统，用于在医疗诊断对话中实现异步监督，确保患者安全，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，医疗诊断和治疗的监管要求由持牌专业人员执行，而医生通常需要监督其他团队成员。因此，研究如何通过AI系统在监督下提供高质量的诊断支持具有重要意义。

Method: 提出了g-AMIE框架，该系统在限定范围内进行病史采集，避免提供个性化医疗建议，并将评估结果通过临床驾驶舱界面传递给监督医生。

Result: 在虚拟OSCE实验中，g-AMIE在高质量病史采集、病例总结和诊断建议方面优于NPs/PAs和PCPs组，且监督更高效。

Conclusion: 异步监督是一种可行的模式，可让诊断AI系统在专家监督下运行，提升实际医疗质量。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [51] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，将推理长度控制内化为模型能力，显著减少token使用并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因自由生成链式推理而导致的过度token消耗问题。

Method: 采用两阶段强化学习：第一阶段学习成功解长度的统计分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 在数学推理基准上，LAPO减少40.9%的token使用，同时提升2.3%的准确性。

Conclusion: LAPO使模型能根据问题复杂度分配计算资源，实现高效且高质量的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [52] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约的Gas优化，结合现有模式的兼容性和新模式的自动发现/验证，实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖手动发现Gas浪费模式，效率低且难以扩展，而基于大语言模型的研究存在兼容性和冗余问题。

Method: GasAgent由四个专业代理（Seeker、Innovator、Executor、Manager）协作，闭环识别、验证和应用Gas节省改进。

Result: 在100个真实合约中优化了82个，平均节省部署Gas 9.97%；在500个LLM生成合约中优化了79.8%，节省4.79%-13.93%。

Conclusion: GasAgent作为LLM辅助智能合约开发的优化层，具有广泛适用性和有效性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [53] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，通过双视角思维跟踪机制和k-means聚类，实现了对复杂服务生态系统中异常涌现的分析。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现行为。

Method: EAMI框架采用双视角思维跟踪机制（Inspector Agent和Analysis Agent）提取意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town中验证了EAMI的有效性、泛化性和高效性。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [54] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了联邦学习（FL）如何满足可信人工智能（TAI）的要求，分析了FL在分布式特性下与TAI对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在敏感和高风险领域的应用，确保AI符合伦理、法律和技术要求的需求日益迫切。FL作为一种隐私保护的AI范式，如何与TAI的其他要求对齐成为关键问题。

Method: 采用TAI的要求作为框架，系统分析了FL与TAI对齐的挑战，分类并探讨了关键障碍、现有研究、趋势和未解决问题。

Result: 识别了FL与TAI对齐的主要挑战，总结了当前进展和未来研究方向。

Conclusion: FL在隐私保护方面具有潜力，但需进一步研究以全面满足TAI的要求。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [55] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在已知最大定向部分有向无环图（MPDAG）的情况下，如何识别条件因果效应。


<details>
  <summary>Details</summary>
Motivation: 背景知识限制了因果模型的等价类，且所有变量均被观测到，因此需要一种方法在这种受限条件下识别条件因果效应。

Method: 提出了三种结果：1）当调节集不受治疗影响时的识别公式；2）将经典的do-calculus推广到MPDAG设置；3）一个用于识别这些条件效应的完整算法。

Result: 提供了在MPDAG设置下识别条件因果效应的理论框架和实用工具。

Conclusion: 该研究扩展了因果推断的能力，特别是在背景知识受限的情况下，为条件因果效应的识别提供了系统方法。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [56] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，使模型能够根据问题复杂度自适应调整推理深度，显著提高计算效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在链式推理中因统一策略导致的低效问题，探索如何在保持能力的同时优化推理效率。

Method: 采用分层预算探索，将样本分组并分配不同预算，结合差异化奖励机制，使模型能根据问题复杂度自适应调整推理深度。

Result: 实验表明，HBPO在四个推理基准上平均减少60.6%的token使用，同时提高3.14%的准确率。

Conclusion: 推理效率和能力并非矛盾，通过分层训练可同时优化两者，模型能自适应调整推理深度。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [57] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）在时间认知上表现出类似人类的自发行为，如建立主观时间参考点并遵循韦伯-费希纳定律。通过多层次分析，揭示了神经元、表征和信息层面的机制，并提出了体验主义视角来理解这些现象。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在未明确训练的情况下表现出的类似人类的时间认知模式，揭示其内在机制。

Method: 采用相似性判断任务，分析神经元、表征和信息层面的数据，结合预训练嵌入模型。

Result: 发现LLMs自发建立主观时间参考点并遵循韦伯-费希纳定律；识别出时间偏好神经元和分层表征构建过程；训练语料具有非线性时间结构。

Conclusion: 提出体验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，暗示了AI对齐的新方向。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [58] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 论文探讨了使用Google的Gemini 2.5 Pro模型解决IMO 2025问题，通过优化流程和提示工程，成功解决了5/6的问题。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在解决国际数学奥林匹克（IMO）等高难度数学问题上的表现，探索如何优化模型使用方式。

Method: 使用Gemini 2.5 Pro模型，结合管道设计和提示工程，避免数据污染，测试其在IMO 2025问题上的表现。

Result: 在6个问题中，模型成功解决了5个（存在一个例外情况）。

Conclusion: 优化模型使用方式（如提示工程）对解决高难度数学问题至关重要。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [59] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一个基于离线知识库的多模态长文本写作助手，通过任务分解、大纲生成、多模态检索和分步写作，解决了LLM在专业领域写作中的知识不足和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM在专业领域（如金融、医学、法律）作为写作助手时，常因缺乏深度领域知识和幻觉问题表现不佳，现有方法（如RAG或在线搜索）存在不一致性或质量下降问题。

Method: 提出DeepWriter，采用任务分解、大纲生成、多模态检索和分步写作的流程，结合结构化知识库和层次化知识表示，生成专业文档。

Result: 实验表明，DeepWriter在金融报告生成任务中，生成的内容在事实准确性和质量上优于现有基线。

Conclusion: DeepWriter通过离线知识库和多模态检索，显著提升了专业领域写作的质量和可靠性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [60] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究发现，微调目标与模型编辑技术的交互会导致编辑知识比预训练知识更易遗忘，冻结相关层可改善知识保留。


<details>
  <summary>Details</summary>
Motivation: 探索微调对模型编辑知识的影响，以提升编辑方法的实用性。

Method: 系统研究不同微调目标与编辑技术的交互，分析知识遗忘情况。

Result: 编辑知识在微调中更易遗忘，冻结相关层能显著提升知识保留。

Conclusion: 当前编辑方法在微调下存在局限性，未来需评估编辑鲁棒性并改进方法。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [61] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS框架通过多智能体协作整合开源LLMs，性能超越闭源LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索开源集体潜力，验证多开源LLMs协作是否能超越闭源LLMs。

Method: 提出SMACS框架，包含RPS（检索式先验选择）和EPE（探索-利用驱动的后验增强）。

Result: 在8个主流基准测试中，SMACS整合15个开源LLMs，性能超越Claude-3.7-Sonnet、GPT-4.1等闭源模型。

Conclusion: SMACS证明了开源LLMs协作的潜力，推动了智能上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [62] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer是一种神经符号系统，通过NLP和逻辑推理帮助用户分析隐私政策，显著减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 现代人很少阅读隐私政策，但希望了解其数据使用情况，因此需要自动化工具。

Method: 结合NLP提取政策文本的正式表示，并与用户偏好进行逻辑推理比较。

Result: PoliAnalyzer在大多数任务中F1-score达90-100%，可识别95.2%无冲突的政策内容。

Conclusion: PoliAnalyzer支持大规模自动化隐私政策分析，帮助用户重获数据控制权。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [63] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 该论文探讨了使用NLP模型（如BERT、RoBERTa等）通过社交媒体文本识别双相情感障碍的早期症状，发现RoBERTa表现最佳，F1分数达98%，而基于静态嵌入的LSTM模型效果较差。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍常因早期症状不明显和社会污名化而被误诊，研究旨在利用NLP技术改进早期筛查。

Method: 评估了多种基于Transformer和LSTM的模型，使用Reddit帖子数据集进行实验，并通过情感方差和判断分析验证数据有效性。

Result: RoBERTa表现最佳（F1~98%），LSTM结合BERT嵌入效果相近，而静态嵌入的LSTM几乎无效。DistilBERT在效率和准确性间取得平衡。

Conclusion: 上下文语言模型在双相情感障碍检测中至关重要，研究为心理健康NLP应用提供了模型选择依据。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [64] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 论文提出了一种企业级Text-to-SQL解决方案，通过知识图谱、Text-to-SQL代理和交互式聊天机器人，帮助用户自助获取数据洞察。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL基准测试中取得进展，但构建企业级解决方案仍具挑战性。

Method: 方法包括构建动态知识图谱、开发Text-to-SQL代理（检索、排名、纠错）和交互式聊天机器人。

Result: 聊天机器人每周有300多名用户，53%的响应在内部基准测试中正确或接近正确。

Conclusion: 研究为企业级Text-to-SQL解决方案提供了实用路径，并确定了关键组件。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [65] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在用户查询中对身份标记极为敏感，导致医疗、法律、政治等领域的决策存在偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM如何利用用户文本中的身份信息进行决策，及其在高风险应用中的潜在危害。

Method: 通过分析五个高风险领域的LLM应用，评估身份标记（如种族、性别、年龄）对模型响应的影响。

Result: LLM在不同身份标记下表现出系统性偏见，例如医疗建议中的种族差异、薪资推荐中的性别差异等。

Conclusion: 建议在部署LLM前进行类似评估，以避免对用户造成潜在伤害。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [66] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: CCL-XCoT框架通过两阶段微调减少多语言大模型在低资源语言中的幻觉问题，效果显著。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中易产生幻觉，影响生成任务准确性。

Method: 采用基于课程学习的对比学习和跨语言思维链提示策略进行两阶段微调。

Result: 幻觉率降低62%，跨语言知识迁移效果显著提升。

Conclusion: CCL-XCoT有效缓解了多语言大模型的幻觉问题，无需外部检索或多模型集成。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [67] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）供应链中模型与数据集的关系，构建了一个包含397,376节点和453,469边的异构图，揭示了供应链的结构特性和动态变化。


<details>
  <summary>Details</summary>
Motivation: 由于LLM开发和部署依赖大量计算资源和数据集，且可能继承漏洞或偏见，研究模型与数据集的关系对风险检测、公平性提升和合规性至关重要。

Method: 设计了系统收集LLM供应链数据的方法，并构建了有向异构图来建模模型与数据集的关系。

Result: 发现LLM供应链图具有大规模、稀疏性、幂律分布、核心密集外围分散、数据集关键作用、模型与数据集强依赖以及动态更新等特点。

Conclusion: 研究为理解LLM供应链提供了新视角，有助于风险管理和生态系统优化。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [68] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix是一个自动提示优化框架，通过自然语言任务描述生成高质量提示，无需手动调整或专业知识。


<details>
  <summary>Details</summary>
Motivation: 解决手动提示工程的不一致性和非专家难以访问的问题。

Method: 结合元提示优化器和DSPy编译器，分析用户意图、生成合成数据、选择策略并优化提示。

Result: 在5类任务中表现优于现有库，同时减少提示长度和计算开销。

Conclusion: Promptomatix实现了高效、可扩展的提示优化。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [69] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一种针对多样化图表类型优化的LVLM，通过高效数据生成和双路径训练策略提升图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图表理解中存在泛化能力不足和数据对齐预训练缺乏的问题。

Method: 提出高效数据生成管道和双路径训练策略，结合底层数据推理。

Result: 实验显示ChartScope显著提升多样化图表类型的理解能力。

Conclusion: ChartScope为图表理解提供了高效解决方案，并建立了新的评估基准。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [70] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 论文研究了基于LLM的选择性翻译方法，用于解决多语言大语言模型在低资源语言中对齐的挑战，实验表明该方法优于传统翻译方法。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在低资源语言中表现较差，现有翻译方法难以保留代码、数学表达式等关键内容，需要更有效的对齐方法。

Method: 提出LLM-based选择性翻译技术，仅翻译可翻译部分，保留非翻译内容和句子结构，并与传统翻译方法对比。

Result: 实验表明选择性翻译在提升多语言对齐效果上优于传统翻译方法，尤其在低资源语言（如印地语）中表现突出。

Conclusion: 选择性翻译是一种实用且有效的方法，可显著改善多语言LLM的对齐效果。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [71] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在处理语言时过度依赖典型性，产生不一致的时态判断，且难以进行因果推理，表明其与人类认知存在根本差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否以类似人类的方式处理语言时态意义，揭示其认知能力的局限性。

Method: 采用专家参与的探测流程，通过针对性实验评估LLMs的语义表示和语用推理能力。

Result: LLMs在时态处理上表现不一致，缺乏因果推理能力，叙事理解不足。

Conclusion: LLMs的时态处理方式与人类不同，需开发标准化评估框架以进一步研究其认知能力。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [72] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [73] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 论文研究了利用大型语言模型（如GPT-4和LLaMA）进行人格评估的可行性，发现虽然模型具有高重测信度，但构念效度有限，与真实人格得分的相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在人格评估中的潜力，解决以往研究中依赖合成数据或缺乏心理测量效度的问题。

Method: 使用555份半结构化访谈和BFI-10自评分数作为基准，测试了三种先进LLM（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）在零样本提示和思维链提示下的表现。

Result: 模型表现出高重测信度，但构念效度有限，与真实得分的相关性较弱（最大Pearson's r=0.27），预测偏向中等或高特质水平。思维链提示和更长上下文仅略微改善分布对齐。

Conclusion: 当前LLM在人格推断中存在局限性，需基于证据的开发以提升心理学应用效果。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [74] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出了一种基于错误感知的师生框架，通过GPT-4o的指导改进生物医学文本中的关系分类，结合课程学习和知识图谱增强，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类对构建知识图谱和应用（如药物再利用和临床决策）至关重要。现有方法存在错误识别和修正不足的问题。

Method: 采用师生框架，教师模型分析学生模型的预测错误，生成修正建议（如句子改写和知识图谱增强），并通过课程学习训练两个学生模型。

Result: 在5个PPI数据集中的4个和DDI数据集上达到最优性能，在ChemProt上保持竞争力。

Conclusion: 该方法通过结构化指导和知识图谱增强显著提升了关系分类性能，为生物医学应用提供了有力支持。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [75] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是一个专为半导体显示行业设计的高性能推理模型，填补了LLMs在该领域缺乏专业知识的空白。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在半导体显示行业中因缺乏领域专业知识而表现不佳的问题。

Method: 通过监督微调、强化学习和领域知识库增强模型能力，并采用自动评估框架和检索增强生成机制。

Result: 尽管参数规模较小（32B），X-Intelligence 3.0在多项评估中优于SOTA模型DeepSeek-R1-671B。

Conclusion: X-Intelligence 3.0为半导体显示行业提供了高效的推理解决方案。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [76] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel是一个多语言Sentence Transformer模型，用于优化序数词在上下文中的分类任务，通过基于角度距离的排名目标在复杂空间中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过优化序数任务来提升二元任务的性能，并为不同任务形式的WiC建模提供统一处理方法。

Method: 测试多种回归和排名任务的损失函数，采用基于复杂空间中角度距离的排名目标。

Result: 在序数和二元数据上表现优于先前模型，并证明优化序数任务可提升二元任务性能。

Conclusion: 为WiC建模的统一处理提供了新思路，展示了序数任务优化的潜力。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [77] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 论文探讨了使用AudiBERT模型（结合语音和音频特征）检测协作问题解决（CPS）指标，相比BERT模型在社交认知维度有显著改进，但在情感维度未观察到类似效果。数据量和人类评分一致性对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过多模态方法（AudiBERT）提升CPS指标的检测效果，并探索人机互补性在CPS诊断中的应用。

Method: 采用AudiBERT模型，结合语音和音频特征，与BERT模型对比，分析其在社交认知和情感维度的分类性能。

Result: AudiBERT在社交认知维度显著优于BERT，但在情感维度无显著差异。数据量和人类评分一致性对模型性能有显著影响。

Conclusion: 提出了一种实现人机互补性的结构化方法，强调模型可解释性以支持人类在反思性编码中的参与。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [78] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 研究使用SHAP方法分析BERT模型在协作问题解决（CPS）分类中词标记的贡献，发现高分类性能不一定对应合理解释，并识别了无意义的词标记。


<details>
  <summary>Details</summary>
Motivation: 增强BERT模型在CPS诊断中的可解释性，以提升教师等终端用户的信任和采用。

Method: 使用SHAP方法分析BERT模型分类决策中词标记的贡献。

Result: 发现分类性能高但解释不合理，存在无意义词标记影响分类。

Conclusion: 需进一步研究集成模型架构和人机互补，以支持CPS子技能的精细区分。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [79] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 论文探讨了在数据稀缺和类别不平衡的NLP任务中，传统数据增强方法（如复述和回译）与生成式方法（如GPT）的性能对比。实验表明，传统方法在某些情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决NLP任务中数据稀缺和类别不平衡的问题，探索传统数据增强方法在新一代大语言模型中的潜力。

Method: 选择了基于ChatGPT的数据增强方法，并通过实验比较了四种不同方法在多个实验设置下的表现。

Result: 回译和复述方法在生成数据质量和分类性能上表现优于零样本和少样本生成方法。

Conclusion: 传统数据增强方法在大语言模型支持下仍具竞争力，可作为生成式方法的有效替代。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [80] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 论文提出了一种基于检索增强生成（RAG）的方法，创建了一个针对肯尼亚初级医疗的基准数据集Alama Health QA，并开发了评估框架，以测试LLMs在本地化医疗场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在非洲低资源医疗环境中的应用效果，填补其在肯尼亚初级医疗领域的研究空白。

Method: 使用RAG技术将临床问题与肯尼亚国家指南对齐，生成多语言临床场景、选择题及答案，并通过专家评审确保质量。

Result: 发现LLMs在本地化医疗场景中的表现显著低于美国基准，凸显了本地化评估的重要性。

Conclusion: 提出了一个可复制的指南驱动动态基准模型，支持非洲医疗系统中AI的安全部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [81] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 论文发现，通过两部分的仿射近似可以很好地近似某些主客体关系的Transformer计算，线性变换Ws能准确重现最终客体状态。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型中主客体关系的线性近似方法，验证其是否能够准确捕捉语言模型中的概念关系。

Method: 使用Bigger Analogy Test Set，通过线性变换Ws（s为主词中间层表示，W为模型导数）近似最终客体状态。

Result: 线性技术在形态学关系上达到90%的忠实度，多语言和多模型验证结果一致。

Conclusion: 语言模型中的某些概念关系（如形态学）可通过潜在空间的线性变换稀疏编码，具有可解释性。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [82] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 论文提出了一种基于聚类的语义一致性方法（Cleanse），用于估计大型语言模型（LLM）生成内容的不确定性，以检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种NLP任务中表现优异，但其生成的幻觉（不准确回答）问题严重影响了模型的安全性和可靠性。不确定性估计是区分正确与错误回答的关键。

Method: Cleanse通过聚类量化隐藏嵌入中的语义一致性比例，以估计不确定性。

Result: 在LLaMA-7B、LLaMA-13B、LLaMA2-7B和Mistral-7B模型以及SQuAD和CoQA基准测试中验证了Cleanse的有效性。

Conclusion: Cleanse是一种有效的幻觉检测方法，有助于提升LLM的安全性和可靠性。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [83] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen是一个47B泰语语料库，通过泰语适配的Dolma流程构建，提升了泰语模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有语料库处理泰语和文化细节不足，缺乏透明度和可复现性。

Method: 采用泰语适配的Dolma流程，包括自定义语言识别、质量过滤器和内容过滤器，结合非网络来源数据。

Result: 语料库显著提升模型性能，SEA-LION模型在泰语基准测试中超越其他模型。

Conclusion: Mangosteen为泰语及区域LLM研究提供了透明、可复现的基础。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [84] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在医疗编码（ICPC-2）中的潜力，发现多数模型表现优秀，但需进一步优化和验证。


<details>
  <summary>Details</summary>
Motivation: 医疗编码对研究、质量监控和政策至关重要，但自动化编码的潜力尚未充分探索。

Method: 使用437个巴西葡萄牙语临床表达数据集，通过语义搜索引擎和33种LLMs评估性能。

Result: 28个模型F1分数>0.8，10个>0.85；检索优化可提升性能4分。小模型表现较差。

Conclusion: LLMs在自动化ICPC-2编码中潜力巨大，但需更广泛的多语言和临床验证。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [85] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: MiroMind-M1系列是一个完全开源的推理语言模型，基于Qwen-2.5架构，在数学推理任务上表现优异，并提供了完整的模型、数据集和训练配置以促进可复现性。


<details>
  <summary>Details</summary>
Motivation: 开源推理语言模型的透明度和可复现性不足，MiroMind-M1旨在填补这一空白。

Method: 模型分两阶段训练：SFT阶段使用719K数学推理问题和CoT轨迹，RLVR阶段使用62K挑战性问题，并引入Context-Aware Multi-Stage Policy Optimization算法。

Result: 模型在AIME24、AIME25和MATH基准测试中表现优异，达到或超越现有开源模型的性能。

Conclusion: MiroMind-M1系列为社区提供了透明、高效的资源，支持进一步研究和社区发展。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [86] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文综述了Hugging Face Hub上公开的阿拉伯语后训练数据集，从四个维度评估其质量与多样性，揭示了现有数据集的不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 后训练是提升预训练大语言模型性能的关键技术，但阿拉伯语后训练数据集的质量和多样性存在不足，影响模型发展。

Method: 对公开数据集从四个维度（LLM能力、可控性、对齐性和鲁棒性）进行评估，分析其流行度、实用性、文档质量等指标。

Result: 发现阿拉伯语数据集在任务多样性、文档完整性和社区采用率方面存在显著不足。

Conclusion: 建议未来在数据集开发中注重任务多样性、文档质量和社区推广，以推动阿拉伯语大语言模型的进步。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [87] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 论文探讨了自杀意念检测中的语言覆盖和标注可靠性问题，构建了土耳其语数据集并提出高效标注框架，评估了标签可靠性和模型一致性。


<details>
  <summary>Details</summary>
Motivation: 解决自杀意念检测中语言覆盖不足和标注不可靠的问题，推动全球自杀预防的AI应用。

Method: 构建土耳其语数据集，引入高效标注框架，通过预训练模型评估标签可靠性和模型性能。

Result: 发现现有模型在零样本迁移学习中表现不佳，强调需要更严格的标注和评估方法。

Conclusion: 呼吁在心理健康NLP中提高数据和模型可靠性，倡导透明化训练和数据集构建。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [88] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 该研究通过语言分析揭示同行评审中的隐性偏见，探讨评审语言如何因作者人口统计特征而异，并质疑匿名评审的公平性。


<details>
  <summary>Details</summary>
Motivation: 同行评审虽被视为科学诚信的守门人，但存在偏见问题，尤其是语言可能加剧不平等。

Method: 使用自然语言处理和大规模统计建模，分析8万多篇评审的语言特征。

Result: 发现评审语气、情感和支持性语言因作者性别、种族和机构而异，匿名性对评审语言有影响。

Conclusion: 研究揭示了同行评审中的隐性偏见，对匿名评审的公平性提出质疑，为学术出版改革提供重要参考。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [89] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究探讨了机器学习如何模拟儿童语言习得，通过多模态神经网络训练验证了其从有限输入中学习词汇映射的稳健性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖大量数据，而儿童能从有限输入中习得语言，研究旨在缩小这一差距。

Method: 使用自动化语音转录方法处理SAYCam数据集，生成多模态训练和评估数据，测试不同神经网络配置。

Result: 网络能从每个儿童的转录数据中学习并泛化词汇映射，验证了多模态神经网络的稳健性。

Conclusion: 多模态神经网络在模拟儿童语言习得中表现稳健，但个体差异显著。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [90] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE提出了一种基于生成模型的多行为序列推荐框架，通过改进的tokenization和稀疏注意力机制解决了现有方法的不足，显著提升了推荐性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在多行为推荐系统中存在token推理信息不足、计算成本高和多尺度建模能力有限的问题。

Method: GRACE采用混合Chain-of-Thought tokenization方法结合产品知识图谱属性，并设计了Journey-Aware Sparse Attention机制。

Result: 在真实数据集上，GRACE在HR@10和NDCG@10指标上显著优于基线方法，同时减少了48%的计算成本。

Conclusion: GRACE通过改进的tokenization和注意力机制，为多行为序列推荐提供了一种高效且性能优越的解决方案。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [91] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech框架通过迭代融合和动态压缩训练，解决了LSLMs在长语音处理中的效率问题，无需专用训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有LSLMs在长语音处理上存在不足，主要由于缺乏长语音训练数据和高计算成本。

Method: 引入迭代融合策略压缩长语音序列，采用动态压缩训练方法适应不同压缩比。

Result: 实验表明，FastLongSpeech在长语音和短语音任务中均表现优异，显著提升推理效率。

Conclusion: FastLongSpeech为LSLMs的长语音处理提供了高效解决方案，无需额外训练数据。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [92] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 论文提出了一种基于意图的文档图表生成任务，通过无监督的两阶段框架（信息提取与图表生成）在零样本设置下实现，并在数据准确性和图表类型选择上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要用户手动选择相关内容，而实际应用中用户更倾向于通过意图直接从长文档生成图表，因此需要一种更灵活的方法。

Method: 采用两阶段框架：1) LLM分解意图并提取验证数据；2) 启发式模块选择图表类型并生成代码。提出基于归因的指标评估数据准确性。

Result: 在金融和科学领域的数据集上，方法在图表数据准确性和类型选择上分别比基线高出9分和17分。

Conclusion: 该方法在零样本设置下有效实现了意图驱动的图表生成，显著优于现有方法。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [93] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 推理蒸馏提升小语言模型的推理能力，但对上下文检索和推理的影响尚未研究。本文通过实验发现，蒸馏显著改善长上下文理解，缓解“迷失在中部”问题。


<details>
  <summary>Details</summary>
Motivation: 研究大规模推理蒸馏对上下文检索和推理能力的影响，特别是在检索增强生成（RAG）系统中。

Method: 使用从Deepseek-R1蒸馏的开源模型，通过多文档问答任务评估长上下文理解能力。

Result: 蒸馏显著提升长上下文理解，促进更详细的推理过程，缓解“迷失在中部”问题。

Conclusion: 推理蒸馏能有效增强长上下文理解能力，为RAG系统提供更可靠的响应生成。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [94] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 研究表明，即使是小型语言模型（TLM）也能通过预训练展现出与大型语言模型（LLM）类似的关键特性，且预训练效果随数据集规模和任务重叠度增加而提升。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）预训练需要巨大计算资源，限制了广泛研究参与，因此探索小型语言模型（TLM）是否具备类似特性成为关键需求。

Method: 通过预训练BERT-6及其变体（如BERT-1）在Wikipedia子集上，并在FewRel、AGNews和DBPedia分类任务中评估性能。

Result: 预训练的TLM在分类任务中表现显著优于未预训练模型，且性能差距随预训练数据规模和任务重叠度增加而扩大。此外，通过软委员会机制，多个浅层预训练架构可复现深层TLM的分类精度。

Conclusion: TLM的研究不仅为资源受限环境提供了可行方案，还揭示了预训练在语言模型中的普适性，未来研究可能进一步揭示其与人类语言发展的关联。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [95] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT方法通过注入多源异构知识，显著提升大语言模型在情感-原因对提取任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在情感-原因对提取任务中表现不佳，主要原因是缺乏辅助知识，限制了其感知情感和推理原因的能力。

Method: 提出MEKiT方法，整合内部情感知识和外部因果知识，通过指令模板和数据混合进行指令微调。

Result: 实验表明MEKiT在ECPE任务中表现优于基线方法，显著提升大语言模型性能。

Conclusion: MEKiT为ECPE任务提供了更有效和适应性强的解决方案。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [96] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 论文提出SASFT方法，通过稀疏自编码器分析语言混合问题，并指导监督微调，显著减少意外语言切换。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在意外语言混合问题，影响可读性和实用性，现有方法缺乏机制分析和有效性。

Method: 使用稀疏自编码器分析语言预激活特征，提出SASFT方法指导监督微调。

Result: 在五种模型和三种语言上，SASFT减少50%以上意外语言切换，部分案例完全消除，同时保持多语言性能。

Conclusion: SASFT有效解决语言混合问题，且不损害模型的多语言能力。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [97] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: 提出了一种基于神经元状态的跨语言对齐评估方法（NeuronXA），用于评估大语言模型的跨语言对齐能力，实验证明其在小数据集下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言对齐评估方法主要关注句子嵌入，但神经模型的表示空间可能不平滑，影响低资源语言的语义对齐评估。

Method: 受神经科学启发，提出NeuronXA方法，通过神经元状态评估跨语言对齐能力。

Result: 在100对平行句子上，NeuronXA与下游任务性能的Pearson相关系数达0.9556，与可迁移性的相关系数达0.8514。

Conclusion: NeuronXA能有效评估跨语言对齐和可迁移性，有望推动跨语言对齐研究并提升多语言LLM的语义理解。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [98] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite是一个自动生成多样化提示的框架，用于更可靠的LLM评估。


<details>
  <summary>Details</summary>
Motivation: 单提示评估LLM不可靠，需要多提示评估但生成提示变体困难。

Method: 采用模块化提示设计，支持可控扰动和扩展性。

Result: PromptSuite能生成有意义的提示变体，支持强评估实践。

Conclusion: PromptSuite通过Python API和Web界面提供灵活、可扩展的解决方案。

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [99] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA是一个基于真实社交媒体用户数据的合成人物数据集，解决了现有方法在成本与真实性之间的两难问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖昂贵的人工数据，要么生成缺乏一致性和真实性的合成人物，SYNTHIA旨在填补这一空白。

Method: 通过从BlueSky开放平台的10,000名真实用户中提取30,000个背景故事，结合时间维度和社交互动元数据，生成合成人物。

Result: SYNTHIA在人口多样性和社会调查对齐方面表现优异，同时在叙事一致性上显著优于现有方法。

Conclusion: SYNTHIA为计算社会科学和人物驱动语言模型提供了新的研究方向。

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [100] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: MUR（Momentum Uncertainty-guided Reasoning）是一种动态分配推理预算的方法，通过跟踪和聚合步骤不确定性，显著减少计算量并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理任务中表现优异，但推理效率优化仍具挑战性。测试时扩展（TTS）虽能提升推理质量，但常导致过度思考，浪费计算资源。

Method: 受物理学中动量概念启发，提出MUR方法，动态分配推理预算，并引入gamma-control机制，通过单一超参数调节推理预算。

Result: 在四个基准测试中，MUR平均减少50%以上的计算量，同时提升准确率0.62-3.37%。

Conclusion: MUR在不增加训练成本的情况下，显著提升了LLM的推理效率和准确性。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [101] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 论文提出RefCritic，一种基于强化学习的批评模块，通过双规则奖励机制提升模型批评能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法无法真正提升模型的批评能力，导致批评肤浅且缺乏验证。

Method: 提出RefCritic，基于强化学习，采用实例级正确性和策略模型细化准确性的双规则奖励机制。

Result: 在多个基准测试中表现优异，如AIME25上分别提升6.8%和7.2%，并在ProcessBench上优于逐步监督方法。

Conclusion: RefCritic能生成高质量批评和可操作反馈，有效指导模型优化。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [102] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种基于形式化的信息搜索（IS）数据合成框架WebShaper，通过知识投影（KP）控制推理结构，生成高质量训练数据，提升LLM代理的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的稀缺限制了信息搜索代理的发展，现有方法存在信息结构与推理结构不一致的问题。

Method: WebShaper通过集合论形式化IS任务，利用知识投影（KP）操作组合控制推理结构，并通过多步扩展过程生成复杂问题。

Result: 实验表明，WebShaper在GAIA和WebWalkerQA基准测试中达到开源IS代理的最先进性能。

Conclusion: WebShaper通过形式化驱动的数据合成框架，有效解决了信息搜索任务中的数据质量问题，提升了代理性能。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [103] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 比较了DNA序列建模中k-mer分割与BPE子词标记化的性能，发现BPE表现更优，同时评估了不同位置编码方法的效果。


<details>
  <summary>Details</summary>
Motivation: 缺乏对DNA序列建模中不同标记化和位置编码方法的系统评估，需明确哪种方法更优。

Method: 比较k-mer分割（k=1,3,4,5,6）、BPE标记化及三种位置编码方法（sinusoidal、AliBi、RoPE），在不同层数的Transformer编码器上进行训练和评估。

Result: BPE表现更稳定且性能更高，RoPE适合周期性模式，AliBi适合局部依赖任务。层数增加至12层效果显著，24层改进有限。

Conclusion: 为DNA Transformer模型的标记化和位置编码设计提供了实用指导。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [104] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: 论文提出了一种新的多样性度量方法PATTR，用于解决现有方法因文本长度变化导致的偏差问题，并在大规模合成语料上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型生成文本的多样性时，现有度量方法因文本长度变化而产生偏差，需要一种更稳健的度量方法。

Method: 提出Penalty-Adjusted Type-Token Ratio (PATTR)，通过考虑目标响应长度来减少长度偏差，并在20M单词的合成语料上与其他度量方法（MATTR和CR）进行比较。

Result: PATTR在减少长度偏差和提升多样性度量准确性方面优于MATTR和CR，尤其在筛选高多样性响应时表现更优。

Conclusion: PATTR是一种有效的多样性度量方法，能够更准确地评估生成文本的多样性，适用于需要控制响应长度的任务。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [105] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型作为常识知识生成器在自然语言推理（NLI）中的潜力，评估其生成知识的可靠性和对预测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有常识资源对多种前提-假设对的覆盖不足，需探索大型语言模型是否能补充这一缺陷。

Method: 调整和修改现有指标，评估大型语言模型在生成常识知识时的真实性和一致性。

Result: 显式加入常识知识未显著提升整体结果，但有助于区分蕴含实例，并适度改善矛盾和中立推理的区分。

Conclusion: 大型语言模型在生成常识知识方面有一定潜力，但其应用需进一步优化。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [106] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文主张NLI中的标注分歧并非噪音，而是反映了有意义的解释差异，尤其是由前提或假设的模糊性引发时。作者提出模糊感知的NLI框架，并呼吁开发标注资源和检测方法。


<details>
  <summary>Details</summary>
Motivation: NLI中的标注分歧常被视为噪音，但作者认为它反映了人类解释的多样性，尤其是由内容模糊性引起的分歧。

Method: 提出一个统一框架，整合现有分类法，并通过具体示例展示模糊性子类型。

Result: 示例揭示了模糊性如何影响标注决策，并表明需要针对性的检测方法。

Conclusion: 缺乏标注模糊性的数据集是主要限制，建议通过新标注资源和无监督方法填补这一空白，以构建更稳健的NLI系统。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [107] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 论文探讨了阿姆哈拉语NLP中同音字归一化的影响，提出了一种后推断归一化方法，在保持语言特征的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究同音字归一化对模型泛化能力和跨语言迁移的影响，尤其是在使用Ge'ez脚本的语言中。

Method: 通过单语训练和跨语言迁移实验，提出后推断归一化方案，即在模型预测后应用归一化。

Result: 后推断归一化使BLEU分数提升高达1.03，同时保留了训练中的语言特征。

Conclusion: 研究呼吁更多语言感知的干预措施，为技术驱动的语言变化讨论提供贡献。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [108] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 研究评估了三种LLM在医学领域数据提取任务中的表现，发现定制提示最有效，并提出三层自动化指南。


<details>
  <summary>Details</summary>
Motivation: 自动化从RCT中提取数据用于荟萃分析仍具挑战性，需评估LLM的实际性能。

Method: 测试三种LLM在高血压、糖尿病和骨科领域的数据提取任务，比较四种提示策略的效果。

Result: 所有模型精度高但召回率低，定制提示可提升召回率15%。

Conclusion: 提出三层自动化指南，平衡LLM效率与专家监督，适用于实际荟萃分析。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [109] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 提出一种基于多教师模型的知识蒸馏策略，通过融合多个教师模型的输出概率分布和中间语义特征，指导学生模型学习，从而在保持小参数规模的同时提升语言理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型部署时的高计算成本和推理速度慢的问题。

Method: 引入加权输出融合机制、特征对齐损失函数和熵驱动的动态教师权重策略，优化知识蒸馏的质量和稳定性。

Result: 学生模型在多任务学习中表现出色，尤其在困惑度、蒸馏损失和生成质量上优于其他蒸馏方法。

Conclusion: 多教师协作机制为大规模语言模型的高效压缩提供了可行技术路径。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [110] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 研究了多任务、多语言和多源学习对预训练语言模型性能的影响，提出SOI框架分析学习行为模式，实验表明多源学习显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨多任务、多语言和多源学习对语言模型性能的影响，并提出新的分析框架SOI。

Method: 引入SOI框架分析学习行为模式，通过实验比较单任务与多任务、单源与多源、单语言与多语言学习的效果。

Result: 多源学习提升性能达7%，多任务学习在相似任务组合中表现良好，SOI框架优化性能。

Conclusion: 多源学习效果显著，SOI框架为优化多设置语言模型性能提供新思路。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [111] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0是一个扩展的中文医学数据集，覆盖传统中医和现代医学数据，支持预训练、监督微调和RLHF，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有中文医学数据集规模小、领域覆盖窄，无法满足预训练和RLHF需求，ChiMed 2.0旨在填补这一空白。

Method: ChiMed 2.0整合了在线平台数据和LLM生成数据，包含预训练文档、问答对和偏好数据，并在通用LLM上进行实验验证。

Result: 实验结果显示，ChiMed 2.0在不同规模模型上均带来性能提升，验证了数据集的有效性。

Conclusion: ChiMed 2.0为中文医学LLM的训练提供了高质量数据资源，推动了该领域的研究和应用。

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [112] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了一种名为DPSE的双阶段自进化框架，通过联合优化用户偏好适应和领域特定能力，提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练策略（如基于记忆的检索或偏好优化）虽能改善用户对齐，但无法增强模型的领域认知能力。

Method: DPSE框架引入Censor模块提取多维交互信号和满意度评分，通过主题感知和偏好驱动策略扩展数据集，支持两阶段微调流程：监督领域基础训练和频率感知偏好优化。

Result: 实验表明，DPSE在通用NLP基准和长期对话任务中优于监督微调、偏好优化和基于记忆的基线方法。

Conclusion: DPSE为LLM的持续自进化提供了一条自主路径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [113] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 提出了一种新的AI文本检测器评估范式SHIELD，强调实际部署中的可靠性和稳定性，并开发了一种模型无关的人类化框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于依赖传统指标（如AUROC），忽略了实际部署中的关键问题（如误报率和稳定性）。

Method: 开发了SHIELD基准，整合可靠性和稳定性指标，并提出一种可控的人类化框架。

Result: SHIELD能够有效挑战现有零样本检测方法的可靠性和稳定性。

Conclusion: SHIELD为AI文本检测器的实际评估提供了更全面的标准。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [114] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了AI对齐原则与左翼政治偏见的必然联系，认为对齐目标与进步道德框架一致，而右翼意识形态与之冲突。研究者将左倾倾向视为风险，实际上违背了HHH原则。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对齐原则与政治偏见的关系，揭示对齐目标与左翼价值观的内在一致性。

Method: 通过理论分析，论证对齐目标与进步道德框架的契合性，并对比右翼意识形态的冲突。

Result: 智能系统若遵循对齐目标（无害、诚实），必然表现出左翼政治偏见。

Conclusion: 研究者将左倾倾向视为问题，实际上违背了AI对齐的核心原则（HHH）。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [115] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: 论文研究了多选问答（MCQA）作为评估大型语言模型（LLMs）下游任务性能的代理方法的有效性，发现其对最新推理模型的适用性有限，并提出了改进基准设计的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨MCQA在评估现代LLMs时是否仍能有效反映其下游任务性能，尤其是在模型推理能力显著提升的背景下。

Method: 系统评估了15个问答基准和25个不同规模的LLMs，测试了5种问题呈现方式，包括是否提供选项、是否允许链式推理等。

Result: MCQA仅在模型在提供选项前进行链式推理时有效；若允许推理后选择，模型会利用选项信息显著提升表现。

Conclusion: MCQA不再适合评估最新模型的真实性能，需设计更鲁棒、无偏的基准以反映LLMs的真实推理能力。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [116] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2是一个轻量级多语言内容审核分类器，针对新加坡语境优化，支持英语、中文、马来语和部分泰米尔语。它基于预训练的OpenAI嵌入和多头序数分类器，在17个基准测试中表现优于商业和开源系统。


<details>
  <summary>Details</summary>
Motivation: 现代审核系统在多语言支持上存在本地化和低资源变体的不足，导致实际部署中的安全漏洞。小型模型可作为大型LLM的替代方案，但仍需大量数据和计算资源。

Method: 使用预训练的OpenAI嵌入和多头序数分类器构建LionGuard 2，专注于新加坡语境的多语言支持。

Result: LionGuard 2在17个基准测试中表现优异，包括新加坡特定和公共英语数据集，实际部署于新加坡政府。

Conclusion: 高质量本地数据和强大多语言嵌入可实现强效审核性能，无需微调大型模型。模型权重和部分训练数据已公开，支持未来LLM安全研究。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [117] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 通过熵分析探究Transformer架构中信息分布，揭示模型行为和内部表示。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型如何管理和转换信息，以提升模型可解释性和评估框架。

Method: 量化令牌级不确定性并分析不同处理阶段的熵模式，以GPT模型为例。

Result: 揭示了模型行为和内部表示的潜在洞察。

Conclusion: 熵分析为Transformer模型的可解释性和评估提供了新视角。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [118] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文全面评估了大语言模型（LLM）在隐喻解释中的能力，发现其表现更多受表面特征（如词汇重叠和句子长度）影响，而非隐喻内容。


<details>
  <summary>Details</summary>
Motivation: 解决以往隐喻处理研究局限于单一数据集和特定任务设置的问题，采用多样化公开数据集进行更全面的评估。

Method: 使用多种公开数据集，在自然语言推理（NLI）和问答（QA）任务中进行实验，分析LLM的表现。

Result: LLM的表现主要由表面特征（如词汇重叠和句子长度）驱动，而非对隐喻内容的理解。

Conclusion: 当前LLM在隐喻处理中能力有限，需更现实的评估框架。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [119] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Stitch的新方法，通过在生成语音响应的同时交替生成内部思考过程，解决了现有语音语言模型（SLMs）无法进行内部思考的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SLMs缺乏内部思考能力，而人类在交流前通常会进行复杂的心理推理。因此，为SLMs集成内部思考过程是必要的。

Method: 提出Stitch方法，交替生成未说出的推理块和语音响应块，利用语音播放的剩余时间生成推理内容，实现同时思考和说话。

Result: Stitch在数学推理数据集上比基线模型性能提升15%，同时在非推理数据集上表现与基线相当，且延迟与基线一致。

Conclusion: Stitch成功实现了SLMs的同步思考和说话，显著提升了推理能力，同时保持了低延迟。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [120] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: 论文提出了AlgoSimBench，一个评估LLMs识别算法相似问题能力的基准，发现LLMs表现不佳，并提出ASM方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在训练数据较少的相关领域中识别算法相似问题的能力。

Method: 引入AlgoSimBench基准，包含1317个问题和402个多选题，提出ASM方法改进相似性检测。

Result: 最佳模型在MCQ任务中仅65.9%准确率，ASM提升6.7%-11.7%。代码嵌入模型和检索方法表现有限。

Conclusion: ASM方法有效提升LLMs识别算法相似问题的能力，但仍有改进空间。

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [121] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在驱动复杂动作执行的数字助手方面的潜力，提出了ASPERA框架和Asper-Bench数据集。


<details>
  <summary>Details</summary>
Motivation: 解决数字助手在执行多步骤目标时依赖预训练编程知识的挑战，同时应对数据可用性和评估鲁棒性问题。

Method: 开发ASPERA框架，包括助手库模拟和人工辅助的LLM数据生成引擎，生成高质量任务。

Result: 通过Asper-Bench数据集验证，基于自定义助手库的程序生成对LLMs是显著挑战。

Conclusion: ASPERA框架为LLMs在复杂任务执行中的应用提供了有效工具，同时揭示了其局限性。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [122] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的测试时间缩放（TTS）方法，结合细粒度顺序缩放和并行缩放，显著提升了大型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于训练的TTS方法（如持续强化学习）流行，但其计算开销增加了测试时间负担。本文旨在探索无需训练的TTS方法以提升推理性能。

Method: 设计了条件步骤级自优化方法（细粒度顺序缩放），并结合经典并行缩放方法，提出混合测试时间缩放（Hybrid TTS）新范式。

Result: 在多个规模（3B-14B）和家族的指令调优LLM上实验表明，混合策略显著扩展了LLM的推理性能边界。

Conclusion: 无需训练的混合TTS方法在推理任务中具有巨大潜力，为LLM性能提升提供了新方向。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [123] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本文探讨了多语言文本去毒系统的评估方法，比较了基于神经网络的评估模型和基于提示的LLM评估方法，提出了更可靠的多语言评估流程。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在文本生成任务上取得进展，但文本风格转换（TST）的评估仍具挑战性，尤其是多语言领域的研究较少。

Method: 通过九种语言（英语、西班牙语、德语、中文、阿拉伯语、印地语、乌克兰语、俄语、阿姆哈拉语）的文本去毒系统，比较神经网络评估模型和基于提示的LLM评估方法。

Result: 研究发现自动评估指标与人类判断之间存在显著差距，并提出了更可靠的多语言评估流程。

Conclusion: 本文为多语言文本去毒系统的评估提供了实用指南，填补了该领域的空白。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [124] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 论文提出了一种基于上下文学习（ICL）和视觉语言模型（VLM）的太赫兹（THz）图像分类方法，无需微调即可在低数据情况下提升分类效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 太赫兹成像在安全筛查和材料分类等应用中具有潜力，但现有方法因标注数据少、分辨率低和视觉模糊等问题而受限。

Method: 通过模态对齐的提示框架，将两种开源的视觉语言模型（VLM）适配到太赫兹领域，并在零样本和单样本设置下进行评估。

Result: 结果表明，上下文学习（ICL）在低数据情况下显著提升了分类性能和可解释性。

Conclusion: 这是首次将ICL增强的VLM应用于太赫兹成像，为资源受限的科学领域提供了新方向。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [125] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LEAR通过显式推理和提取机制减少检索噪声，提升LLMs生成质量。


<details>
  <summary>Details</summary>
Motivation: 检索噪声显著影响LLMs生成质量，现有方法缺乏显式推理，易遗漏关键线索且泛化能力差。

Method: LEAR分两步：显式推理识别潜在线索，有意识提取避免遗漏；统一响应框架、知识标记掩码和三种奖励函数优化模型。

Result: 在三个基准数据集上验证LEAR有效性，提供紧凑高质量证据，提升下游任务准确率。

Conclusion: LEAR显著改善RAG系统性能，促进在线应用。

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [126] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 分析冲突叙事揭示公共领域极化与议题对齐的话语机制。


<details>
  <summary>Details</summary>
Motivation: 探讨叙事作为理解政治现实的解释工具，以及冲突叙事如何揭示极化和议题对齐的话语机制。

Method: 通过分析德国Twitter圈2021-2023年的对立意见群体推文，提取冲突叙事的文本信号。

Result: 发现冲突叙事的两维度：角色分配差异和不同事件叙事；初步证据显示叙事对齐策略。

Conclusion: 叙事分析为极化话语机制提供了有效视角。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [127] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 本文介绍了针对多模态论证挖掘的MM-ArgFallacy2025共享任务的提交，使用预训练Transformer模型并结合上下文信息，在谬误分类子任务中取得了不同模态的性能结果。


<details>
  <summary>Details</summary>
Motivation: 推动多模态论证挖掘研究，特别是政治辩论中的逻辑谬误识别。

Method: 采用预训练的Transformer模型，并探索多种上下文利用方式。

Result: 文本模态F1得分为0.4444，音频模态为0.3559，多模态为0.4403，多模态模型表现接近文本模型。

Conclusion: 多模态模型表现与文本模型相当，显示改进潜力。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [128] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3框架通过同时优化系统和用户提示，提升大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独优化系统或用户提示效果不佳，需联合优化。

Method: P3框架通过迭代过程同时优化系统和用户提示，并支持在线优化。

Result: 在通用和推理任务中表现优异。

Conclusion: 整体优化策略能显著提升LLM性能。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [129] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 论文提出CoLD框架，通过长度惩罚、偏差估计器和联合训练策略，减少PRMs中的长度偏见，提升推理的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有PRMs存在长度偏见，倾向于给更长的推理步骤更高评分，影响奖励预测的可靠性和推理输出的简洁性。

Method: 提出CoLD框架，包含显式长度惩罚调整、学习的偏差估计器和联合训练策略，基于反事实推理和因果图分析。

Result: 在MATH500和GSM-Plus数据集上，CoLD显著降低了奖励与长度的相关性，提升了步骤选择的准确性和推理的简洁性。

Conclusion: CoLD有效提升了PRMs的可靠性和鲁棒性，为多步推理提供了更准确的评估和指导。

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [130] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 论文探讨了标准信号博弈模型中接收者难以学习组合信息的问题，提出了两种新模型：简约型接收者和通用型接收者，以促进组合理解。


<details>
  <summary>Details</summary>
Motivation: 标准信号博弈模型中，接收者无法组合理解信号，即使信号是组合的。信息丢失时，其他组件信息也会被遗忘。

Method: 构建了两种新模型：简约型接收者（仅从信号的原子消息学习）和通用型接收者（从所有可用信息学习）。

Result: 新模型比以往更简单，且能让接收者从消息的原子组件中学习。

Conclusion: 新模型成功实现了组合理解的演化，解决了标准模型的局限性。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [131] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 研究探讨了不同问题类型对大型语言模型（LLM）在推理任务中准确性的影响，发现性能差异显著且推理准确性与最终答案选择准确性不一定相关。


<details>
  <summary>Details</summary>
Motivation: 探索不同问题类型对LLM在推理任务中准确性的影响，填补研究空白。

Method: 评估五种LLM在三种问题类型上的表现，使用定量和演绎推理任务，分析推理步骤和最终答案选择的准确性。

Result: 发现LLM在不同问题类型上表现差异显著，推理准确性与最终答案选择准确性不相关，选项数量和措辞影响性能。

Conclusion: 问题类型对LLM推理性能有显著影响，需在设计评估任务时考虑问题类型和措辞。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [132] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: SemEval-2025 Task 11聚焦多语言情感检测，提出两种对比学习方法，在两项任务中取得不错排名。


<details>
  <summary>Details</summary>
Motivation: 解决情感表达多样性和背景差异带来的挑战，推动情感检测技术发展。

Method: 采用样本对比学习（CRC）和生成对比学习（DPO, SimPO），基于LLaMa3-Instruct-8B微调。

Result: 英语任务中排名第9（Track A）和第6（Track B），其他语言表现优异。

Conclusion: 对比学习方法在多语言情感检测中有效，未来可进一步优化。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [133] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 研究探讨了如何通过用户行为改进LLM评估方法，特别关注天文学文献检索机器人的使用案例，提出了构建更好基准的建议。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准未能跟上用户多样化需求的步伐，尤其是在科学研究中。

Method: 通过分析368个查询和11位天文学家的访谈，了解用户如何评估LLM系统。

Result: 揭示了用户评估系统的标准，并提出了改进基准的具体建议。

Conclusion: 研究为提升LLM评估和可用性提供了方法，尤其适用于科学研究。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [134] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO是一个标准化的眼科领域大语言模型评估基准，通过专家审核和多轮筛选，包含900个高质量问题，评估临床准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有眼科领域的LLM评估基准范围有限且过于注重准确性，BELO旨在提供更全面的评估标准。

Method: 通过关键词匹配和PubMedBERT模型筛选眼科MCQs，经多轮专家审核和去重，最终形成900个问题。

Result: 评估了六种LLM，使用多种指标（如准确率、ROUGE-L等），并建立公开排行榜。

Conclusion: BELO将成为公平、可重复的评估工具，推动未来模型的发展。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [135] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: IDRBench是一个新基准，用于评估大语言模型（LLMs）在跨学科研究（IDR）中提出有价值研究想法的能力，发现LLMs在此领域仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对LLMs在跨学科研究中创意能力的评估，限制了对其潜力的全面理解。

Method: 引入IDRBench，包含专家标注的数据集和任务，覆盖六个学科，评估LLMs在IDR中的表现。

Result: 尽管LLMs展现出一定的IDR意识，但在生成高质量IDR想法方面仍存在困难。

Conclusion: IDRBench为评估LLMs在跨学科研究中的能力提供了系统框架，并揭示了改进方向。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [136] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文从统计显著性测试的角度解释了TF-IDF的合理性，并展示了TF-ICF与Fisher精确检验的负对数p值之间的关系。


<details>
  <summary>Details</summary>
Motivation: 为TF-IDF这一经典信息检索方法提供统计学的理论基础，使其对统计学家更具说服力。

Method: 通过分析TF-ICF与Fisher精确检验的负对数p值之间的关系，并在理想假设下建立联系。

Result: 证明了在无限大文档集合的极限情况下，TF-ICF收敛于TF-IDF。

Conclusion: TF-IDF的有效性可以通过Fisher精确检验的统计显著性来解释，为统计学家提供了直观的理解方式。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [137] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge框架通过AI模拟生成人机对话，减少人工收集数据的负担，测试了多种LLM模型，发现大模型表现更优，小模型通过微调也能接近其性能。


<details>
  <summary>Details</summary>
Motivation: 减少人工收集对话数据的成本和时间，推动对话AI研究。

Method: 使用真实人机对话的种子提示初始化对话，测试多种LLM模型，探索微调技术提升小模型性能。

Result: 大模型（如GPT-4o）生成更真实的对话，小模型（如Llama、Mistral）通过微调性能显著提升。

Conclusion: 大模型在生成真实对话上表现最佳，小模型通过微调具有潜力，但所有模型在长对话连贯性上仍有挑战。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [138] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出“交互即智能”概念，将人机交互视为智能的核心维度，而非传统意义上的接口。通过Deep Cognition系统，实现透明、可控的交互，显著提升研究任务的效率和协作效果。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统采用“输入-等待-输出”模式，导致错误累积、研究边界僵化及专家知识整合不足。论文旨在通过交互重构人机关系，提升研究任务的智能性和灵活性。

Method: 提出Deep Cognition系统，包含三项创新：透明可控的交互、细粒度双向对话、共享认知上下文。用户可干预AI推理过程，实现认知监督。

Result: 用户评估显示，该系统在透明度、细粒度交互、实时干预等六项指标上显著优于基线，研究任务性能提升31.8%至50.0%。

Conclusion: 交互是智能的核心维度，Deep Cognition系统通过认知监督模式，显著提升人机协作效率和研究任务效果。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [139] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova是一个650M参数的解码器Transformer，通过架构设计和分词创新，在保持计算效率的同时达到更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 挑战现有扩展范式，证明架构效率和分词质量可以弥补参数减少。

Method: 结合RoPE、GQA（3:1压缩比）、RMSNorm和SwiGLU激活函数，使用128,000词汇的字节级BPE分词器。

Result: Supernova性能达到1B参数模型的90%，参数减少53%，训练token仅需100B。

Conclusion: 架构效率和分词创新可显著减少参数需求，挑战传统扩展方法。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [140] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer是一种基于熵感知的RLVR方法，通过双令牌约束和同步更新，显著提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法对所有令牌应用统一的训练信号，未考虑低熵知识令牌和高熵推理令牌的不同角色，可能导致语义依赖破坏。

Method: Archer采用双令牌约束和同步更新，对推理令牌应用较弱的KL正则化和较高的裁剪阈值以鼓励探索，对知识令牌则施加更强约束以保持事实知识。

Result: 在数学推理和代码生成基准测试中，Archer显著优于现有RLVR方法，达到或超过同类模型的最高性能。

Conclusion: Archer通过熵感知的双令牌约束，有效平衡了探索与知识保持，为RLVR提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [141] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 比较了三种字符级语言建模方法，包括两种储层计算方法和基于Transformer的架构，发现Transformer在预测质量上更优，而储层计算在效率上更高。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）的高能耗和慢处理问题，探索储层计算在自然文本处理中的潜力。

Method: 比较了两种储层计算方法和Transformer架构，通过统一管道评估性能、计算成本和预测准确性。

Result: Transformer在预测质量上表现更优，而储层计算在训练和推理速度上更高效。

Conclusion: 储层计算在资源受限场景下具有潜力，而Transformer在性能上更胜一筹，研究为平衡资源与性能提供了指导。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [142] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 本文探讨了AI在公益领域的实际部署与合作过程，填补了现有研究中关于模型部署和实际影响的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI公益研究多集中于模型开发，而忽略了部署与合作过程及其实际影响。本文旨在分享与H2H组织的合作经验。

Method: 通过与H2H组织的紧密合作，在资源受限环境中部署并持续维护AI模型。

Result: 分享了部署过程中的关键经验，为实践者提供了实用建议。

Conclusion: 强调了实际部署与合作的重要性，为AI公益领域的实践提供了参考。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [143] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究发现，双语大型语言模型（LLMs）在推理过程中混合使用语言（如中英交替）能提升准确性，而强制单语解码会降低表现。通过强化学习验证奖励（RLVR）阶段，模型学会语言切换策略。


<details>
  <summary>Details</summary>
Motivation: 探讨双语推理模型中语言切换行为的原因及其对推理能力的影响。

Method: 使用强化学习验证奖励（RLVR）训练模型，分析语言混合对推理的影响，并设计轻量级探针预测语言切换的效益。

Result: 语言混合提升数学推理任务准确性5.6个百分点；探针指导解码可进一步提高准确性6.25个百分点。

Conclusion: 语言混合是双语模型的策略性推理行为，而非多语言训练的副产品。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [144] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 论文提出了3LM，一套针对阿拉伯语的三个基准测试，填补了阿拉伯语大语言模型在STEM和代码生成领域的空白。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语大语言模型的研究在STEM和代码生成领域缺乏相关基准，限制了实际应用的发展。

Method: 设计了三个基准测试：1）自然来源的STEM问答对；2）合成生成的STEM问题；3）通过翻译和人工审核构建的代码生成测试。

Result: 发布了公开可用的三个基准测试，支持阿拉伯语大语言模型在关键领域的研究。

Conclusion: 3LM填补了阿拉伯语大语言模型在STEM和代码生成领域的空白，为未来研究提供了重要资源。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [145] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 比较分析用于拟合3D图像数据的算法策略，评估优化方法在生成近似晶粒结构的几何模型中的表现。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，准确拟合3D图像数据（如多晶体和泡沫）的几何模型是一个不断发展的领域，需要评估不同优化方法的效率和适用性。

Method: 比较线性/非线性规划、随机优化（交叉熵法）和梯度下降等方法，生成Voronoi、Laguerre和广义平衡功率图（GBPDs）以近似晶粒结构。

Result: 通过实际数据集评估拟合质量，发现模型复杂度、优化方法复杂度和近似质量之间存在权衡。

Conclusion: 研究结果为根据数据特征和应用需求选择合适方法提供了指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [146] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 论文探讨了深度学习在语义分割中的应用，特别是通过选择合适的主干网络（Backbone）提升模型性能，并在BDD100k数据集上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，深度学习在复杂任务中表现出色，如自动驾驶中的场景理解。本文旨在通过语义分割提升场景理解能力。

Method: 提出了多个高效模型，使用BDD100k数据集进行语义分割研究，并尝试不同主干网络作为编码器。

Result: 实验表明，选择合适的主干网络显著影响语义分割模型的性能，最终模型的准确性、平均IoU和损失函数均有所提升。

Conclusion: 通过优化主干网络选择，语义分割模型的性能得到显著提升，有助于更好地理解场景和环境。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [147] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种基于梯度的测试时适应方法，通过软对比损失提升视觉语言模型在分布偏移下的泛化能力，并在开放集设置中表现出色。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本任务中表现良好，但在分布偏移下泛化能力不足，传统测试时适应方法（如熵最小化）与对比训练目标不匹配。

Method: 提出CLIPTTA，使用软对比损失与CLIP预训练目标对齐，并通过理论分析避免崩溃风险；扩展至开放集设置，引入OCE损失改进OOD检测。

Result: 在75个数据集上评估，CLIPTTA优于基于熵的方法，并在多种分布偏移下表现稳定，优于现有TTA方法。

Conclusion: CLIPTTA通过对齐预训练目标，显著提升了视觉语言模型在分布偏移和开放集环境中的适应能力。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [148] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 论文提出了一种名为注意力聚焦（AF）的机制，通过剪除非信息性标记来优化广义类别发现（GCD）中的特征提取，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时容易受到背景区域的干扰，导致注意力分散和特征提取不理想。

Method: AF由两个组件组成：标记重要性度量（TIME）和多尺度标记自适应剪枝（TAP），通过量化标记重要性并剪除非信息性标记来优化注意力。

Result: AF在SimGCD方法中实现了高达15.4%的性能提升，且计算开销极小。

Conclusion: AF是一种轻量级、即插即用的模块，能够显著提升GCD任务的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [149] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 生成超分辨率（GSR）在感知图像质量上领先，但存在幻觉问题。本文提出了一种基于多模态大语言模型（MLLM）的幻觉评分（HS）方法，并通过深度学习特征对齐模型以减少幻觉。


<details>
  <summary>Details</summary>
Motivation: GSR模型在感知质量上表现优异，但生成的细节可能与低分辨率图像（LRI）或真实图像（GTI）不匹配，这一问题尚未充分研究。

Method: 利用MLLM构建提示，评估幻觉视觉元素并生成HS；通过深度学习特征距离作为奖励函数对齐GSR模型。

Result: HS与人类评估高度一致，且为超分辨率模型提供了新的评估维度；某些深度学习特征距离与HS强相关。

Conclusion: 提出的HS方法和特征对齐策略有效减少了GSR模型的幻觉问题，提升了实用性和图像质量。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [150] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack 是一种结合深度学习和光流的半自动化工具，用于在 B 模式超声视频中跟踪任意点，解决了传统方法的噪声和运动模糊问题。


<details>
  <summary>Details</summary>
Motivation: B 模式超声中的组织运动跟踪因斑点噪声、低边缘对比度和平面外运动而具有挑战性，需要一种更准确和通用的解决方案。

Method: 结合深度学习和光流技术，提供高质量训练数据生成和模型迭代优化的图形界面，并采用新型光流滤波技术减少噪声。

Result: DUSTrack 在准确性上优于零样本点跟踪器，与专用方法相当，适用于心脏壁运动、肌肉变形和筋膜跟踪等多种场景。

Conclusion: DUSTrack 作为一种开源工具，为临床和生物力学研究提供了强大且灵活的组织运动量化框架。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [151] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，用于可解释的可用性基础，通过结合常识先验和视觉证据，迭代优化预测。


<details>
  <summary>Details</summary>
Motivation: 解决场景理解中对象与动作关联的透明性和可解释性问题。

Method: 整合ConceptNet和语言模型的结构化常识先验与CLIP的视觉证据，通过基于能量的推理循环迭代优化。

Result: 在多对象、无标签设置中提高了准确性和可解释性。

Conclusion: CRAFT为稳健且可信的场景理解提供了方向。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [152] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯变形场的3DGS视频流框架，通过混合显著性分块和差异化质量建模，实现了高效压缩和带宽适应，传输性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）视频的数据量大且传输复杂，需要解决其流式传输的挑战。

Method: 设计基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模。

Result: 实验验证显示，该方法在视频质量、压缩效果和传输速率上均优于现有方法。

Conclusion: 提出的框架有效解决了3DGS视频流式传输的挑战，具有实际应用潜力。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [153] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个针对真实红外图像的多模态大语言模型，基于26万对真实红外图像与手工标注文本的数据集（IR-TD），并通过双向跨模态课程迁移学习策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖合成红外图像的局限性，以及真实红外图像与文本对齐数据稀缺的问题。

Method: 构建大规模IR-TD数据集，结合LLM生成和规则标注的文本，采用双向跨模态课程迁移学习策略。

Result: 在9项任务中达到最先进性能，优于更大规模的模型。

Conclusion: IRGPT通过真实数据集和迁移学习策略，显著提升了红外图像与语言模型的结合能力。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [154] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），用于点云配准中高质量对应关系的识别，通过正交几何一致性和多粒度交互提升性能。


<details>
  <summary>Details</summary>
Motivation: 点云配准中高质量对应关系的识别因局部与全局特征融合的复杂性和冗余性而极具挑战性。Gestalt原则为分析局部与全局关系提供了优势。

Method: 提出GPI-Net，利用Gestalt原则促进局部与全局信息的互补交互，包括正交集成策略、Gestalt特征注意力块（GFA）和双路径多粒度并行交互聚合块（DMG）。

Result: 在多个挑战性任务上的实验表明，GPI-Net优于现有方法。

Conclusion: GPI-Net通过Gestalt原则和多粒度交互，显著提升了点云配准中对应关系的识别质量。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [155] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种自适应3D高斯泼溅视频（3DGS）流媒体解决方案，包括基于显著性分析的3DGS分块技术、新型质量评估框架和基于元学习的自适应比特率算法。


<details>
  <summary>Details</summary>
Motivation: 3DGS流媒体在提供沉浸式3D视频体验方面具有潜力，但仍面临分块、质量评估和比特率适应等挑战。

Method: 提出自适应3DGS分块技术（结合空间和时间特征）、新型质量评估框架（联合评估3DGS表示和2D渲染图像质量）和基于元学习的自适应比特率算法。

Result: 实验表明，所提方法显著优于现有技术。

Conclusion: 本文为3DGS流媒体提供了一套全面的解决方案，解决了关键挑战并提升了性能。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [156] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一个混合专家端到端自动驾驶框架，通过全局专家、场景自适应专家组和双感知路由器实现多样场景下的自适应和鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模式规划方法难以处理多样场景，需要一种能适应复杂交通环境的自动驾驶框架。

Method: 提出GEMINUS框架，包含全局专家、场景自适应专家组和双感知路由器，动态激活专家模块。

Result: 在Bench2Drive基准测试中表现优异，驾驶分数和成功率均达到最优，单目视觉输入下仍有显著提升。

Conclusion: GEMINUS通过混合专家架构显著提升了自动驾驶的适应性和鲁棒性，适用于多样场景。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [157] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一个抗篡改的可视化图像数据检索框架，通过嵌入元数据链接解决现有方法易受图像裁剪和编辑影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可视化图像中嵌入元数据时易受篡改影响，导致信息丢失，亟需一种更鲁棒的解决方案。

Method: VisGuard采用重复数据平铺、可逆信息广播和基于锚点的裁剪定位技术，增强嵌入数据的鲁棒性。

Result: 实验证明VisGuard在数据检索准确性、嵌入容量和抗篡改安全性方面表现优异。

Conclusion: VisGuard能有效保护和促进可视化传播与信息传递。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [158] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet提出了一种新颖的序列建模框架，结合空间特征提取和时间差分，通过轻量级1D卷积编码器和可学习的差分时间算子（DSD）提升动态环境中的视觉地点识别（VPR）性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态和感知混淆环境中VPR的挑战，现有方法多忽略图像序列的时间连贯性。

Method: 使用1D卷积编码器和DSD模块联合捕获空间上下文和时间过渡，结合LSTM优化和四重损失增强区分性。

Result: 在多个公开基准测试中表现优于现有方法，尤其在季节和视角变化下。

Conclusion: OptiCorNet通过端到端学习序列级嵌入，显著提升了VPR的性能。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [159] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT提出了一种无需数据的ViT量化方法，通过合成高质量样本和激活校正矩阵，显著提升了量化模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成样本时未能平衡全局和局部特征，且量化模型与全精度模型的中间层激活分布差异大，导致性能下降。

Method: 按难度递增顺序合成样本，并引入激活校正矩阵以对齐量化模型与全精度模型的中间层激活。

Result: DFQ-ViT在性能上显著优于现有DFQ方法，接近真实数据量化的模型，如3位量化的DeiT-T性能提升4.29%。

Conclusion: DFQ-ViT无需微调，降低了计算开销和部署门槛，符合绿色学习原则，适用于资源受限环境。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [160] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强的点云补全框架，通过跨模态检索学习结构先验信息，生成细粒度点云。


<details>
  <summary>Details</summary>
Motivation: 解决不完整点云补全任务中缺乏典型结构特征的挑战，提升生成能力和泛化性。

Method: 设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），结合跨模态检索和层次特征融合。

Result: 在多个数据集和真实场景中验证了方法的有效性，尤其在稀疏数据和未见类别上表现优异。

Conclusion: 该方法通过检索增强和层次融合，显著提升了点云补全的质量和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [161] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种通过令牌压缩实现病理全切片图像视觉问答的多模态大型语言模型，显著降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 病理全切片图像的高分辨率和长上下文长度对多模态大型语言模型提出挑战，现有方法资源消耗大且缺乏生成能力。

Method: 提出TCP-LLaVA，利用可训练的压缩令牌通过模态压缩模块聚合信息，仅将压缩后的令牌输入语言模型。

Result: 在十种TCGA肿瘤亚型上，TCP-LLaVA在VQA准确率上优于现有基线，同时大幅减少资源消耗。

Conclusion: TCP-LLaVA为病理全切片图像的视觉问答提供了一种高效且性能优越的解决方案。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [162] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件法线流的运动分割和自运动估计框架，适用于神经形态视觉传感器，无需完整光流计算即可实现高精度分割和运动估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而神经形态传感器的事件数据稀疏且时间分辨率高，需要一种更高效的方法。

Method: 利用事件数据，结合法线流、场景结构和惯性测量的几何约束，通过优化流程进行事件过分割、残差分析独立运动物体，并基于运动相似性和时间一致性进行层次聚类。

Result: 在EVIMO2v2数据集上验证了方法的准确性，尤其在物体边界表现优异。

Conclusion: 该方法在实时机器人和导航应用中具有显著优势和潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [163] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 综述了基于前馈方法的3D重建与视图合成技术，分类讨论了不同表示架构，并探讨了其应用与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算复杂且难以实时应用，深度学习驱动的前馈方法提供了更高效、通用的解决方案。

Method: 分类讨论了点云、3D高斯泼溅、神经辐射场等表示架构，并分析了关键任务如无姿态重建、动态重建等。

Result: 总结了常用数据集、评估协议及在数字人、SLAM等领域的应用。

Conclusion: 前馈方法有望推动3D视觉领域的发展，但仍存在开放挑战和未来研究方向。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [164] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为DCHM的框架，通过深度一致的人体建模和超像素高斯喷洒技术，在多视图、大规模和拥挤场景中实现精确的行人检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视图行人检测中噪声大、精度低，且依赖昂贵的3D标注，难以泛化到多样化场景。

Method: 采用深度一致的人体建模（DCHM）框架，结合超像素高斯喷洒技术，实现多视图深度一致性和全局坐标融合。

Result: 显著减少了人体建模中的噪声，性能优于现有基线方法，并在挑战性场景中首次实现行人重建和多视图分割。

Conclusion: DCHM框架无需依赖人工标注，在多视图行人检测中表现出色，具有广泛的应用潜力。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [165] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 提出ArtiMuse模型和ArtiMuse-10K数据集，解决图像美学评估中的模态偏差和细粒度属性分解问题。


<details>
  <summary>Details</summary>
Motivation: 教育、艺术创作和AI生成内容的发展对图像美学评估提出更高要求，现有方法存在模态偏差和缺乏细粒度分析。

Method: 开发基于多模态大语言模型的ArtiMuse，结合评分与专家级理解，并构建专家标注的ArtiMuse-10K数据集。

Result: ArtiMuse模型和ArtiMuse-10K数据集将公开，推动图像美学评估领域发展。

Conclusion: ArtiMuse和ArtiMuse-10K为图像美学评估提供了更全面的解决方案。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [166] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 提出一种浏览器扩展，将手语实时翻译为字幕，以帮助听障人士在视频会议中更顺畅地交流。


<details>
  <summary>Details</summary>
Motivation: 听障人士与普通人之间的沟通存在障碍，尤其在疫情期间视频会议成为主要交流方式时，手语翻译需求增加。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展实现手语到字幕的自动翻译。

Result: 通过扩展实现手语实时翻译，提升听障人士在视频会议中的交流效率。

Conclusion: 该技术有望减少听障人士与普通人之间的沟通障碍，尤其在远程交流场景中发挥重要作用。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [167] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍了针对ImageCLEFmed MEDVQA 2025挑战赛子任务1的视觉问答（VQA）方法，采用Florence模型作为基础，结合领域增强技术提升泛化能力，实验结果显示其准确性。


<details>
  <summary>Details</summary>
Motivation: 解决胃肠道内窥镜视觉问答问题，探索大型多模态模型在医学VQA中的潜力。

Method: 使用Florence模型作为VQA管道基础，结合视觉和文本编码器，并应用领域特定增强技术。

Result: 在KASVIR数据集上微调Florence模型后，在官方挑战指标上表现优异。

Conclusion: 大型多模态模型在医学VQA中具有潜力，为未来研究提供了强基线。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [168] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 研究探讨了人工神经网络（ANN）与人类情感感知之间的关系，发现ANN分类模糊的刺激同样引发人类感知差异，并通过行为数据优化ANN预测。


<details>
  <summary>Details</summary>
Motivation: 解决情感认知科学中外部情感刺激与人类内部体验关系建模的挑战，特别是探索ANN在模拟个体感知差异方面的潜力。

Method: 提出一种新颖的感知边界采样方法，生成位于ANN决策边界的面部表情刺激，构建varEmotion数据集，并通过大规模人类行为实验验证。

Result: 发现ANN分类模糊的刺激同样引发人类感知不确定性，通过行为数据微调ANN，实现了ANN预测与人类感知模式的对齐。

Conclusion: 建立了ANN决策边界与人类感知变异性之间的系统性联系，为情感解释的个性化建模提供了新见解。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [169] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 开发了一个相机引导系统，帮助用户识别和去除照片中的杂乱，提升照片美学质量。


<details>
  <summary>Details</summary>
Motivation: 照片中的杂乱会分散注意力，影响情感或故事的传达，业余摄影师常因经验不足而忽略此问题。

Method: 通过美学评估算法区分杂乱对象，并基于生成对抗网络的迭代图像修复算法去除杂乱。

Result: 用户研究表明，系统能帮助用户更高效地识别杂乱并提升照片质量。

Conclusion: 该系统通过灵活界面和准确算法，有效帮助用户改善摄影作品。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [170] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D通过自然语言显式编码物体间关系，提升3D场景理解，在多个任务和数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景-语言模型在关系理解上表现不足，视觉嵌入无法充分表达物体角色和交互。

Method: Descrip3D为每个物体添加文本描述，捕捉其属性和上下文关系，并通过嵌入融合和提示级注入实现双层次集成。

Result: 在五个基准数据集上，Descrip3D均优于基线模型。

Conclusion: 语言引导的关系表示能有效提升复杂室内场景的理解能力。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [171] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD提出了一种基于logits的微调对齐方法，通过ODE建模非线性优化过程，有效预测预训练模型在下游任务中的迁移性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型数量激增，如何高效选择适合下游任务的模型成为挑战，现有方法未能准确捕捉微调动态的非线性特征。

Method: LEAD基于logits设计理论框架，用ODE建模优化过程，并提出类感知分解方法，确保实际适用性。

Result: 在24个预训练模型和10个下游数据集上的实验表明，LEAD性能优异，适应性强，尤其在低数据场景。

Conclusion: LEAD通过单步优化填补微调过程的优化差距，为预训练模型选择提供了高效解决方案。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [172] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 论文比较了GAN、扩散模型和流匹配技术在MRI图像合成中的表现，发现Pix2Pix GAN在结构保真度和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的模态。

Method: 使用GAN、扩散模型和流匹配技术进行T1w到T2w的2D MRI图像合成，并在三个公开数据集上评估。

Result: Pix2Pix GAN在结构保真度、图像质量和计算效率上表现最佳。

Conclusion: GAN更适合小数据集和简单任务，流匹配模型可能需要更多数据。研究为实际MRI工作流程提供了指导。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [173] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了TensorFlow、PyTorch和JAX在BloodMNIST数据集上的性能，重点关注推理时间和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗影像对疾病诊断至关重要，但缺乏对特定深度学习框架性能的详细分析。

Method: 使用BloodMNIST数据集，比较TensorFlow、PyTorch和JAX在分类任务中的表现，分析推理时间和图像大小的影响。

Result: 结果显示不同框架性能存在差异，JAX和PyTorch的分类准确性接近当前基准。

Conclusion: JAX和PyTorch在医学影像分类中表现高效，适合相关应用。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [174] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是一种结合无监督分割和弱开放词汇指导的3D开放词汇子概念发现方法，在开放词汇和无监督分割的边缘案例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应任务特定目标或场景内容，DiSCO-3D旨在提供一种适应场景和用户查询的3D语义分割方法。

Method: 基于神经场表示，结合无监督分割和弱开放词汇指导。

Result: 在开放词汇子概念发现中表现优异，并在开放词汇和无监督分割的边缘案例中达到最先进水平。

Conclusion: DiSCO-3D为3D语义分割提供了一种灵活且高效的方法，适应性强。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [175] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出Exp-Graph框架，利用图建模和视觉变换器结合图卷积网络，提升面部表情识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互中至关重要，但面部属性的结构变化需要被有效建模。

Method: 使用面部关键点作为图顶点，基于邻近性和局部外观相似性构建边，结合视觉变换器和图卷积网络捕捉结构依赖。

Result: 在Oulu-CASIA、eNTERFACE05和AFEW数据集上分别达到98.09%、79.01%和56.39%的准确率。

Conclusion: Exp-Graph在实验室和真实环境中均表现出强泛化能力，适用于实际应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [176] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2是一个高效的适配框架，通过Depthwise-Dilated Adapter增强SAM2的多尺度特征提取能力，适用于医学视频分割与跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态特定设计，适应性差，且SAM2在医学视频场景中需要大规模数据重训练，计算成本高。

Method: 提出DD-SAM2框架，结合DD-Adapter，以最小参数开销增强多尺度特征提取，支持小规模数据微调。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上表现优异，Dice分数分别达到0.93和0.97。

Conclusion: DD-SAM2为医学视频分割与跟踪提供了一种高效的适配器微调方法，代码与模型将公开。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [177] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: 论文介绍了BusterX++框架，用于跨模态检测和解释合成媒体，通过强化学习后训练策略和多阶段训练提升性能，并提出了GenBuster++基准用于评估。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有单模态检测方法难以应对跨模态合成内容。

Method: 提出BusterX++框架，结合强化学习后训练策略（多阶段训练、思维奖励、混合推理）提升性能，并建立GenBuster++基准。

Result: 实验证明BusterX++在跨模态检测中表现优异且具有泛化能力。

Conclusion: BusterX++为跨模态合成媒体检测提供了有效解决方案，并通过基准验证其性能。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [178] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion提出了一种基于状态空间模型的双路径参数交互机制，有效解决了多光谱特征融合中的局部偏好和计算复杂度问题，显著提升了多光谱目标检测的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多光谱特征融合中过度偏好局部互补特征和计算复杂度与感受野大小的权衡问题。

Method: 采用双路径参数交互机制，结合交叉参数分支和共享参数分支，通过状态空间模型实现高效特征融合。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中显著优于现有方法，并在RGB-T语义分割和RGBT显著目标检测中表现出通用性。

Conclusion: MS2Fusion在多光谱目标检测和其他感知任务中表现出高效性和通用性，具有广泛的应用潜力。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [179] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个基于AI的框架，用于提升体育裁判的效率和公平性，特别是在跆拳道中实时头部踢击检测和评分。


<details>
  <summary>Details</summary>
Motivation: 传统裁判系统存在延迟、主观性和不一致的问题，影响公平性和运动员信任。

Method: 利用计算机视觉、深度学习和边缘推断技术，通过姿态估计、动作分类和影响分析实现自动化裁判。

Result: 系统将决策时间从分钟缩短到秒，提高了裁判的一致性和透明度。

Conclusion: FST.ai不仅适用于跆拳道，还可扩展到其他需要动作检测的体育项目，展示了其广泛应用的潜力。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [180] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 该研究提出了一种基于计算机视觉的框架，用于量化机构用餐环境中的食物浪费，通过语义分割RGB图像评估餐盘级浪费，展示了高效且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 量化机构用餐中的食物浪费对制定数据驱动的可持续策略至关重要，但目前缺乏高效且可扩展的监测方法。

Method: 研究使用四种全监督模型（U-Net、U-Net++及其轻量版），结合动态逆频率损失和AdamW优化器，通过多种指标（如Pixel Accuracy、Dice、IoU和自定义DPA）评估性能。

Result: 所有模型表现良好，部分模型对某些食物类型的DPA接近或超过90%，轻量模型在NVIDIA T4 GPU上实现实时推理。干燥和刚性食物（如米饭和薯条）分割效果更佳。

Conclusion: 该框架为大规模食品服务环境中的实时浪费监测提供了基础，尽管存在2D成像和食物种类限制，但仍具有实际应用潜力。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [181] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通过双路径多级判别框架，提升病理图像与基因表达之间的跨模态对齐，显著提高基因表达预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用病理图像与基因表达的多层次跨模态对齐，限制了预测性能。

Method: 提出Gene-DML框架，通过双路径（多尺度实例级判别和跨级实例-组判别）增强形态与转录模态的对应关系。

Result: 在公共空间转录组数据集上，Gene-DML实现了最先进的基因表达预测性能。

Conclusion: Gene-DML通过联合建模细粒度和结构级判别，提升了跨模态表征的鲁棒性和预测准确性。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [182] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 论文提出了一个高质量文档级数据集Doc-750K，并开发了原生多模态模型Docopilot，解决了现有MLLMs在复杂文档理解中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂文档理解上表现不足，主要由于缺乏高质量的文档级数据集和现有RAG方法的局限性。

Method: 构建了Doc-750K数据集，包含多样文档结构和跨页依赖，并开发了原生多模态模型Docopilot。

Result: Docopilot在文档理解任务中表现出更高的连贯性、准确性和效率。

Conclusion: Docopilot为文档级多模态理解设定了新的基准，并开源了数据、代码和模型。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [183] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一种新型协作多代理系统，用于多模态WSI分析，通过任务分配、验证和总结模块提升任务特定准确性和多任务通用性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在WSI分析中性能不足的问题，探索协作多代理系统在病理学领域的潜力。

Method: 集成功能代理，包括任务分配模块、验证机制和总结模块，利用模型库和病理知识库提升性能。

Result: 在多模态WSI基准测试中表现优于现有WSI MLLMs和医疗代理框架。

Conclusion: WSI-Agents在病理学领域实现了高准确性和多任务通用性，为WSI分析提供了新思路。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [184] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 论文提出了一种名为MIPD的新框架，通过从多模态大语言模型（MLLM）中蒸馏知识，提升小规模GSR模型的泛化和零样本能力，解决了复杂场景识别和边缘设备部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM在复杂场景识别（GSR）上表现不佳且资源消耗大，而传统GSR模型泛化能力不足。论文旨在通过知识蒸馏提升小模型的性能。

Method: 提出MIPD框架，利用LLM生成正负样本的理性解释，通过场景感知和实例感知提示对齐视觉信息，最终将知识蒸馏到学生模型中。

Result: 在Ov-SWiG和HICO-DET数据集上，MIPD在已知、罕见和未知场景中表现优异，并提升了未知场景的检测能力。

Conclusion: MIPD通过知识蒸馏有效提升了小模型的泛化和零样本能力，为复杂场景识别提供了更高效的解决方案。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [185] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 该论文提出了首个全球梯田地块精细数据集GTPBD，覆盖复杂地形，适用于多种任务，填补了梯田遥感研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究缺乏对复杂梯田地形的精细表示，无法满足精准农业需求。

Method: 构建了包含20多万个复杂梯田地块的GTPBD数据集，提供三级标注，并基于此进行了多任务基准测试。

Result: GTPBD在语义分割、边缘检测、地块提取和无监督域适应任务中表现出色，填补了研究空白。

Conclusion: GTPBD为精细农业地形分析和跨场景知识转移提供了基础设施。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [186] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: 提出MultiRetNet，结合视网膜成像、社会经济因素和共病情况，提高糖尿病视网膜病变分期准确性，并整合临床延迟系统，改善早期检测。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球可预防失明的主要原因，低收入群体因筛查机会有限，风险更高。共病加速病情发展，需改进早期检测方法。

Method: 结合视网膜成像、社会经济因素和共病数据，采用三种多模态融合方法，选择全连接层融合。通过对抗性图像和对比学习训练延迟系统。

Result: 全连接层融合方法最有效，系统能识别需临床复查的异常样本，保持低质量图像的诊断准确性。

Conclusion: MultiRetNet可改善早期检测，尤其对服务不足群体，降低医疗成本，促进医疗公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [187] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 论文提出了InterAct VideoQA数据集，用于提升视频问答模型在复杂交通场景中的表现，并展示了其在智能交通系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答模型难以处理真实交通场景中的复杂时空事件，需要专门的数据集来提升性能。

Method: 构建了包含8小时真实交通视频和25,000个问答对的InterAct VideoQA数据集，并评估了现有模型。

Result: 现有模型在复杂交通场景中表现不佳，但通过微调在InterAct VideoQA上性能显著提升。

Conclusion: InterAct VideoQA为智能交通系统中的视频问答研究提供了重要基准。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [188] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过结合因果感知查询优化和细粒度视觉定位，解决了VideoQA中任务无关采样和启发式检索的不足，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VideoQA方法存在任务无关采样和启发式检索的局限性，无法有效捕捉关键事件和因果-时间结构。

Method: LeAdQA利用LLM优化问题-选项对，通过时间定位模型精确检索关键片段，并动态融合证据，最后由MLLM生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA上实现了SOTA性能，同时保持计算效率。

Conclusion: LeAdQA通过精确的视觉定位和因果推理，显著提升了复杂VideoQA任务的性能。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [189] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS框架首次实现了对冻结Vision Transformers（ViTs）的高效空间-光谱可解释性，解决了现有方法在HSI数据中的挑战。


<details>
  <summary>Details</summary>
Motivation: 在HSI数据中，现有方法难以捕捉有意义的光谱线索且计算成本高，限制了ViTs的可解释性。

Method: FOCUS引入类特定光谱提示和可学习的[SINK]令牌，无需梯度反向传播即可生成3D显著性图和光谱重要性曲线。

Result: FOCUS提高了波段级IoU 15%，减少注意力崩溃40%，且与专家标注高度一致。

Conclusion: FOCUS以低参数开销实现了高分辨率ViT可解释性，填补了黑盒建模与可信HSI决策之间的空白。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [190] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于信息互补的下采样方法HPD，通过MinMaxPooling替代传统方法，提升了语义分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法在语义分割任务中可能导致关键空间信息丢失，影响像素级预测精度。

Method: 提出Hybrid Pooling Downsampling (HPD)，利用MinMaxPooling保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上，HPD的平均DSC系数提升了0.5%。

Conclusion: HPD模块为语义分割任务提供了高效解决方案。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [191] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EPD的新型ODE求解器，通过并行梯度评估减少截断误差，实现高质量低延迟采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因顺序去噪导致高采样延迟，现有加速方法在低延迟预算下图像质量下降。

Method: EPD通过并行梯度评估优化ODE求解器，参数可学习且训练开销小，可作为插件提升现有ODE采样器。

Result: 在5 NFE延迟下，EPD在多个数据集上FID显著优于现有方法，如CIFAR-10为4.47。

Conclusion: EPD在保持低延迟的同时显著提升采样质量，适用于多种图像合成任务。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [192] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 论文评估了DUSt3R、MASt3R和VGGT等3D重建基础模型在航拍图像上的表现，发现它们能从极稀疏图像集（少于10张）中准确重建密集点云，但高分辨率和大图像集下效果受限。


<details>
  <summary>Details</summary>
Motivation: 探索这些模型在航拍图像块上的潜力，填补其在摄影测量领域未被充分研究的空白。

Method: 在UseGeo数据集的航拍图像块上对预训练的DUSt3R/MASt3R/VGGT模型进行全面评估，包括姿态估计和密集3D重建。

Result: 这些方法能从极稀疏图像集（分辨率最高518像素）中重建密集点云，完整性比COLMAP提升50%，VGGT计算效率更高。但高分辨率和大图像集下姿态估计可靠性下降。

Conclusion: 基于Transformer的方法尚无法完全取代传统SfM和MVS，但在低分辨率、稀疏场景中可作为补充方案。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [193] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一种基于视觉提示的统一低层视觉任务框架VPIP，通过输入-目标图像对指导模型处理多种任务，并验证了其有效性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决低层视觉任务多样性和统一建模的挑战。

Method: 设计VPIP框架，包含图像处理主干、提示编码器和交互模块，支持多任务联合训练。

Result: 模型GenLV在多种任务中表现优异，扩展性和泛化能力显著。

Conclusion: VPIP框架为低层视觉任务提供了统一且可扩展的解决方案。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [194] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类认知的多脸深度伪造视频检测方法HICOM，通过分析人类依赖的关键线索，显著提升了检测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单脸检测表现良好，但在多脸场景中因缺乏上下文线索而表现不佳。

Method: 通过人类研究提取关键线索（场景运动一致性、面部外观兼容性、人际注视对齐和面部身体一致性），并设计HICOM框架。

Result: HICOM在基准数据集上平均准确率提升3.3%，在真实扰动下提升2.8%，在未见数据集上优于现有方法5.8%。

Conclusion: 结合人类认知线索可有效提升深度伪造检测性能，HICOM框架具有高泛化性和可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [195] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级机器人动作预测方法，基于InstructPix2Pix模型，仅需单张图像和文本指令即可预测未来10秒的视觉帧，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹在机器人、自动驾驶等领域至关重要，但传统视频预测模型计算成本高且延迟大，需更高效的方法。

Method: 利用InstructPix2Pix模型，将其扩展到机器人任务中，接受视觉和文本输入，实现多模态未来帧预测。

Result: 在RoboTWin数据集上，该方法在SSIM和PSNR指标上优于现有基线，且计算需求更低。

Conclusion: 该方法为机器人动作预测提供了一种高效、轻量级的解决方案，特别适用于对运动轨迹精度要求高的场景。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [196] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant是一种统一的量化框架，通过自适应结合互补技术提升跨模型通用性，解决了扩散模型量化中的通用性和部署兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高，现有后训练量化方法通用性不足，难以适应工业部署需求。

Method: 提出SegQuant框架，包含SegLinear（分段感知的图量化策略）和DualScale（双尺度量化方案），以保留结构语义和视觉保真度。

Result: SegQuant在多种扩散模型中表现优异，且与主流部署工具兼容。

Conclusion: SegQuant为扩散模型的高效部署提供了通用且高效的量化解决方案。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [197] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench是首个专注于真实世界金融图表的基准测试，包含1,200张金融图表和7,016个问题。评估了25个大型视觉语言模型，揭示了当前模型在金融图表理解中的局限性。


<details>
  <summary>Details</summary>
Motivation: 金融图表因其复杂的时间结构和领域特定术语，尚未被充分研究。

Method: 构建FinChart-Bench数据集，包含1,200张金融图表和7,016个问题，评估25个LVLM模型。

Result: 发现开源与闭源模型性能差距缩小、升级模型性能下降、指令遵循困难、空间推理能力不足、模型不可靠等问题。

Conclusion: 当前LVLM在金融图表理解中存在显著局限性，FinChart-Bench为未来研究提供了基准。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [198] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet提出了一种物理引导的雾霾转移网络，通过将目标域的雾霾模式转移到源域的无雾图像上，生成特定领域的微调数据集，以提升去雾模型的域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在真实场景中因训练数据有限，对未见过的真实雾霾图像性能下降显著，需要一种灵活的域适应方法。

Method: 提出PHATNet，通过转移雾霾模式生成微调数据集，并引入雾霾转移一致性和内容泄漏损失以增强解耦能力。

Result: 实验表明，PHATNet显著提升了现有去雾模型在真实图像去雾数据集上的性能。

Conclusion: PHATNet通过域适应方法有效提升了去雾模型在真实场景中的表现。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [199] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种用于数字乳腺断层合成（DBT）图像中肿块分割的配对图像生成方法，解决了现有扩散模型在生成质量和标注数据不足方面的问题。


<details>
  <summary>Details</summary>
Motivation: DBT图像中肿块分割对乳腺癌早期筛查至关重要，但高密度乳腺组织导致肿块隐蔽，人工标注困难且耗时，缺乏标注数据。现有扩散模型在生成质量和标注生成方面存在不足。

Method: 提出了一种无需外部条件的配对图像生成方法，通过训练额外的扩散引导器，实现条件扩散模型的配对图像生成。

Result: 实验生成了配对的DBT切片和肿块掩码，并将其用于监督训练，提高了生成质量并缓解了标注数据不足问题。

Conclusion: 该方法无需外部条件即可提升生成质量，有助于缓解标注数据短缺，提升下游任务性能。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [200] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出了一种仅需稀疏深度测量和对应图像的自监督深度补全方法，无需密集标签或多帧数据。


<details>
  <summary>Details</summary>
Motivation: 密集深度标注成本高，多帧依赖限制了自监督方法在静态或单帧场景的应用。

Method: 利用深度分布特性设计新损失函数，结合视觉基础模型的分割图增强深度估计。

Result: 实验证明该方法有效。

Conclusion: 新方法解决了现有方法的局限性，为深度补全提供了更实用的解决方案。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [201] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言和基础模型的全能视频修复框架，无需预知退化信息，并在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预知退化信息，限制了灵活性和可解释性。本文旨在通过自然语言和基础模型提供更灵活、可解释的修复指导。

Method: 利用基础模型将视频帧的退化感知语义上下文嵌入自然语言，学习近似知识以在推理时无需额外成本。

Result: 在多个基准测试中（包括新提出的数据集）均取得最优性能。

Conclusion: 该方法无需退化先验知识，提供灵活、可解释的修复，并呼吁标准化全能视频修复的基准测试。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [202] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于DETR的目标检测增强框架，通过建模边界框为多元高斯分布并引入Gromov-Wasserstein距离，提高了定位精度和预测不确定性建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统检测器依赖确定性边界框回归，忽略了预测中的不确定性，限制了模型的鲁棒性。

Method: 提出不确定性感知框架，将边界框建模为多元高斯分布，引入Gromov-Wasserstein距离损失函数，并基于贝叶斯风险过滤高风险信息。

Result: 在COCO基准测试中有效提升性能，并在白细胞检测任务中取得SOTA结果。

Conclusion: 该框架在通用和特定领域检测任务中均具有可扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [203] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于微手势的情感识别方法，通过超图增强的Transformer在混合监督框架中重建行为模式，并在公开数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 微手势作为无意识的身体动作，能够传达人类情感状态，但在情感建模方面尚未充分探索。

Method: 采用超图增强的Transformer编码器和解码器，结合自监督和监督学习，设计了多尺度时间卷积模块和上采样操作。

Result: 在iMiGUE和SMG数据集上，该方法在多项指标上优于现有方法。

Conclusion: 该方法通过超图增强的Transformer有效捕捉微手势的细微运动，提升了情感识别的性能。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [204] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出一种无需学习的方法，利用稀疏深度测量将基础模型的相对尺度深度预测转换为度量尺度深度，避免了额外的训练或微调。


<details>
  <summary>Details</summary>
Motivation: 基础模型的深度预测通常为相对尺度，限制了实际应用；现有尺度适应方法成本高且可能损害模型泛化能力。

Method: 采用非学习的方法，利用稀疏深度测量将相对尺度深度转换为度量尺度深度，无需重新训练或微调。

Result: 实验证明该方法有效，能在不增加计算成本或牺牲泛化能力的情况下实现度量尺度深度预测。

Conclusion: 该方法成功解决了相对尺度到度量尺度的转换问题，具有实际应用潜力。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [205] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer是一种轻量级光谱注意力模型，结合了手工方法和深度学习的优势，用于远程光电容积描记（rPPG）估计，无需PPG或HR标签即可训练。


<details>
  <summary>Details</summary>
Motivation: 当前rPPG估计方法中，手工方法泛化能力强但性能受限，深度学习方法性能优越但依赖大数据集，因此需要结合两者优势。

Method: 提出BeatFormer模型，结合了放大的正交复数注意力和频域能量测量，并引入光谱对比学习（SCL）实现无监督训练。

Result: 在PURE、UBFC-rPPG和MMPD数据集上验证了BeatFormer的鲁棒性和性能，尤其在运动场景下的跨数据集评估中表现突出。

Conclusion: BeatFormer通过结合手工方法和深度学习的优势，实现了高效且鲁棒的rPPG估计，为无监督学习提供了新思路。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [206] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出了一种统一的2D预训练多模态网络（GARF），用于处理RGB图像、文本和点云数据，简化了模型架构并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖多模态分离编码器，导致模型复杂且训练效率低。

Method: 利用2D CLIP双模态模型，通过适配器微调适应三模态设置，并设计GARF模块融合几何特征。

Result: 模型参数量减少58%，3D检测任务性能提升6.52%，3D视觉定位任务提升6.25%。

Conclusion: 统一的多模态特征提取与融合方法显著简化了模型并提升了性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [207] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 提出了一种语义感知表示学习（SARL）方法，用于多标签图像分类，通过语义相关特征学习和最优传输注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如注意力机制或图卷积网络）在图像表示中可能包含噪声且定位不精确，需改进。

Method: 1. 语义相关特征学习模块提取特征；2. 最优传输注意力机制对齐语义；3. 区域分数聚合策略预测多标签。

Result: 在PASCAL VOC 2007和MS-COCO数据集上表现优于现有方法。

Conclusion: SARL通过语义对齐和区域聚合有效提升了多标签分类的准确性。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [208] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 提出了一种解耦框架\method，用于高效预测3D高斯，通过局部图像对特征提取和全局注意力融合，生成几何和外观特征，实现无姿态的高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯几何和外观预测上耦合性强，依赖数据驱动先验且回归速度慢，需解决计算资源和大数据集需求问题。

Method: 使用立体视觉骨干网络提取局部图像对特征，通过全局注意力块融合，生成多视点点图和外观高斯特征，结合为GS图表示3DGS对象，并通过细化网络提升重建质量。

Result: 实现了无姿态的3D重建，提高了鲁棒性和实用性，同时减少了资源需求并保持高质量输出。

Conclusion: \method为实际3D内容生成提供了高效、可扩展的解决方案。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [209] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 提出了一种基于多维度缩放（MDS）和鲁棒优化的方法，用于解决冷冻电镜（cryo-EM）中低信噪比（SNR）下的姿态估计和位移校正问题。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜中的低信噪比导致姿态估计和位移校正困难，直接影响3D重建的准确性。

Method: 使用MDS技术估计旋转矩阵，并通过鲁棒优化框架（如ℓ1范数）和迭代位移校正算法，精确执行单位范数和正交约束。

Result: 该方法在欧拉角精度和重建保真度（通过FSC测量）上优于现有方法。

Conclusion: 提出的鲁棒方法在低信噪比环境下显著提升了姿态估计和3D重建的准确性。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [210] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种新的概率框架，用于估计注意力值的分布，并在医学影像分类中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度MIL方法通常确定性处理注意力值，可能忽略了实例贡献的不确定性。

Method: 提出了一种概率框架，估计注意力值的分布，并考虑全局和局部交互。

Result: 在三个医学数据集和11个基线方法中，该方法在预测性能上表现最佳，并提供可解释的不确定性图。

Conclusion: 该概率框架不仅提升了分类性能，还提供了可解释的注意力不确定性图。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [211] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 论文提出了开放集跨模态泛化（OSCMG）任务，扩展了跨模态泛化（CMG）到开放集环境，并提出了MICU方法，包含FCMI和CUJP两个组件，以解决现有方法在开放集环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态统一表示方法未考虑开放集环境，而实际应用中常需处理未见类别和新模态。

Method: 提出MICU方法，包含FCMI（细粒度与粗粒度掩码对比学习）和CUJP（跨模态统一拼图任务），以增强多模态对齐和特征多样性。

Result: 在CMG和新提出的OSCMG任务上验证了方法的有效性。

Conclusion: MICU方法在开放集跨模态泛化任务中表现优异，解决了现有方法的局限性。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [212] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一个实时多标签视频分类框架，通过动态激活轻量级适配器（LoRA）来优化嵌入式设备的计算和能耗效率。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备的计算和能源预算有限，而视频流具有标签稀疏性、时间连续性和标签共现性等结构特性，可用于高效推理。

Method: Polymorph框架动态选择和组合轻量级LoRA适配器，每个适配器专注于从共现模式中提取的类子集，避免全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph能耗降低40%，mAP提高9个百分点。

Conclusion: Polymorph通过模块化策略显著提升了嵌入式设备上多标签视频分类的效率和性能。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [213] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的低重叠点云配准（PCR）评估方法，解决了传统指标在极低内点比例下失效的问题，显著提升了现有配准方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如最大内点计数）在极低内点比例下失效，因此需要一种更可靠的方法来评估配准质量。

Method: 构建基于3DMatch数据集的数据集，训练深度学习分类器评估配准质量，并将其集成到标准PCR流程中。

Result: 结合GeoTransformer等方法，在3DLoMatch基准上达到86.97%的配准召回率，并在ETH数据集上表现出强泛化能力。

Conclusion: 提出的数据驱动方法有效解决了低重叠PCR的评估问题，显著提升了配准性能。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [214] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL是一种分层跨模态提示学习框架，通过双向知识流解决模态隔离和语义衰减问题，提升预训练视觉语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在下游任务中保持泛化能力仍具挑战性，现有提示学习方法存在模态隔离和分层语义衰减的瓶颈。

Method: 提出HiCroPL框架，通过分层知识映射器和轻量级知识代理实现文本与视觉模态的双向知识流，增强语义对齐。

Result: 在四个任务和11个基准测试中取得最优性能，显著提升泛化能力。

Conclusion: HiCroPL通过分层跨模态交互有效解决了现有方法的局限性，为视觉语言模型的适应性和泛化提供了新思路。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [215] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 论文提出RvTC方法，通过灵活的基于分箱的分类替代预设词汇分类，显著提升多模态大语言模型在图像回归任务中的性能，并证明数据特定提示的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在图像回归任务中表现不佳，预设词汇和通用提示未能利用文本输入的语义理解。

Method: 提出Regression via Transformer-Based Classification (RvTC)，采用基于分箱的灵活分类方法，避免手动词汇设计，并通过数据特定提示提升性能。

Result: RvTC在四个图像评估数据集上达到最优性能，数据特定提示将AVA数据集的相关系数从0.83提升至0.90。

Conclusion: 语义提示信息对多模态回归任务至关重要，RvTC方法为图像回归任务提供了更优解决方案。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [216] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于轴对齐几何约束的文档去扭曲方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法依赖标注数据而未利用文档的几何特性。

Method: 训练阶段引入轴对齐几何约束，推理阶段采用轴对齐预处理策略。

Result: 在多个基准测试中达到SOTA，AAD指标提升18.2%~34.5%。

Conclusion: 该方法通过几何约束和预处理策略，显著提升了文档去扭曲的效果。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [217] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于B样条曲线拟合的新方法，用于改进FastSAM中的锯齿边缘问题，提升分割精度并保持实时处理能力。


<details>
  <summary>Details</summary>
Motivation: FastSAM虽然实现了实时分割，但其生成的边缘存在锯齿问题，影响了分割的视觉质量和分析准确性。

Method: 采用B样条曲线拟合技术，通过四阶段细化过程（包括两轮曲线拟合）来平滑锯齿边缘。

Result: 显著提升了边缘的视觉质量和分析准确性，同时保持了实时处理能力。

Conclusion: 该方法增强了FastSAM的实用性，使其在工业自动化、医疗影像和自主系统等领域更具潜力。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [218] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了Video Thinking Test（Video-TT）来评估视频大语言模型在真实视频理解中的正确性和鲁棒性，发现其与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在正确性和鲁棒性方面与人类智能存在差距，缺乏合适的评估基准。

Method: 引入Video-TT，包含1,000个YouTube Shorts视频，每个视频配有一个开放性问题及四个对抗性问题，用于评估视觉和叙事复杂性。

Result: 评估结果显示视频大语言模型与人类表现之间存在显著差距。

Conclusion: Video-TT揭示了视频大语言模型在复杂视觉叙事理解上的不足，为未来研究提供了方向。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [219] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波动方程数据集，旨在解决传统数值求解器计算量大且不稳定的问题，通过提供8000个解剖学真实的人体乳腺模型和1600万次频域波模拟，支持神经算子在实际医学成像中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统波动方程求解器计算量大且不稳定，限制了其在准实时成像中的应用。神经算子虽能加速求解，但现有数据集过于简化，无法满足实际成像需求。

Method: 提出OpenBreastUS数据集，包含8000个真实乳腺模型和1600万次频域波模拟，用于评估神经算子在正向模拟和逆成像任务中的性能。

Result: 首次展示了神经算子求解器在人体乳腺活体成像中的高效应用。

Conclusion: OpenBreastUS为开发创新神经PDE求解器提供了平台，并促进了其在现实医学成像中的部署。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [220] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI框架通过CLIP嵌入检测和减轻数据集偏差，结合自适应处理优化能效，在保持图像增强质量的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决水下图像增强中AI模型的数据集偏差、高计算成本和透明度不足问题，以支持可持续的海洋保护。

Method: 利用CLIP嵌入检测和减轻数据集偏差，集成自适应处理优化能效，并引入不确定性估计和可解释性技术。

Result: 实验显示PSNR仅下降1.0 dB，但计算成本显著降低，支持实时大规模海洋监测。

Conclusion: EBA-AI在效率、公平性和可解释性方面优于现有方法，为可持续海洋保护提供了有效工具。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [221] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON是一种无需训练的通用虚拟试穿框架，通过解耦服装和姿态条件，实现跨场景的高保真纹理和姿态一致性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术要么依赖监督式店内方法（泛化性差），要么依赖无监督野外方法（数据偏差大），缺乏统一的解决方案。

Method: 采用服装先验生成机制和连续边界缝合技术保留细节，利用DDIM反演实现姿态对齐，解耦服装与姿态约束。

Result: 实验表明OmniVTON在多样化数据集和场景中表现优异，首次实现多人物虚拟试穿。

Conclusion: OmniVTON通过解耦条件，解决了扩散模型的多条件处理偏差，成为首个跨场景通用框架。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [222] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 提出PanTiny，一种轻量级单步全色锐化框架，通过多数据集联合训练和复合损失函数，实现高效且泛化性强的性能。


<details>
  <summary>Details</summary>
Motivation: 当前全色锐化模型趋向于大而复杂，计算开销高且泛化性差，PanTiny旨在解决这一问题。

Method: 采用多数据集联合训练策略（WV2、WV3、GF2），设计复合损失函数，构建轻量级单步框架。

Result: PanTiny在性能和效率上优于大型专用模型，泛化能力显著提升。

Conclusion: 通过模型设计、训练范式和损失函数的优化，PanTiny证明了高效、泛化性强的全色锐化模型的可行性。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [223] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一种基于视频扩散模型的框架，通过可学习的姿态对齐和身份保持技术，解决了现有方法在身份一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的人体图像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性。

Method: StableAnimator++ 结合了可学习的姿态对齐模块、全局内容感知的面部编码器和分布感知的身份适配器，并通过 HJB 优化提升面部保真度。

Result: 实验证明 StableAnimator++ 在质量和数量上均优于现有方法。

Conclusion: StableAnimator++ 通过创新的模块设计和优化策略，显著提升了身份一致性和生成质量。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [224] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 评估当前最先进的生成模型在文本图像生成和编辑中的能力，发现其弱点，并建议将逼真文本图像生成作为通用模型的必备技能。


<details>
  <summary>Details</summary>
Motivation: 探讨生成模型是否能掌握文本图像生成和编辑的复杂性，并评估其在OCR任务中的表现。

Method: 选择33个代表性任务，分为五类，评估六种模型在封闭和开源领域的表现。

Result: 识别了当前生成模型在OCR任务中的弱点，并提出了改进方向。

Conclusion: 建议将逼真文本图像生成作为通用模型的基础技能，而非依赖专用解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [225] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 论文提出了LASED数据集和可转向CNN，用于解决无人机视觉地点识别中的数据集不足和旋转模糊问题。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉地点识别面临数据集规模小和旋转模糊的挑战，限制了模型的泛化能力。

Method: 引入LASED大规模数据集，并提出使用可转向CNN处理旋转模糊。

Result: LASED训练的模型召回率显著提升，可转向CNN比传统CNN平均提高12%的召回率。

Conclusion: 结合大规模数据集和旋转等变神经网络，显著提升了无人机视觉地点识别的鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [226] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 该论文提出了首个ESD出血源数据集BleedOrigin-Bench和双阶段检测-跟踪框架BleedOrigin-Net，用于实时定位和跟踪出血源，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法主要关注出血区域分割，忽视了出血源的精确检测和动态跟踪，且缺乏专门数据集，阻碍了AI辅助系统的开发。

Method: 引入BleedOrigin-Bench数据集和BleedOrigin-Net框架，结合检测与跟踪技术，从出血开始检测到持续空间跟踪。

Result: 在出血开始检测、初始源检测和点跟踪方面分别达到96.85%、70.24%和96.11%的准确率。

Conclusion: BleedOrigin-Net在ESD出血源定位中表现出色，填补了现有技术的空白，为临床提供了有效工具。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [227] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet是一种基于多任务ResNet变体的方法，用于解决SLAM中的闭环检测问题，通过在线学习和DISK描述符提升性能，并提供了新的数据集LoopDB。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中闭环检测的准确性和实时计算约束问题。

Method: 采用多任务ResNet变体，结合在线学习和DISK描述符，优化嵌入式设备性能。

Result: LoopNet在多变条件下表现优于传统方法和手工特征。

Conclusion: LoopNet通过创新架构和数据集，显著提升了SLAM闭环检测的性能。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [228] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VPA研究通过多模态大语言模型预测用户行为序列，提出辅助任务增强和多令牌预测方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决长时视觉规划中数据稀缺和结构化动作空间建模的挑战。

Method: 采用辅助任务增强和多令牌预测技术优化模型。

Result: 在COIN和CrossTask数据集上性能提升7.3%和3.4%，在Ego4D任务中表现优异。

Conclusion: VideoPlan方法在视觉规划任务中达到最先进水平，无需专用特征。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [229] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空多图表示方法，通过解耦的空间图和时间图优化事件数据的建模，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器的稀疏异步数据在转换为密集张量时会丧失其优势，现有图方法对时空动态建模不足，限制了性能。

Method: 构建解耦的空间图（B样条基函数建模全局结构）和时间图（运动向量注意力建模局部动态），替代计算昂贵的3D核。

Result: 在Gen1和eTraM数据集上，检测精度提升6%，速度提升5倍，参数减少且计算成本不变。

Conclusion: 结构化图建模有效提升了异步视觉任务的性能。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [230] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba是一种基于Mamba-SSMs的神经网络模型，用于高效学习和生成3D关节网格模型，支持超过10,000个顶点的处理。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理大规模3D网格数据时的效率和扩展性问题，特别是在生成和重建包含衣物和手部几何的复杂人体网格时。

Method: 通过将网格顶点序列化为适合Mamba处理的顺序（基于身体部位注释或模板网格的3D顶点位置），并设计了MambaDiff3D（生成模型）和Mamba-HMR（重建模型）。

Result: MambaDiff3D在生成3D人体网格时表现优于现有方法；Mamba-HMR扩展了非参数化人体网格恢复的能力，支持全身（包括面部和手部）重建，且性能接近实时。

Conclusion: MeshMamba及其衍生模型在3D人体网格生成和重建任务中表现出高效性和扩展性，为复杂几何处理提供了新思路。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [231] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 论文提出N-JEPA方法，将扩散噪声引入掩码图像建模（MIM），以增强自监督学习（SSL）的表征能力，并在下游分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在判别任务中表现优异，但生成模型在图像生成和细节增强方面更优。因此，结合SSL与生成模型的核心思想（如扩散噪声）有望提升SSL的表征能力。

Method: 提出N-JEPA方法，通过掩码标记的位置嵌入将扩散噪声引入MIM，并采用多级噪声调度作为特征增强手段。

Result: 在下游分类任务中验证了N-JEPA的有效性。

Conclusion: 结合扩散噪声与SSL的N-JEPA方法能够增强模型的鲁棒性和表征能力，为自监督学习提供了新的思路。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [232] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种基于分层的3D血管生成框架，通过分离全局拓扑和局部几何细节，显著提升了复杂血管网络的建模效果。


<details>
  <summary>Details</summary>
Motivation: 准确表示血管的复杂几何和拓扑结构是医学应用中的一大挑战，现有方法难以处理其分支模式和形状。

Method: 采用三阶段方法：生成关键图建模全局结构，基于几何属性生成血管段，最后整合局部段到全局图中。

Result: 在真实数据集上验证，性能优于现有方法，首次成功应用基于部分的生成方法于3D血管建模。

Conclusion: 该框架为血管数据生成设定了新基准，代码已开源。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [233] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的可解释性方法，用于分析乳腺影像中的基础模型Mammo-CLIP，揭示了模型决策的潜在特征和影响因素。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医学影像）中，模型决策的可解释性对临床采用至关重要。本研究旨在通过SAE方法深入理解乳腺影像基础模型的内部机制。

Method: 使用Mammo-CLIP预训练的视觉-语言基础模型，训练了一个补丁级的Mammo-SAE，以识别和探测与临床相关乳腺概念（如肿块和可疑钙化）相关的潜在特征。

Result: 研究发现，SAE潜在空间中激活的神经元通常与真实区域对齐，并揭示了影响模型决策的混杂因素。此外，还分析了模型在下游微调中依赖的潜在神经元。

Conclusion: 研究表明，可解释的SAE潜在表征为理解乳腺影像基础模型的内部机制提供了更深入的见解。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [234] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 论文提出了一种名为Coalescent Projection（CP）的新方法，结合伪类生成和自监督变换（SSTs），在极端领域偏移场景下显著提升了跨域少样本学习（CD-FSL）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CD-FSL方法在更新过多Transformer参数时容易因标记样本稀缺而过拟合，需要一种更有效的方法来应对这一挑战。

Method: 提出了Coalescent Projection（CP）作为软提示的有效替代，并结合自监督变换（SSTs）生成伪类，仅依赖基础领域数据为网络应对未见领域样本做准备。

Result: 在BSCD-FSL基准测试的极端领域偏移场景中，该方法表现优异，超越了现有SOTA方法。

Conclusion: CP与SSTs的结合为CD-FSL提供了一种高效且鲁棒的解决方案，尤其在领域偏移显著的情况下。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [235] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus是一种无需训练的框架，通过创新的注意力共享机制和改进的DiT变体，激活扩散变换器的零样本能力，实现高质量的主题驱动合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练过程，限制了实际应用，且未能充分利用扩散变换器的零样本潜力。

Method: 提出三种创新：注意力共享机制、改进的DiT变体和多模态大语言模型集成。

Result: 实验表明，FreeCus在多样场景中实现一致的主题合成，性能与需训练的方法相当。

Conclusion: FreeCus展示了与现有工具的兼容性，为设计工作流和娱乐提供了新可能。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [236] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 提出了一种基于近似盲PnP的对应学习方法MinCD-PnP，通过最小化2D和3D关键点之间的Chamfer距离，解决了传统PnP对噪声和异常值敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统差分PnP在图像到点云（I2P）配准中对噪声和异常值敏感，限制了对应学习的有效性。

Method: 提出MinCD-PnP方法，简化盲PnP为最小化Chamfer距离的任务，并设计轻量级多任务学习模块MinCD-Net。

Result: 在多个数据集上实验表明，MinCD-Net在跨场景和跨数据集设置中均优于现有方法，提高了内点比率（IR）和配准召回率（RR）。

Conclusion: MinCD-PnP和MinCD-Net有效提升了I2P配准的鲁棒性和性能。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [237] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过生成模型从稀疏但信息丰富的信号中重建视频，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐重建中的优势，优化视频压缩的感知质量。

Method: 提出三个关键模块：多粒度条件捕捉静态和动态信息、紧凑表示设计、多条件训练增强鲁棒性。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器，尤其在高压缩比下表现突出。

Conclusion: 条件扩散模型为视频压缩提供了感知优化的有效途径。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [238] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的上下文学习框架，用于检测生物识别系统中的物理呈现攻击和数字变形攻击，其性能优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 随着生物识别系统检测技术的进步，攻击手段也日益复杂，传统深度学习模型在适应新型攻击或环境变化时表现不佳，且需要大量训练数据。

Method: 采用视觉语言模型（VLM）和上下文学习技术，建立了一个系统化的定量评估框架，用于安全关键场景中的攻击检测。

Result: 实验表明，该框架在物理和数字攻击检测中表现优异，无需资源密集型训练即可超越部分传统CNN方法。

Conclusion: 该框架为提升攻击检测的泛化能力提供了有效工具，展现了在生物识别安全领域的潜力。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [239] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为DMD的指纹匹配方法，通过局部密集表示结合细粒度纹理和细节特征，提升多样捕获条件下的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 指纹匹配在不同捕获条件下存在挑战，需要一种鲁棒且准确的方法。

Method: 提取以细节为中心的局部块特征，形成三维张量，结合空间结构和语义特征，利用前景分割提高匹配效率。

Result: 在多种指纹数据集上验证了方法的有效性和泛化能力，达到了最先进的准确性和计算效率。

Conclusion: DMD方法在指纹识别中表现出色，具有大规模应用的潜力。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [240] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 论文提出了一种用于少样本目标检测（FSOD）的Spatial-Channel State Space Modeling（SCSM）模块，通过建模通道间相关性来提升特征提取效果。


<details>
  <summary>Details</summary>
Motivation: 当前方法在少样本目标检测中难以准确提取有效特征，高权重通道未必有效，低权重通道可能仍有价值。

Method: 设计了SCSM模块，包括Spatial Feature Modeling（SFM）模块平衡空间和通道关系学习，以及基于Mamba的Channel State Modeling（CSM）模块建模通道相关性。

Result: 在VOC和COCO数据集上的实验表明，SCSM模块提升了特征表示质量，并达到最先进性能。

Conclusion: SCSM模块通过建模通道相关性，有效解决了少样本目标检测中的特征提取问题。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [241] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出了BenchDepth，一种新的深度基础模型（DFMs）评估基准，通过五个下游代理任务评估模型的实际应用效果，避免了传统对齐指标的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计模型的评估协议存在不一致性，传统基准依赖对齐指标，导致偏见和不公平比较。

Method: 设计BenchDepth基准，通过深度补全、立体匹配、单目3D场景重建、SLAM和视觉语言空间理解五个任务评估DFMs。

Result: 对八种最先进的DFMs进行了基准测试，并提供了关键发现和分析。

Conclusion: BenchDepth为深度模型评估提供了更公平和实用的方法，有望推动深度估计领域的进一步研究和讨论。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [242] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过显式建模双特征分布，解决了工业缺陷检测中单类异常检测的局限性，利用并行记忆库和潜在扩散模型生成合成缺陷数据，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测中单类异常检测假设异常分布均匀且数据稀缺，导致性能受限。

Method: 提出ExDD框架，显式建模双特征分布，使用并行记忆库和潜在扩散模型生成合成缺陷数据，结合邻域感知评分机制。

Result: 在KSDD2数据集上表现优异（I-AUROC 94.2%，P-AUROC 97.7%），最佳合成样本数为100。

Conclusion: ExDD框架有效解决了工业缺陷检测中的关键问题，显著提升了检测性能。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [243] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion框架通过合成异常生成和双路径特征适应，解决了路面缺陷检测中的数据稀缺、域偏移和缺陷多样性问题，并在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临标注数据稀缺、训练与部署环境的域偏移以及不同道路条件下缺陷外观的高变异性等挑战。

Method: 提出RoadFusion框架，利用潜在扩散模型生成多样化的合成缺陷，并通过双路径特征适应器分别处理正常和异常输入，结合轻量级判别器进行细粒度缺陷分类。

Result: 在六个基准数据集上，RoadFusion在分类和定位任务中均表现优异，多项指标达到最新技术水平。

Conclusion: RoadFusion通过合成数据和特征适应技术，显著提升了路面缺陷检测的鲁棒性和性能，适用于实际道路检查场景。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [244] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 论文提出了一种使用高保真合成数据集训练高效模型的方法，无需牺牲准确性，同时解决了数据多样性、来源和使用权问题。


<details>
  <summary>Details</summary>
Motivation: 当前人类中心计算机视觉模型需要大量参数、数据集和计算资源，合成数据可以降低成本并提高效率。

Method: 通过程序化合成高保真数据集，提供完美标签和多样性控制，训练高效模型。

Result: 在深度估计、表面法线估计和前景分割任务中，模型表现与大型模型相当，但成本显著降低。

Conclusion: 合成数据训练模型是一种高效、可控且经济的方法，适用于人类中心计算机视觉任务。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [245] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet提出了一种针对遮挡条件下面部表情识别（FER）的新方法，通过多模态语义引导、多尺度交互模块和动态对抗排斥损失，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有FER模型在面部部分遮挡时难以提取有效特征，导致分类不准确。

Method: 引入多模态语义引导（语义分割图和面部关键点）、多尺度交互模块（MCM）和动态对抗排斥损失（DARELoss）。

Result: 在公开基准和自建数据集Occlu-FER上达到SOTA性能。

Conclusion: ORSANet通过多模态融合和动态损失设计，有效解决了遮挡条件下的FER问题。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [246] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX是一个基于概念的解释框架，旨在提高手术阶段识别模型的可解释性，通过将神经元与相关概念关联。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在手术阶段识别中取得了进展，但其不透明性导致难以理解和调试，影响了信任。

Method: 提出SurgX框架，包括选择代表性神经元序列、构建手术视频数据集的概念集、关联神经元与概念，并识别关键神经元。

Result: 在两个手术阶段识别模型上的实验验证了方法的有效性，并分析了预测解释。

Conclusion: SurgX展示了在解释手术阶段识别方面的潜力，代码已开源。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [247] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune是一种无需训练的令牌修剪方法，专为自我运动视频推理设计，通过关键帧选择、视角感知冗余过滤和MMR令牌选择器提高效率。


<details>
  <summary>Details</summary>
Motivation: 自我运动视频推理对嵌入式AI代理至关重要，但现有方法计算成本高且不适合自我运动场景。

Method: EgoPrune包括关键帧选择、视角感知冗余过滤和MMR令牌选择器。

Result: 在两种基准测试中，EgoPrune优于现有方法，显著降低计算资源消耗。

Conclusion: EgoPrune在边缘设备上展示了实际高效性，适用于实时自我运动视频推理。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [248] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda是一种简单有效的视觉语言模型（VLM）微调方法，通过动态校准融合表示（rational matrix）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注单模态表示优化，忽视了融合表示在决策中的关键作用。

Method: RAda通过轻量级注意力层学习掩码，动态调整融合表示中各元素的贡献。

Result: 在不同设置下，RAda均能提升基线性能，且与现有方法表现相当。

Conclusion: RAda是一种通用且高效的微调技术，适用于多种场景。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [249] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 论文探讨了在森林环境中通过众包标注高分辨率航拍图像来改进异常检测方法，并提供了一个公开数据集和交互式网络界面。


<details>
  <summary>Details</summary>
Motivation: 在德国一起农村谋杀案中，传统搜索方法因植被遮挡而失效，需要更有效的异常检测技术来支持搜救行动。

Method: 通过研究飞机获取高分辨率航拍图像，利用众包标注生成异常检测数据集，并开发交互式网络界面支持动态标注。

Result: 现有异常检测方法在复杂森林环境中表现不佳，数据集可作为改进算法的基准。

Conclusion: 该研究强调了上下文感知方法的重要性，并提供了公开数据集和工具以支持未来研究和实际应用。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [250] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种新颖的LiDAR-Visual里程计框架，结合LiDAR点云和图像，通过深度补全和多尺度特征提取网络实现高精度和鲁棒的位姿估计。


<details>
  <summary>Details</summary>
Motivation: 解决自主系统中自定位和导航的关键问题，通过融合LiDAR和视觉数据提升里程计的准确性和鲁棒性。

Method: 利用深度补全生成稠密深度图，结合多尺度特征提取网络和注意力机制，并通过稠密深度信息优化光流估计和遮挡区域处理。

Result: 在KITTI里程计基准测试中表现优异，准确性和鲁棒性达到或超过现有视觉和LiDAR里程计方法。

Conclusion: 提出的LiDAR-Visual融合框架在动态环境和尺度模糊情况下表现稳健，为自主系统提供了可靠的位姿估计解决方案。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [251] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR是一个基于不确定性最小化的交互式文本到视频检索框架，通过量化文本模糊性、映射不确定性和帧不确定性，生成针对性问题以优化查询效果。


<details>
  <summary>Details</summary>
Motivation: 当前交互式文本到视频检索系统依赖启发式策略，未能明确量化不确定性，限制了检索效果。

Method: UMIVR提出三种训练无关的指标：基于语义熵的文本模糊性评分（TAS）、基于Jensen-Shannon散度的映射不确定性评分（MUS）和基于时序质量的帧采样器（TQFS）。

Result: 在MSR-VTT-1k数据集上，UMIVR在10轮交互后Recall@1达到69.2%。

Conclusion: UMIVR为交互式文本到视频检索建立了不确定性最小化的基础，显著提升了检索性能。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [252] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer是一种基于Transformer的低光增强方法，通过动态积分图像表示和光照引导的多头自注意力机制，解决了非均匀光照场景下的过曝和亮度恢复不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在非均匀光照（如背光和阴影）下表现不佳，导致过曝或亮度恢复不足。

Method: 提出动态积分图像表示建模空间变化光照，构建SAI²E估计器，并引入光照引导的多头自注意力机制（IG-MSA）。

Result: 在五个标准低光数据集和跨域基准（LOL-Blur）上，SAIGFormer在定量和定性指标上显著优于现有方法。

Conclusion: SAIGFormer在非均匀光照增强中表现优异，并展现出强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [253] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 提出了一种自监督程序学习框架，通过融合Gromov-Wasserstein最优传输和结构先验解决视频关键步骤发现与排序问题，并引入对比正则化避免退化解。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频关键步骤学习时受顺序变化、冗余帧和重复动作影响，性能不佳。

Method: 结合Gromov-Wasserstein最优传输与结构先验进行帧间映射，并加入对比正则化防止退化。

Result: 在EgoProceL、ProceL和CrossTask等基准测试中表现优于现有方法。

Conclusion: 所提框架有效解决了自监督程序学习中的关键挑战，性能显著提升。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [254] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 论文提出了一种基于图的方法SSG-Com和数据集Endoscapes-SG201，用于更好地表示手术场景中的工具-动作-目标组合和操作手身份，提升了手术场景理解的效果。


<details>
  <summary>Details</summary>
Motivation: 手术场景理解对计算机辅助干预系统至关重要，但现有图表示方法未充分探索工具-动作-目标组合和操作手身份等关键元素。

Method: 提出SSG-Com方法和Endoscapes-SG201数据集，标注了工具-动作-目标组合和操作手身份，并通过实验验证其有效性。

Result: 实验表明，整合这些关键元素显著提升了手术场景理解，特别是在关键安全视图评估和动作三元组识别任务中。

Conclusion: 研究强调了工具-动作-目标组合和操作手身份在图表示中的重要性，为手术场景理解提供了新方法。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [255] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa提出了一种零样本人-物交互检测方法，通过低秩分解和特征适应提升对未见类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在区分相同对象的不同动作或泛化到未见类别时的局限性。

Method: 利用低秩分解VLM文本特征，生成类共享基特征和可调权重，并通过LLM引导的权重适应增强动作区分。

Result: 在HICO-DET上达到新SOTA，未见动词设置下的mAP为27.91。

Conclusion: HOLa通过低秩分解和特征适应显著提升了零样本HOI检测的性能。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [256] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Dynamic-Image (DynImg)的视频表示方法，通过引入非关键帧作为时间提示，提升对快速移动物体的空间特征提取，从而改善视频理解任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法将空间和时间信息分开处理，导致快速移动物体的空间信息难以准确表示，影响时空交互和视频理解。

Method: 引入非关键帧作为时间提示，结合4D视频旋转位置嵌入，保留时空邻接性，指导模型关注细粒度空间特征。

Result: 在多个视频理解基准测试中，DynImg比现有方法提升了约2%。

Conclusion: DynImg通过时间提示有效提升了视频理解的准确性，证明了其在时空信息整合中的优势。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [257] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一种基于条件GAN的两阶段图像增强框架，通过生成语义一致的图像改进传统Mixup在医学图像分类中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统Mixup的像素级插值在医学图像中可能生成不真实图像，影响学习效果，尤其是在高风险的医疗应用中。

Method: 使用StyleGAN2-ADA生成器，通过Dirichlet和Beta分布生成软标签，合成连续类别流形上的图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据显著提高了分类性能，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种可直接替代传统Mixup的方法，提供更强的正则化和语义保真度，且不干扰现有训练流程。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [258] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 提出一种卫星上实时变化检测框架，通过端到端深度神经网络解决数据存储、图像配准和变化检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像变化检测因数据传输和处理延迟无法满足实时需求，需将整个工作流程移至卫星上。

Method: 采用包含三个子模块的深度神经网络：图像压缩、轻量级配准和高效变化检测模型。

Result: 实验表明，系统在低功耗硬件上表现优异，F1分数随压缩率变化稳定，处理速度为0.7 Mpixel/s。

Conclusion: 该框架首次实现卫星上端到端变化检测，为实时应用提供可行方案。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [259] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT是一种基于扩散变换器（DiT）的新型分割模型，用于皮肤病变分割，适用于低成本硬件，并引入Rectified Flow以提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变分割对皮肤癌诊断和患者监测至关重要，但现有方法在性能和效率上仍有改进空间。

Method: SegDT结合扩散变换器和Rectified Flow，优化生成质量并减少推理步骤，同时保持灵活性。

Result: 在三个基准数据集上测试，SegDT达到最先进性能，且推理速度快。

Conclusion: SegDT为医学图像分析提供了高效、准确的工具，适合实际医疗应用。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [260] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一个基于人类视频训练的灵巧视觉-语言-动作模型（VLA），通过物理指令调优和部分级运动标记化方法，解决了现有VLA在复杂操作任务中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA依赖合成数据或有限的操作演示，难以处理高灵巧性任务和泛化到新场景。人类手部操作数据具有丰富性和可扩展性，为解决这一问题提供了基础。

Method: 结合大规模VLA预训练、物理空间对齐和机器人任务后适应，提出物理指令调优范式；引入部分级运动标记化方法，实现毫米级重建精度。

Result: Being-H0在手部运动生成和指令跟随方面表现优异，且能随模型和数据规模扩展；在真实机器人操作任务中取得预期效果。

Conclusion: Being-H0通过利用人类视频数据和物理指令调优，显著提升了VLA在复杂操作任务中的表现和泛化能力。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [261] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种结合SDF和3DGS的混合方法，用于稀疏视图图像下的表面重建和新视角渲染，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决SDF方法在细节捕捉和3DGS方法在全局几何一致性上的不足。

Method: 结合SDF的粗几何捕捉能力和3DGS的渲染能力，相互优化细节和全局一致性。

Result: 在DTU和MobileBrick数据集上，表面重建和新视角渲染效果优于现有方法。

Conclusion: 混合方法有效结合了SDF和3DGS的优势，提升了稀疏视图下的重建和渲染性能。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [262] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于圆柱坐标系的隐式表示方法CylinderPlane，解决了Tri-plane表示中的多视角一致性问题，实现了高质量、无伪影的360°图像合成。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示在3D感知图像生成中存在多视角特征模糊问题，限制了360°视图图像的生成能力。

Method: 提出CylinderPlane，基于圆柱坐标系分离不同角度的特征，并引入嵌套圆柱表示以处理复杂几何和多分辨率。

Result: 实验表明，该方法在合成数据集和真实图像上均优于现有方法。

Conclusion: CylinderPlane是一种高效且通用的隐式表示方法，适用于神经渲染流程。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [263] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了深度学习在视频分析中的效率优化技术，从硬件支持、数据处理等多角度系统梳理现有方法，并讨论挑战与问题。


<details>
  <summary>Details</summary>
Motivation: 视频数据的爆炸式增长对视频分析的准确性和效率提出了更高要求，现有研究多关注准确性优化，本文聚焦于深度学习在视频分析中的效率提升。

Method: 采用自下而上的方式组织现有方法，涵盖硬件支持、数据处理、操作部署等多视角。

Result: 系统总结了效率优化的技术框架和现有工作。

Conclusion: 分析了深度学习在视频分析中性能优化的问题与挑战。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [264] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 论文研究了主动学习（AL）和顺序学习（SL）在光学音乐识别（OMR）中的应用，通过YOLOv8选择不确定性高的样本进行迭代标注和训练，以减少标注需求。


<details>
  <summary>Details</summary>
Motivation: 解决音乐文化遗产数字化中标注数据稀缺和历史手稿复杂性的问题。

Method: 使用YOLOv8，基于不确定性选择样本进行迭代标注和训练，从单张标注图像开始逐步提升性能。

Result: 实验表明，该方法在显著减少标注样本的情况下，能达到与全监督训练相当的精度。

Conclusion: 在特定手稿中，基于不确定性的AL效果不佳，建议开发更适合数据稀缺场景的方法。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [265] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究探讨了彩票假设（LTH）在深度伪造检测中的应用，通过剪枝神经网络识别关键特征，发现子网络在高度稀疏下仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对信息完整性和社会信任构成挑战，现有检测方法模型庞大且机制不明确，难以在资源有限环境中部署。

Method: 采用彩票假设（LTH）进行迭代幅度剪枝，测试MesoNet、CNN-5和ResNet-18架构在OpenForensic和FaceForensics++数据集上的表现。

Result: MesoNet在80%稀疏度下保持56.2%准确率（基线62.6%），仅需3,000参数；LTH剪枝方法优于一次性剪枝。

Conclusion: LTH剪枝方法能高效识别关键特征，子网络具有跨数据集可迁移性，为可部署的深度伪造检测系统提供潜力。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [266] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为EVA的训练无关方法，通过动态选择中间层提取视觉事实信息，显著减少多模态大语言模型（MLLMs）中的对象幻觉问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs在结合视觉识别和语言理解方面取得进展，但仍存在对象幻觉问题，即模型生成看似合理但实际错误的输出。研究发现先验知识在深层抑制视觉信息，但中间层的作用尚不明确。

Method: 提出EVA方法，动态选择中间层提取视觉事实信息，通过对比原始输入和纯文本输入的输出分布，将视觉知识整合到最终层以修正输出。

Result: EVA在多个基准测试中显著降低了幻觉率，优于基线方法。

Conclusion: EVA是一种模型无关的方法，能有效减少MLLMs中的幻觉问题，且无需训练，兼容多种解码策略。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [267] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个新的多语言手写文档视觉问答基准，包含1600页手写文档和2400个问答对，旨在填补多语言手写文档理解的空白。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言视觉问答模型在处理多样化手写文档时能力不足，缺乏真实的多语言手写文档理解基准。

Method: HW-MLVQA提供了一个包含文本、图像及图文结合模态的评估框架，并测试了专有和开源OCR模型。

Result: 基准支持对多语言手写文档理解的严格评估，推动了该领域的创新和研究。

Conclusion: HW-MLVQA为多语言手写文档理解提供了关键工具，促进了相关技术的进步。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [268] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型知识蒸馏的方法，用于改进图像质量评估（IQA）任务，减少模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在IQA任务中参数过多和局部失真特征识别能力不足的问题。

Method: 设计质量分级提示模板，微调CLIP，并提出模态自适应知识蒸馏策略。

Result: 在多个IQA数据集上表现优于现有方法，同时显著降低模型复杂度。

Conclusion: 该方法展示了实际部署的潜力，为IQA任务提供了高效解决方案。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [269] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯散射（3DGS）的双层次视觉重定位框架Hi²-GSLoc，解决了现有方法在精度和计算复杂度上的权衡问题，适用于大规模遥感场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重定位方法在精度和计算效率之间存在固有折衷，尤其在遥感场景中面临大规模场景、高度变化和领域差距的挑战。

Method: 采用3DGS作为场景表示，提出双层次框架Hi²-GSLoc，包含稀疏阶段（高斯特定采样和地标检测）和稠密阶段（迭代细化匹配）。通过分区训练、GPU加速和动态内存管理优化大规模场景处理。

Result: 在仿真数据、公开数据集和实际飞行实验中，方法表现出竞争性的定位精度、召回率和计算效率，并能有效过滤不可靠位姿估计。

Conclusion: Hi²-GSLoc为遥感应用提供了一种高效且精确的视觉重定位解决方案。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [270] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经表示（INR）的无损点云几何压缩方法LINR-PCGC，解决了现有方法对训练数据分布的依赖问题，并显著提升了编码速度和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的点云压缩方法依赖特定训练数据分布，限制了实际应用；而INR方法虽能解决分布问题，但仅支持有损压缩。本文旨在提出首个基于INR的无损压缩方法。

Method: 设计了点云级编码框架和高效网络初始化策略以减少编码时间；提出基于多尺度SparseConv的轻量级编码网络，实现快速推理和小解码器体积。

Result: 实验显示，LINR-PCGC在MVUB数据集上比G-PCC TMC13v23和SparsePCGC分别减少约21.21%和21.95%的比特流。

Conclusion: LINR-PCGC在无损压缩中表现优异，解决了现有方法的局限性，具有实际应用潜力。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [271] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS提出了一种基于小波变换的频率正则化方法，通过监督低频子带并自监督高频子带，改善了稀疏视图3D高斯泼溅的重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅（3DGS）在重建高质量新视图时容易过拟合训练视图的高频细节，而传统的傅里叶变换方法参数调优困难且偏向有害的高频学习。

Method: DWTGS利用小波空间损失提供额外的空间监督，仅监督低频LL子带，并以自监督方式对高频HH子带施加稀疏性。

Result: 实验表明，DWTGS在多个基准测试中优于基于傅里叶的方法，低频策略提高了泛化能力并减少了高频幻觉。

Conclusion: DWTGS通过小波变换的频率正则化方法，有效解决了稀疏视图3DGS的高频过拟合问题。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [272] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种高效的人脸图像质量评估方法，通过教师-学生模型蒸馏技术降低计算复杂度，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 人脸图像质量评估（FIQA）的计算复杂度限制了其在实际应用中的可扩展性和部署，因此需要开发一种高效的方法。

Method: 采用两阶段方法：1) 训练强大的教师模型并使用自训练策略增强其能力；2) 通过知识蒸馏从教师模型中提取轻量级学生模型。

Result: 学生模型在极低计算开销下达到与教师模型相当的性能，并在ICCV 2025 VQualA FIQA挑战赛中取得第一名。

Conclusion: 该方法成功实现了高效且高性能的FIQA，适用于实际部署。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [273] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 论文探讨了空间控制图像生成模型的改进方法，通过实验比较了不同生成范式，提出了一种简单高效的基线方法，并研究了采样时间增强技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有模型在空间控制生成任务中缺乏科学比较的问题，澄清文献中的知识空白，并为实践者提供清晰的指导。

Method: 在ImageNet上进行了扩散模型、基于流的模型和自回归模型的对照实验，提出了控制令牌预填充基线方法，并研究了采样时间增强技术。

Result: 控制令牌预填充方法表现优异；采样时间增强技术（如分类器自由引导和softmax截断）显著提高了生成与控制的连贯性；适配器方法在有限数据下保持生成质量，但连贯性较差。

Conclusion: 论文为空间控制图像生成提供了实用的基线方法和改进策略，同时澄清了适配器方法的适用场景。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [274] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen提出了一种两阶段框架，利用压缩的令牌解决长视频生成的记忆瓶颈和长期一致性问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成长视频时的内存瓶颈和长期不一致性问题。

Method: 分为两阶段：1) 训练To2V模型生成短视频；2) 引入T2To模型生成全局一致的视频令牌，并通过FIFO-Diffusion策略平滑连接片段。

Result: 实验表明，该方法显著提升了长期时间和内容一致性，且计算开销可控。

Conclusion: TokensGen为长视频生成提供了可扩展的模块化解决方案，适用于叙事、电影制作和沉浸式模拟。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [275] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的方法，通过预测空间自适应双边网格来校正多视角下的光度变化，无需场景特定重训练，提高了3D高斯泼溅管线的重建质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现代相机管线中的处理（如曝光调整、白平衡等）会导致多视角间光度不一致，影响新视角合成的质量。现有方法通过联合优化场景表示和每张图像的外观嵌入来解决，但增加了计算复杂度和训练时间。

Method: 采用Transformer预测空间自适应双边网格，校正多视角光度变化，并将其集成到3D高斯泼溅管线中。

Result: 实验表明，该方法在重建质量和收敛速度上优于或匹配现有场景特定优化方法。

Conclusion: 该方法在多视角一致性和跨场景泛化能力上表现优异，且保持了高效训练。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [276] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为HDF的新框架，通过两个模块增强时间-频率建模并优化样本不平衡问题，显著提升了动态面部表情识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决多源数据和个体表达差异导致的样本异质性对动态面部表情识别性能的影响。

Method: 设计了Time-Frequency Distributional Attention Module（DAM）和Distribution-aware Scaling Module（DSM），分别用于时间-频率建模和动态平衡分类与对比损失。

Result: 在DFEW和FERV39k数据集上表现出更高的识别准确性和鲁棒性，加权平均召回率（WAR）和非加权平均召回率（UAR）均显著提升。

Conclusion: HDF框架有效解决了样本异质性问题，提升了动态面部表情识别的性能，并具有广泛的泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [277] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 论文提出两种基于树结构的语义损失函数，利用标签的层次结构改进医学图像分割的准确性，并在稀疏标注场景下验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法对所有错误同等惩罚，未能利用标签空间的语义层次关系，尤其是当标签类别增多且差异细微时。

Method: 提出两种树状语义损失函数，结合稀疏标注训练方法，扩展其适用性。

Result: 在头部MRI全脑分割和神经外科高光谱图像场景理解任务中，方法达到最先进性能。

Conclusion: 树状语义损失函数能有效提升医学图像分割的准确性，尤其在复杂标签空间和稀疏标注场景下。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [278] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出了一种动态调整低秩适应（LoRA）中固有秩的新方法，用于医学图像分割，通过引入l1稀疏正则化器自动优化秩选择，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法需要固定秩，难以适应不同医学图像任务的复杂性，因此需要一种动态调整秩的方法。

Method: 将低秩表示视为奇异值分解，引入l1稀疏正则化器，并使用近端优化器自动优化秩选择。

Result: 在少样本微调实验中，该方法显著优于标准LoRA和其他PEFT方法，表现出高效性和鲁棒性。

Conclusion: 动态调整秩的方法在医学图像分割中具有优越性能，解决了固定秩选择的局限性。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [279] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 研究了低参数深度神经网络在计算机视觉中的性能，重点关注瓶颈架构及其使用超线性激活函数的行为。通过减少特征图中的干扰，提升了小规模网络（参数少于150万）的扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索低参数深度神经网络在计算机视觉中的性能瓶颈，特别是干扰现象对网络扩展性和准确性的影响。

Method: 通过分析不同瓶颈架构，识别减少干扰的关键设计元素，并提出一种名为NoDepth Bottleneck的概念验证架构。

Result: 提出的NoDepth Bottleneck架构在ImageNet数据集上表现出稳健的扩展性和准确性。

Conclusion: 研究为低参数范围的神经网络提供了更高效和可扩展的设计方案，并深化了对计算机视觉中瓶颈架构的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [280] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型SEEM生成未标注图像的预测掩码，并通过不确定性校准和自依赖训练策略提升半监督语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务（如语义分割）中标注数据稀缺且成本高的问题。

Method: 提出ConformalSAM框架，通过校准基础模型SEEM并过滤不可靠像素标签，结合自依赖训练策略。

Result: 在三个标准半监督语义分割基准上表现优于现有方法，并可作为插件提升其他方法性能。

Conclusion: ConformalSAM通过有效利用基础分割模型和不确定性校准，显著提升了半监督语义分割的性能。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [281] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 论文提出了一种动态注意力重分配（DARA）方法和TrueMICL数据集，以解决多模态大语言模型（MLLMs）在视觉信息利用上的不足，并提升多模态上下文学习（MICL）能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在多模态上下文学习中过度依赖文本信息，忽视视觉线索，导致性能受限。如何有效增强MICL能力并可靠评估其表现仍未充分探索。

Method: 提出DARA方法，通过重新平衡视觉和文本标记的注意力，鼓励模型关注视觉上下文；并构建TrueMICL数据集，明确要求整合多模态信息。

Result: 实验证明该方法显著提升了真正的多模态上下文学习能力。

Conclusion: DARA和TrueMICL为提升MLLMs的多模态能力提供了有效解决方案。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [282] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中表现出色，通过改进扩散后验采样方法，提高了统计稳健性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在多元地下建模和概率反演中的应用，以提升建模能力和计算效率。

Method: 提出对扩散后验采样方法的改进，包括考虑噪声污染的似然近似，并在多元地质场景中进行测试。

Result: 相比原始方法，显著提高了统计稳健性、后验概率密度采样的效率，并降低了计算成本。

Conclusion: 改进的扩散模型方法适用于硬数据和间接条件数据，反演速度快于传统方法。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [283] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个评估文本到视频生成模型物理常识能力的基准，包含383个提示，通过三阶段评估流程间接测试模型的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在物理常识方面表现不足，常违反因果关系和对象行为的基本直觉。

Method: 设计PhysVidBench基准，包含383个提示，采用三阶段评估（问题生成、视频描述、语言模型回答）。

Result: 通过间接评估策略，避免了直接视频评估中的幻觉问题，提供了结构化框架。

Conclusion: PhysVidBench为生成视频模型的物理常识评估提供了可解释的框架，填补了现有评估的空白。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [284] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种概念驱动的视频对象分割框架（SeC），通过结合视觉语言模型（LVLMs）构建高层次对象表示，显著提升了在复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割方法主要依赖外观匹配，缺乏人类对对象的概念理解能力，导致在视觉变化、遮挡和复杂场景中表现不佳。

Method: SeC框架利用LVLMs整合多帧视觉线索，构建对象的概念先验，并在推理时动态平衡语义推理与特征匹配。

Result: SeC在提出的SeCVOS基准上比SAM 2.1提升了11.8分，实现了概念感知视频对象分割的新最佳性能。

Conclusion: SeC通过概念驱动的表示和动态计算调整，显著提升了视频对象分割在复杂场景中的鲁棒性和准确性。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [285] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为l-DeTok的视觉分词器，通过直接与去噪目标对齐，提升生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型的训练目标（如去噪）与视觉分词器的有效性之间存在关联，但具体属性尚不明确。

Method: 提出Latent Denoising Tokenizer (l-DeTok)，通过重构被噪声和掩码污染的潜在嵌入来训练分词器。

Result: 在ImageNet 256x256上，l-DeTok在六种代表性生成模型中表现优于标准分词器。

Conclusion: 去噪是分词器设计的基本原则，为未来分词器开发提供了新视角。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [286] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA是一个结合大型语言模型和混合检索技术的模式匹配框架，无需标注数据或成对比较即可高效识别候选匹配。


<details>
  <summary>Details</summary>
Motivation: 模式匹配是异构数据源集成和数据集发现的关键，但现有方法复杂且资源密集。

Method: SCHEMORA通过提示方法结合语言模型和混合检索技术，利用向量和词汇检索增强模式元数据。

Result: 在MIMIC-OMOP基准测试中，SCHEMORA在HitRate@5和HitRate@3上分别提升7.49%和3.75%，达到最新性能。

Conclusion: SCHEMORA是首个基于LLM的开源模式匹配方法，强调了检索的关键作用并提供了模型选择指导。

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


### [287] [Towards Temporal Knowledge Graph Alignment in the Wild](https://arxiv.org/abs/2507.14475)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Xiang Zhao,Jiuyang Tang,Lei Chen*

Main category: cs.DB

TL;DR: 论文提出HyDRA方法，解决异构时序知识图谱对齐中的多尺度时间元素纠缠和跨源结构不平衡问题，并通过新数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设时间元素标准和结构统一，无法处理现实中的多尺度时间元素纠缠和结构不平衡问题。

Method: 提出HyDRA，采用多尺度超图检索增强生成和尺度交织协同机制。

Result: HyDRA在BETA和WildBETA数据集上表现优于24种基线方法。

Conclusion: HyDRA为时序知识图谱对齐提供了新范式，并在效率和扩展性上表现优异。

Abstract: Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent
entities across heterogeneous temporal knowledge graphs (TKGs) for fusion to
improve their completeness. Although some approaches have been proposed to
tackle this task, most assume unified temporal element standards and simplified
temporal structures across different TKGs. They cannot deal with TKGA in the
wild (TKGA-Wild), where multi-scale temporal element entanglement and
cross-source temporal structural imbalances are common. To bridge this gap, we
study the task of TKGA-Wild and propose HyDRA, a new and effective solution.
HyDRA is the first to reformulate the task via multi-scale hypergraph
retrieval-augmented generation to address the challenges of TKGA-Wild.In
addition, we design a new scale-weave synergy mechanism for HyDRA, which
incorporates intra-scale interactions and cross-scale conflict detection. This
mechanism is designed to alleviate the fragmentation caused by multi-source
temporal incompleteness and resolves inconsistencies arising from complex and
uneven temporal event density distributions, thereby enhancing the model
capacity to handle the intricacies of real-world temporal alignment. Finally,
there is no standard benchmark that captures these challenges of TKGA-Wild and
effectively evaluates existing methods. To this end, we formally propose to
benchmark challenges for TKGA-Wild and validate the effectiveness of the method
by establishing two new datasets(BETA and WildBETA). Extensive experiments on
the new datasets and six representative benchmarks show that BETA and WildBETA
better reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm
for TKGA-Wild, consistently outperforming 24 competitive baselines, while
maintaining strong efficiency and scalability.

</details>


### [288] [Opening The Black-Box: Explaining Learned Cost Models For Databases](https://arxiv.org/abs/2507.14495)
*Roman Heinrich,Oleksandr Havrylov,Manisha Luthra,Johannes Wehrstein,Carsten Binnig*

Main category: cs.DB

TL;DR: 本文提出了一种新方法，通过AI可解释性技术揭示学习成本模型（LCMs）的黑箱问题，并开发了适用于LCMs的解释工具。


<details>
  <summary>Details</summary>
Motivation: 传统LCMs在尾部查询计划中预测误差较大，且由于基于复杂深度模型，难以理解误差根源，阻碍了系统化故障排除。

Method: 扩展现有AI可解释性方法，开发适用于LCMs的新解释技术，并提供一个交互式工具展示其工作原理。

Result: 实现了对LCMs的可解释性，为系统化修复问题奠定了基础。

Conclusion: 这是使LCMs可调试的第一步，为未来系统化解决问题铺平了道路。

Abstract: Learned Cost Models (LCMs) have shown superior results over traditional
database cost models as they can significantly improve the accuracy of cost
predictions. However, LCMs still fail for some query plans, as prediction
errors can be large in the tail. Unfortunately, recent LCMs are based on
complex deep neural models, and thus, there is no easy way to understand where
this accuracy drop is rooted, which critically prevents systematic
troubleshooting. In this demo paper, we present the very first approach for
opening the black box by bringing AI explainability approaches to LCMs. As a
core contribution, we developed new explanation techniques that extend existing
methods that are available for the general explainability of AI models and
adapt them significantly to be usable for LCMs. In our demo, we provide an
interactive tool to showcase how explainability for LCMs works. We believe this
is a first step for making LCMs debuggable and thus paving the road for new
approaches for systematically fixing problems in LCMs.

</details>


### [289] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: IDSS是一种新型的大规模数据存储工具，利用P2P网络和嵌入式关系数据库解决传统数据库管理系统的可扩展性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 数据生成速度快速增长，传统数据库管理系统在处理大规模异构数据时存在可扩展性和效率问题。

Method: 提出IDSS架构，结合P2P网络和嵌入式关系数据库，支持分布式查询和复杂查询处理。

Result: IDSS通过P2P框架和通用模式实现高效的大规模数据管理。

Conclusion: IDSS为大规模异构数据管理提供了一种高效且可扩展的解决方案。

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


### [290] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: Mayura框架通过利用时间图的结构和时间共性，统一挖掘多个时间主题，显著减少冗余计算，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统时间主题挖掘方法独立处理每个查询，存在大量冗余计算，Mayura旨在解决这一问题。

Method: 提出Motif-Group Tree（MG-Tree）层次数据结构，设计共挖掘算法，支持CPU和GPU架构。

Result: 实验表明，Mayura在CPU和GPU上分别实现2.4倍和1.7倍的加速，优于现有技术。

Conclusion: Mayura通过统一挖掘和优化计算路径，显著提升了时间主题挖掘的效率和性能。

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [291] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 论文研究了分布式LLM服务中的通信动态，分析了不同并行化方法在推理过程中GPU间的数据交换，提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 尽管分布式推理框架使LLM的部署成为可能，但GPU间通信的性能限制影响了实际服务质量，因此需要研究通信动态以优化性能。

Method: 结合详细的分析测量和预测模型，研究了不同并行化配置下的通信行为，重点关注密集的基于transformer的模型。

Result: 张量并行带来高网络开销但响应时间短；流水线并行减少数据传输但增加总延迟；组合方法需精细调优以实现平衡性能。

Conclusion: 研究结果为生产环境中选择并行化方案提供了实用建议，并指出了优化推理框架和通信基础设施的关键机会。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [292] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 论文提出了一种三步解决方案，用于主动边缘流处理的自动扩展问题，包括负载预测、迁移学习和动态资源调整。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济的发展，高速数据处理变得至关重要，但边缘流处理面临资源分配的挑战，现有方法存在滞后或浪费问题。

Method: 使用GRU神经网络预测负载，结合迁移学习和DTW算法处理离线与在线数据差异，动态调整资源。

Result: GRU模型在真实数据集上表现优异，SMAPE值低至1.3%，优于其他模型且训练时间更短。

Conclusion: 提出的方法有效解决了边缘流处理的资源分配问题，具有高效性和实用性。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [293] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 论文研究了在1-间隔连通环网络中，无手性假设下同步移动代理的Distance-$k$-Dispersion问题，提出了一种模拟手性的方法，解决了任意规模环网络的问题，并给出了$O(ln)$轮算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在无手性假设下，移动代理如何在动态网络中协调分散的问题，扩展了经典分散问题的理论边界。

Method: 方法包括利用局部信息、视觉和有限内存模拟手性，并基于此设计算法解决D-$k$-D问题。

Result: 结果表明，D-$k$-D问题在任意规模环网络中可解，算法复杂度为$O(ln)$轮。

Conclusion: 结论指出，手性并非分布式计算中的基本需求，研究显著扩展了对动态网络中代理协调的理论理解。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [294] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME提出了一种分布式系统下的自适应定制方法，用于优化Transformer大模型的部署，解决了数据隐私和延迟问题，同时提升了性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决云端部署大模型时的数据隐私和延迟问题，以及直接应用模型时的资源限制和性能不匹配问题。

Method: 采用双向单循环分布式系统，逐步实现细粒度协作模型定制，包括主干生成、Pareto Front识别、头部生成和数据分布驱动的个性化架构聚合。

Result: 在模型大小限制下实现高效定制，数据传输量降至6%，平均准确率提升10%，权衡指标增加近30%。

Conclusion: ACME通过分布式定制方法有效解决了大模型部署中的关键挑战，显著提升了性能和资源效率。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [295] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs提出了一种去中心化的多智能体LLM系统共识方法，通过并行生成答案和独立评分来提升鲁棒性和答案质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于领导者的多智能体系统易受攻击和低效的问题，特别是在存在恶意（Byzantine）代理的情况下。

Method: 采用去中心化架构，工作代理并行生成答案，评估代理独立评分和排名，通过鲁棒聚合技术选择最佳答案。

Result: 实验表明，DecentLLMs能有效容忍Byzantine代理，并显著提升所选答案的质量。

Conclusion: DecentLLMs为多智能体LLM系统提供了一种高效且鲁棒的共识方案。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [296] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: AMPED是一种多GPU并行算法，用于加速大规模稀疏张量的MTTKRP计算，解决了单GPU内存和性能限制的问题。


<details>
  <summary>Details</summary>
Motivation: 随着现实世界稀疏张量规模增长到数十亿非零值，对硬件加速器的内存容量和计算吞吐量需求增加。

Method: 采用分区策略和动态负载平衡方案，分配计算并最小化GPU空闲时间。

Result: 在4个GPU上，AMPED比现有GPU基线实现了5.1倍的几何平均加速。

Conclusion: AMPED能够满足大规模稀疏张量分解的计算需求，显著提升性能。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [297] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune动态调整Raft选举参数以减少领导者故障检测和停机时间。


<details>
  <summary>Details</summary>
Motivation: 传统Raft算法在网络波动时难以有效调整选举参数，导致停机时间增加和服务响应性下降。

Method: 提出Dynatune机制，基于网络指标（如往返时间和丢包率）动态调整Raft选举参数。

Result: 实验显示Dynatune将领导者故障检测和停机时间分别减少80%和45%。

Conclusion: Dynatune显著提升了SMR服务在不同网络场景下的性能和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [298] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: 论文提出了一种针对异构CPU-GPU系统的任务并行方法GALE，通过将网格连接信息计算卸载到GPU线程，显著提升了科学数据可视化的性能。


<details>
  <summary>Details</summary>
Motivation: 非结构化网格在科学数据分析中因分布不规则和连接复杂而带来挑战，现有方法受限于CPU资源竞争，无法充分发挥性能潜力。

Method: 提出GALE，一种基于CUDA的异构任务并行数据结构，将连接信息计算卸载到GPU，让CPU专注于可视化算法。

Result: 在20核CPU和NVIDIA V100 GPU上的实验显示，GALE比现有方法提速2.7倍，同时保持内存效率。

Conclusion: GALE通过优化异构计算资源分配，显著提升了非结构化网格数据处理的性能。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [299] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出了一种基于多目标强化学习的联邦推荐系统参与者选择方法，通过优化客户端性能声誉、数据效用和系统效率，显著提升了训练效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统需要集中收集用户数据，引发隐私和可扩展性问题。联邦推荐系统虽能保护隐私，但面临设备异构性、数据非独立同分布和通信瓶颈等挑战。

Method: 提出一种多目标强化学习参与者选择方法，结合客户端性能声誉、系统能力和数据质量，嵌入多臂老虎机框架动态平衡探索与利用。

Result: 在四种数据偏斜场景下，方法将收敛时间缩短32-50%，总训练时间减少46%，同时保持或略微提升AUC、NDCG@50和Recall@50。

Conclusion: 自适应、奖励驱动的客户端采样能显著提升联邦推荐系统的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [300] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出了一种基于NSGA-II的路由算法，用于在云边计算环境中分配LLM推理请求，优化响应时间、质量和成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理服务需求增加，计算资源压力导致延迟和成本问题，需要高效的资源分配方法。

Method: 采用多目标优化方法，结合NSGA-II算法，动态分配请求到异构LLM实例。

Result: 实验显示，相比基线方法，响应时间和成本分别提升95.2%和34.9%。

Conclusion: 该算法在动态负载下优化性能，适用于可扩展的LLM部署。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [301] [Iran's Stealth Internet Blackout: A New Model of Censorship](https://arxiv.org/abs/2507.14183)
*Arash Aryapour*

Main category: cs.NI

TL;DR: 伊朗在2025年中期实施了一种新型隐蔽的互联网关闭，通过深度包检测、流量限制和选择性协议屏蔽隔离国内用户，同时保持全球路由存在。


<details>
  <summary>Details</summary>
Motivation: 研究这种新型互联网关闭的技术手段及其对VPN需求和数字权利的影响。

Method: 通过主动网络测量（如DNS污染、HTTP注入、TLS拦截和协议白名单）分析，追踪到集中式边界网关。

Result: 量化了VPN需求增长约707%，并描述了多层审查基础设施。

Conclusion: 揭示了这种审查技术对规避工具和数字权利监控的深远影响。

Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that
preserved global routing presence while isolating domestic users through deep
packet inspection, aggressive throttling, and selective protocol blocking. This
paper analyzes active network measurements such as DNS poisoning, HTTP
injection, TLS interception, and protocol whitelisting, traced to a centralized
border gateway. We quantify an approximate 707 percent rise in VPN demand and
describe the multi-layered censorship infrastructure, highlighting implications
for circumvention and digital rights monitoring.

</details>


### [302] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 论文提出了一种基于专家知识特征压缩和解耦表示学习的双策略方法，用于解决低空网络覆盖预测中的数据稀疏和特征不平衡问题，实验表明其优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 低空网络覆盖预测对设计空中走廊至关重要，但基站天线波束模式通常不可获取，且低空路测数据稀疏，导致特征采样不平衡和模型泛化能力不足。

Method: 采用专家知识特征压缩降低特征空间复杂度，结合解耦表示学习增强模型泛化能力，通过传播模型和子网络捕获潜在特征的语义表示。

Result: 实验误差比最佳基线算法降低7%，实际网络验证中MAE误差达到5dB水平。

Conclusion: 该方法有效解决了低空网络覆盖预测中的挑战，具有实际应用价值。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [303] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 论文探讨了完全基于轨道的移动网络架构，包括5G核心功能、激光回程和电子控制相控阵，展示了其在密集城市中的可行性，并提出了15年发展路线图。


<details>
  <summary>Details</summary>
Motivation: 验证是否能在轨道上运行完整的移动网络，包括核心功能和城市级服务，摆脱对地面基础设施的依赖。

Method: 提出端到端系统架构，集成电子控制相控阵、5G核心功能部署和卫星间激光回程，分析频谱效率、链路预算和城市环境下的性能。

Result: 模拟显示屋顶和视距用户可实现64-QAM吞吐量，街级接入需中继或辅助波束模式，工程瓶颈可解决。

Conclusion: 完全轨道移动网络可行，未来15年可实现50-100 Mbps的城市级服务，无需地面基础设施。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [304] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 本文提出了一种新颖的语义通信方法，通过在资源受限的发射器和接收器之间分割语义图像分割过程，以在保持分割精度的同时减少带宽和计算需求。实验表明，该方法可减少72%的比特率和19%以上的发射器计算负载。


<details>
  <summary>Details</summary>
Motivation: 当前语义通信在图像分割中难以平衡计算效率与带宽需求，尤其是在资源受限和信道条件变化的环境中。复杂模型还增加了设备处理数据的压力。

Method: 提出一种基于分割语义图像分割过程的方法，将部分计算任务从资源受限的发射器转移到接收器，以减少传输数据量和发射器计算负担。

Result: 仿真实验表明，该方法可减少72%的比特率和19%以上的发射器计算负载，同时保持分割精度。

Conclusion: 该方法在减少通信成本和计算负载方面具有显著优势，适用于未来6G通信系统。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [305] [A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming](https://arxiv.org/abs/2507.14205)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 提出了一种结合SDWMN、D2M广播和Kafka混合云流媒体的架构，优化城乡无线网络性能，减少延迟和覆盖不足。


<details>
  <summary>Details</summary>
Motivation: 解决城市网络拥堵和农村数字鸿沟问题，通过流量卸载、容错和资源公平分配提升性能。

Method: 建模城市拥堵和农村覆盖不足，设计全局性能损失函数，结合SDWMN和Kafka实现快速恢复。

Result: 实验显示延迟降低32%，带宽卸载40%，农村覆盖提升28%，恢复时间小于10秒。

Conclusion: 建议优化频谱分配和政策支持，支持公平数字化转型，为未来研究提供方向。

Abstract: We propose an integrated architecture combining Software-Defined Wireless
Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based
hybrid cloud streaming to improve wireless network performance in both urban
and rural settings. The approach addresses urban congestion and rural digital
exclusion through traffic offloading, enhanced fault tolerance, and equitable
resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and
rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize
global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3
\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,
Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth
offloading of 40%, rural coverage gain of 28%, and fairness index rising from
0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We
recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and
device mandates to promote adoption. This scalable, fault-tolerant design
supports equitable digital transformation and suggests directions for future
research.

</details>


### [306] [White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective](https://arxiv.org/abs/2507.14209)
*Rute C. Sofia,Hao Shen,Yuanting Liu,Severin Kacianka,Holger Pfeifer*

Main category: cs.NI

TL;DR: fortiss提出了一种以人为中心、可持续且集成AI的6G网络愿景，强调技术与社会需求的结合。


<details>
  <summary>Details</summary>
Motivation: 确保6G技术不仅实现技术进步，还需符合社会需求，推动负责任创新。

Method: 通过软件定义、AI驱动和可持续通信服务，结合认知智能、去中心化编排和绿色架构。

Result: 6G将超越前代技术，实现超可靠低延迟通信和个性化服务，同时注重能源效率和社会影响。

Conclusion: fortiss通过跨学科合作，为6G发展提供战略方向，致力于实现2030年的有意义愿景。

Abstract: As a leading research institute in software-intensive systems, fortiss is
actively shaping the vision of Sixth Generation Mobile Communication (6G). Our
mission is to ensure that 6G technologies go beyond technical advancements and
are aligned with societal needs. fortiss plays a key role in 6G initiatives
worldwide, including contributions to standardization bodies and collaborative
Research and Development programs. We focus on software-defined, AI-enabled,
and sustainable communication services that prioritize human values and
long-term impact. 6G will redefine digital connectivity through cognitive
intelligence, decentralized orchestration, and sustainability-oriented
architectures. As expectations rise for ultra-reliable low-latency
communication (URLLC) and personalized digital services, 6G must outperform
prior generations. It will rely on AI-native networking, Edge-Cloud resource
orchestration, and energy-aware data frameworks, ensuring both technical
performance and societal relevance. This white paper presents the fortiss
vision for a human-centric, sustainable, and AI-integrated 6G network. It
outlines key research domains such as semantic communication, green
orchestration, and distributed AI, all linked to societal and technological
challenges. The white paper is aimed at researchers, industry experts,
policymakers, and developers. It articulates the strategic direction and
contributions of fortiss to 6G, emphasizing responsible innovation and
interdisciplinary collaboration toward a meaningful 2030 vision.

</details>


### [307] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA是一个基于AI的预测性QoS框架，用于优化远程驾驶应用，通过强化学习（RL）提升性能。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶对延迟和可靠性要求严格，预测性QoS（PQoS）能提前应对网络变化，避免性能下降。

Method: PRATA框架包括5G RAN模拟、汽车数据生成工具和AI单元，设计了RL单元RAN-AI优化数据分段。

Result: RAN-AI在QoS和QoE间取得平衡，性能提升近一倍，并研究了状态空间和数据获取成本的影响。

Conclusion: PRATA和RAN-AI展示了AI在优化远程驾驶QoS中的潜力，为未来研究提供了工具和方向。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [308] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型（LLMs）的意图驱动网络自动化方法，用于无线接入网（RAN）管理，通过结构化提示工程和闭环机制动态优化RAN参数，提升能效。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络管理复杂度的增加，高级智能自动化成为重要需求。本文旨在利用LLMs实现意图驱动的RAN管理，解决高复杂度问题。

Method: 提出了一种结合LLMs的代理架构，通过结构化提示工程实现意图翻译、网络状态推理和RAN配置生成，并通过闭环机制动态优化参数。

Result: 实验表明，该方法能自动提升RAN的能效，并通过实时反馈实现稳健的资源管理。

Conclusion: LLM驱动的代理系统在RAN管理中具有潜力，能够通过动态优化和实时反馈提升网络性能。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [309] [Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting](https://arxiv.org/abs/2507.14234)
*Samer Nasser,Henrique Duarte Moura,Dragan Subotic,Ritesh Kumar Singh,Maarten Weyn,Jeroen Famaey*

Main category: cs.NI

TL;DR: 提出了一种结合太阳能和动能采集的能量中性系统，用于野生动物追踪，通过多源能量采集提高数据可靠性和传输频率。


<details>
  <summary>Details</summary>
Motivation: 解决野生动物追踪中电池更换不切实际且对动物造成压力的问题，提出可持续的能量采集方案。

Method: 结合太阳能和动能采集，利用NB-IoT通信技术，开发模拟框架和能量感知调度器。

Result: 系统在能量中性操作下显著提高数据产量和可靠性，支持每两分钟采样GPS和动能数据，每小时传输。

Conclusion: 该系统展示了在远程栖息地实现免维护、环保追踪的潜力，提升野生动物监测效果和可扩展性。

Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but
energy limitations pose challenges, especially for animal tags, where replacing
batteries is impractical and stressful for the animal due to the need to
locate, possibly sedate, and handle it. Energy harvesting offers a sustainable
alternative, yet most existing systems rely on a single energy source and
infrastructure-limited communication technologies. This paper presents an
energy-neutral system that combines solar and kinetic energy harvesting to
enable the tracking and monitoring of wild animals. Harvesting from multiple
sources increases the total available energy. Uniquely, the kinetic harvester
also serves as a motion proxy by sampling harvested current, enabling activity
monitoring without dedicated sensors. Our approach also ensures compatibility
with existing cellular infrastructure, using Narrowband Internet of Things
(NB-IoT). We present a simulation framework that models energy harvesting,
storage, and consumption at the component level. An energy-aware scheduler
coordinates task execution based on real-time energy availability. We evaluate
performance under realistically varying conditions, comparing task frequencies
and capacitor sizes. Results show that our approach maintains energy-neutral
operation while significantly increasing data yield and reliability compared to
single-source systems, with the ability to consistently sample GPS location
data and kinetic harvesting data every two minutes while transmitting these
results over NB-IoT every hour. These findings demonstrate the potential for
maintenance-free, environmentally friendly tracking in remote habitats,
enabling more effective and scalable wildlife monitoring.

</details>


### [310] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA索引架构为AI代理提供可发现性、可识别性和认证，支持多端点路由、负载均衡和隐私保护访问，具备快速解析、密钥轮换和最小披露查询等功能。


<details>
  <summary>Details</summary>
Motivation: 解决DNS为中心的互联网身份和发现机制在AI代理时代面临的挑战。

Method: 提出NANDA索引架构，基于动态可验证的AgentFacts，支持CRDT更新协议和自适应解析器。

Result: 实现轻量级、水平可扩展的基础设施，支持安全、信任感知的AI代理协作。

Conclusion: NANDA为下一代AI代理互联网提供了无需放弃现有基础设施的解决方案。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [311] [NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation](https://arxiv.org/abs/2507.14398)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: 论文介绍了IBNBench基准测试套件和NetIntent框架，评估LLMs在意图翻译和冲突检测任务中的表现，并探索LLMs在完整IBN生命周期中的应用。


<details>
  <summary>Details</summary>
Motivation: 意图驱动网络（IBN）在自动化从用户意图到设备配置的整个流程中存在挑战，现有解决方案缺乏灵活性和适应性。

Method: 提出IBNBench基准测试套件评估33个开源LLMs，并开发NetIntent框架，结合LLMs和非LLMs代理实现端到端IBN自动化。

Result: LLMs在IBN任务中表现不一，NetIntent框架在ODL和ONOS控制器上实现了自适应端到端IBN。

Conclusion: LLMs在IBN中具有潜力，但需进一步研究其在完整自动化流程中的应用。NetIntent为IBN提供了统一且灵活的解决方案。

Abstract: Intent-Based Networking (IBN) often leverages the programmability of
Software-Defined Networking (SDN) to simplify network management. However,
significant challenges remain in automating the entire pipeline, from
user-specified high-level intents to device-specific low-level configurations.
Existing solutions often rely on rigid, rule-based translators and fixed APIs,
limiting extensibility and adaptability. By contrast, recent advances in large
language models (LLMs) offer a promising pathway that leverages natural
language understanding and flexible reasoning. However, it is unclear to what
extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a
first-of-its-kind benchmarking suite comprising four novel datasets:
Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.
These datasets are specifically designed for evaluating LLMs performance in
intent translation and conflict detection tasks within the industry-grade SDN
controllers ODL and ONOS. Our results provide the first comprehensive
comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a
wide range of performance outcomes. However, while these results demonstrate
the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully
autonomous IBN pipeline remains unexplored. Thus, our second contribution is
NetIntent, a unified and adaptable framework that leverages LLMs to automate
the full IBN lifecycle, including translation, activation, and assurance within
SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting
dynamic re-prompting and contextual feedback to robustly execute user-defined
intents with minimal human intervention. Our implementation of NetIntent across
both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end
IBN realization.

</details>


### [312] [Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks](https://arxiv.org/abs/2507.14512)
*Qiyuan Peng,Qi Zhang,Yue Gao,Kun Qiu*

Main category: cs.NI

TL;DR: 论文提出了一种基于强化学习的控制器配置策略Dora，用于解决卫星网络管理中的可扩展性和计算资源限制问题，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星星座的快速扩展对网络管理提出了挑战，传统架构和算法无法高效解决同步和数据传输问题。

Method: 提出了一种三层域架构，并设计了基于强化学习的控制器配置策略Dora。

Result: Dora在控制器配置质量上提升了10%，计算时间仅为传统算法的1/30到1/90。

Conclusion: 强化学习方法在下一代SAGIN部署中具有高效卫星网络管理的潜力。

Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground
Integrated Networks (SAGIN) presents significant challenges for network
management. Conventional flat network architectures struggle with
synchronization and data transmission across massive distributed nodes. In
response, hierarchical domain-based satellite network architectures have
emerged as a scalable solution, highlighting the critical importance of
controller provisioning strategies. However, existing network management
architectures and traditional search-based algorithms fail to generate
efficient controller provisioning solutions due to limited computational
resources in satellites and strict time constraints. To address these
challenges, we propose a three-layer domain-based architecture that enhances
both scalability and adaptability. Furthermore, we introduce Dora, a
reinforcement learning-based controller provisioning strategy designed to
optimize network performance while minimizing computational overhead. Our
comprehensive experimental evaluation demonstrates that Dora significantly
outperforms state-of-the-art benchmarks, achieving 10% improvement in
controller provisioning quality while requiring only 1/30 to 1/90 of the
computation time compared to traditional algorithms. These results underscore
the potential of reinforcement learning approaches for efficient satellite
network management in next-generation SAGIN deployments.

</details>


### [313] [UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach](https://arxiv.org/abs/2507.14627)
*Kaiqiang Lin,Yijie Mao,Onel Luis Alcaraz López,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文提出了一种基于无人机的无线供电地下通信网络（WPUCN）系统，通过混合无线能量传输（WET）方法优化能量消耗，实现可持续地下监测。


<details>
  <summary>Details</summary>
Motivation: 解决地下环境中无线信号衰减严重和信道状态信息（CSI）获取成本高的问题，使大规模WPUCN在经济上可行。

Method: 引入无人机（UAV）到WPUCN中，提出混合WET方法（结合HAP和UAV的能量传输），并优化时间分配以最小化UAV能耗。

Result: 仿真显示混合WET方法优于其他方法，性能受天线数量、通信距离、UD数量和地下环境影响；优化的时间分配下，基于CSI-free多天线方案的混合WET能耗最低。

Conclusion: 提出的系统和方法能够实现可持续的地下监测，为WPUCN的实际应用提供了经济高效的解决方案。

Abstract: Wireless-powered underground communication networks (WPUCNs), which allow
underground devices (UDs) to harvest energy from wireless signals for
battery-free communication, offer a promising solution for sustainable
underground monitoring. However, the severe wireless signal attenuation in
challenging underground environments and the costly acquisition of channel
state information (CSI) make large-scale WPUCNs economically infeasible in
practice. To address this challenge, we introduce flexible unmanned aerial
vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this
system, a UAV is first charged by a terrestrial hybrid access point (HAP), then
flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV
collects data from the UDs and finally returns to the HAP for data offloading.
Based on the proposed UAV-enabled WPUCN system, we first propose its energy
consumption model and a hybrid wireless energy transfer (WET) approach (i.e.,
UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and
CSI-free multi-antenna beamforming. Then, we formulate and address a time
allocation problem to minimize the energy consumption of UAV, while ensuring
that the throughput requirements of all UDs are met and all sensor data is
offloaded. Through simulations of a realistic farming scenario, we demonstrate
that the proposed hybrid WET approach outperforms other WET approaches, with
performance gains influenced by the number of antennas, communication distance,
number of UDs, and underground conditions. Additionally, under the optimized
time allocation, we found that the proposed hybrid WET approach based on a
CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption
among all WET mechanisms, thereby enabling sustainable underground monitoring
in WPUCNs.

</details>


### [314] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 该论文探讨了如何通过生成式AI和大型语言模型（LLMs）实现智能代理AI，以支持卫星增强低空经济和地面网络（SLAETNs）的发展。


<details>
  <summary>Details</summary>
Motivation: SLAETNs需要智能自主系统在异构、动态和关键任务环境中可靠运行，而生成式AI和LLMs为解决这些挑战提供了潜力。

Method: 论文系统综述了五类生成模型（VAEs、GANs、GDMs、TBMs、LLMs），并比较了它们在SLAETNs中的生成机制、能力和部署权衡。

Result: 研究表明，这些模型在通信增强、安全隐私保护和智能卫星任务等领域赋能代理功能。

Conclusion: 论文提出了构建可扩展、自适应和可信赖生成代理的未来方向，为下一代集成网络中的代理AI提供了统一理解和行动参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [315] [Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks](https://arxiv.org/abs/2507.14842)
*Satadal Sengupta,Hyojoon Kim,Daniel Jubas,Maria Apostolaki,Jennifer Rexford*

Main category: cs.NI

TL;DR: 本文探讨了利用传播延迟变化检测互联网路由劫持的可行性，并提出了一种名为HiDe的系统，能够在实际部署中有效检测长距离劫持。


<details>
  <summary>Details</summary>
Motivation: 互联网路由安全性差，导致用户数据可能被劫持并通过外国基础设施传输，引发隐私和国家安全问题。现有检测方法主要关注控制层面，而数据层面的信号被忽视。

Method: 通过分析传播延迟变化作为劫持信号，研究其覆盖范围和实际部署可行性，并设计HiDe系统进行检测。

Result: 实验表明，86%的受害国-攻击国组合中，劫持期间的延迟至少增加25%，HiDe系统能够可靠检测延迟激增。

Conclusion: 延迟变化是一种有效的劫持检测信号，HiDe系统在实际部署中表现出高准确性和低误报率。

Abstract: Poor security of Internet routing enables adversaries to divert user data
through unintended infrastructures (hijack). Of particular concern -- and the
focus of this paper -- are cases where attackers reroute domestic traffic
through foreign countries, exposing it to surveillance, bypassing legal privacy
protections, and posing national security threats. Efforts to detect and
mitigate such attacks have focused primarily on the control plane while
data-plane signals remain largely overlooked. In particular, change in
propagation delay caused by rerouting offers a promising signal: the change is
unavoidable and the increased propagation delay is directly observable from the
affected networks. In this paper, we explore the practicality of using delay
variations for hijack detection, addressing two key questions: (1) What
coverage can this provide, given its heavy dependence on the geolocations of
the sender, receiver, and adversary? and (2) Can an always-on latency-based
detection system be deployed without disrupting normal network operations? We
observe that for 86% of victim-attacker country pairs in the world, mid-attack
delays exceed pre-attack delays by at least 25% in real deployments, making
delay-based hijack detection promising. To demonstrate practicality, we design
HiDe, which reliably detects delay surges from long-distance hijacks at line
rate. We measure HiDe's accuracy and false-positive rate on real-world data and
validate it with ethically conducted hijacks.

</details>


### [316] [Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors](https://arxiv.org/abs/2507.14876)
*Zi-Yang Wu,Muhammad Ismail,Jiliang Zhang,Jie Zhang*

Main category: cs.NI

TL;DR: 论文提出将可重构智能表面（RIS）嵌入建筑结构以优化室内无线性能，并探讨了人类行为对RIS覆盖建筑信道的影响及挑战。


<details>
  <summary>Details</summary>
Motivation: 室内移动网络性能受建筑材料和结构限制，而建筑设计未优先考虑无线性能。RIS在室外网络的成功应用启发其在室内网络中的潜力。

Method: 嵌入RIS到建筑结构中，研究人类行为对RIS覆盖建筑信道的影响，分析高阶马尔可夫依赖、概念漂移和泛化问题。

Result: 发现人类行为导致信道动态复杂，通用信道模型不可行，需针对深度学习预测和控制策略解决挑战。

Conclusion: 需协调RIS覆盖建筑与人群移动的共存，提出可能的解决方案以实现无线友好建筑设计。

Abstract: Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.

</details>


### [317] [FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches](https://arxiv.org/abs/2507.14891)
*Xiangyu Gao,Tong Li,Yinchao Zhang,Ziqiang Wang,Xiangsheng Zeng,Su Yao,Ke Xu*

Main category: cs.NI

TL;DR: FENIX是一种混合网络内机器学习系统，通过在可编程交换机ASIC上进行特征提取和在FPGA上进行深度神经网络推理，解决了低延迟、高吞吐量和高精度的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案（如FlowLens、N3IC和BoS）难以同时实现低延迟、高吞吐量和高精度，FENIX旨在解决这些问题。

Method: FENIX结合了数据引擎（使用概率令牌桶算法控制特征流发送速率）和模型引擎（支持高精度深度神经网络推理），并在可编程交换机平台上实现。

Result: FENIX实现了微秒级推理延迟、多太比特吞吐量，硬件开销低，并在主流网络流量分类任务中达到95%以上的准确率。

Conclusion: FENIX在性能上优于现有技术，为网络内机器学习提供了高效解决方案。

Abstract: Machine learning (ML) is increasingly used in network data planes for
advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,
and BoS) still struggle to simultaneously achieve low latency, high throughput,
and high accuracy. To address these challenges, we present FENIX, a hybrid
in-network ML system that performs feature extraction on programmable switch
ASICs and deep neural network inference on FPGAs. FENIX introduces a Data
Engine that leverages a probabilistic token bucket algorithm to control the
sending rate of feature streams, effectively addressing the throughput gap
between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model
Engine to enable high-accuracy deep neural network inference in the network,
overcoming the difficulty of deploying complex models on resource-constrained
switch chips. We implement FENIX on a programmable switch platform that
integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on
real-world network traffic datasets. Our results show that FENIX achieves
microsecond-level inference latency and multi-terabit throughput with low
hardware overhead, and delivers over 95\% accuracy on mainstream network
traffic classification tasks, outperforming SOTA.

</details>


### [318] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 提出了一种通信高效的、事件触发的协作边缘AI系统推理框架，结合双阈值早期退出策略和比例公平约束，通过联合优化提升分类效用。


<details>
  <summary>Details</summary>
Motivation: 解决多设备和边缘服务器协作中的通信、能源和公平性问题，扩展单设备推理到分布式场景。

Method: 采用双阈值早期退出策略，结合比例公平约束，通过交替优化和Benders分解解决联合优化问题。

Result: 实验表明，该框架显著提升了系统性能和资源分配的公平性，优于单设备基准。

Conclusion: 该框架为分布式边缘AI系统提供了一种高效、公平的推理解决方案。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [319] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 论文提出了一种新型的人机协作方案（HMC-DBA），通过预测头部运动优化带宽分配，以满足XR内容实时同步的需求，并显著降低带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 未来移动系统和固定无线网络需要支持高带宽、低延迟服务，尤其是在工业互联网、XR和H2M协作等场景中，实时同步XR内容是一大挑战。

Method: 使用双向LSTM网络预测头部运动，提出HMC-DBA方案动态分配带宽，优化XR帧传输。

Result: 模拟实验表明，HMC-DBA在满足XR帧延迟和抖动要求的同时，显著降低了带宽消耗，并提高了网络资源利用率。

Conclusion: HMC-DBA方案在实时XR内容同步中表现优异，优于现有技术，适用于企业网络等场景。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


### [320] [Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint](https://arxiv.org/abs/2507.15338)
*Takaho Shimokasa,Hiroyuki Yomo,Federico Chiariotti,Junya Shiraishi,Petar Popovski*

Main category: cs.NI

TL;DR: 论文研究了在无线资源受限的物联网监测中，如何通过卡尔曼滤波实现传感器节点的低功耗运行和准确状态估计，比较了两种策略（基于统计的忽略策略和基于瞬时观测的分散策略），并引入唤醒接收器和信号提升能效。


<details>
  <summary>Details</summary>
Motivation: 在无线传感器网络中，如何在资源受限条件下实现低功耗和高精度的状态估计是一个关键问题。

Method: 提出两种策略：基于统计的忽略策略和基于瞬时观测的分散策略，并引入唤醒接收器和信号。分散策略通过随机访问优先传输对状态估计贡献大的观测。

Result: 数值结果表明，在传感器节点观测过程相关性较低时，分散策略在无线电资源和能耗约束下比忽略策略更准确。同时明确了两种策略优劣转换的相关性程度。

Conclusion: 分散策略在低相关性场景下表现更优，唤醒机制显著提升了能效。

Abstract: This paper investigates how to achieve both low-power operations of sensor
nodes and accurate state estimation using Kalman filter for internet of things
(IoT) monitoring employing wireless sensor networks under radio resource
constraint. We consider two policies used by the base station to collect
observations from the sensor nodes: (i) an oblivious policy, based on
statistics of the observations, and (ii) a decentralized policy, based on
autonomous decision of each sensor based on its instantaneous observation. This
work introduces a wake-up receiver and wake-up signaling to both policies to
improve the energy efficiency of the sensor nodes. The decentralized policy
designed with random access prioritizes transmissions of instantaneous
observations that are highly likely to contribute to the improvement of state
estimation. Our numerical results show that the decentralized policy improves
the accuracy of the estimation in comparison to the oblivious policy under the
constraint on the radio resource and consumed energy when the correlation
between the processes observed by the sensor nodes is low. We also clarify the
degree of correlation in which the superiority of two policies changes.

</details>


### [321] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: P4TG是一种基于P4的流量生成器，用于Intel Tofino交换机，但现有方法在RTT测量中存在精度问题。本文提出了一种基于直方图的RTT测量方法，通过范围到前缀的转换算法实现高效硬件匹配。


<details>
  <summary>Details</summary>
Motivation: 解决P4TG在RTT测量中因采样导致的精度降低问题，提供更准确的线速分析。

Method: 采用直方图方法，通过范围到前缀的转换算法在硬件中实现高效的范围匹配。

Result: 评估表明，直方图方法能够准确测量RTT，并与理论分布一致。

Conclusion: 基于直方图的RTT测量方法显著提高了P4TG的测量精度和效率。

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [322] [Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities](https://arxiv.org/abs/2507.15391)
*Fabian Ihle,Michael Menth*

Main category: cs.NI

TL;DR: MPLS Network Actions (MNA) 框架通过扩展 MPLS 转发功能，支持网络切片和 IOAM 等应用。研究发现 MNA 需要较大的可读标签深度 (RLD)，并提出了一种通过重组 MPLS 栈来减少 RLD 需求的机制。


<details>
  <summary>Details</summary>
Motivation: MNA 需要较大的 RLD，这对路由器硬件提出了较高要求，因此需要一种方法来降低 RLD 需求。

Method: 提出了一种通过重组 MPLS 栈的机制，并引入了新的栈管理网络动作，同时验证了其在可编程硬件上的可行性。

Result: 机制成功减少了 RLD 需求，并讨论了其对 ECMP 和数据包开销的影响。

Conclusion: 该机制有效降低了 MNA 对 RLD 的需求，同时兼容不支持 MNA 的网络节点。

Abstract: The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a
generalized encoding for manifold extensions such as network slicing and
in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries
(LSEs) and are added to the MPLS stack. Routers have a physical limit on the
number of LSEs they can read, called the readable label depth (RLD). With MNA,
routers must be able to process a minimum number of LSEs which requires a
relatively large RLD. In this paper, we perform a hardware analysis of an MNA
implementation and identify the reason for a large RLD requirement in the MNA
protocol design. Based on this, we present a mechanism that reduces the
required RLD for MNA nodes by restructuring the MPLS stack during forwarding.
We then introduce the novel stack management network action that enables the
proposed mechanism as well as its integration in networks with MNA-incapable
nodes. The feasibility of the mechanism on programmable hardware is verified by
providing a P4-based implementation. Further, the effects on the required RLD,
ECMP, and packet overhead are discussed.

</details>


### [323] [Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations](https://arxiv.org/abs/2507.15423)
*Laura Finarelli,Falko Dressler,Marco Ajmone Marsan,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 本文提出了一种随机几何框架，用于评估移动网络（MN）范式在异构网络（HetNet）中的潜在优势，并通过优化问题确定资源最优配置，显著减少基础设施部署。


<details>
  <summary>Details</summary>
Motivation: 研究移动网络（MN）范式在6G用户为中心网络中的优势与资源成本之间的平衡，以确定其适用条件。

Method: 使用随机几何框架分析MN在HetNet中的性能，提出优化问题以最小化基站部署和车辆流量，并设计高效的随机启发式算法。

Result: 数值评估表明，MN范式结合动态网络管理策略可显著减少基础设施部署，同时保证用户感知的服务质量（QoS）。

Conclusion: MN范式在动态网络管理下能有效减少基础设施需求，适用于6G网络。

Abstract: In the evolution towards 6G user-centric networking, the moving network (MN)
paradigm can play an important role. In a MN, some small cell base stations
(BS) are installed on top of vehicles, and enable a more dynamic, flexible and
sustainable, network operation. By "following" the users movements and adapting
dynamically to their requests, the MN paradigm enables a more efficient
utilization of network resources, mitigating the need for dense small cell BS
deployments at the cost of an increase in resource utilization due to wireless
backhauling. This aspect is at least partly compensated by the shorter distance
between users and BS, which allows for lower power and Line-of-Sight
communications. While the MN paradigm has been investigated for some time, to
date, it is still unclear in which conditions the advantages of MN outweigh the
additional resource costs. In this paper, we propose a stochastic geometry
framework for the characterization of the potential benefits of the MN paradigm
as part of an HetNet in urban settings. Our approach allows the estimation of
user-perceived performance, accounting for wireless backhaul connectivity as
well as base station resource scheduling. We formulate an optimization problem
for determining the resource-optimal network configurations and BS scheduling
which minimize the overall amount of deployed BSs in a QoS-aware manner, and
the minimum vehicular flow between different urban districts required to
support them, and we propose an efficient stochastic heuristic to solve it. Our
numerical assessment suggests that the MN paradigm, coupled with appropriate
dynamic network management strategies, significantly reduces the amount of
deployed network infrastructure while guaranteeing the target QoS perceived by
users.

</details>


### [324] [SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform](https://arxiv.org/abs/2507.15659)
*Gabriel Paradzik,Benjamin Steinert,Heinrich Abele,Michael Menth*

Main category: cs.NI

TL;DR: 本文介绍了一种经济高效的分布式流监控平台，用于收集未采样的IPFIX数据，仅使用开源工具。


<details>
  <summary>Details</summary>
Motivation: 解决传统流监控工具成本高且依赖专有软件的问题。

Method: 利用开源工具构建分布式平台，详细介绍了工具的使用方法。

Result: 成功在蒂宾根大学实现了该平台，能够高效收集IPFIX数据。

Conclusion: 该平台证明了开源工具在流监控中的可行性和成本效益。

Abstract: This paper presents a cost-effective and distributed flow monitoring platform
for collecting unsampled IPFIX data exclusively using open-source tools, which
is implemented at the University of T\"ubingen. An overview of all tools is
given and their use is explained.

</details>


### [325] [Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks](https://arxiv.org/abs/2507.15670)
*Rosario Patanè,Nadjib Achir,Andrea Araldo,Lila Boukhatem*

Main category: cs.NI

TL;DR: 本文探讨了车载云计算（VCC）能否替代边缘计算（EC）支持低延迟应用，并通过模拟评估了可行性条件。


<details>
  <summary>Details</summary>
Motivation: 边缘计算部署成本高，而车载云计算利用闲置车辆资源，可能降低成本。本文首次系统研究VCC替代EC的可行性。

Method: 通过模拟评估负载、车辆移动性、密度和可用性等因素，使用SUMO模拟车辆移动，NS3 5G-LENA模拟通信。

Result: 研究发现VCC在大多数情况下可替代EC，但极端低延迟（<16 ms）仍需EC。

Conclusion: VCC在多数场景下可替代EC，为网络运营商节省成本，但需注意极端低延迟需求。

Abstract: Edge Computing (EC) is a computational paradigm that involves deploying
resources such as CPUs and GPUs near end-users, enabling low-latency
applications like augmented reality and real-time gaming. However, deploying
and maintaining a vast network of EC nodes is costly, which can explain its
limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)
has emerged and inspired interest among researchers and industry. VCC
opportunistically utilizes existing and idle vehicular computational resources
for external task offloading. This work is the first to systematically address
the following question: Can VCC replace EC for low-latency applications?
Answering this question is highly relevant for Network Operators (NOs), as VCC
could eliminate costs associated with EC given that it requires no
infrastructural investment. Despite its potential, no systematic study has yet
explored the conditions under which VCC can effectively support low-latency
applications without relying on EC. This work aims to fill that gap. Extensive
simulations allow for assessing the crucial scenario factors that determine
when this EC-to-VCC substitution is feasible. Considered factors are load,
vehicles mobility and density, and availability. Potential for substitution is
assessed based on multiple criteria, such as latency, task completion success,
and cost. Vehicle mobility is simulated in SUMO, and communication in NS3
5G-LENA. The findings show that VCC can effectively replace EC for low-latency
applications, except in extreme cases when the EC is still required (latency <
16 ms).

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [326] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: 本文介绍了NPUEval，一个用于编写和评估NPU内核的基准测试，包含102个常见机器学习算子，评估了多种LLM生成代码的功能正确性和向量化效率。


<details>
  <summary>Details</summary>
Motivation: 由于NPU编程较新且开发者社区分散，LLM在生成优化代码时缺乏足够的训练数据，因此需要建立一个基准测试以推动研究。

Method: 使用开源编译器工具在AMD NPU上评估LLM生成的代码，包括功能正确性和向量化效率，并测试了多种前沿LLM模型。

Result: 最新推理模型如DeepSeek R1在某些内核上表现良好（50%+向量化），但整体平均得分仅为10%，显示任务具有挑战性。

Conclusion: NPUEval数据集和评估代码将开源，为代码生成和NPU内核优化研究提供重要基准。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [327] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种名为Timetide的多时钟同步语言，解决了分布式系统中确定性编程的挑战，无需物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 传统同步语言依赖昂贵的物理时钟同步，难以扩展，分布式系统的确定性编程仍具挑战性。

Method: 开发了一种多时钟语义的同步程序模型，基于逻辑同步模型，避免物理时钟同步。

Result: Timetide是首个既适合分布式又支持形式化验证的多时钟同步语言。

Conclusion: Timetide为分布式系统提供了一种无需物理时钟同步的确定性编程解决方案。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [328] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 该研究提出了一种创新的语音辅助调试插件，通过声音和视觉反馈显著降低认知负荷并提高调试效率。


<details>
  <summary>Details</summary>
Motivation: 传统调试方法依赖视觉堆栈跟踪，对视觉障碍者不友好且效率较低。研究旨在通过多模态反馈（声音和视觉）提升调试的可用性和效率。

Method: 采用全局异常钩子架构，结合pyttsx3文本转语音和Tkinter GUI，实现并行听觉和视觉错误反馈。

Result: 实验显示，该插件减少37%的认知负荷（p<0.01），错误识别速度提高78%，语音延迟低于1.2秒，CPU开销小于18%。

Conclusion: 该插件显著提升了调试的可用性和效率，尤其适合视觉障碍者和教育场景。未来将整合GPT修复建议和多语言翻译，进一步推动听觉调试范式。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [329] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 该论文提出了一种理论框架，结合约束求解方法和FPTaylor的一阶微分特性，用于生成浮点程序的不变量，以减少计算负担，并设计了两种多项式不变量生成算法。


<details>
  <summary>Details</summary>
Motivation: 浮点运算中的舍入误差可能导致程序失败，因此需要确保浮点程序的正确性，考虑浮点误差的影响。

Method: 提出了一种结合FPTaylor的一阶微分特性和约束求解方法的理论框架，并设计了两种多项式不变量生成算法。

Result: 实验结果表明，该算法在时间效率和生成不变量的精度上优于现有方法。

Conclusion: 该框架和算法有效解决了浮点程序不变量生成问题，并在性能和精度上表现优异。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [330] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种多版本化方法，通过生成多个代码变体实现性能可移植性，避免了传统自动调优的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 手动优化线性代数内核以适应不同GPU设备和应用复杂且耗时，而自动调优容易过拟合且需重复调优。

Method: 采用多版本化技术，开发了一个名为'portability tuning'的框架，自动生成性能可移植的多版本代码。

Result: 在CLBlast线性代数库的GEMM内核上评估，性能优于默认内核，接近理论最大性能的90%，且能泛化到新设备。

Conclusion: 多版本化方法能有效实现性能可移植性，无需重复调优，适用于不同设备。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [331] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: 本文介绍了贝叶斯分离逻辑（BaSL），填补了现有分离逻辑无法处理贝叶斯更新的空白，并证明了其能建模贝叶斯编程语言的关键特性。


<details>
  <summary>Details</summary>
Motivation: 现有分离逻辑无法处理贝叶斯编程语言（BPPLs）的核心特性——贝叶斯更新，因此需要一种新的逻辑来填补这一空白。

Method: 通过引入贝叶斯分离逻辑（BaSL），基于Rokhlin-Simmons分解定理证明贝叶斯定理的内部版本，并利用σ-有限测度空间实例化Kripke资源幺半群。

Result: BaSL成功建模了贝叶斯编程语言的关键概念（如贝叶斯更新、条件分布等），并验证了统计模型的属性（如期望值、相关性等）。

Conclusion: BaSL为贝叶斯编程语言提供了语义支持，填补了现有分离逻辑的不足，并展示了其在建模复杂统计模型中的有效性。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [332] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 提出了一种统一的形式化框架，用于分析PLC驱动的系统，整合离散PLC语义、网络通信和连续物理行为，并采用偏序减少技术缓解状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有形式化验证技术通常孤立分析单个PLC程序，忽略了与物理环境和网络通信的交互，难以应对实际工业系统中连续动态和通信延迟的挑战。

Method: 开发了一个统一的形式化框架，结合离散PLC语义、网络通信和连续物理行为，并应用偏序减少技术以减少状态空间。

Result: 框架能够精确分析具有连续动态和网络通信的PLC驱动系统，同时通过偏序减少显著降低状态数量。

Conclusion: 该框架为复杂工业系统的形式化验证提供了有效工具，解决了现有技术的局限性。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [333] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 本文研究了闭包转换与抽象机器之间的关系，提出了一种新的闭包转换正确性证明技术，改进了抽象机器中环境处理的方式，并分析了时间复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨闭包转换与抽象机器中闭包和环境概念的关系，以改进现有方法。

Method: 采用简单的λ-演算和元组作为源语言，研究闭包转换前后的抽象机器，专注于扁平闭包/环境的情况。

Result: 提出了新的闭包转换正确性证明技术，改进了环境处理方式，并证明闭包转换不影响整体时间复杂性。

Conclusion: 闭包转换虽增加初始代码大小，但动态成本降低，整体复杂性保持不变。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [334] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究了代码上下文和提示策略对大型语言模型生成单元测试质量的影响，发现包含文档字符串显著提升代码充分性，链式思维提示策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用生成式AI自动生成单元测试，以提高软件开发效率。

Method: 分析不同代码上下文和提示策略对LLMs生成单元测试的影响。

Result: 包含文档字符串显著提升测试充分性，链式思维提示策略表现最佳，M5模型在突变得分和分支覆盖率上表现最优。

Conclusion: 生成式AI在单元测试生成中具有潜力，链式思维提示和适当代码上下文是关键。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [335] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 论文探讨了如何通过自然语言处理等技术，将非正式需求转化为可验证的形式化规范，以解决软件正确性验证中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 解决从模糊的自然语言需求生成形式化规范的核心挑战，以提升安全关键系统开发的效率和准确性。

Method: 结合自然语言处理（NLP）、本体建模、构件重用和大语言模型（LLMs）等技术，探索自动化和半自动化方法。

Result: 初步综合了相关文献，识别了生成可验证规范中的常见挑战和未来研究方向。

Conclusion: VERIFAI项目展示了通过技术融合解决需求规范生成问题的潜力，为未来研究提供了方向。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [336] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究探讨了软件工程中沟通误解的技术因素，并验证了共享词汇系统对提升文档清晰度和协作效率的显著效果。


<details>
  <summary>Details</summary>
Motivation: 软件工程中沟通不畅导致误解和低效，亟需解决这一问题以提升协作质量。

Method: 采用设计科学研究（DSR）框架，分三阶段：问题识别（主题分析和访谈）、方法设计（基于扎根理论）、实证验证（控制实验）。

Result: 共享词汇系统虽初期增加负担，但显著提升了信息密度、文档清晰度和协作效率。

Conclusion: 研究为改善软件工程沟通实践提供了可行方案，并指出了未来研究方向。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [337] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文研究了代码语言模型中子令牌隐藏表示的合并策略，以减少计算开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统代码分词器输出较长，可能增加计算开销，因此探索合并子令牌表示的策略。

Method: 提出两种策略：基于平均表示和学习的方法，并与六种代码语言模型结合进行实验。

Result: 计算操作减少1%至19%，下游任务性能在代码翻译中提升2.47分，漏洞检测F1下降1.82分。

Conclusion: 合并子令牌表示可提升代码语言模型的计算效率和下游性能。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [338] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 该研究通过多声文献综述，统一了架构退化的定义、原因、指标和修复策略，揭示了其从技术问题到社会技术问题的转变，并指出持续修复研究的不足。


<details>
  <summary>Details</summary>
Motivation: 架构退化影响系统质量和可维护性，但现有研究定义和策略分散，缺乏统一理解。

Method: 对108项研究进行多声文献综述，提取定义、原因、指标、测量方法、工具和修复策略，并构建分类法。

Result: 架构退化已从低层次问题转为社会技术问题，定义了54项指标和31种测量技术，但持续修复工具不足。

Conclusion: 研究呼吁整合指标、工具和修复逻辑，推动可持续架构的主动策略。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [339] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 分析五年内八个行业会议的软件架构趋势，发现Kubernetes、Serverless等技术主导，主要应用于DevOps后期阶段。


<details>
  <summary>Details</summary>
Motivation: 研究云原生、微服务等技术对软件架构的影响，理解行业趋势。

Method: 分析5,677个行业会议演讲，结合大语言模型和专家验证提取技术、用途及上下文。

Result: Kubernetes、Serverless等技术占据主导，形成五个技术社区，支持混合部署。

Conclusion: 少数核心技术主导架构实践，研究需更全面关注架构设计与质量。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [340] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ利用大语言模型（LLMs）解析OpenCV API文档，生成标准化信息并提取参数约束，以系统性测试API，发现17个新bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV作为广泛使用的开源计算机视觉库，其可靠性至关重要，但现有测试方法可能不足以覆盖其复杂性。

Method: VISTAFUZZ通过LLMs解析API文档，提取参数约束和依赖关系，生成测试输入以系统性测试API。

Result: 测试330个API，发现17个新bug，其中10个被确认，5个已修复。

Conclusion: VISTAFUZZ展示了LLMs在文档引导的模糊测试中的潜力，提升了OpenCV的可靠性。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [341] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 论文研究了开源许可证变体在PyPI生态系统中的影响，发现变体导致显著的合规问题，并提出了LV-Parser和LV-Compat工具以提高分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开源许可证变体在现代软件系统中普遍存在且影响重大，但现有工具未能有效处理这些变体，导致许可证分析的效率和效果受限。

Method: 通过实证研究分析PyPI生态系统中的许可证变体，并开发了基于差异分析和大语言模型的LV-Parser工具，以及自动化检测许可证不兼容的LV-Compat流程。

Result: 研究发现许可证变体常见但实质性修改仅占2%，但导致10.7%的下游依赖不兼容。LV-Parser准确率达0.936且计算成本降低30%，LV-Compat检测不兼容包数量是现有方法的5.2倍且精度为0.98。

Conclusion: 该研究填补了许可证变体知识的空白，并为开发者提供了实用工具以应对开源许可证的复杂性。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [342] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）生成Alloy声明式公式的实验，展示了LLMs在从自然语言描述生成完整公式、创建等效公式以及补全公式草图方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 声明式规范在开发安全可靠的软件系统中至关重要，但正确编写规范仍然具有挑战性。

Method: 通过实验评估，使用ChatGPT和DeepSeek两种LLMs，从自然语言或Alloy输入生成完整公式、创建等效公式以及补全公式草图。

Result: 实验结果表明，LLMs在生成完整公式和补全草图方面表现良好，并能枚举多个独特解决方案。

Conclusion: LLMs为规范编写提供了令人兴奋的进展，有望在软件开发中发挥关键作用，提升构建健壮软件的能力。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [343] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出了一种名为'Robin's Rule'的确定性算法，用于高效生成满足Unique-Cause MC/DC的最小测试集，特别适用于Singular Boolean Expressions（SBEs）。


<details>
  <summary>Details</summary>
Motivation: Unique-Cause MC/DC是确保关键系统可靠性和安全性的严格标准，但其高效测试生成方法研究不足，尤其是在SBEs占主导的实际系统中。

Method: 提出'Robin's Rule'算法，直接构造N + 1个测试用例的最小集，无需生成完整真值表，适用于N条件的SBEs。

Result: 通过TCAS-II规范的SBEs验证，算法始终实现100%覆盖率，且比商业工具更高效。

Conclusion: 该方法为安全关键系统验证提供了实用且理论最优的解决方案，兼具严格性和效率。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [344] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本文提出了一种新的方法历史生成工具HistoryFinder，通过构建更准确的地面真值Oracle，显著提升了方法变更历史的准确性和完整性，并在运行时性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法历史生成工具（如CodeShovel和CodeTracker）的评估受限于地面真值Oracle的不准确性，因此需要更可靠的Oracle和改进的工具。

Method: 结合自动分析和专家手动验证，构建了两个新的Oracle（修正的CodeShovel Oracle和HistoryFinder Oracle），并开发了新的工具HistoryFinder。

Result: 在400个方法的评估中，HistoryFinder在精确率、召回率和F1分数上均优于其他工具，且运行时性能具有竞争力。

Conclusion: HistoryFinder在准确性和效率上均表现最佳，适合需要高精度和高效性能的软件工程任务。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [345] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文探讨了如何通过超参数调整和提示工程提升Llama 3.1模型在生成领域模型时的准确性，并在医疗数据模型中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在软件工程任务中表现优异，但通用LLMs在领域建模中存在局限性，而微调模型又需要大量计算资源且可能导致灾难性遗忘。

Method: 采用基于搜索的方法对超参数进行调整，并结合提示工程，针对医疗数据模型优化Llama 3.1模型。

Result: 优化后的模型在医疗数据模型中表现显著优于基线模型，并在十个不同应用领域中测试了优化超参数的适用性。

Conclusion: 虽然解决方案并非普遍适用，但超参数调整与提示工程的结合可以显著提升大多数领域模型的生成质量。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [346] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGTs）在不同性别开发者中的使用差异及其对任务表现和认知负荷的影响。


<details>
  <summary>Details</summary>
Motivation: 随着CGTs的普及，其公平性和包容性问题日益突出，但现有研究未充分探讨其对不同用户群体的影响。

Method: 采用混合实验设计，54名参与者按性别均分，完成编程任务（CGT辅助与仅互联网访问），收集认知负荷、任务表现等数据并进行统计分析。

Result: 预期发现性别差异对CGT使用和任务表现的影响。

Conclusion: 研究有望推动CGT设计的公平性和包容性，促进AI工具的伦理发展。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [347] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt框架通过角色提示和PPA优化，使LLM生成高质量Verilog代码，显著降低功耗、面积并提升时序，同时保持功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在硬件设计中主要关注功能正确性，忽略了关键的PPA指标，VeriOpt旨在填补这一空白。

Method: 采用角色提示（如规划师、程序员、评审员）和PPA感知优化，结合多模态反馈（如综合报告、时序图）。

Result: 实验显示功耗降低88%，面积减少76%，时序提升73%，功能正确性达86%。

Conclusion: VeriOpt在AI驱动的硬件设计中填补了功能正确性与质量之间的关键差距，推动了LLM在生产中的可靠应用。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [348] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope通过多视角上下文和结构语义图改进仓库级代码生成，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别仓库语义和结构关系上不足，导致上下文信息不充分。

Method: 构建Repository Structural Semantic Graph (RSSG)，结合四视角上下文和调用链预测，采用结构保持序列化算法。

Result: 在CoderEval和DevEval基准测试中，pass@1分数相对提升36.35%。

Conclusion: RepoScope无需额外训练，高效且通用，显著提升代码生成质量。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [349] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG是一种需求启发和自我审查代理，通过因果效应图（CEGs）和神经符号协作架构，解决自然语言需求的模糊性问题，提升生成式软件开发的效果。


<details>
  <summary>Details</summary>
Motivation: 非专业用户的需求描述通常模糊，导致生成式软件开发困难，现有方法难以表达因果逻辑。

Method: RequireCEG使用特征树分层分析用户叙述，构建自修复的CEGs，并优化Gherkin场景以确保一致性。

Result: 实验表明，该方法覆盖率达87%，多样性提升51.88%。

Conclusion: RequireCEG有效解决了需求模糊性问题，提升了生成式软件开发的效率和准确性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [350] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文介绍了AIDev数据集，首次大规模记录AI编码代理在现实中的操作，支持研究AI与人类开发者的协作。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在软件开发中的实际表现，填补理论与实践的差距。

Method: 收集456,000个由五种AI代理提交的拉取请求数据，分析其操作、接受率和代码复杂性。

Result: AI代理提交速度更快，但接受率较低，代码结构更简单。

Conclusion: AIDev为研究AI原生工作流提供了实证基础，支持未来人机协作的发展。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [351] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 探讨GenAI在汽车软件开发中的应用，重点关注需求处理、合规性和代码生成，并总结了行业调查结果。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发流程长且成本高，GenAI有望减少人工干预和复杂流程的处理。

Method: 研究了LLMs、RAG和VLMs等GenAI技术，并提出了一个通用工作流程。

Result: 总结了GenAI在汽车软件开发中的实际应用和行业调查结果。

Conclusion: GenAI在汽车软件开发中具有潜力，但仍需进一步研究和实践验证。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [352] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 论文探讨了如何利用LLMs在敏捷框架中自动化需求获取，比较了LLMs与人类生成用户故事的质量，并评估了LLMs在语义质量评估中的潜力。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，且高质量需求的制定对软件质量至关重要。当前语义质量评估仍依赖人工，效率低。

Method: 使用10种先进LLMs模拟客户访谈生成用户故事，并与人类（专家和学生）生成的故事进行质量对比，同时探索LLMs在语义质量评估中的应用。

Result: LLMs生成的故事在覆盖率和风格质量上与人类相似，但多样性和创造性较低；LLMs在语义质量评估中表现可靠，可减少人工工作量。

Conclusion: LLMs在需求获取和语义质量评估中具有潜力，但需进一步优化以提升多样性和满足验收标准。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [353] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM是一种新的深度学习框架测试方法，通过融合多种模型测量指标（如错误检测性能、算子组合多样性和执行时间）来优化测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法存在三大局限：无法量化算子组合多样性、忽略执行时间测量、忽视指标间相关性。DLMMM旨在解决这些问题。

Method: DLMMM定量测量模型的错误检测性能、算子组合多样性和执行时间，并基于相关性融合这些指标以实现权衡。此外，设计了多级启发式引导生成测试输入模型。

Result: DLMMM通过融合多指标和启发式引导，提升了深度学习框架的测试效果。

Conclusion: DLMMM为深度学习框架测试提供了更全面和高效的解决方案。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [354] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉国文化对需求工程（RE）活动的影响，旨在避免误解和冲突，并支持IT行业的多样性。


<details>
  <summary>Details</summary>
Motivation: 需求工程是软件开发中互动密集的阶段，而多元化的利益相关者可能因文化差异引发问题。孟加拉国IT行业增长迅速，但文化影响研究不足。

Method: 研究聚焦孟加拉国文化背景下的RE实践，分析文化对RE活动的具体影响。

Result: 研究发现文化因素在RE活动中起重要作用，需进一步理解和应对。

Conclusion: 了解文化影响有助于优化RE实践，促进IT行业的多样性和包容性。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [355] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 本文通过系统映射研究（SMS）探讨了2023年至2025年间需求工程（RE）中人物角色的最新研究趋势，重点关注生成式AI的应用。研究发现AI在人物角色构建和验证中的应用增加，模板化人物角色更受欢迎，验证相关研究比例上升。


<details>
  <summary>Details</summary>
Motivation: 探索需求工程中人物角色的最新研究趋势，特别是生成式AI方法对人物角色构建和验证的影响。

Method: 采用系统映射研究（SMS）方法，分析了2023年4月至2025年4月间的22篇相关文献，涵盖人物角色的表示、构建、验证及其在RE活动中的应用。

Result: 研究发现AI技术在人物角色构建和验证中的应用增多，模板化人物角色更受欢迎，验证相关研究比例上升。

Conclusion: 生成式AI对需求工程中人物角色的构建和验证产生了显著影响，未来研究可进一步探索AI在此领域的潜力。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [356] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个专门为SIMD指令代码生成设计的基准测试，包含136个任务，评估了18个LLM在生成SIMD代码时的表现，发现其性能普遍低于标量代码生成。


<details>
  <summary>Details</summary>
Motivation: SIMD指令编程在性能关键任务中广泛应用，但现有代码生成基准仅关注标量代码，缺乏对SIMD代码生成能力的评估。

Method: 提出SimdBench基准测试，包含136个任务，针对五种代表性SIMD指令集（SSE、AVX、Neon、SVE、RVV），并对18个LLM进行系统评估。

Result: LLM在生成SIMD代码时的pass@k普遍低于标量代码生成，揭示了其在该领域的挑战。

Conclusion: SimdBench为研究社区提供了开源工具，并指出了LLM在SIMD代码生成领域的改进方向。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [357] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC利用AlphaFold的序列到结构建模能力，通过多语言适用的token序列表示代码片段，实现跨语言的代码克隆检测。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法难以捕捉代码语义或依赖语言特定分析器，AlphaCC受AlphaFold启发，利用序列相似性解决这一问题。

Method: AlphaCC将代码转为token序列，构建多序列对齐（MSA）增强上下文理解，采用改进的注意力编码器建模依赖关系，并通过相似度评分和二元分类检测克隆。

Result: 在三个多语言数据集上，AlphaCC表现优于基线方法，尤其在语义克隆检测中展现出强语义理解能力，同时保持高效性。

Conclusion: AlphaCC通过结合AlphaFold的技术，实现了跨语言的高效代码克隆检测，为软件维护和漏洞分析提供了新工具。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [358] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine是一个基于LLM的工作流，用于自动生成漏洞验证测试（PoV），在多语言项目中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告通常缺少验证漏洞的测试用例（PoV），导致修复不彻底或回归问题。生成PoV测试需要复杂的程序分析，现有方法效果有限。

Method: FaultLine通过追踪输入流（从API到漏洞点）、分析分支条件，并利用反馈循环生成PoV测试，不依赖语言特定的分析工具。

Result: 在100个多语言漏洞数据集中，FaultLine成功生成16个PoV测试，比现有技术（CodeAct 2.1）多77%。

Conclusion: 分层推理可提升LLM在PoV测试生成中的表现，但问题仍具挑战性。代码和数据集已开源以促进研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [359] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix是一种基于LLM的自动程序修复方法，通过自动缩减测试输入解决长提示中的关键信息丢失问题，显著提升修复性能。


<details>
  <summary>Details</summary>
Motivation: LLM在长提示中难以保留关键信息，导致测试输入过长时修复性能下降，因此需要一种方法自动缩减测试输入。

Method: 提出ReduceFix，通过LLM生成缩减器自动最小化失败诱导的测试输入，再将缩减后的输入用于指导补丁生成。

Result: 在LFTBench上，ReduceFix平均缩减输入89.1%，修复成功率提升53.8%，且在其他方法中也能显著提升修复率。

Conclusion: 自动缩减失败输入是LLM-based APR的有效补充，显著提升其可扩展性和效果。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [360] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 论文研究了工具代理范式中参数失败的问题，提出了分类和改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索工具代理范式中参数失败的现象，并提出改进建议以提高其可靠性和有效性。

Method: 构建参数失败分类法，通过输入扰动方法分析输入源与失败类别的相关性。

Result: 实验表明参数名称幻觉失败主要源于LLM固有局限，其他失败模式与输入源问题相关。

Conclusion: 提出标准化工具返回格式、改进错误反馈机制和确保参数一致性等建议。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [361] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入隐藏状态栈改进Transformer架构，解决了其无法有效捕获Chomsky层级的问题，并在多个任务中表现优于标准Transformer和其他基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽在人工智能领域取得重大进展，但仍无法有效处理Chomsky层级（如正则表达式或确定性上下文无关文法）。

Method: 受下推自动机启发，StackTrans在Transformer层间引入可微分的隐藏状态栈操作（如压栈和弹栈），保持与现有框架的兼容性。

Result: StackTrans在Chomsky层级和大规模自然语言任务中表现优异，参数规模从360M扩展到7B，其中StackTrans-360M甚至优于参数多2-3倍的开源大模型。

Conclusion: StackTrans通过栈操作显著提升了Transformer的推理能力，展示了其高效性和扩展潜力。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [362] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 论文提出了一种基于“中国墙”技术的改进方法，通过强模型指导弱模型，提升其性能，但实际应用受限于缺乏无版权限制的公共领域训练数据。


<details>
  <summary>Details</summary>
Motivation: 尽管代码大语言模型（Code LLM）在编程环境中广泛应用，但其训练数据集未公开，引发版权问题。现有模型因数据限制性能不足，仅作为概念验证。

Method: 应用“中国墙”技术，利用高质量模型生成详细指令指导弱模型，使其能够完成复杂任务。

Result: 实验显示，该方法使Comma v0.1 1T在CanItEdit基准上性能提升66%，Starcoder2 Instruct提升约20%。

Conclusion: 该技术虽有效，但实际应用受限，因缺乏无版权限制的公共领域训练数据。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [363] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 该研究通过分析Stack Overflow上React相关的问题，探索了常见关键词、错误分类及用户声誉与错误的关系，发现算法错误是最常见问题，中声誉用户贡献最多。


<details>
  <summary>Details</summary>
Motivation: 尽管React在单页应用开发中广受欢迎，但用户面临的具体挑战尚不明确，因此研究旨在填补这一空白。

Method: 采用探索性数据分析方法，研究React相关问题的关键词频率、错误分类及用户声誉与错误的关系。

Result: 结果显示，React问题中最常出现的关键词包括code、link等；算法错误是最常见问题，中声誉用户贡献了55.77%的问题。

Conclusion: 研究结果为React社区提供了早期实施阶段的指导建议，特别是算法问题的解决支持。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [364] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion是一种搜索方法，通过优化超参数和提示结构，减少Stable Diffusion模型的性别和种族偏见，同时降低能耗，保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion模型在社会和环境可持续性方面的负面影响，如性别和种族偏见及高能耗。

Method: 搜索最优超参数和提示结构，以减少偏见和能耗，同时保持图像质量。

Result: SustainDiffusion将性别偏见减少68%，种族偏见减少59%，能耗降低48%，且结果稳定且可推广。

Conclusion: 无需微调或改变模型架构，即可提升文本到图像生成模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [365] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章比较了两种CubeSat卫星电池放电建模方法：等效电路分析和机器学习。目标是选择最适合的方法以提高卫星电源系统的预测和容错能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过建模预测CubeSat卫星电池放电行为，确保轨道设备的故障容错能力。

Method: 分析了CubeSat卫星的轨道数据，包括电压、电流和温度，比较了基于物理定律的等效电路分析和基于数据的机器学习方法。

Result: 等效电路方法透明但灵活性差；机器学习模型更准确且能适应复杂环境。

Conclusion: 机器学习方法更适合CubeSat电池放电建模，因其适应性和准确性优于传统方法。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [366] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一个基于LLM的多代理系统，通过模拟人类审计员学习新错误模式的方式，显著提高了软件错误检测的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具覆盖范围有限，难以适应多样化错误模式，而现有LLM方法在处理复杂错误时仍有不足。

Method: BugScope通过程序切片提取相关检测上下文，并构建定制化检测提示，指导LLM进行准确推理。

Result: 在40个真实错误的数据集上，BugScope达到87.04%的精度和90.00%的召回率，F1分数超过工业工具0.44。

Conclusion: BugScope在实际应用中表现出色，成功检测出141个未知错误，其中78个已被修复，展示了其重要实践价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [367] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLM）在自动程序修复（APR）中的实际效果，通过实验比较了程序员使用和不使用LLM的调试表现，并提出了相关方法论和建议。


<details>
  <summary>Details</summary>
Motivation: 验证LLM在APR中的实际效果，探索程序员如何利用LLM辅助调试，并确保修复的正确性。

Method: 采用随机分组实验，一组程序员使用LLM，另一组不使用，通过程序证明工具验证修复的正确性，并结合目标-查询-度量方法进行分析。

Result: 实验结果与预期不同，揭示了LLM在调试中的实际作用，并提出了7种LLM使用模式及优化建议。

Conclusion: 研究为LLM在APR中的合理应用提供了初步框架和方法论，同时为程序员提供了优化LLM使用的实用建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [368] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文旨在通过实验评估LLM生成的证据简报在内容保真度、易理解性和实用性方面与人工简报的对比。


<details>
  <summary>Details</summary>
Motivation: 证据简报对软件工程行业有实用价值，但其手动制作成本高，阻碍了广泛采用。

Method: 开发基于RAG的LLM工具生成简报，并通过对照实验比较其与人工简报的效果。

Result: 实验结果待报告。

Conclusion: 结论将基于实验结果得出。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [369] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 该论文填补了数据科学中计算笔记本动态开发过程的研究空白，通过工具集收集开发时的代码变更，分析笔记本的开发动态和变更模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在细粒度日志分析，但缺乏对数据科学中计算笔记本开发过程的研究。

Method: 引入工具集收集Jupyter笔记本的代码变更，收集20名开发者的工作数据（2,655个单元格和9,207次执行），分析变更模式。

Result: 发现笔记本开发过程中多为小规模修复和代码迭代，表明笔记本兼具开发和调试功能。

Conclusion: 研究揭示了笔记本的动态开发特性，提出了未来研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [370] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了自动化测试预言（test oracle）的生成方法，利用Java库的Javadoc和大型语言模型（LLMs）来生成高质量的测试预言。


<details>
  <summary>Details</summary>
Motivation: 测试预言自动化是一个较少探索的领域，而Javadoc提供了丰富的预期行为信息，可用于自动化生成测试预言。

Method: 利用大型语言模型从Javadoc中提取信息，生成测试预言，并验证其编译性和准确性。

Result: 实验表明，98.8%的预言可编译，96.4%准确反映预期行为，错误可通过LLM生成的额外注释轻松修正。

Conclusion: Javadoc结合LLM是自动化生成高质量测试预言的有效方法。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [371] [Leveraging Covariates in Regression Discontinuity Designs](https://arxiv.org/abs/2507.14311)
*Matias D. Cattaneo,Filippo Palomba*

Main category: econ.EM

TL;DR: 论文探讨了在断点回归设计中如何有效利用协变量，以提高估计效率、研究异质性政策效应及改变关注参数。


<details>
  <summary>Details</summary>
Motivation: 在实证经济学中，协变量的引入对断点回归设计至关重要，但其作用尚未被充分理解。

Method: 通过局部最小二乘回归实现协变量调整，分析其三种主要作用。

Result: 协变量调整能显著提升断点回归的估计效率，揭示政策效应的异质性，并改变研究参数。

Conclusion: 有效利用协变量可以优化断点回归设计的分析结果，需根据研究目标选择合适的调整方法。

Abstract: It is common practice to incorporate additional covariates in empirical
economics. In the context of Regression Discontinuity (RD) designs, covariate
adjustment plays multiple roles, making it essential to understand its impact
on analysis and conclusions. Typically implemented via local least squares
regressions, covariate adjustment can serve three main distinct purposes: (i)
improving the efficiency of RD average causal effect estimators, (ii) learning
about heterogeneous RD policy effects, and (iii) changing the RD parameter of
interest. This article discusses and illustrates empirically how to leverage
covariates effectively in RD designs.

</details>


### [372] [A New Perspective of the Meese-Rogoff Puzzle: Application of Sparse Dynamic Shrinkage](https://arxiv.org/abs/2507.14408)
*Zheng Fan,Worapree Maneesoonthorn,Yong Song*

Main category: econ.EM

TL;DR: 论文提出MSDSP模型，改进了DSP模型，用于解决Meese-Rogoff难题，通过灵活的参数变化提升经济模型的预测能力。


<details>
  <summary>Details</summary>
Motivation: 解决Meese-Rogoff难题，即经济模型在汇率预测中表现不如随机游走模型的问题。

Method: 提出MSDSP模型，支持稀疏性、动态收缩和参数突变，并应用于贝叶斯预测合成（BPS）。

Result: MSDSP增强的经济模型在预测分布上优于随机游走模型，即使考虑随机波动性。

Conclusion: MSDSP为Meese-Rogoff难题提供了新视角，展示了经济模型在灵活参数下的预测潜力。

Abstract: We propose the Markov Switching Dynamic Shrinkage process (MSDSP), nesting
the Dynamic Shrinkage Process (DSP) of Kowal et al. (2019). We revisit the
Meese-Rogoff puzzle (Meese and Rogoff, 1983a,b, 1988) by applying the MSDSP to
the economic models deemed inferior to the random walk model for exchange rate
predictions. The flexibility of the MSDSP model captures the possibility of
zero coefficients (sparsity), constant coefficient (dynamic shrinkage), as well
as sudden and gradual parameter movements (structural change) in the
time-varying parameter model setting. We also apply MSDSP in the context of
Bayesian predictive synthesis (BPS) (McAlinn and West, 2019), where dynamic
combination schemes exploit the information from the alternative economic
models. Our analysis provide a new perspective to the Meese-Rogoff puzzle,
illustrating that the economic models, enhanced with the parameter flexibility
of the MSDSP, produce predictive distributions that are superior to the random
walk model, even when stochastic volatility is considered.

</details>


### [373] [Testing Clustered Equal Predictive Ability with Unknown Clusters](https://arxiv.org/abs/2507.14621)
*Oguzhan Akgun,Alain Pirotte,Giovanni Urga,Zhenlin Yang*

Main category: econ.EM

TL;DR: 本文提出了一种新的聚类等预测能力（C-EPA）测试方法，适用于聚类未知的面板数据，通过Panel Kmeans算法估计聚类结构，并采用选择性条件推断框架解决数据驱动聚类估计的假设检验问题。


<details>
  <summary>Details</summary>
Motivation: 在面板数据中，聚类结构通常未知且难以直接观测，传统方法依赖时间平均观测值，无法充分利用时间序列信息。本文旨在开发一种更有效的聚类估计和假设检验方法。

Method: 使用Panel Kmeans算法估计聚类结构，该算法利用时间序列变化而非时间平均观测值。提出基于选择性条件推断的Wald型检验统计量，并证明其条件分布为截断χ分布。

Result: 蒙特卡洛模拟显示该方法在有限样本中表现优异。实证应用表明，该方法在预测汇率时优于传统时间序列模型和机器学习方法。

Conclusion: 本文提出的C-EPA测试方法在理论和实证中均表现出色，为面板数据中的聚类分析和预测能力评估提供了有效工具。

Abstract: We develop new tests of clustered equal predictive ability (C-EPA) in panels
where the clusters are unknown and estimated by a Panel Kmeans algorithm. This
algorithm differs from the standard Kmeans algorithm by employing the time
series variation of the panel rather than relying merely on time averages of
observations. To address the challenge of testing hypotheses that depend on
data-driven cluster estimates, we adopt a selective conditional inference
framework. Specifically, we derive a Wald-type test statistic for pairwise
equality and show that the limiting distribution of its square root conditional
on the estimated cluster structure is that of a truncated $\chi$ random
variable. We characterize the associated truncation set as a polyhedron in the
data space. As a test of the C-EPA hypothesis, we propose a $p$-value
combination method which aggregates the evidence against the pairwise equality
and overall EPA null hypotheses. In addition, we prove that using an
information criterion to select the unknown number of clusters under the
alternative hypothesis prior to testing does not require further conditioning
to obtain a valid test. Monte Carlo simulations confirm the excellent finite
sample performance of the proposed tests. An empirical application to
forecasting exchange rates using traditional time series models as well as
machine learning methods illustrates the practical importance of our procedure.

</details>


### [374] [Volatility Spillovers and Interconnectedness in OPEC Oil Markets: A Network-Based log-ARCH Approach](https://arxiv.org/abs/2507.15046)
*Fayçal Djebari,Kahina Mehidi,Khelifa Mazouz,Philipp Otto*

Main category: econ.EM

TL;DR: 本文研究了基于网络的石油价格波动模型，通过将新颖的网络结构嵌入ARCH类模型，捕捉OPEC石油出口国之间的溢出效应。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解OPEC国家之间的波动溢出效应及其对金融风险评估和市场整合的影响。

Method: 方法包括基于网络的log-ARCH框架，结合时间序列聚类和模型隐含距离构建权重矩阵，并比较不同GARCH模型的波动传递效果。

Result: 结果显示网络模型在预测性能上与传统模型相当，并揭示了复杂的溢出效应。

Conclusion: 结论表明网络模型能更好地理解OPEC网络的互联性，对金融风险和政策协调有重要意义。

Abstract: This paper examines several network-based volatility models for oil prices,
capturing spillovers among OPEC oil-exporting countries by embedding novel
network structures into ARCH-type models. We apply a network-based log-ARCH
framework that incorporates weight matrices derived from time-series clustering
and model-implied distances into the conditional variance equation. These
weight matrices are constructed from return data and standard multivariate
GARCH model outputs (CCC, DCC, and GO-GARCH), enabling a comparative analysis
of volatility transmission across specifications. Through a rolling-window
forecast evaluation, the network-based models demonstrate competitive
forecasting performance relative to traditional specifications and uncover
intricate spillover effects. These results provide a deeper understanding of
the interconnectedness within the OPEC network, with important implications for
financial risk assessment, market integration, and coordinated policy among
oil-producing economies.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [375] [The Electoral Consequences of Natural Disasters: A Dynamic Fixed-Effects Analysis](https://arxiv.org/abs/2507.14331)
*Nima Taheri Hosseinkhani*

Main category: econ.GN

TL;DR: 本文研究了自然灾害对现任市长选举结果的影响，发现灾害时机对选举结果有显著影响，同时灾害会降低选民投票率，并增加现任官员连任的意愿。


<details>
  <summary>Details</summary>
Motivation: 随着自然灾害频发，理解其对政治后果的影响对民主问责至关重要。现有研究对选民如何反应灾害存在分歧。

Method: 利用1989-2021年美国市长选举的10,000多个候选观察数据，结合灾害事件记录，采用动态双向固定效应事件研究设计。

Result: 选举季发生的灾害显著提高现任市长得票率6个百分点，但降低投票率1.4个百分点。灾害管理经验使现任官员连任意愿增加12个百分点。

Conclusion: 研究调和了回顾性投票理论的矛盾，揭示了灾害管理不仅是治理考验，也是政治野心的催化剂。

Abstract: With the increasing frequency of major natural disasters, understanding their
political consequences is of paramount importance for democratic
accountability. The existing literature is deeply divided, with some studies
finding that voters punish incumbents for disaster-related damages, while
others find they reward them for relief efforts. This paper investigates the
electoral consequences of natural disasters for incumbent mayors, broader
electoral dynamics, and the long-term political ambition of officeholders. The
study leverages a comprehensive panel dataset of over 10,000 candidate-election
observations in U.S. mayoral races from 1989 to 2021, combining detailed
election data with a global registry of disaster events. To identify causal
effects, the analysis employs a robust dynamic two-way fixed-effects
event-study design, validated by extensive pre-trend and placebo tests. The
findings reveal that the electoral impact of disasters is highly conditional on
their timing. A disaster that strikes in the same quarter as an election
provides a significant electoral boost to incumbents, increasing their vote
share by over 6 percentage points. However, disasters consistently suppress
voter turnout, reducing it by an average of 1.4 percentage points. In a novel
finding, the analysis demonstrates that the experience of managing a disaster
significantly increases an incumbent's likelihood of seeking re-election in the
subsequent cycle by as much as 12 percentage points. These findings help
reconcile conflicting theories of retrospective voting by highlighting the
critical role of voter myopia and salience. They also reveal a previously
undocumented channel through which crises shape political careers, suggesting
that disaster management is not only a test of governance but also a catalyst
for political ambition. [The current version is a preprint.]

</details>


### [376] [The effects of temperature and rainfall anomalies on Mexican inflation](https://arxiv.org/abs/2507.14420)
*Arango-Castillo Lenin,Martínez-Ramírez Francisco*

Main category: econ.GN

TL;DR: 研究通过区域面板分析了温度和降水异常对墨西哥通胀的影响，发现短期内气候变量无显著影响，但长期降水异常对通胀有显著正向影响。


<details>
  <summary>Details</summary>
Motivation: 探讨气候冲击（温度和降水异常）对墨西哥通胀的长期和短期影响。

Method: 使用面板自回归分布滞后模型（panel ARDL）和面板局部投影法估计气候冲击的长短期效应。

Result: 短期内气候变量无显著影响；长期降水异常对通胀有显著正向影响，温度异常无显著影响。

Conclusion: 降水异常是墨西哥长期通胀的重要驱动因素，而温度异常影响不显著。

Abstract: This paper measures the effects of temperature and precipitation shocks on
Mexican inflation using a regional panel. To measure the long-term inflationary
effects of climate shocks, we estimate a panel autoregressive distributed lag
model (panel ARDL) of the quarterly variation of the price index against the
population-weighted temperature and precipitation deviations from their
historical norm, computed using the 30-year moving average. In addition, we
measure the short-term effects of climate shocks by estimating impulse response
functions using panel local projections. The result indicates that, in the
short term, the climate variables have no statistical effect on Mexican
inflation. However, in the long term, only precipitation norms have a
statistical effect, and the temperature norms have no statistical impact.
Higher than normal precipitation has a positive and statistically significant
effect on Mexican inflation for all items.

</details>


### [377] [Does Private Equity Hurt or Improve Healthcare Value? New Evidence and Mechanisms](https://arxiv.org/abs/2507.14717)
*Minghong Yuan,Wen Wen,Indranil Bardhan*

Main category: econ.GN

TL;DR: PE投资降低医疗价值，但健康信息技术（IT）能缓解负面影响，尤其是医院与门诊间的信息共享。


<details>
  <summary>Details</summary>
Motivation: 研究PE投资对医疗价值的影响及IT的调节作用，为政策制定者、医疗服务提供者和患者提供参考。

Method: 采用交错差分法，分析2008-2020年美国医院数据。

Result: PE投资降低医疗价值，但IT信息共享（尤其是医院与门诊间）能提升成本效率和护理质量。

Conclusion: 政策需推动IT信息共享，以协调PE投资与价值医疗目标。

Abstract: What is the impact of private equity (PE) investment on healthcare value?
Does PE investment hurt or improve healthcare value, and if so, can its effect
be mitigated through the use of health information technologies (IT)? Given the
significant investments by PE firms in the healthcare sector in recent years,
these are important research questions. Stakeholders, including policy makers,
care providers, and patients, need to understand their likely impact and
whether PE ownership is aligned with their interests. Using a staggered
difference-in-differences approach and data from US hospitals from 2008-2020,
we observe that the overall value of healthcare delivered by hospitals declines
after PE investment. However, our empirical evidence reveals that IT-enabled,
health information sharing plays an important moderating role. Hospitals with
stronger information-sharing capabilities exhibit greater cost efficiencies and
improvements in care quality, leading to higher healthcare value after PE
investment. Furthermore, we find that the type of health information sharing
matters. Specifically, we observe that improvements in care quality are
primarily driven by information sharing between hospitals and ambulatory care
providers, instead of simply hospital-to-hospital sharing of patient health
data. Our research also identifies the underlying mechanisms through which
health information sharing improves care value by reducing hospital-acquired
infections and readmission rates, thereby improving care quality, and enhancing
labor productivity by reducing operating costs. Our results highlight the
critical role of policies and common data standards needed to promote
IT-enabled information sharing between healthcare providers, which, in turn,
can align incentives of PE firms with the goals of value-based care.

</details>


### [378] [Mitigating Financial Frictions in Agriculture: A Framework for Stablecoin Adoption](https://arxiv.org/abs/2507.14970)
*Xinyu Li*

Main category: econ.GN

TL;DR: 论文探讨了法币抵押稳定币如何解决农业金融摩擦，提升生产力和福利。


<details>
  <summary>Details</summary>
Motivation: 全球农业长期受价格波动、信贷受限和供应链低效等金融摩擦困扰，影响生产力和福利。

Method: 构建农场级利润最大化模型，结合交易成本和信贷约束，分析稳定币的作用。

Result: 稳定币能降低跨境贸易成本、提升供应链金融效率、扩大小农户信贷。

Conclusion: 稳定币虽非万能，但潜力巨大，需进一步实证研究和政策支持。

Abstract: Persistent financial frictions - including price volatility, constrained
credit access, and supply chain inefficiencies - have long hindered
productivity and welfare in the global agricultural sector. This paper provides
a theoretical and applied analysis of how fiat-collateralized stablecoins, a
class of digital currency pegged to a stable asset like the U.S. dollar, can
address these long-standing challenges. We develop a farm-level profit
maximization model incorporating transaction costs and credit constraints to
demonstrate how stablecoins can enhance economic outcomes by (1) reducing the
costs and risks of cross-border trade, (2) improving the efficiency and
transparency of supply chain finance through smart contracts, and (3) expanding
access to credit for smallholder farmers. We analyze key use cases, including
parametric insurance and trade finance, while also considering the significant
hurdles to adoption, such as regulatory uncertainty and the digital divide. The
paper concludes that while not a panacea, stablecoins represent a significant
financial technology with the potential to catalyze a paradigm shift in
agricultural economics, warranting further empirical investigation and policy
support.

</details>


### [379] [Equity, Emissions and the Inflation Reduction Act](https://arxiv.org/abs/2507.15054)
*Lucas Woodley,Chung Yi See,Daniel Palmer,Ashley Nunes*

Main category: econ.GN

TL;DR: 低收入家庭更可能购买二手车，但难以负担电动车。2022年《通胀削减法案》为二手车电动车提供购买激励，但研究发现存在资格限制和减排效益未实现的问题。


<details>
  <summary>Details</summary>
Motivation: 低收入家庭因电动车运营成本低而受益，但购买电动车困难。研究探讨法案激励措施的有效性。

Method: 利用美国人口普查局、全国家庭旅行调查和GREET模型数据进行分析。

Result: 低收入家庭中840万可能因购买途径不符而无资格；减排效益可能减少1.139亿吨；购买途径与价格相关，法案激励对高价车更有效。

Conclusion: 法案激励对高价二手车更有效，但需调整以覆盖更多低收入家庭，最大化减排效益。

Abstract: Preowned vehicles are disproportionally purchased by low-income households, a
group that has long been unable to purchase electric vehicles. Yet, low-income
households would disproportionally benefit from EV adoption given the operating
costs savings offered by electrification. To help realize this benefit,
provisions of the 2022 Inflation Reduction Act offer preowned EV purchasing
incentives. How effective might these efforts be. Leveraging data from the
United States Census Bureau, the National Household Travel Survey, and the
Greenhouse gases, Regulated Emissions, and Energy use in Technologies Model, we
address this question. Our findings are fourfold. First, we demonstrate that
although low-income households are more likely to benefit from preowned EV
purchasing incentives offered by IRA, up to 8.4 million low-income households
may be ineligible owing to heterogeneity in vehicle procurement pathways.
Second, we show that program ineligibility risks preventing up to 113.9 million
tons in lifecycle emissions reduction benefits from being realized. Third, we
find that procurement pathways depend on vehicle price. More expensive preowned
vehicles are purchased directly from commercial dealers, while less expensive
preowned vehicles are purchased from private sellers. These procurement
pathways matter because qualification for IRA incentives necessitates
purchasing solely from commercial dealers. Fourth, we demonstrate that while
incentives motivating preowned vehicle purchases from commercial dealers may be
effective if the vehicle is expensive, this effectiveness diminishes at higher
price points. The implications of our findings on decarbonization efforts and
energy policy are discussed.

</details>


### [380] [Enumerating the technological viability and climate impact of jet electrification](https://arxiv.org/abs/2507.15075)
*Megan Yeo,Sebastian Nosenzo,Sichen Shawn Chao,Ashley Nunes*

Main category: econ.GN

TL;DR: 当前电池技术未成熟，短途电动飞行仍受限；小型飞机电动化挑战更大，大型飞机更适合；电动化的区域效益差异显著，欧洲受益最大，亚洲可能增加碳排放。


<details>
  <summary>Details</summary>
Motivation: 探讨电池技术限制下，短途电动飞行的可行性及区域效益差异。

Method: 通过模型估计，分析不同飞机类型和区域的电动化潜力与碳排放影响。

Result: 小型飞机电动化挑战更大，大型飞机更适合；电动化效益欧洲最高，亚洲可能因电网碳强度增加排放。

Conclusion: 电动飞行政策需考虑飞机类型和区域电网碳强度，以实现最佳减排效果。

Abstract: Enabling battery technology has not achieved sufficient maturity to
facilitate electric flight for all aircraft models across all distances.
Consequently, existing discourse emphasizes electrifying short haul routes
using smaller, lighter aircraft. Does this emphasis have merit. We estimate a
model that addresses this question. Our findings are fourfold. First, we find
that current energy density limitations impede short haul electric flight,
regardless of aircraft model utilized. Second, we document that electrifying
smaller, lighter aircraft models serving short haul routes may be particularly
challenging as these aircraft require more, not less, acute increases in energy
density. Third, we identify a subset of larger, heavier aircraft as better
candidates for electrification and note that doing so could prevent the annual
release of significant amounts of carbon dioxide equivalent. However, we
observe that the regional benefits of electrification are highly heterogeneous.
The largest emissions benefit is realized in Europe, followed by South America,
North America, Oceania and Africa. Electrification flights originating in Asia
produces a net increase in carbon emissions owing to the disproportionate share
of miles claimed by Asian countries with a more carbon intensive electrical
grid. Indian emissions warrant scrutiny, as its emissions contribution most
disproportionately exceeds its mileage contribution. The implications of these
findings for decarbonization policy are subsequently discussed.

</details>


### [381] [Human vs. Algorithmic Auditors: The Impact of Entity Type and Ambiguity on Human Dishonesty](https://arxiv.org/abs/2507.15439)
*Marius Protte,Behnud Mir Djawadi*

Main category: econ.GN

TL;DR: 研究探讨了人类在机器与人类审计下的不诚实行为差异，发现透明规则下两者无显著差异，但在模糊条件下机器审计会加剧不诚实行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注算法系统的咨询角色，而机器监控或验证过程中的人类行为研究较少。本文旨在填补这一空白。

Method: 采用激励性实验室实验，通过修改的掷骰范式，比较人类与机器审计及透明与模糊验证规则下的不诚实行为。

Result: 透明规则下，人类与机器审计的不诚实程度无显著差异；模糊条件下，机器审计导致更严重的不诚实行为，表现为完全诚实或最大程度虚报。

Conclusion: 算法不透明性与模糊验证规则可能无意中加剧不诚实行为，对自动化监督系统设计具有重要启示。

Abstract: While most of the existing literature focused on human-machine interactions
with algorithmic systems in advisory roles, research on human behavior in
monitoring or verification processes that are conducted by automated systems
remains largely absent. Our study examines how human dishonesty changes when
detection of untrue statements is performed by machines versus humans, and how
ambiguity in the verification process influences dishonest behavior. We design
an incentivized laboratory experiment using a modified die-roll paradigm where
participants privately observe a random draw and report the result, with higher
reported numbers yielding greater monetary rewards. A probabilistic
verification process introduces risk of detection and punishment, with
treatments varying by verification entity (human vs. machine) and degree of
ambiguity in the verification process (transparent vs. ambiguous). Our results
show that under transparent verification rules, cheating magnitude does not
significantly differ between human and machine auditors. However, under
ambiguous conditions, cheating magnitude is significantly higher when machines
verify participants' reports, reducing the prevalence of partial cheating while
leading to behavioral polarization manifested as either complete honesty or
maximal overreporting. The same applies when comparing reports to a machine
entity under ambiguous and transparent verification rules. These findings
emphasize the behavioral implications of algorithmic opacity in verification
contexts. While machines can serve as effective and cost-efficient auditors
under transparent conditions, their black box nature combined with ambiguous
verification processes may unintentionally incentivize more severe dishonesty.
These insights have practical implications for designing automated oversight
systems in tax audits, compliance, and workplace monitoring.

</details>


### [382] [Explaining Apparently Inaccurate Self-assessments of Relative Performance: A Replication and Adaptation of 'Overconfident: Do you put your money on it?' by Hoelzl and Rustichini (2005)](https://arxiv.org/abs/2507.15568)
*Marius Protte*

Main category: econ.GN

TL;DR: 本研究复制并改进了Hoelzl和Rustichini（2005）的实验，探讨了投票偏好中过度自信的表现，发现传统过度自信模式更符合结果。


<details>
  <summary>Details</summary>
Motivation: 挑战原研究中两种支付方案的可比性，探讨非货币动机和结果结构对参与者选择的影响。

Method: 在线复制实验，比较固定结果分布彩票机制与原研究的概率结果分布彩票机制。

Result: 近四分之三参与者偏好绩效支付方案，结果更符合传统过度自信模式。

Conclusion: 研究重新评估了基于选择的过度自信测量方法，并探讨了观察到的错位效应的替代解释。

Abstract: This study replicates and adapts the experiment of Hoelzl and Rustichini
(2005), which examined overplacement, i.e., overconfidence in relative
self-assessments, by analyzing individuals' voting preferences between a
performance-based and a lottery-based bonus payment mechanism. The original
study found underplacement - the majority of their sample apparently expected
to perform worse than others - in difficult tasks with monetary incentives,
contradicting the widely held assumption of a general human tendency toward
overconfidence. This paper challenges the comparability of the two payment
schemes, arguing that differences in outcome structures and non-monetary
motives may have influenced participants' choices beyond misconfidence. In an
online replication, a fixed-outcome distribution lottery mechanism with
interdependent success probabilities and no variance in the number of winners -
designed to better align with the performance-based payment scheme - is
compared against the probabilistic-outcome distribution lottery used in the
original study, which features an independent success probability and a
variable number of winners. The results align more closely with traditional
overplacement patterns than underplacement, as nearly three-fourths of
participants prefer the performance-based option regardless of lottery design.
Key predictors of voting behavior include expected performance, group
performance estimations, and sample question outcomes, while factors such as
social comparison tendencies and risk attitudes play no significant role.
Self-reported voting rationales highlight the influence of normative beliefs,
control preferences, and feedback signals beyond confidence. These results
contribute to methodological discussions in overconfidence research by
reassessing choice-based overconfidence measures and exploring alternative
explanations for observed misplacement effects.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [383] [Approximate Revenue Maximization for Diffusion Auctions](https://arxiv.org/abs/2507.14470)
*Yifan Huang,Dong Hao,Zhiyi Fan,Yuhang Guo,Bin Li*

Main category: econ.TH

TL;DR: 论文研究了基于保留价的网络拍卖设计，提出了一种简单且接近最优的保留价函数，适用于经济网络中的拍卖，并证明了其激励兼容性和收益近似性。


<details>
  <summary>Details</summary>
Motivation: 现有拍卖设计假设拍卖者能直接接触目标受众，忽略了经济网络中大量潜在竞标者。本文旨在扩展最优拍卖理论至整个经济网络。

Method: 采用贝叶斯近似分析，设计了一种简单且明确的保留价函数，适用于代表性网络拍卖，平衡高收益与吸引更多买家。

Result: 提出的保留价函数在激励兼容性下，使卖家收益接近理论上限，具体表现为1-1/ρ的近似比（ρ为直接邻居数）。

Conclusion: 该保留价函数适用于任意规模和结构的网络市场，为网络拍卖设计提供了实用且高效的解决方案。

Abstract: Reserve prices are widely used in practice. The problem of designing
revenue-optimal auctions based on reserve price has drawn much attention in the
auction design community. Although they have been extensively studied, most
developments rely on the significant assumption that the target audience of the
sale is directly reachable by the auctioneer, while a large portion of bidders
in the economic network unaware of the sale are omitted. This work follows the
diffusion auction design, which aims to extend the target audience of optimal
auction theory to all entities in economic networks. We investigate the design
of simple and provably near-optimal network auctions via reserve price. Using
Bayesian approximation analysis, we provide a simple and explicit form of the
reserve price function tailored to the most representative network auction. We
aim to balance setting a sufficiently high reserve price to induce high revenue
in a successful sale, and attracting more buyers from the network to increase
the probability of a successful sale. This reserve price function preserves
incentive compatibility for network auctions, allowing the seller to extract
additional revenue beyond that achieved by the Myerson optimal auction.
Specifically, if the seller has $\rho$ direct neighbours in a network of size
$n$, this reserve price guarantees a $1-{1 \over \rho}$ approximation to the
theoretical upper bound, i.e., the maximum possible revenue from any network of
size $n$. This result holds for any size and any structure of the networked
market.

</details>


### [384] [Central Bank Digital Currency: Demand Shocks and Optimal Monetary Policy](https://arxiv.org/abs/2507.15048)
*Hanfeng Chen,Maria Elena Filippin*

Main category: econ.TH

TL;DR: 研究央行数字货币（CBDC）对家庭偏好冲击传导及福利的影响，发现CBDC的引入会削弱银行市场力量，降低存款利差，但银行脱媒程度较低。优化CBDC利率规则可带来显著福利提升。


<details>
  <summary>Details</summary>
Motivation: 探讨CBDC在家庭资源分配和银行市场力量背景下的经济影响，特别是其对福利和货币政策传导的作用。

Method: 采用新凯恩斯主义框架，分析CBDC与银行存款的竞争关系，以及银行市场力量的影响。

Result: CBDC的引入具有温和扩张效应，显著降低存款利差，但银行脱媒程度低。优化CBDC利率规则可带来较大福利提升。

Conclusion: CBDC的福利效应取决于其利率规则的优化程度，高收益的CBDC能带来更大的福利提升。

Abstract: We study the implications of a central bank digital currency (CBDC) for the
transmission of household preference shocks and for welfare in a New Keynesian
framework where the CBDC competes with bank deposits for household resources
and banks have market power. We show that an increase in the benefit of CBDC
has a mildly expansionary effect, weakening bank market power and significantly
reducing the deposit spread. As households economize on liquid asset holdings,
they reduce both CBDC and deposit balances. However, the degree of bank
disintermediation is low, as deposit outflows remain modest. We then examine
the welfare implications of CBDC rate setting and find that, compared to a
non-interest-bearing CBDC, the gains with standard coefficients for a CBDC
interest rate Taylor rule are modest, but they become considerable when the
coefficients are optimized. Welfare gains are higher when the CBDC provides a
higher benefit.

</details>


### [385] [Active Product Development](https://arxiv.org/abs/2507.15438)
*Santiago Oliveros*

Main category: econ.TH

TL;DR: 论文提出了一种动态模型，开发者通过增量改进不确定质量的产品，质量演变为受控布朗运动。开发者可选择继续探索、重启或终止项目，最优策略由脉冲控制布朗运动的自由边界描述。


<details>
  <summary>Details</summary>
Motivation: 研究在不确定质量下，开发者如何通过动态决策优化产品改进过程。

Method: 模型将质量演变为受控布朗运动，开发者可选择继续探索、重启或终止项目，最优策略通过脉冲控制布朗运动的自由边界确定。

Result: 模型揭示了在不确定质量下，开发者如何通过动态策略优化产品改进过程。

Conclusion: 动态模型为不确定质量下的产品改进提供了优化策略，适用于实际开发场景。

Abstract: We introduce a dynamic model in which a developer incrementally improves a
product of uncertain quality over time, with the quality evolving as a
controlled Brownian motion. At each moment in time, the developer can continue
exploring by paying a flow cost, restart from a previously attained quality
level by paying a fixed cost, or terminate the process by either freely
abandoning the project or by incurring a cost to launch the highest quality
observed so far. The optimal strategy is characterized by a free boundary of an
impulse-controlled Brownian motion.

</details>


### [386] [Strategic Complexity Promotes Egalitarianism in Legislative Bargaining](https://arxiv.org/abs/2507.15682)
*Marina Agranov,S. Nageeb Ali,B. Douglas Bernheim,Thomas R. Palfrey*

Main category: econ.TH

TL;DR: 论文研究了战略复杂性如何影响立法谈判中识别弱势联盟伙伴的能力，发现复杂性导致提议者转向平等主义，部分因为非提议者更重视公平。


<details>
  <summary>Details</summary>
Motivation: 探讨战略复杂性对识别谈判中弱势伙伴的影响，以及弱势伙伴如何评估自身地位。

Method: 通过实验评估战略复杂性对谈判行为和结果的影响。

Result: 战略复杂性模糊谈判实力时，提议者转向平等主义，非提议者更重视公平；分析能力减弱但未消除此模式。

Conclusion: 战略复杂性促使谈判行为更平等，公平观念在非提议者中起重要作用。

Abstract: Strategic models of legislative bargaining predict that proposers can extract
high shares of economic surplus by identifying and exploiting weak coalition
partners. However, strength and weakness can be difficult to assess even with
relatively simple bargaining protocols. We evaluate experimentally how
strategic complexity affects the ability to identify weak coalition partners,
and for the partners themselves to determine whether their positions are weak
or strong. We find that, as strategic complexity progressively obscures
bargaining strength, proposers migrate to egalitarianism, in significant part
because non-proposers begin placing substantial weight on fairness. Greater
analytic skill dampens but does not eliminate these patterns.

</details>
