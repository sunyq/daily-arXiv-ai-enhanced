{"id": "2507.00094", "categories": ["cs.DB", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.00094", "abs": "https://arxiv.org/abs/2507.00094", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u6570\u636e\u611f\u77e5Declare\u6a21\u578b\u7684\u6700\u4f18\u5bf9\u9f50\uff0c\u7ed3\u5408A*\u641c\u7d22\u548cSMT\u6c42\u89e3\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u4ec5\u652f\u6301\u63a7\u5236\u6d41\u6216\u6709\u9650\u7684\u6570\u636e\u611f\u77e5\u6269\u5c55\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408A*\u641c\u7d22\u548cSMT\u6c42\u89e3\uff0c\u5f15\u5165\u4fee\u590d\u52a8\u4f5c\u9010\u6b65\u89e3\u51b3\u7ea6\u675f\u51b2\u7a81\uff0c\u9ad8\u6548\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u6570\u636e\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u611f\u77e5\u8fc7\u7a0b\u6a21\u578b\u7684\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.00188", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00188", "abs": "https://arxiv.org/abs/2507.00188", "authors": ["Qihan Zhang", "Shaolin Xie", "Ibrahim Sabek"], "title": "LIMAO: A Framework for Lifelong Modular Learned Query Optimization", "comment": "To appear at VLDB 2025 (https://vldb.org/2025/)", "summary": "Query optimizers are crucial for the performance of database systems.\nRecently, many learned query optimizers (LQOs) have demonstrated significant\nperformance improvements over traditional optimizers. However, most of them\noperate under a limited assumption: a static query environment. This limitation\nprevents them from effectively handling complex, dynamic query environments in\nreal-world scenarios. Extensive retraining can lead to the well-known\ncatastrophic forgetting problem, which reduces the LQO generalizability over\ntime. In this paper, we address this limitation and introduce LIMAO (Lifelong\nModular Learned Query Optimizer), a framework for lifelong learning of plan\ncost prediction that can be seamlessly integrated into existing LQOs. LIMAO\nleverages a modular lifelong learning technique, an attention-based neural\nnetwork composition architecture, and an efficient training paradigm designed\nto retain prior knowledge while continuously adapting to new environments. We\nimplement LIMAO in two LQOs, showing that our approach is agnostic to\nunderlying engines. Experimental results show that LIMAO significantly enhances\nthe performance of LQOs, achieving up to a 40% improvement in query execution\ntime and reducing the variance of execution time by up to 60% under dynamic\nworkloads. By leveraging a precise and self-consistent design, LIMAO\neffectively mitigates catastrophic forgetting, ensuring stable and reliable\nplan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup\non selected benchmarks, highlighting its practical advantages in real-world\nquery optimization.", "AI": {"tldr": "LIMAO\u662f\u4e00\u79cd\u7ec8\u8eab\u5b66\u4e60\u7684\u67e5\u8be2\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u67e5\u8be2\u73af\u5883\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u6267\u884c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u578b\u67e5\u8be2\u4f18\u5316\u5668\uff08LQO\uff09\u5728\u52a8\u6001\u67e5\u8be2\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "LIMAO\u91c7\u7528\u6a21\u5757\u5316\u7ec8\u8eab\u5b66\u4e60\u6280\u672f\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\uff0c\u4fdd\u7559\u65e7\u77e5\u8bc6\u5e76\u9002\u5e94\u65b0\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLIMAO\u5c06\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u63d0\u534740%\uff0c\u6267\u884c\u65f6\u95f4\u65b9\u5dee\u964d\u4f4e60%\uff0c\u5e76\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "LIMAO\u663e\u8457\u63d0\u5347\u4e86LQO\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u67e5\u8be2\u4f18\u5316\u573a\u666f\u3002"}}
{"id": "2507.00343", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00343", "abs": "https://arxiv.org/abs/2507.00343", "authors": ["Vishal Chakraborty", "Youri Kaminsky", "Sharad Mehrotra", "Felix Naumann", "Faisal Nawab", "Primal Pappachan", "Mohammad Sadoghi", "Nalini Venkatasubramanian"], "title": "Meaningful Data Erasure in the Presence of Dependencies", "comment": "VLDB 2025 Preprint", "summary": "Data regulations like GDPR require systems to support data erasure but leave\nthe definition of \"erasure\" open to interpretation. This ambiguity makes\ncompliance challenging, especially in databases where data dependencies can\nlead to erased data being inferred from remaining data. We formally define a\nprecise notion of data erasure that ensures any inference about deleted data,\nthrough dependencies, remains bounded to what could have been inferred before\nits insertion. We design erasure mechanisms that enforce this guarantee at\nminimal cost. Additionally, we explore strategies to balance cost and\nthroughput, batch multiple erasures, and proactively compute data retention\ntimes when possible. We demonstrate the practicality and scalability of our\nalgorithms using both real and synthetic datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u7684\u6570\u636e\u5220\u9664\u5b9a\u4e49\uff0c\u786e\u4fdd\u901a\u8fc7\u4f9d\u8d56\u5173\u7cfb\u63a8\u65ad\u5220\u9664\u6570\u636e\u7684\u8303\u56f4\u53d7\u5230\u9650\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4f4e\u6210\u672c\u5b9e\u73b0\u673a\u5236\u3002", "motivation": "GDPR\u7b49\u6cd5\u89c4\u5bf9\u6570\u636e\u5220\u9664\u7684\u5b9a\u4e49\u6a21\u7cca\uff0c\u5bfc\u81f4\u5408\u89c4\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\u590d\u6742\u7684\u6570\u636e\u5e93\u4e2d\u3002", "method": "\u6b63\u5f0f\u5b9a\u4e49\u4e86\u6570\u636e\u5220\u9664\u7684\u7cbe\u786e\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u4f4e\u6210\u672c\u5220\u9664\u673a\u5236\uff0c\u5e76\u63a2\u7d22\u4e86\u6279\u91cf\u5220\u9664\u548c\u9884\u8ba1\u7b97\u6570\u636e\u4fdd\u7559\u65f6\u95f4\u7684\u7b56\u7565\u3002", "result": "\u7b97\u6cd5\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u6570\u636e\u5220\u9664\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u540c\u65f6\u517c\u987e\u4e86\u6210\u672c\u548c\u6548\u7387\u3002"}}
{"id": "2507.00379", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00379", "abs": "https://arxiv.org/abs/2507.00379", "authors": ["Zikai Wang", "Qianxi Zhang", "Baotong Lu", "Qi Chen", "Cheng Tan"], "title": "Towards Robustness: A Critique of Current Vector Database Assessments", "comment": null, "summary": "Vector databases are critical infrastructure in AI systems, and average\nrecall is the dominant metric for their evaluation. Both users and researchers\nrely on it to choose and optimize their systems. We show that relying on\naverage recall is problematic. It hides variability across queries, allowing\nsystems with strong mean performance to underperform significantly on hard\nqueries. These tail cases confuse users and can lead to failure in downstream\napplications such as RAG. We argue that robustness consistently achieving\nacceptable recall across queries is crucial to vector database evaluation. We\npropose Robustness-$\\delta$@K, a new metric that captures the fraction of\nqueries with recall above a threshold $\\delta$. This metric offers a deeper\nview of recall distribution, helps vector index selection regarding application\nneeds, and guides the optimization of tail performance. We integrate\nRobustness-$\\delta$@K into existing benchmarks and evaluate mainstream vector\nindexes, revealing significant robustness differences. More robust vector\nindexes yield better application performance, even with the same average\nrecall. We also identify design factors that influence robustness, providing\nguidance for improving real-world performance.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5e73\u5747\u53ec\u56de\u7387\u4f5c\u4e3a\u5411\u91cf\u6570\u636e\u5e93\u8bc4\u4f30\u7684\u4e3b\u8981\u6307\u6807\u5b58\u5728\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u6307\u6807Robustness-\u03b4@K\u4ee5\u66f4\u5168\u9762\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u5e73\u5747\u53ec\u56de\u7387\u63a9\u76d6\u4e86\u67e5\u8be2\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u5728\u56f0\u96be\u67e5\u8be2\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u63d0\u51faRobustness-\u03b4@K\u6307\u6807\uff0c\u8861\u91cf\u67e5\u8be2\u4e2d\u53ec\u56de\u7387\u8d85\u8fc7\u9608\u503c\u03b4\u7684\u6bd4\u4f8b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u73b0\u6709\u57fa\u51c6\u4e2d\u3002", "result": "\u4e3b\u6d41\u5411\u91cf\u7d22\u5f15\u5728\u7a33\u5065\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7a33\u5065\u6027\u66f4\u9ad8\u7684\u7d22\u5f15\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "Robustness-\u03b4@K\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5411\u91cf\u6570\u636e\u5e93\u6027\u80fd\uff0c\u4e3a\u7d22\u5f15\u9009\u62e9\u548c\u4f18\u5316\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2507.00217", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.00217", "abs": "https://arxiv.org/abs/2507.00217", "authors": ["Tiancheng Chen", "Ales Kubicek", "Langwen Huang", "Torsten Hoefler"], "title": "CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training", "comment": "USENIX ATC '25", "summary": "Training large language models (LLMs) now requires resources that exceed a\nsingle datacenter, making cross-datacenter strategies increasingly crucial. We\npresent CrossPipe, a framework designed to optimize model training across\ngeographically distributed datacenters by explicitly modeling and mitigating\nthe impact of network latency and limited bandwidth. It enables unified\nanalysis and optimization incorporating both pipeline parallelism (PP) and\nopportunities for overlapping data parallelism (DP) communication. CrossPipe\ngenerates optimized pipeline schedules using either solver-based optimal or\nfast near-optimal greedy algorithms, built upon a flexible execution engine\nthat separates scheduling logic from communication details. Our evaluation\nshows that CrossPipe reduces training time by up to 33.6\\% compared to\ntraditional pipeline schedules under identical memory constraints. When memory\nconstraints are relaxed, CrossPipe maintains strong performance despite\ncommunication delays, approaching the efficiency of idealized schedules without\ndelays. CrossPipe offers improved scalability and resource utilization,\nparticularly in environments with high network latency or limited bandwidth.", "AI": {"tldr": "CrossPipe\u662f\u4e00\u4e2a\u4f18\u5316\u8de8\u6570\u636e\u4e2d\u5fc3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u7f51\u7edc\u5ef6\u8fdf\u548c\u5e26\u5bbd\u9650\u5236\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u8d85\u8fc7\u5355\u4e2a\u6570\u636e\u4e2d\u5fc3\uff0c\u8de8\u6570\u636e\u4e2d\u5fc3\u7b56\u7565\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "CrossPipe\u7ed3\u5408\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\u901a\u4fe1\uff0c\u91c7\u7528\u6c42\u89e3\u5668\u4f18\u5316\u6216\u5feb\u901f\u8d2a\u5fc3\u7b97\u6cd5\u751f\u6210\u4f18\u5316\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCrossPipe\u5728\u76f8\u540c\u5185\u5b58\u7ea6\u675f\u4e0b\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u8fbe33.6%\uff0c\u5728\u901a\u4fe1\u5ef6\u8fdf\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "CrossPipe\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.00289", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.00289", "abs": "https://arxiv.org/abs/2507.00289", "authors": ["Ben Deaner", "Soonwoo Kwon"], "title": "Extrapolation in Regression Discontinuity Design Using Comonotonicity", "comment": "48 pages", "summary": "We present a novel approach for extrapolating causal effects away from the\nmargin between treatment and non-treatment in sharp regression discontinuity\ndesigns with multiple covariates. Our methods apply both to settings in which\ntreatment is a function of multiple observables and settings in which treatment\nis determined based on a single running variable. Our key identifying\nassumption is that conditional average treated and untreated potential outcomes\nare comonotonic: covariate values associated with higher average untreated\npotential outcomes are also associated with higher average treated potential\noutcomes. We provide an estimation method based on local linear regression. Our\nestimands are weighted average causal effects, even if comonotonicity fails. We\napply our methods to evaluate counterfactual mandatory summer school policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5177\u6709\u591a\u4e2a\u534f\u53d8\u91cf\u7684\u5c16\u9510\u56de\u5f52\u4e0d\u8fde\u7eed\u8bbe\u8ba1\u4e2d\u63a8\u65ad\u56e0\u679c\u6548\u5e94\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u89c2\u6d4b\u53d8\u91cf\u6216\u5355\u4e00\u8fd0\u884c\u53d8\u91cf\u7684\u60c5\u51b5\u3002", "motivation": "\u89e3\u51b3\u5728\u5c16\u9510\u56de\u5f52\u4e0d\u8fde\u7eed\u8bbe\u8ba1\u4e2d\uff0c\u5982\u4f55\u4ece\u6cbb\u7597\u4e0e\u975e\u6cbb\u7597\u7684\u8fb9\u754c\u5916\u63a8\u56e0\u679c\u6548\u5e94\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5c40\u90e8\u7ebf\u6027\u56de\u5f52\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5047\u8bbe\u6761\u4ef6\u5e73\u5747\u5904\u7406\u548c\u975e\u5904\u7406\u6f5c\u5728\u7ed3\u679c\u662f\u5171\u5355\u8c03\u7684\u3002", "result": "\u5373\u4f7f\u5171\u5355\u8c03\u6027\u4e0d\u6210\u7acb\uff0c\u4f30\u8ba1\u91cf\u4ecd\u4e3a\u52a0\u6743\u5e73\u5747\u56e0\u679c\u6548\u5e94\u3002", "conclusion": "\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u8bc4\u4f30\u53cd\u4e8b\u5b9e\u5f3a\u5236\u6691\u671f\u5b66\u6821\u653f\u7b56\u3002"}}
{"id": "2507.00347", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "AI": {"tldr": "VTS-AI \u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u601d\u7ef4\u7b56\u7565\u7684 AI \u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u62a5\u544a\u4e2d\u63d0\u53d6\u5546\u4e1a\u6d1e\u5bdf\uff0c\u901f\u5ea6\u5feb\u4e14\u7ed3\u679c\u4e30\u5bcc\u3002", "motivation": "\u73b0\u4ee3\u4f01\u4e1a\u9762\u4e34\u5927\u91cf\u975e\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e0d\u654f\u6377\uff0cVTS-AI \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u4e09\u5c42\uff08\u5fae\u3001\u4e2d\u3001\u5b8f\u89c2\uff09\uff0c\u901a\u8fc7\u6807\u8bb0\u95ee\u9898\u3001\u94fe\u63a5\u6765\u6e90\u5e76\u751f\u6210\u53ef\u641c\u7d22\u7684 YAML \u6587\u4ef6\uff0c\u63d0\u53d6\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u50cf\u4e2d\u7684\u6d1e\u5bdf\u3002", "result": "\u5728\u6d4b\u8bd5\u4e2d\uff0cVTS-AI \u901f\u5ea6\u4e0e ChatGPT \u76f8\u5f53\uff0c\u4f46\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u7ed3\u679c\uff08\u5982\u9875\u9762\u4f4d\u7f6e\u3001\u5f15\u7528\u3001\u4e25\u91cd\u6027\u8bc4\u5206\u548c\u56e0\u679c\u94fe\u63a5\uff09\u3002", "conclusion": "VTS-AI \u6709\u671b\u6210\u4e3a\u5feb\u901f\u5546\u4e1a\u5206\u6790\u7684\u751f\u4ea7\u7ea7\u5de5\u5177\uff0c\u672a\u6765\u5c06\u6269\u5c55\u91d1\u878d\u6a21\u578b\u548c\u98ce\u9669\u5c42\u529f\u80fd\u3002"}}
{"id": "2507.00008", "categories": ["cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00008", "abs": "https://arxiv.org/abs/2507.00008", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "AI": {"tldr": "DiMo-GUI\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684GUI\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89c6\u89c9\u5b9a\u4f4d\u548c\u6a21\u6001\u611f\u77e5\u4f18\u5316\u89e3\u51b3GUI\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u591a\u6837\u6027\u548c\u8bed\u8a00\u6b67\u4e49\u95ee\u9898\u3002", "motivation": "GUI\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u591a\u6837\u6027\u548c\u8bed\u8a00\u6b67\u4e49\u4f7f\u5f97\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5c06GUI\u5206\u4e3a\u6587\u672c\u548c\u56fe\u6807\u5143\u7d20\uff0c\u5229\u7528\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u72ec\u7acb\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u805a\u7126\u548c\u5206\u5c42\u7ec6\u5316\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6a21\u6001\u5206\u79bb\u548c\u533a\u57df\u805a\u7126\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "DiMo-GUI\u901a\u8fc7\u52a8\u6001\u548c\u6a21\u6001\u611f\u77e5\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347GUI\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2507.00249", "categories": ["econ.TH", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.00249", "abs": "https://arxiv.org/abs/2507.00249", "authors": ["Nikhil Kumar"], "title": "Endogenous Network Structures with Precision and Dimension Choices", "comment": null, "summary": "This paper presents a social learning model where the network structure is\nendogenously determined by signal precision and dimension choices. Agents not\nonly choose the precision of their signals and what dimension of the state to\nlearn about, but these decisions directly determine the underlying network\nstructure on which social learning occurs. We show that under a fixed network\nstructure, the optimal precision choice is sublinear in the agent's stationary\ninfluence in the network, and this individually optimal choice is worse than\nthe socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network\nstructure, we specify the network by defining a kernel distance between agents,\nwhich then determines how much weight agents place on one another. Agents\nchoose dimensions to learn about such that their choice minimizes the squared\nsum of influences of all agents: a network with equally distributed influence\nacross agents is ideal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u4ea4\u5b66\u4e60\u6a21\u578b\uff0c\u7f51\u7edc\u7ed3\u6784\u7531\u4fe1\u53f7\u7cbe\u5ea6\u548c\u7ef4\u5ea6\u9009\u62e9\u5185\u751f\u51b3\u5b9a\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u56fa\u5b9a\u7f51\u7edc\u7ed3\u6784\u4e0b\uff0c\u4e2a\u4f53\u6700\u4f18\u7cbe\u5ea6\u9009\u62e9\u4f4e\u4e8e\u793e\u4f1a\u6700\u4f18\u9009\u62e9\uff0c\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\u4e0b\uff0c\u7406\u60f3\u7f51\u7edc\u5e94\u5747\u8861\u5206\u914d\u5f71\u54cd\u529b\u3002", "motivation": "\u7814\u7a76\u7f51\u7edc\u7ed3\u6784\u5982\u4f55\u7531\u4fe1\u53f7\u7cbe\u5ea6\u548c\u7ef4\u5ea6\u9009\u62e9\u5185\u751f\u51b3\u5b9a\uff0c\u4ee5\u53ca\u4e2a\u4f53\u4e0e\u793e\u4f1a\u6700\u4f18\u9009\u62e9\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u5206\u6790\u56fa\u5b9a\u548c\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\u4e0b\uff0c\u4ee3\u7406\u4eba\u7684\u4fe1\u53f7\u7cbe\u5ea6\u548c\u7ef4\u5ea6\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u7f51\u7edc\u7ed3\u6784\u548c\u793e\u4f1a\u5b66\u4e60\u3002", "result": "\u56fa\u5b9a\u7f51\u7edc\u4e0b\uff0c\u4e2a\u4f53\u6700\u4f18\u7cbe\u5ea6\u9009\u62e9\u4f4e\u4e8e\u793e\u4f1a\u6700\u4f18\uff1b\u52a8\u6001\u7f51\u7edc\u4e0b\uff0c\u7406\u60f3\u7f51\u7edc\u5e94\u5747\u8861\u5206\u914d\u5f71\u54cd\u529b\u3002", "conclusion": "\u7f51\u7edc\u7ed3\u6784\u7684\u5185\u751f\u6027\u5bf9\u793e\u4ea4\u5b66\u4e60\u6548\u7387\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5747\u8861\u5206\u914d\u5f71\u54cd\u529b\u662f\u7406\u60f3\u72b6\u6001\u3002"}}
{"id": "2507.00152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian M\u00f6ller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u548c\u591a\u6a21\u6001LLM\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u79d1\u5b66\u8868\u683c\u5904\u7406\u4e2d\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u8868\u683c\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46LLM\u5904\u7406\u8868\u683c\u6570\u636e\u7684\u6548\u7387\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u8de8\u9886\u57df\u548c\u8de8\u6a21\u6001\u8bc4\u4f30\uff0c\u6bd4\u8f83LLM\u5728\u4e0d\u540c\u8868\u683c\u7c7b\u578b\uff08\u79d1\u5b66\u4e0e\u975e\u79d1\u5b66\u3001\u56fe\u50cf\u4e0e\u6587\u672c\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "LLM\u5728\u8868\u683c\u6a21\u6001\u95f4\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u5728\u79d1\u5b66\u8868\u683c\u5904\u7406\u4e2d\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "conclusion": "\u63d0\u51fa\u4e86TableEval\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5e76\u6307\u51faLLM\u5728\u79d1\u5b66\u8868\u683c\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.00237", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00237", "abs": "https://arxiv.org/abs/2507.00237", "authors": ["Oleg Kolosov", "David Breitgand", "Dean H. Lorenz", "Gala Yadgar"], "title": "Plan-Based Scalable Online Virtual Network Embedding", "comment": "Accepted to IEEE ICDCS 2025", "summary": "Network virtualization allows hosting applications with diverse computation\nand communication requirements on shared edge infrastructure. Given a set of\nrequests for deploying virtualized applications, the edge provider has to\ndeploy a maximum number of them to the underlying physical network, subject to\ncapacity constraints. This challenge is known as the virtual network embedding\n(VNE) problem: it models applications as virtual networks, where virtual nodes\nrepresent functions and virtual links represent communication between the\nvirtual nodes.\n  All variants of VNE are known to be strongly NP-hard. Because of its\ncentrality to network virtualization, VNE has been extensively studied. We\nfocus on the online variant of VNE, in which deployment requests are not known\nin advance. This reflects the highly skewed and unpredictable demand intrinsic\nto the edge. Unfortunately, existing solutions to online VNE do not scale well\nwith the number of requests per second and the physical topology size.\n  We propose a novel approach in which our new online algorithm, OLIVE,\nleverages a nearly optimal embedding for an aggregated expected demand. This\nembedding is computed offline. It serves as a plan that OLIVE uses as a guide\nfor handling actual individual requests while dynamically compensating for\ndeviations from the plan. We demonstrate that our solution can handle a number\nof requests per second greater by two orders of magnitude than the best results\nreported in the literature. Thus, it is particularly suitable for realistic\nedge environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOLIVE\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u865a\u62df\u7f51\u7edc\u5d4c\u5165\uff08VNE\uff09\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u8bf7\u6c42\u7684\u901f\u5ea6\u548c\u89c4\u6a21\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u865a\u62df\u5316\u5e94\u7528\u7684\u90e8\u7f72\u9700\u6c42\u9ad8\u5ea6\u4e0d\u53ef\u9884\u6d4b\u4e14\u89c4\u6a21\u5e9e\u5927\uff0c\u73b0\u6709\u5728\u7ebfVNE\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u3002", "method": "OLIVE\u7b97\u6cd5\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u7684\u805a\u5408\u9700\u6c42\u5d4c\u5165\u4f5c\u4e3a\u8ba1\u5212\uff0c\u52a8\u6001\u8c03\u6574\u4ee5\u5904\u7406\u5b9e\u9645\u8bf7\u6c42\u3002", "result": "OLIVE\u5904\u7406\u8bf7\u6c42\u7684\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8fb9\u7f18\u73af\u5883\u3002", "conclusion": "OLIVE\u4e3a\u5728\u7ebfVNE\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2507.00033", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00033", "abs": "https://arxiv.org/abs/2507.00033", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cmoment sampling\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5230\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6a21\u578b\u6307\u5bfc\u5e27\u91c7\u6837\uff0c\u63d0\u5347\u957f\u89c6\u9891\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e27\u5b50\u91c7\u6837\u5e38\u4e22\u5931\u5173\u952e\u5e27\u6216\u5f15\u5165\u5197\u4f59\u4fe1\u606f\uff0c\u5f71\u54cd\u95ee\u7b54\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u65f6\u523b\u68c0\u7d22\u6a21\u578b\u4f18\u5148\u9009\u62e9\u4e0e\u95ee\u9898\u6700\u76f8\u5173\u7684\u5e27\u3002", "result": "\u5728\u56db\u4e2a\u957f\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u548c\u56db\u79cd\u5148\u8fdb\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "moment sampling\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.00207", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.00207", "abs": "https://arxiv.org/abs/2507.00207", "authors": ["Md. Al-Amin", "Muneeb Tahir", "Amit Talukder", "Abdullah Al Mamun", "Md Tanjim Hossain", "Nigar Sultana"], "title": "Unraveling Global Threads: Pandemic, Geopolitical Conflict, and Resilience in Fashion and Textile Supply Chain", "comment": "64 pages, 5 figures, 5 tables", "summary": "Several noteworthy scenarios emerged in the global textile and fashion supply\nchains during and after the COVID-19 pandemic. The destabilizing influences of\na global pandemic and a geographically localized conflict are being acutely\nnoticed in the worldwide fashion and textile supply chains. This work examines\nthe impact of the COVID-19 pandemic, the Russo-Ukraine conflict,\nIsrael-Palestine conflict, and Indo-Pak conflict on supply chains within the\ntextile and fashion industry. This research employed a content analysis method\nto identify relevant articles and news from sources such as Google Scholar, the\nSummon database of North Carolina State University, and the scholarly news\nportal NexisUni. The selected papers, news articles, and reports provide a\ncomprehensive overview of the fashion, textile, and apparel supply chain\ndisruptions caused by the pandemic and the war in Ukraine, accompanied by\ndiscussions from common supply chain perspectives. Disruptions due to COVID-19\ninclude international brands and retailers canceling orders, closures of stores\nand factories in developing countries, layoffs, and furloughs of workers in\nboth retail stores and supplier factories, the increased prominence of online\nand e-commerce businesses, the growing importance of automation and\ndigitalization in the fashion supply chain, considerations of sustainability,\nand the need for a resilient supply chain system to facilitate post-pandemic\nrecovery. In the case of the Russo-Ukraine war, Israel-Palestine war, and\nIndo-Pak war, the second-order effects of the conflict have had a more\nsignificant impact on the textile supply chain than the direct military\noperations themselves. In addition to these topics, the study delves into the\npotential strategies for restoring and strengthening the fashion supply chain", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86COVID-19\u5927\u6d41\u884c\u53ca\u591a\u573a\u5730\u533a\u51b2\u7a81\u5bf9\u5168\u7403\u7eba\u7ec7\u548c\u65f6\u5c1a\u4f9b\u5e94\u94fe\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u4f9b\u5e94\u94fe\u4e2d\u65ad\u7684\u8868\u73b0\u53ca\u6062\u590d\u7b56\u7565\u3002", "motivation": "\u5168\u7403\u75ab\u60c5\u548c\u5730\u533a\u51b2\u7a81\u5bf9\u7eba\u7ec7\u548c\u65f6\u5c1a\u4f9b\u5e94\u94fe\u9020\u6210\u4e86\u663e\u8457\u7834\u574f\uff0c\u7814\u7a76\u65e8\u5728\u5206\u6790\u5176\u5f71\u54cd\u5e76\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5185\u5bb9\u5206\u6790\u6cd5\uff0c\u4eceGoogle Scholar\u3001Summon\u6570\u636e\u5e93\u548cNexisUni\u7b49\u6765\u6e90\u7b5b\u9009\u76f8\u5173\u6587\u732e\u548c\u65b0\u95fb\u3002", "result": "\u75ab\u60c5\u5bfc\u81f4\u8ba2\u5355\u53d6\u6d88\u3001\u5de5\u5382\u5173\u95ed\u3001\u5de5\u4eba\u5931\u4e1a\uff0c\u5e76\u52a0\u901f\u4e86\u6570\u5b57\u5316\u548c\u53ef\u6301\u7eed\u53d1\u5c55\uff1b\u5730\u533a\u51b2\u7a81\u7684\u95f4\u63a5\u5f71\u54cd\u6bd4\u76f4\u63a5\u519b\u4e8b\u884c\u52a8\u66f4\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6784\u5efa\u5f39\u6027\u4f9b\u5e94\u94fe\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6062\u590d\u548c\u5f3a\u5316\u4f9b\u5e94\u94fe\u7684\u7b56\u7565\u3002"}}
{"id": "2507.00057", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel B\u00f6hme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e0d\u8fde\u8d2f\u6027\u201d\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u6b63\u786e\u5b9e\u73b0\uff08oracle\uff09\u7684\u60c5\u51b5\u4e0b\u91cf\u5316LLM\u751f\u6210\u4ee3\u7801\u7684\u9519\u8bef\u6982\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u8bc6\u522b\u7ea6\u4e09\u5206\u4e4b\u4e8c\u7684\u9519\u8bef\u7a0b\u5e8f\uff0c\u4e14\u65e0\u5047\u9633\u6027\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u5b58\u5728\u8bed\u6cd5\u6b63\u786e\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u4e00\u79cd\u65e0\u9700oracle\u7684\u9519\u8bef\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u201c\u4e0d\u8fde\u8d2f\u6027\u201d\u5ea6\u91cf\uff0c\u4f5c\u4e3a\u9519\u8bef\u6982\u7387\u7684\u4e0b\u754c\uff0c\u53ef\u9ad8\u6548\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u81ea\u52a8\u8bc6\u522b\u7ea6\u4e09\u5206\u4e4b\u4e8c\u7684\u9519\u8bef\u7a0b\u5e8f\uff0c\u4e14\u4e0eoracle\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u4e0d\u8fde\u8d2f\u6027\u8bc4\u4f30\u53ef\u66ff\u4ee3oracle\u8bc4\u4f30\uff0c\u4e14\u4e0eLLM\u6392\u540d\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2507.00427", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00427", "abs": "https://arxiv.org/abs/2507.00427", "authors": ["Hao Wu", "Changzheng Wei", "Yanhao Wang", "Li Lin", "Yilong Leng", "Shiyu He", "Minghao Zhao", "Hanghang Wu", "Ying Yan", "Aoying Zhou"], "title": "Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition", "comment": null, "summary": "This paper investigates the feasibility of achieving zero-knowledge\nverifiability for graph databases, enabling database owners to\ncryptographically prove the query execution correctness without disclosing the\nunderlying data. Although similar capabilities have been explored for\nrelational databases, their implementation for graph databases presents unique\nchallenges. This is mainly attributed to the relatively large complexity of\nqueries in graph databases. When translating graph queries into arithmetic\ncircuits, the circuit scale can be too large to be practically evaluated. To\naddress this issue, we propose to break down graph queries into more\nfine-grained, primitive operators, enabling a step-by-step evaluation through\nsmaller-scale circuits. Accordingly, the verification with ZKP circuits of\ncomplex graph queries can be decomposed into a series of composable\ncryptographic primitives, each designed to verify a fundamental structural\nproperty such as path ordering or edge directionality. Especially, having\nnoticed that the graph expansion (i.e., traversing from nodes to their\nneighbors along edges) operation serves as the backbone of graph query\nevaluation, we design the expansion centric operator decomposition. In addition\nto constructing circuits for the expansion primitives, we also design\nspecialized ZKP circuits for the various attributes that augment this\ntraversal. The circuits are meticulously designed to take advantage of PLONKish\narithmetization. By integrating these optimized circuits, we implement ZKGraph,\na system that provides verifiable query processing while preserving data\nprivacy. Performance evaluation indicates that ZKGraph significantly\noutperforms naive in circuit implementations of graph operators, achieving\nsubstantial improvements in both runtime and memory consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u6570\u636e\u5e93\u7684\u96f6\u77e5\u8bc6\u53ef\u9a8c\u8bc1\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u67e5\u8be2\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u64cd\u4f5c\u7b26\uff0c\u4f18\u5316\u4e86ZKP\u7535\u8def\u7684\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u56fe\u6570\u636e\u5e93\u7684\u96f6\u77e5\u8bc6\u53ef\u9a8c\u8bc1\u6027\uff0c\u89e3\u51b3\u67e5\u8be2\u6267\u884c\u6b63\u786e\u6027\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u514b\u670d\u56fe\u67e5\u8be2\u590d\u6742\u5ea6\u9ad8\u7684\u6311\u6218\u3002", "method": "\u5c06\u56fe\u67e5\u8be2\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u7b26\uff0c\u8bbe\u8ba1\u57fa\u4e8ePLONKish\u7b97\u672f\u5316\u7684\u4e13\u7528ZKP\u7535\u8def\uff0c\u91cd\u70b9\u4f18\u5316\u56fe\u6269\u5c55\u64cd\u4f5c\u3002", "result": "\u5b9e\u73b0\u4e86ZKGraph\u7cfb\u7edf\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u5747\u6709\u5927\u5e45\u6539\u5584\u3002", "conclusion": "\u901a\u8fc7\u5206\u89e3\u548c\u4f18\u5316ZKP\u7535\u8def\uff0cZKGraph\u4e3a\u56fe\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u67e5\u8be2\u9a8c\u8bc1\u65b9\u6848\u3002"}}
{"id": "2507.00418", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00418", "abs": "https://arxiv.org/abs/2507.00418", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank W\u00fcrthwein"], "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).", "AI": {"tldr": "\u5bf9Qualcomm Cloud AI 100 Ultra\uff08QAic\uff09\u52a0\u901f\u5668\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7684\u80fd\u6548\u548c\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0eNVIDIA\u548cAMD\u7684GPU\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "motivation": "\u8bc4\u4f30QAic\u52a0\u901f\u5668\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u80fd\u6548\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528vLLM\u6846\u67b6\u5bf915\u4e2a\u5f00\u6e90LLM\uff08\u53c2\u6570\u4ece1.17\u4ebf\u5230900\u4ebf\u4e0d\u7b49\uff09\u8fdb\u884c\u63a8\u7406\u6d4b\u8bd5\uff0c\u5e76\u4e0eNVIDIA\uff08A100\u3001H200\uff09\u548cAMD\uff08MI300A\uff09GPU\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "QAic\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u80fd\u6548\uff08\u6bcf\u74e6\u541e\u5410\u91cf\uff09\uff0c\u6027\u80fd\u826f\u597d\u3002", "conclusion": "QAic\u52a0\u901f\u5668\u5728HPC\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.00307", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2507.00307", "abs": "https://arxiv.org/abs/2507.00307", "authors": ["Joseph Fry"], "title": "Robust Inference when Nuisance Parameters may be Partially Identified with Applications to Synthetic Controls", "comment": null, "summary": "When conducting inference for the average treatment effect on the treated\nwith a Synthetic Control Estimator, the vector of control weights is a nuisance\nparameter which is often constrained, high-dimensional, and may be only\npartially identified even when the average treatment effect on the treated is\npoint-identified. All three of these features of a nuisance parameter can lead\nto failure of asymptotic normality for the estimate of the parameter of\ninterest when using standard methods. I provide a new method yielding\nasymptotic normality for an estimate of the parameter of interest, even when\nall three of these complications are present. This is accomplished by first\nestimating the nuisance parameter using a regularization penalty to achieve a\nform of identification, and then estimating the parameter of interest using\nmoment conditions that have been orthogonalized with respect to the nuisance\nparameter. I present high-level sufficient conditions for the estimator and\nverify these conditions in an example involving Synthetic Controls.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5728\u5408\u6210\u63a7\u5236\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u5373\u4f7f\u5b58\u5728\u7ea6\u675f\u3001\u9ad8\u7ef4\u6216\u90e8\u5206\u8bc6\u522b\u7684\u5e72\u6270\u53c2\u6570\u3002", "motivation": "\u89e3\u51b3\u5408\u6210\u63a7\u5236\u4f30\u8ba1\u4e2d\u5e72\u6270\u53c2\u6570\u5bfc\u81f4\u7684\u6e10\u8fd1\u6b63\u6001\u6027\u5931\u6548\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6b63\u5219\u5316\u60e9\u7f5a\u4f30\u8ba1\u5e72\u6270\u53c2\u6570\uff0c\u5e76\u4f7f\u7528\u6b63\u4ea4\u5316\u77e9\u6761\u4ef6\u4f30\u8ba1\u76ee\u6807\u53c2\u6570\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5408\u6210\u63a7\u5236\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u6e10\u8fd1\u6b63\u6001\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e72\u6270\u53c2\u6570\u5e26\u6765\u7684\u6e10\u8fd1\u6b63\u6001\u6027\u95ee\u9898\u3002"}}
{"id": "2507.00352", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AST\u5d4c\u5165\u548cRAG\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347SVRF\u4ee3\u7801\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5728740\u6761DRC\u89c4\u5219\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd540%\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u6280\u672f\u8282\u70b9\u7684\u8fdb\u6b65\uff0c\u4f20\u7edfSVRF\u5f00\u53d1\u65b9\u6cd5\u56e0\u590d\u6742\u8bbe\u8ba1\u89c4\u5219\u800c\u5931\u6548\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u586b\u8865\u6280\u672f\u7a7a\u767d\u3002", "method": "\u91c7\u7528AST\u5d4c\u5165\u8fdb\u884c\u7ed3\u6784\u9a8c\u8bc1\uff0c\u7ed3\u5408RAG\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u51faSVRF\u4e13\u7528\u8bc4\u5206\u6846\u67b6\u3002", "result": "\u5728740\u6761DRC\u89c4\u5219\u6d4b\u8bd5\u4e2d\uff0c\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u63d0\u534740%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86SVRF\u5f00\u53d1\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u751f\u4ea7\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002"}}
{"id": "2507.00041", "categories": ["cs.AI", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00041", "abs": "https://arxiv.org/abs/2507.00041", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "AI": {"tldr": "TalentMine\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u7684\u8868\u683c\u8868\u793a\u89e3\u51b3\u4f20\u7edf\u8868\u683c\u63d0\u53d6\u65b9\u6cd5\u5728\u8bed\u4e49\u5173\u7cfb\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u624d\u7ba1\u7406\u6587\u6863\u7684\u67e5\u8be2\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u63d0\u53d6\u65b9\u6cd5\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u4fe1\u606f\u68c0\u7d22\u548c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5931\u6548\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u8868\u683c\u6570\u636e\u4e2d\u3002", "method": "TalentMine\u91c7\u7528\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\uff0c\u5c06\u63d0\u53d6\u7684\u8868\u683c\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u4fdd\u7559\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTalentMine\u5728\u67e5\u8be2\u4efb\u52a1\u4e2d\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u8fdc\u8d85AWS Textract\u76840%\u548c40%\u3002", "conclusion": "TalentMine\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u7684\u8868\u683c\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u624d\u7ba1\u7406\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00288", "categories": ["econ.TH", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00288", "abs": "https://arxiv.org/abs/2507.00288", "authors": ["Claire Li", "David Freeborn"], "title": "Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context", "comment": "22 pages", "summary": "This study explores how AI-powered digital innovations are reshaping\norganisational accountability in a transnational governance context. As AI\nsystems increasingly mediate decision-making in domains such as auditing and\nfinancial reporting, traditional mechanisms of accountability, based on\ncontrol, transparency, and auditability, are being destabilised. We integrate\nthe Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and\ninstitutional theory to examine how organisations adopt AI technologies in\nresponse to regulatory, ethical, and cultural pressures that transcend national\nboundaries. We argue that accountability is co-constructed within global\nsocio-technical networks, shaped not only by user perceptions but also by\ngovernance logics and normative expectations. Extending TAM, we incorporate\ncompliance and legitimacy as key factors in perceived usefulness and usability.\nDrawing on ANT, we reconceptualise accountability as a relational and emergent\nproperty of networked assemblages. We propose two organisational strategies\nincluding internal governance reconfiguration and external actor-network\nengagement to foster responsible, legitimate, and globally accepted AI adoption\nin the accounting domain.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u9a71\u52a8\u7684\u6570\u5b57\u521b\u65b0\u5982\u4f55\u91cd\u5851\u8de8\u56fd\u6cbb\u7406\u4e2d\u7684\u7ec4\u7ec7\u95ee\u8d23\u5236\uff0c\u7ed3\u5408TAM\u3001ANT\u548c\u5236\u5ea6\u7406\u8bba\u5206\u6790AI\u6280\u672f\u7684\u91c7\u7eb3\u53ca\u5176\u5bf9\u95ee\u8d23\u673a\u5236\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u5ba1\u8ba1\u548c\u8d22\u52a1\u62a5\u544a\u7b49\u9886\u57df\u7684\u51b3\u7b56\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f20\u7edf\u7684\u95ee\u8d23\u673a\u5236\uff08\u5982\u63a7\u5236\u3001\u900f\u660e\u5ea6\u548c\u53ef\u5ba1\u8ba1\u6027\uff09\u53d7\u5230\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u9002\u5e94\u8de8\u56fd\u6cbb\u7406\u73af\u5883\u3002", "method": "\u6574\u5408\u6280\u672f\u63a5\u53d7\u6a21\u578b\uff08TAM\uff09\u3001\u884c\u52a8\u8005\u7f51\u7edc\u7406\u8bba\uff08ANT\uff09\u548c\u5236\u5ea6\u7406\u8bba\uff0c\u5206\u6790\u7ec4\u7ec7\u5728\u8de8\u56fd\u76d1\u7ba1\u3001\u4f26\u7406\u548c\u6587\u5316\u538b\u529b\u4e0b\u91c7\u7eb3AI\u6280\u672f\u7684\u65b9\u5f0f\u3002", "result": "\u95ee\u8d23\u5236\u662f\u5168\u7403\u793e\u4f1a\u6280\u672f\u7f51\u7edc\u4e2d\u5171\u540c\u6784\u5efa\u7684\uff0c\u53d7\u7528\u6237\u611f\u77e5\u3001\u6cbb\u7406\u903b\u8f91\u548c\u89c4\u8303\u671f\u671b\u5f71\u54cd\u3002\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u7ec4\u7ec7\u7b56\u7565\uff1a\u5185\u90e8\u6cbb\u7406\u91cd\u6784\u548c\u5916\u90e8\u884c\u52a8\u8005\u7f51\u7edc\u53c2\u4e0e\u3002", "conclusion": "\u901a\u8fc7\u5185\u90e8\u6cbb\u7406\u8c03\u6574\u548c\u5916\u90e8\u7f51\u7edc\u5408\u4f5c\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u4f1a\u8ba1\u9886\u57df\u8d1f\u8d23\u4efb\u3001\u5408\u6cd5\u4e14\u5168\u7403\u63a5\u53d7\u7684AI\u6280\u672f\u91c7\u7eb3\u3002"}}
{"id": "2507.00163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00163", "abs": "https://arxiv.org/abs/2507.00163", "authors": ["Ari Holtzman", "Chenhao Tan"], "title": "Prompting as Scientific Inquiry", "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "\u63d0\u793a\uff08prompting\uff09\u662f\u7814\u7a76\u548c\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4e5f\u662f\u5176\u6700\u5f3a\u5927\u7684\u5de5\u5177\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u5b83\u5e38\u88ab\u89c6\u4e3a\u201c\u70bc\u91d1\u672f\u201d\u800c\u975e\u79d1\u5b66\u3002\u672c\u6587\u8ba4\u4e3a\u5e94\u5c06\u63d0\u793a\u89c6\u4e3a\u884c\u4e3a\u79d1\u5b66\u7684\u4e00\u90e8\u5206\uff0c\u800c\u975e\u6743\u5b9c\u4e4b\u8ba1\u3002", "motivation": "\u63a2\u8ba8\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u7684\u79d1\u5b66\u5730\u4f4d\uff0c\u53cd\u9a73\u5c06\u5176\u89c6\u4e3a\u201c\u70bc\u91d1\u672f\u201d\u7684\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u4e00\u79cd\u590d\u6742\u4e14\u4e0d\u900f\u660e\u7684\u751f\u7269\u4f53\uff0c\u5c06\u63d0\u793a\u7c7b\u6bd4\u4e3a\u884c\u4e3a\u79d1\u5b66\u7684\u7814\u7a76\u65b9\u6cd5\u3002", "result": "\u63d0\u793a\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u79d1\u5b66\u4e2d\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u800c\u975e\u6b21\u8981\u5de5\u5177\u3002", "conclusion": "\u63d0\u793a\u5e94\u88ab\u89c6\u4e3a\u4e00\u95e8\u79d1\u5b66\uff0c\u662f\u7406\u89e3\u548c\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u65b9\u6cd5\u3002"}}
{"id": "2507.00337", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00337", "abs": "https://arxiv.org/abs/2507.00337", "authors": ["Yuxin Liu", "Tianyang Zhang", "Qiang Wu", "Ju Ren", "Kyle Jamieson", "Yaxiong Xie"], "title": "Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols", "comment": null, "summary": "Delay-based protocols rely on end-to-end delay measurements to detect network\ncongestion. However, in cellular networks, Radio Access Network (RAN) buffers\nintroduce significant delays unrelated to congestion, fundamentally challenging\nthese protocols' assumptions. We identify two major types of RAN buffers -\nretransmission buffers and uplink scheduling buffers - that can introduce\ndelays comparable to congestion-induced delays, severely degrading protocol\nperformance. We present CellNinjia, a software-based system providing real-time\nvisibility into RAN operations, and Gandalf, which leverages this visibility to\nsystematically handle RAN-induced delays. Unlike existing approaches that treat\nthese delays as random noise, Gandalf identifies specific RAN operations and\ncompensates for their effects. Our evaluation in commercial 4G LTE and 5G\nnetworks shows that Gandalf enables substantial performance improvements - up\nto 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols'\ncore algorithms, demonstrating that delay-based protocols can realize their\nfull potential in cellular networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCellNinjia\u548cGandalf\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4bRAN\u64cd\u4f5c\u5e76\u8865\u507f\u5176\u5f15\u5165\u7684\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ef6\u8fdf\u534f\u8bae\u5728\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u8702\u7a9d\u7f51\u7edc\u4e2dRAN\u7f13\u51b2\u533a\u5f15\u5165\u7684\u5ef6\u8fdf\u4e0e\u62e5\u585e\u65e0\u5173\uff0c\u6311\u6218\u4e86\u5ef6\u8fdf\u534f\u8bae\u7684\u5047\u8bbe\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f00\u53d1CellNinjia\u63d0\u4f9bRAN\u64cd\u4f5c\u5b9e\u65f6\u53ef\u89c1\u6027\uff0cGandalf\u5229\u7528\u6b64\u4fe1\u606f\u8bc6\u522b\u5e76\u8865\u507fRAN\u64cd\u4f5c\u5f15\u5165\u7684\u5ef6\u8fdf\u3002", "result": "\u57284G LTE\u548c5G\u7f51\u7edc\u4e2d\uff0cGandalf\u4f7fCopa\u548cPCC Vivace\u6027\u80fd\u5206\u522b\u63d0\u53477.49\u500d\u548c9.53\u500d\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u5904\u7406RAN\u5ef6\u8fdf\uff0c\u5ef6\u8fdf\u534f\u8bae\u5728\u8702\u7a9d\u7f51\u7edc\u4e2d\u53ef\u5b9e\u73b0\u5176\u5168\u90e8\u6f5c\u529b\u3002"}}
{"id": "2507.00042", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00042", "abs": "https://arxiv.org/abs/2507.00042", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "comment": "ICANN 2025", "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "AI": {"tldr": "ER-EMU\u7b97\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u7ecf\u9a8c\u56de\u653e\u548c\u57df\u8ddd\u79bb\u5ea6\u91cf\u9009\u62e9\u5386\u53f2\u6570\u636e\uff0c\u89e3\u51b3\u8fb9\u7f18\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u5468\u671f\u6027\u53d8\u5316\u5bfc\u81f4\u6a21\u578b\u5728\u9002\u5e94\u65b0\u6570\u636e\u5206\u5e03\u65f6\u9057\u5fd8\u65e7\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5386\u53f2\u6570\u636e\u3002", "method": "\u63d0\u51faER-EMU\u7b97\u6cd5\uff0c\u7ed3\u5408FIFO\u7ecf\u9a8c\u7f13\u51b2\u533a\u548cDDM-ES\u7b97\u6cd5\uff0c\u4f7f\u7528MK-MMD\u5ea6\u91cf\u57df\u8ddd\u79bb\uff0c\u4f18\u5148\u9009\u62e9\u4e0e\u5f53\u524d\u57df\u5dee\u5f02\u5927\u7684\u5386\u53f2\u6570\u636e\u3002", "result": "\u5728Bellevue\u4ea4\u901a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0cER-EMU\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u4e91\u8fb9\u534f\u540c\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u7684\u6027\u80fd\u3002", "conclusion": "ER-EMU\u901a\u8fc7\u4f18\u5316\u5386\u53f2\u6570\u636e\u9009\u62e9\u548c\u591a\u6837\u6027\u8bad\u7ec3\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u3002"}}
{"id": "2507.00279", "categories": ["econ.GN", "cs.SI", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.00279", "abs": "https://arxiv.org/abs/2507.00279", "authors": ["Xiao Hui Tai", "Suraj R. Nair", "Shikhar Mehra", "Joshua E. Blumenstock"], "title": "Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan", "comment": null, "summary": "Seasonal migration plays a critical role in stabilizing rural economies and\nsustaining the livelihoods of agricultural households. Violence and civil\nconflict have long been thought to disrupt these labor flows, but this\nhypothesis has historically been hard to test given the lack of reliable data\non migration in conflict zones. Focusing on Afghanistan in the 8-year period\nprior to the Taliban's takeover in 2021, we first demonstrate how satellite\nimagery can be used to infer the timing of the opium harvest, which employs a\nlarge number of seasonal workers in relatively well-paid jobs. We then use a\ndataset of nationwide mobile phone records to characterize the migration\nresponse to this harvest, and examine whether and how violence and civil\nconflict disrupt this migration. We find that, on average, districts with high\nlevels of poppy cultivation receive significantly more seasonal migrants than\ndistricts with no poppy cultivation. These labor flows are surprisingly\nresilient to idiosyncratic violent events at the source or destination,\nincluding extreme violence resulting in large numbers of fatalities. However,\nseasonal migration is affected by longer-term patterns of conflict, such as the\nextent of Taliban control in origin and destination locations.", "AI": {"tldr": "\u5229\u7528\u536b\u661f\u5f71\u50cf\u548c\u624b\u673a\u6570\u636e\u7814\u7a76\u963f\u5bcc\u6c57\u5b63\u8282\u6027\u79fb\u6c11\u5bf9\u9e26\u7247\u6536\u83b7\u7684\u54cd\u5e94\uff0c\u53d1\u73b0\u9ad8\u7f42\u7c9f\u79cd\u690d\u533a\u5438\u5f15\u66f4\u591a\u79fb\u6c11\uff0c\u77ed\u671f\u66b4\u529b\u4e8b\u4ef6\u5f71\u54cd\u6709\u9650\uff0c\u4f46\u957f\u671f\u51b2\u7a81\u6a21\u5f0f\uff08\u5982\u5854\u5229\u73ed\u63a7\u5236\uff09\u663e\u8457\u5f71\u54cd\u79fb\u6c11\u6d41\u52a8\u3002", "motivation": "\u7814\u7a76\u66b4\u529b\u4e0e\u51b2\u7a81\u5982\u4f55\u5f71\u54cd\u5b63\u8282\u6027\u79fb\u6c11\u6d41\u52a8\uff0c\u586b\u8865\u51b2\u7a81\u5730\u533a\u79fb\u6c11\u6570\u636e\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u536b\u661f\u5f71\u50cf\u63a8\u65ad\u9e26\u7247\u6536\u83b7\u65f6\u95f4\uff0c\u5229\u7528\u5168\u56fd\u624b\u673a\u8bb0\u5f55\u5206\u6790\u79fb\u6c11\u6d41\u52a8\uff0c\u8003\u5bdf\u66b4\u529b\u4e8b\u4ef6\u548c\u957f\u671f\u51b2\u7a81\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u7f42\u7c9f\u79cd\u690d\u533a\u5438\u5f15\u66f4\u591a\u79fb\u6c11\uff1b\u77ed\u671f\u66b4\u529b\u4e8b\u4ef6\u5f71\u54cd\u5c0f\uff0c\u957f\u671f\u51b2\u7a81\u6a21\u5f0f\uff08\u5982\u5854\u5229\u73ed\u63a7\u5236\uff09\u663e\u8457\u5f71\u54cd\u79fb\u6c11\u3002", "conclusion": "\u5b63\u8282\u6027\u79fb\u6c11\u5bf9\u77ed\u671f\u66b4\u529b\u4e8b\u4ef6\u5177\u6709\u97e7\u6027\uff0c\u4f46\u957f\u671f\u51b2\u7a81\u6a21\u5f0f\u4f1a\u663e\u8457\u6539\u53d8\u79fb\u6c11\u6d41\u52a8\u3002"}}
{"id": "2507.00264", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility.", "AI": {"tldr": "\u6bd4\u8f83\u7814\u7a76\u8bc4\u4f30\u4e86PyO3\u3001ctypes\u548ccffi\u5728\u6027\u80fd\u548c\u6613\u7528\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0PyO3\u7ed3\u5408Rust\u5de5\u5177\u94fe\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u65e0\u9700\u62c5\u5fc3API\u517c\u5bb9\u6027\u3002", "motivation": "Python\u56e0\u5176\u8bed\u6cd5\u548c\u79d1\u5b66\u5e93\u800c\u53d7\u6b22\u8fce\uff0c\u4f46\u89e3\u91ca\u5668\u901f\u5ea6\u6162\uff0c\u4f18\u5316\u5173\u952e\u90e8\u5206\u9700\u8981\u590d\u6742\u7684\u8de8\u8bed\u8a00\u4ea4\u4e92\u77e5\u8bc6\uff0c\u624b\u52a8\u5b9e\u73b0\u7e41\u7410\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83PyO3\u3001ctypes\u548ccffi\u7684\u6027\u80fd\u548c\u6613\u7528\u6027\uff0c\u8bc4\u4f30Rust\u5de5\u5177\u94fe\u5728Python\u4e2d\u7684\u8868\u73b0\u3002", "result": "PyO3\u7ed3\u5408Rust\u5de5\u5177\u94fe\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u62c5\u5fc3API\u517c\u5bb9\u6027\u95ee\u9898\u3002", "conclusion": "PyO3\u4e3aPython\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00489", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00489", "abs": "https://arxiv.org/abs/2507.00489", "authors": ["Pengyu Chen", "Zizheng Guo", "Jianwei Yang", "Dongjing Miao"], "title": "Towards Efficient Random-Order Enumeration for Join Queries", "comment": null, "summary": "In many data analysis pipelines, a basic and time-consuming process is to\nproduce join results and feed them into downstream tasks. Numerous enumeration\nalgorithms have been developed for this purpose. To be a statistically\nmeaningful representation of the whole join result, the result tuples are\nrequired to be enumerated in uniformly random order. However, existing studies\nlack an efficient random-order enumeration algorithm with a worst-case runtime\nguarantee for (cyclic) join queries. In this paper, we study the problem of\nenumerating the results of a join query in random order. We develop an\nefficient random-order enumeration algorithm for join queries with no large\nhidden constants in its complexity, achieving expected\n$O(\\frac{\\mathrm{AGM}(Q)}{|Res(Q)|}\\log^2|Q|)$ delay,\n$O(\\mathrm{AGM}(Q)\\log|Q|)$ total running time after $O(|Q|\\log|Q|)$-time index\nconstruction, where $|Q|$ is the size of input, $\\mathrm{AGM}(Q)$ is the AGM\nbound, and $|Res(Q)|$ is the size of the join result. We prove that our\nalgorithm is near-optimal in the worst case, under the combinatorial $k$-clique\nhypothesis. Our algorithm requires no query-specific preprocessing and can be\nflexibly adapted to many common database indexes with only minor modifications.\nWe also devise two non-trivial techniques to speed up the enumeration, and\nprovide an experimental study on our enumeration algorithm along with the\nspeed-up techniques. The experimental results show that our algorithm, enhanced\nwith the proposed techniques, significantly outperforms existing\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u968f\u673a\u987a\u5e8f\u679a\u4e3e\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fde\u63a5\u67e5\u8be2\uff0c\u5177\u6709\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6570\u636e\u5206\u6790\u4e2d\uff0c\u8fde\u63a5\u7ed3\u679c\u7684\u968f\u673a\u987a\u5e8f\u679a\u4e3e\u662f\u4e00\u4e2a\u57fa\u7840\u4f46\u8017\u65f6\u7684\u8fc7\u7a0b\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u6548\u4e14\u5177\u6709\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u7684\u7b97\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u968f\u673a\u987a\u5e8f\u679a\u4e3e\u7b97\u6cd5\uff0c\u590d\u6742\u5ea6\u4e3a$O(\\frac{\\mathrm{AGM}(Q)}{|Res(Q)|}\\log^2|Q|)$\u5ef6\u8fdf\u548c$O(\\mathrm{AGM}(Q)\\log|Q|)$\u603b\u8fd0\u884c\u65f6\u95f4\uff0c\u65e0\u9700\u7279\u5b9a\u67e5\u8be2\u9884\u5904\u7406\u3002", "result": "\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u63a5\u8fd1\u6700\u4f18\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u5e93\u7d22\u5f15\uff0c\u4e3a\u8fde\u63a5\u67e5\u8be2\u7684\u968f\u673a\u679a\u4e3e\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00428", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00428", "abs": "https://arxiv.org/abs/2507.00428", "authors": ["Mohammad Firas Sada", "John J. Graham", "Mahidhar Tatineni", "Dmitry Mishin", "Thomas A. DeFanti", "Frank W\u00fcrthwein"], "title": "Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor", "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC25)", "summary": "As machine learning (ML) applications become integral to modern network\noperations, there is an increasing demand for network programmability that\nenables low-latency ML inference for tasks such as Quality of Service (QoS)\nprediction and anomaly detection in cybersecurity. ML models provide\nadaptability through dynamic weight adjustments, making Programming\nProtocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an\nideal platform for investigating In-Network Machine Learning (INML). These\ndevices offer high-throughput, low-latency packet processing and can be\ndynamically reconfigured via the control plane, allowing for flexible\nintegration of ML models directly at the network edge. This paper explores the\napplication of the P4 programming paradigm to neural networks and regression\nmodels, where weights and biases are stored in control plane table lookups.\nThis approach enables flexible programmability and efficient deployment of\nretrainable ML models at the network edge, independent of core infrastructure\nat the switch level.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528P4\u53ef\u7f16\u7a0bFPGA SmartNICs\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u673a\u5668\u5b66\u4e60\u63a8\u7406\uff0c\u4ee5\u652f\u6301\u7f51\u7edc\u8fb9\u7f18\u7684\u7075\u6d3bML\u6a21\u578b\u90e8\u7f72\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u7f51\u7edc\u64cd\u4f5c\u4e2d\u7684\u91cd\u8981\u6027\u589e\u52a0\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684ML\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u652f\u6301QoS\u9884\u6d4b\u548c\u7f51\u7edc\u5b89\u5168\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002", "method": "\u91c7\u7528P4\u7f16\u7a0b\u8303\u5f0f\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u548c\u56de\u5f52\u6a21\u578b\u7684\u6743\u91cd\u4e0e\u504f\u7f6e\u5b58\u50a8\u5728\u63a7\u5236\u5e73\u9762\u8868\u67e5\u627e\u4e2d\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u53ef\u7f16\u7a0b\u6027\u548c\u9ad8\u6548\u7684\u6a21\u578b\u90e8\u7f72\u3002", "result": "\u901a\u8fc7P4-programmable FPGA SmartNICs\uff0c\u80fd\u591f\u5728\u7f51\u7edc\u8fb9\u7f18\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684ML\u63a8\u7406\uff0c\u4e14\u652f\u6301\u52a8\u6001\u91cd\u914d\u7f6e\u3002", "conclusion": "P4\u7f16\u7a0b\u8303\u5f0f\u4e3a\u7f51\u7edc\u8fb9\u7f18\u7684ML\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u72ec\u7acb\u4e8e\u6838\u5fc3\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.00555", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2507.00555", "abs": "https://arxiv.org/abs/2507.00555", "authors": ["Victor Chernozhukov", "Christian B. Hansen", "Lingwei Kong", "Weining Wang"], "title": "Plausible GMM: A Quasi-Bayesian Approach", "comment": null, "summary": "Structural estimation in economics often makes use of models formulated in\nterms of moment conditions. While these moment conditions are generally\nwell-motivated, it is often unknown whether the moment restrictions hold\nexactly. We consider a framework where researchers model their belief about the\npotential degree of misspecification via a prior distribution and adopt a\nquasi-Bayesian approach for performing inference on structural parameters. We\nprovide quasi-posterior concentration results, verify that quasi-posteriors can\nbe used to obtain approximately optimal Bayesian decision rules under the\nmaintained prior structure over misspecification, and provide a form of\nfrequentist coverage results. We illustrate the approach through empirical\nexamples where we obtain informative inference for structural objects allowing\nfor substantial relaxations of the requirement that moment conditions hold\nexactly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u7ecf\u6d4e\u5b66\u7ed3\u6784\u4f30\u8ba1\u4e2d\u77e9\u6761\u4ef6\u53ef\u80fd\u4e0d\u5b8c\u5168\u6210\u7acb\u7684\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u77e9\u6761\u4ef6\u53ef\u80fd\u4e0d\u5b8c\u5168\u6210\u7acb\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u5148\u9a8c\u5206\u5e03\u5efa\u6a21\u5bf9\u9519\u8bef\u8bbe\u5b9a\u7684\u4fe1\u5ff5\u3002", "method": "\u91c7\u7528\u51c6\u8d1d\u53f6\u65af\u65b9\u6cd5\u8fdb\u884c\u7ed3\u6784\u53c2\u6570\u63a8\u65ad\uff0c\u5141\u8bb8\u77e9\u6761\u4ef6\u5b58\u5728\u4e00\u5b9a\u7a0b\u5ea6\u7684\u9519\u8bef\u8bbe\u5b9a\u3002", "result": "\u63d0\u4f9b\u4e86\u51c6\u540e\u9a8c\u96c6\u4e2d\u6027\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u51c6\u540e\u9a8c\u53ef\u7528\u4e8e\u83b7\u5f97\u8fd1\u4f3c\u6700\u4f18\u7684\u8d1d\u53f6\u65af\u51b3\u7b56\u89c4\u5219\uff0c\u5e76\u63d0\u4f9b\u4e86\u9891\u7387\u8986\u76d6\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u8bc1\u4f8b\u5b50\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u653e\u677e\u77e9\u6761\u4ef6\u4e25\u683c\u6210\u7acb\u8981\u6c42\u65f6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.00378", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "AI": {"tldr": "iPanda\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u534f\u8bae\u4e00\u81f4\u6027\u6d4b\u8bd5\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u4ee3\u7801\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u534f\u8bae\u4e00\u81f4\u6027\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u521b\u5efa\u6d4b\u8bd5\u7528\u4f8b\u548c\u811a\u672c\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u8017\u65f6\u3002LLMs\u7684\u6587\u672c\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e3a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "iPanda\u901a\u8fc7\u5173\u952e\u8bcd\u65b9\u6cd5\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u751f\u6210\u53ef\u6267\u884c\u6d4b\u8bd5\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u81ea\u6821\u6b63\u673a\u5236\u4f18\u5316\u4ee3\u7801\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ciPanda\u5728\u6d4b\u8bd5\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\uff08Pass@1\uff09\u4e0a\u6bd4\u7eafLLM\u65b9\u6cd5\u63d0\u53474.675\u81f310.751\u500d\u3002", "conclusion": "iPanda\u6846\u67b6\u4e3a\u534f\u8bae\u4e00\u81f4\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.00048", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00048", "abs": "https://arxiv.org/abs/2507.00048", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "comment": "10 pages, 5 figures", "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFAIR\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u5206\u5e03\u5f0f\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\u52a0\u901f\u79d1\u5b66\u548c\u5de5\u7a0b\u4e2d\u7684\u4f18\u5316\u4efb\u52a1\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u673a\u5668\u5b66\u4e60\u548cFAIR\u6570\u636e\u57fa\u7840\u8bbe\u65bd\uff0c\u4fc3\u8fdb\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u95f4\u7684\u534f\u4f5c\uff0c\u63d0\u9ad8\u5b9e\u9a8c\u6570\u636e\u7684\u53ef\u91cd\u7528\u6027\u548c\u4f18\u5316\u6548\u7387\u3002", "method": "\u4f7f\u7528nanoHUB\u670d\u52a1\u6784\u5efa\u5206\u5e03\u5f0fSDL\u6846\u67b6\uff0c\u652f\u6301\u6570\u636e\u5171\u4eab\u3001\u81ea\u52a8\u5904\u7406\u548c\u4e3b\u52a8\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8eFAIR\u6570\u636e\u7684\u534f\u4f5c\u4f18\u5316\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u5b9e\u9a8c\uff08\u5982\u98df\u54c1\u67d3\u6599\u914d\u65b9\u4f18\u5316\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5de5\u5177\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79d1\u5b66\u548c\u5de5\u7a0b\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86FAIR\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.00397", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2507.00397", "abs": "https://arxiv.org/abs/2507.00397", "authors": ["Jiasong Han", "Xuehan Wang", "Jintao Wang"], "title": "Dynamic SINR-Guided Iterative Interference Cancellation for ODDM Systems in Doubly Dispersive Channels", "comment": "6 pages, 4 figures, submitted to International Conference on\n  Communications in China", "summary": "Orthogonal delay-Doppler division multiplexing (ODDM) modulation has recently\ngained significant attention as a promising candidate to promote the\ncommunication reliability in high-mobility environments. Low complexity signal\ndetection is one of the most significant challenges for ODDM over general\nphysical channels, due to the large channel spreading caused by the fractional\ndelay and Doppler shifts. In this paper, we investigate the low-complexity data\ndetection for ODDM system by utilizing iterative interference cancellation.\nBased on the theoretical analysis of signal to interference plus noise ratio\n(SINR) during the iteration, a dynamic SINR-guided approach is proposed to\nprovide a better initialization result. Specifically, we analyze the SINR of\neach time domain sample before initial estimate with consideration of off-grid\ndelay and Doppler shifts. The iteration is then started from the multi-carrier\nsymbol index which has the best SINR. The corresponding interference is then\neliminated for other time domain samples while the SINR for symbol awaiting\ndetection is also updated. Based on the updated SINR, the next multi-carrier\nsymbol index is selected for the same processing until all data symbols have\nbeen initialized. Finally, we update the SINR synchronously until the end of\nthe initialization. Simulation experiments indicate that our proposed\nalgorithms demonstrate satisfying convergence and error performance while\navoiding the huge complexity introduced by full linear minimum mean squared\nerror (LMMSE) initialization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001SINR\u5f15\u5bfc\u7684\u4f4e\u590d\u6742\u5ea6\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b63\u4ea4\u5ef6\u8fdf-\u591a\u666e\u52d2\u5206\u590d\u7528\uff08ODDM\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e72\u6270\u6d88\u9664\u63d0\u5347\u901a\u4fe1\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u4e2d\uff0cODDM\u8c03\u5236\u9762\u4e34\u7531\u5206\u6570\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u79fb\u5f15\u8d77\u7684\u4fe1\u9053\u6269\u5c55\u95ee\u9898\uff0c\u5bfc\u81f4\u4fe1\u53f7\u68c0\u6d4b\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u57fa\u4e8eSINR\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u52a8\u6001SINR\u5f15\u5bfc\u65b9\u6cd5\uff0c\u4ece\u6700\u4f73SINR\u7684\u591a\u8f7d\u6ce2\u7b26\u53f7\u5f00\u59cb\u8fed\u4ee3\u5e72\u6270\u6d88\u9664\uff0c\u9010\u6b65\u521d\u59cb\u5316\u6240\u6709\u6570\u636e\u7b26\u53f7\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6536\u655b\u6027\u548c\u8bef\u7801\u6027\u80fd\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e14\u907f\u514d\u4e86\u5168\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08LMMSE\uff09\u521d\u59cb\u5316\u7684\u9ad8\u590d\u6742\u5ea6\u3002", "conclusion": "\u52a8\u6001SINR\u5f15\u5bfc\u65b9\u6cd5\u4e3aODDM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u590d\u6742\u5ea6\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2507.00210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00210", "abs": "https://arxiv.org/abs/2507.00210", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han L\u00f9", "Massimo Caccia", "V\u00e9ronique Eglin", "Alexandre Aussem", "J\u00e9r\u00e9my Espinas", "Alexandre Lacoste"], "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLineRetriever\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u548c\u68c0\u7d22\u4e0e\u672a\u6765\u5bfc\u822a\u6b65\u9aa4\u6700\u76f8\u5173\u7684\u89c2\u5bdf\u884c\uff0c\u89e3\u51b3\u4e86\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e2d\u4e0a\u4e0b\u6587\u8fc7\u957f\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\uff08\u5982\u81ea\u4e0b\u800c\u4e0a\u7684\u622a\u65ad\u6216\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\uff09\u5728\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f1a\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5f71\u54cd\u81ea\u9002\u5e94\u89c4\u5212\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u5386\u53f2\u3002", "method": "\u63d0\u51faLineRetriever\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u5e76\u68c0\u7d22\u5bf9\u9884\u6d4b\u672a\u6765\u52a8\u4f5c\u6700\u6709\u5e2e\u52a9\u7684\u89c2\u5bdf\u884c\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLineRetriever\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u6bcf\u4e00\u6b65\u7684\u89c2\u5bdf\u5185\u5bb9\u5927\u5c0f\u3002", "conclusion": "LineRetriever\u4f18\u5316\u4e86\u68c0\u7d22\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81ea\u9002\u5e94\u89c4\u5212\u7684\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2507.00623", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00623", "abs": "https://arxiv.org/abs/2507.00623", "authors": ["Daniel Mej\u00edas", "Inhar Yeregui", "Roberto Viola", "Miguel Fern\u00e1ndez", "Mario Montagud"], "title": "Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols", "comment": null, "summary": "The increasing complexity of Extended Reality (XR) applications demands\nsubstantial processing power and high bandwidth communications, often\nunavailable on lightweight devices. Remote rendering consists of offloading\nprocessing tasks to a remote node with a powerful GPU, delivering the rendered\ncontent to the end device. The delivery is usually performed through popular\nstreaming protocols such as Web Real-Time Communications (WebRTC), offering a\ndata channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH),\nbetter suitable for scalability. Moreover, new streaming protocols based on\nQUIC are emerging as potential replacements for WebRTC and DASH and offer\nbenefits like connection migration, stream multiplexing and multipath delivery.\nThis work describes the integration of the two most popular multimedia\nframeworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote\nRenderer, and analyzes their performance when offering different protocols for\ndelivering the rendered content to the end device over WIFI or 5G. This\nsolution constitutes a beyond state-of-the-art testbed to conduct cutting-edge\nresearch in the XR field.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8fdc\u7a0b\u6e32\u67d3\u89e3\u51b3XR\u5e94\u7528\u5bf9\u9ad8\u6027\u80fd\u8bbe\u5907\u7684\u9700\u6c42\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6d41\u5a92\u4f53\u534f\u8bae\uff08\u5982WebRTC\u3001DASH\u548c\u57fa\u4e8eQUIC\u7684\u65b0\u534f\u8bae\uff09\u5728WiFi\u548c5G\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u968f\u7740XR\u5e94\u7528\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u8f7b\u91cf\u7ea7\u8bbe\u5907\u5f80\u5f80\u7f3a\u4e4f\u8db3\u591f\u7684\u5904\u7406\u80fd\u529b\u548c\u5e26\u5bbd\uff0c\u56e0\u6b64\u9700\u8981\u8fdc\u7a0b\u6e32\u67d3\u6280\u672f\u6765\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6574\u5408\u4e86GStreamer\u548cFFmpeg\u591a\u5a92\u4f53\u6846\u67b6\u4e0e\u8fdc\u7a0b\u6e32\u67d3\u5f15\u64ce\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6d41\u5a92\u4f53\u534f\u8bae\u5728WiFi\u548c5G\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5148\u8fdb\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301XR\u9886\u57df\u7684\u524d\u6cbf\u7814\u7a76\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u4e3aXR\u9886\u57df\u7684\u8fdc\u7a0b\u6e32\u67d3\u548c\u6d41\u5a92\u4f53\u534f\u8bae\u7814\u7a76\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u7684\u6d4b\u8bd5\u73af\u5883\u3002"}}
{"id": "2507.00043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00043", "abs": "https://arxiv.org/abs/2507.00043", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "comment": null, "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "AI": {"tldr": "MR-CLIP\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06MRI\u56fe\u50cf\u4e0eDICOM\u5143\u6570\u636e\u5bf9\u9f50\uff0c\u5b66\u4e60\u5bf9\u6bd4\u611f\u77e5\u8868\u793a\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "MRI\u626b\u63cf\u7684\u5bf9\u6bd4\u5ea6\u4fe1\u606f\u901a\u5e38\u4e0d\u5b8c\u6574\u6216\u7f3a\u5931\uff0c\u5bfc\u81f4\u56fe\u50cf\u89e3\u91ca\u548c\u68c0\u7d22\u56f0\u96be\uff0c\u5f71\u54cd\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51faMR-CLIP\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5c06MRI\u56fe\u50cf\u4e0eDICOM\u5143\u6570\u636e\u5bf9\u9f50\uff0c\u5b66\u4e60\u5bf9\u6bd4\u611f\u77e5\u8868\u793a\u3002", "result": "\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5bf9\u6bd4\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "MR-CLIP\u4e3a\u89e3\u51b3MRI\u5bf9\u6bd4\u5ea6\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.00281", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.00281", "abs": "https://arxiv.org/abs/2507.00281", "authors": ["Mohamed Khalafalla", "Tejal Mulay", "Shonda L Bernadin"], "title": "Factors Influencing Change Orders in Horizontal Construction Projects: A Comparative Analysis of Unit Price and Lump Sum Contracts", "comment": "https://lupinepublishers.com/civil-engineering-journal/pdf/TCEIA.MS.ID.000192.pdf", "summary": "Change orders (COs) are a common occurrence in construction projects, leading\nto increased costs and extended durations. Design-Bid-Build (DBB) projects,\nfavored by state transportation agencies (STAs), often experience a higher\nfrequency of COs compared to other project delivery methods. This study aims to\nidentify areas of improvement to reduce CO frequency in DBB projects through a\nquantitative analysis. Historical bidding data from the Florida Department of\nTransportation (FDOT) was utilized to evaluate five factors, contracting\ntechnique, project location, type of work, project size, and duration, on\nspecific horizontal construction projects. Two DBB contracting techniques, Unit\nPrice (UP) and Lump Sum (LS), were evaluated using a discrete choice model. The\nanalysis of 581 UP and 189 LS projects revealed that project size, duration,\nand type of work had a statistically significant influence on the frequency of\nchange orders at a 95% confidence level. The discrete choice model showed\nsignificant improvement in identifying the appropriate contract type for a\nspecific project compared to traditional methods used by STAs. By evaluating\nthe contracting technique instead of project delivery methods for horizontal\nconstruction projects, the use of DBB can be enhanced, leading to reduced\nchange orders for STAs.", "AI": {"tldr": "\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\uff0c\u7814\u7a76\u53d1\u73b0\u9879\u76ee\u89c4\u6a21\u3001\u5de5\u671f\u548c\u5de5\u4f5c\u7c7b\u578b\u5bf9\u53d8\u66f4\u5355\u9891\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u79bb\u6563\u9009\u62e9\u6a21\u578b\u80fd\u66f4\u51c6\u786e\u5730\u9009\u62e9\u5408\u540c\u7c7b\u578b\uff0c\u4ece\u800c\u51cf\u5c11\u53d8\u66f4\u5355\u3002", "motivation": "\u8bbe\u8ba1-\u62db\u6807-\u5efa\u9020\uff08DBB\uff09\u9879\u76ee\u53d8\u66f4\u5355\u9891\u7387\u9ad8\uff0c\u589e\u52a0\u4e86\u6210\u672c\u548c\u5de5\u671f\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u51cf\u5c11\u53d8\u66f4\u5355\u3002", "method": "\u5229\u7528\u4f5b\u7f57\u91cc\u8fbe\u4ea4\u901a\u90e8\u7684\u5386\u53f2\u6295\u6807\u6570\u636e\uff0c\u8bc4\u4f30\u4e94\u79cd\u56e0\u7d20\u5bf9\u53d8\u66f4\u5355\u9891\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u79bb\u6563\u9009\u62e9\u6a21\u578b\u6bd4\u8f83\u5355\u4ef7\u548c\u603b\u4ef7\u5408\u540c\u3002", "result": "\u9879\u76ee\u89c4\u6a21\u3001\u5de5\u671f\u548c\u5de5\u4f5c\u7c7b\u578b\u5bf9\u53d8\u66f4\u5355\u9891\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5408\u540c\u7c7b\u578b\u9009\u62e9\uff0cDBB\u9879\u76ee\u53ef\u4ee5\u51cf\u5c11\u53d8\u66f4\u5355\uff0c\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2507.00488", "categories": ["cs.PL", "D.3.3; D.2"], "pdf": "https://arxiv.org/pdf/2507.00488", "abs": "https://arxiv.org/abs/2507.00488", "authors": ["Lloyd Allison"], "title": "Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?", "comment": null, "summary": "Compared to functions in mathematics, functions in programming languages seem\nto be under classified. Functional programming languages based on the lambda\ncalculus famously treat functions as first-class values. Object-oriented\nlanguages have adopted ``lambdas'', notably for call-back routines in\nevent-based programming. Typically a programming language has functions, a\nfunction has a type, and some functions act on other functions and/or return\nfunctions but there is generally a lack of (i) ``class Function'' in the OO\nsense of the word class and particularly (ii) subclasses of Function for\nfunctions having specific properties. Some such classes are presented here and\nprogrammed in some popular programming languages as an experimental\ninvestigation into OO languages missing this opportunity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7f16\u7a0b\u8bed\u8a00\u4e2d\u51fd\u6570\u5206\u7c7b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u9762\u5411\u5bf9\u8c61\u601d\u60f3\u7684\u51fd\u6570\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5728\u6d41\u884c\u8bed\u8a00\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "motivation": "\u6570\u5b66\u4e2d\u7684\u51fd\u6570\u6709\u660e\u786e\u7684\u5206\u7c7b\uff0c\u800c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u51fd\u6570\u7f3a\u4e4f\u7c7b\u4f3c\u7684\u7ed3\u6784\u5316\u5206\u7c7b\uff0c\u5c24\u5176\u662f\u5728\u9762\u5411\u5bf9\u8c61\u8bed\u8a00\u4e2d\u7f3a\u5c11\u5bf9\u51fd\u6570\u7684\u7c7b\u5316\u4e0e\u5b50\u7c7b\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e9b\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u51fd\u6570\u7c7b\uff0c\u5e76\u5728\u6d41\u884c\u7684\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b9e\u73b0\u8fd9\u4e9b\u7c7b\uff0c\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u5728\u9762\u5411\u5bf9\u8c61\u8bed\u8a00\u4e2d\u5b9e\u73b0\u51fd\u6570\u5206\u7c7b\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bed\u8a00\u8bbe\u8ba1\u4e2d\u7684\u4e0d\u8db3\u3002", "conclusion": "\u7f16\u7a0b\u8bed\u8a00\u8bbe\u8ba1\u53ef\u4ee5\u501f\u9274\u6570\u5b66\u4e2d\u51fd\u6570\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9762\u5411\u5bf9\u8c61\u7684\u65b9\u5f0f\u4e30\u5bcc\u51fd\u6570\u7684\u529f\u80fd\u548c\u8868\u8fbe\u529b\u3002"}}
{"id": "2507.00839", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00839", "abs": "https://arxiv.org/abs/2507.00839", "authors": ["Chiyu Hao", "Jixian Su", "Shixuan Sun", "Hao Zhang", "Sen Gao", "Jianwen Zhao", "Chenyi Zhang", "Jieru Zhao", "Chen Chen", "Minyi Guo"], "title": "RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries", "comment": "17 pages, 18 figures", "summary": "Dynamic graph storage systems are essential for real-time applications such\nas social networks and recommendation, where graph data continuously evolves.\nHowever, they face significant challenges in efficiently handling concurrent\nread and write operations. We find that existing methods suffer from write\nqueries interfering with read efficiency, substantial time and space overhead\ndue to per-edge versioning, and an inability to balance performance, such as\nslow searches under concurrent workloads. To address these issues, we propose\nRapidStore, a holistic approach for efficient in-memory dynamic graph storage\ndesigned for read-intensive workloads. Our key idea is to exploit the\ncharacteristics of graph queries through a decoupled system design that\nseparates the management of read and write queries and decouples version data\nfrom graph data. Particularly, we design an efficient dynamic graph store to\ncooperate with the graph concurrency control mechanism. Experimental results\ndemonstrate that RapidStore enables fast and scalable concurrent graph queries,\neffectively balancing the performance of inserts, searches, and scans, and\nsignificantly improving efficiency in dynamic graph storage systems.", "AI": {"tldr": "RapidStore\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5185\u5b58\u52a8\u6001\u56fe\u5b58\u50a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u79bb\u8bfb\u5199\u67e5\u8be2\u7ba1\u7406\u548c\u7248\u672c\u6570\u636e\u4e0e\u56fe\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5e76\u53d1\u8bfb\u5199\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u56fe\u5b58\u50a8\u7cfb\u7edf\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bfb\u5199\u5e72\u6270\u3001\u7248\u672c\u7ba1\u7406\u5f00\u9500\u5927\u4ee5\u53ca\u6027\u80fd\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faRapidStore\uff0c\u91c7\u7528\u89e3\u8026\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5206\u79bb\u8bfb\u5199\u67e5\u8be2\u7ba1\u7406\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u56fe\u5b58\u50a8\u4e0e\u5e76\u53d1\u63a7\u5236\u673a\u5236\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRapidStore\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u5e76\u53d1\u56fe\u67e5\u8be2\uff0c\u5e73\u8861\u4e86\u63d2\u5165\u3001\u641c\u7d22\u548c\u626b\u63cf\u6027\u80fd\u3002", "conclusion": "RapidStore\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u56fe\u5b58\u50a8\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8bfb\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u3002"}}
{"id": "2507.00507", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.00507", "abs": "https://arxiv.org/abs/2507.00507", "authors": ["Chuhao Xu", "Zijun Li", "Quan Chen", "Han Zhao", "Minyi Guo"], "title": "LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference", "comment": null, "summary": "The rise of LLMs has driven demand for private serverless deployments,\ncharacterized by moderate-scale models and infrequent requests. While existing\nsolutions follow exclusive GPU deployment, we take a step back to explore\nmodern platforms and find that: Emerging CPU architectures with built-in\naccelerators are capable of serving LLMs but remain underutilized, and both\nCPUs and GPUs can accommodate multiple LLMs simultaneously.\n  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized\nLLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh\ntackles three fundamental challenges: (1) precise, fine-grained compute\nresource allocation at token-level to handle fluctuating computational demands;\n(2) a coordinated and forward-looking memory scaling mechanism to detect\nout-of-memory hazards and reduce operational overhead; and (3) a dual approach\nthat reduces resource fragmentation through proactive preemption and reactive\nbin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that\nLLM-Meshimproves service capacity by 44% - 63% through sharing, while further\nleveraging CPUs boosts this to 91% - 159%.", "AI": {"tldr": "LLM-Mesh\u662f\u4e00\u79cd\u9488\u5bf9\u4e2d\u5c0f\u578bLLM\u7684\u65e0\u670d\u52a1\u5668\u63a8\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u5f02\u6784\u786c\u4ef6\u5f39\u6027\u5171\u4eab\u63d0\u5347\u670d\u52a1\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u9700\u6c42\u7684\u589e\u957f\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u72ec\u5360GPU\u8d44\u6e90\uff0c\u800c\u73b0\u4ee3CPU\u67b6\u6784\u5185\u7f6e\u52a0\u901f\u5668\u5374\u672a\u5145\u5206\u5229\u7528\u3002", "method": "LLM-Mesh\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u3001\u5185\u5b58\u6269\u5c55\u673a\u5236\u548c\u8d44\u6e90\u788e\u7247\u5316\u51cf\u5c11\u7b56\u7565\uff0c\u5b9e\u73b0\u5f02\u6784\u786c\u4ef6\u5171\u4eab\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM-Mesh\u901a\u8fc7\u5171\u4eab\u63d0\u5347\u670d\u52a1\u80fd\u529b44%-63%\uff0c\u7ed3\u5408CPU\u4f18\u5316\u540e\u53ef\u8fbe91%-159%\u3002", "conclusion": "LLM-Mesh\u5c55\u793a\u4e86\u5f02\u6784\u786c\u4ef6\u5171\u4eab\u5728LLM\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.00763", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.00763", "abs": "https://arxiv.org/abs/2507.00763", "authors": ["Yong Li", "Sushanta K. Mallick", "Tao Zeng", "Junxing Zhang"], "title": "Comparing Misspecified Models with Big Data: A Variational Bayesian Perspective", "comment": null, "summary": "Optimal data detection in massive multiple-input multiple-output (MIMO)\nsystems often requires prohibitively high computational complexity. A variety\nof detection algorithms have been proposed in the literature, offering\ndifferent trade-offs between complexity and detection performance. In recent\nyears, Variational Bayes (VB) has emerged as a widely used method for\naddressing statistical inference in the context of massive data. This study\nfocuses on misspecified models and examines the risk functions associated with\npredictive distributions derived from variational posterior distributions.\nThese risk functions, defined as the expectation of the Kullback-Leibler (KL)\ndivergence between the true data-generating density and the variational\npredictive distributions, provide a framework for assessing predictive\nperformance. We propose two novel information criteria for predictive model\ncomparison based on these risk functions. Under certain regularity conditions,\nwe demonstrate that the proposed information criteria are asymptotically\nunbiased estimators of their respective risk functions. Through comprehensive\nnumerical simulations and empirical applications in economics and finance, we\ndemonstrate the effectiveness of these information criteria in comparing\nmisspecified models in the context of massive data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u6570\u636e\u68c0\u6d4b\u7684\u590d\u6742\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u53d8\u5206\u8d1d\u53f6\u65af\uff08VB\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u9488\u5bf9\u6a21\u578b\u8bef\u8bbe\u60c5\u51b5\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u4fe1\u606f\u51c6\u5219\uff0c\u7528\u4e8e\u8bc4\u4f30\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u68c0\u6d4b\u901a\u5e38\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u590d\u6742\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002VB\u65b9\u6cd5\u5728\u7edf\u8ba1\u63a8\u65ad\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6a21\u578b\u8bef\u8bbe\u60c5\u51b5\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u8bc4\u4f30\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u53d8\u5206\u540e\u9a8c\u5206\u5e03\u7684\u9884\u6d4b\u5206\u5e03\u98ce\u9669\u51fd\u6570\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u4fe1\u606f\u51c6\u5219\uff0c\u7528\u4e8e\u6a21\u578b\u6bd4\u8f83\u3002\u901a\u8fc7KL\u6563\u5ea6\u8861\u91cf\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u63d0\u51fa\u7684\u4fe1\u606f\u51c6\u5219\u88ab\u8bc1\u660e\u662f\u5176\u98ce\u9669\u51fd\u6570\u7684\u6e10\u8fd1\u65e0\u504f\u4f30\u8ba1\u3002\u6570\u503c\u6a21\u62df\u548c\u5b9e\u8bc1\u5e94\u7528\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u6a21\u578b\u8bef\u8bbe\u60c5\u51b5\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63d0\u51fa\u7684\u4fe1\u606f\u51c6\u5219\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.00413", "categories": ["cs.SE", "D.2.7"], "pdf": "https://arxiv.org/pdf/2507.00413", "abs": "https://arxiv.org/abs/2507.00413", "authors": ["Taiming Wang", "Hui Liu", "Yuxia Zhang", "Yanjie Jiang"], "title": "Recommending Variable Names for Extract Local Variable Refactorings", "comment": "Accepted by TOSEM", "summary": "Extract local variable is one of the most popular refactorings, and most IDEs\nand refactoring tools provide automated support for this refactoring. However,\nwe find approximately 70% of the names recommended by these IDEs are different\nfrom what developers manually constructed, adding additional renaming burdens\nto developers and providing limited assistance. In this paper, we introduce\nVarNamer, an automated approach designed to recommend variable names for\nextract local variable refactorings. Through a large-scale empirical study, we\nidentify key contexts that are useful for composing variable names. Leveraging\nthese insights, we developed a set of heuristic rules through program static\nanalysis techniques and employ data mining techniques to recommend variable\nnames effectively. Notably, some of our heuristic rules have been successfully\nintegrated into Eclipse, where they are now distributed with the latest\nreleases of the IDE. Evaluation demonstrates its superiority over\nstate-of-the-art IDEs. Specifically, VarNamer significantly increases the\nchance of exact match by 52.6% compared to Eclipse and 40.7% compared to\nIntelliJ IDEA. We also evaluated the proposed approach with real-world extract\nlocal variable refactorings conducted in C++ projects, and the results suggest\nthat the approach can achieve comparable performance on programming languages\nbesides Java. It may suggest the generalizability of VarNamer. Finally, we\ndesigned and conducted a user study and the results of the user study suggest\nthat our approach can speed up the refactoring by 27.8% and reduce 49.3% edits\non the recommended variable names.", "AI": {"tldr": "VarNamer\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u63d0\u53d6\u5c40\u90e8\u53d8\u91cf\u91cd\u6784\u63a8\u8350\u53d8\u91cf\u540d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u5f00\u53d1\u8005\u624b\u52a8\u547d\u540d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709IDE\u5728\u63d0\u53d6\u5c40\u90e8\u53d8\u91cf\u91cd\u6784\u65f6\u63a8\u8350\u7684\u53d8\u91cf\u540d\u4e0e\u5f00\u53d1\u8005\u624b\u52a8\u547d\u540d\u5dee\u5f02\u8f83\u5927\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u8005\u7684\u91cd\u547d\u540d\u8d1f\u62c5\uff0cVarNamer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u786e\u5b9a\u547d\u540d\u5173\u952e\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u6280\u672f\u548c\u6570\u636e\u6316\u6398\u6280\u672f\uff0c\u5f00\u53d1\u542f\u53d1\u5f0f\u89c4\u5219\u63a8\u8350\u53d8\u91cf\u540d\u3002", "result": "VarNamer\u5728\u7cbe\u786e\u5339\u914d\u7387\u4e0a\u663e\u8457\u4f18\u4e8eEclipse\u548cIntelliJ IDEA\uff0c\u5e76\u5728C++\u9879\u76ee\u4e2d\u8868\u73b0\u51fa\u53ef\u6bd4\u6027\u80fd\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u80fd\u52a0\u901f\u91cd\u6784\u5e76\u51cf\u5c11\u7f16\u8f91\u3002", "conclusion": "VarNamer\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u53d8\u91cf\u547d\u540d\u63a8\u8350\u65b9\u6cd5\uff0c\u5df2\u5728Eclipse\u4e2d\u96c6\u6210\u5e76\u9a8c\u8bc1\u5176\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2507.00050", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.00050", "abs": "https://arxiv.org/abs/2507.00050", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u89e3\u91ca\u96f6\u6837\u672c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7f51\u7edc\uff08SEZ-HARN\uff09\uff0c\u80fd\u591f\u8bc6\u522b\u672a\u5728\u8bad\u7ec3\u4e2d\u9047\u5230\u7684\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7\u9aa8\u67b6\u89c6\u9891\u89e3\u91ca\u5176\u51b3\u7b56\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u96f6\u6837\u672c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08ZS-HAR\uff09\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u6570\u636e\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86SEZ-HARN\u6a21\u578b\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u5b66\u4e60\u548c\u81ea\u89e3\u91ca\u80fd\u529b\uff0c\u901a\u8fc7\u9aa8\u67b6\u89c6\u9891\u63d0\u4f9b\u51b3\u7b56\u89e3\u91ca\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cSEZ-HARN\u7684\u96f6\u6837\u672c\u9884\u6d4b\u51c6\u786e\u7387\u63a5\u8fd1\u6700\u4f73\u9ed1\u76d2\u6a21\u578b\uff08PAMAP2\u4e0a\u76f8\u5dee3%\uff09\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "conclusion": "SEZ-HARN\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.00913", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2507.00913", "abs": "https://arxiv.org/abs/2507.00913", "authors": ["Abinash Panda", "Anup Pramanik", "Ragini Saxena"], "title": "Local Strategy-Proofness and Dictatorship", "comment": null, "summary": "We investigate preference domains where every unanimous and locally\nstrategy-proof social choice function (scf) satisfies dictatorship. We identify\na condition on domains called connected with two distinct neighbours which is\nnecessary for unanimous and locally strategy-proof scfs to satisfy\ndictatorship. Further, we show that this condition is sufficient within the\nclass of domains where every unanimous and locally strategy-proof scf satisfies\ntops-onlyness. While a complete characterization remains open, we make\nsignificant progress by showing that on connected with two distinct neighbours\ndomains, unanimity and strategy-proofness (a stronger requirement) guarantee\ndictatorship.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u504f\u597d\u57df\u4e2d\u4e00\u81f4\u4e14\u5c40\u90e8\u7b56\u7565\u8bc1\u660e\u7684\u793e\u4f1a\u9009\u62e9\u51fd\u6570\uff08scf\uff09\u6ee1\u8db3\u72ec\u88c1\u6027\u7684\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u201c\u4e0e\u4e24\u4e2a\u4e0d\u540c\u90bb\u5c45\u76f8\u8fde\u201d\u7684\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5fc5\u8981\u6027\u548c\u90e8\u5206\u5145\u5206\u6027\u3002", "motivation": "\u63a2\u7d22\u504f\u597d\u57df\u4e2d\u4e00\u81f4\u4e14\u5c40\u90e8\u7b56\u7565\u8bc1\u660e\u7684scf\u6ee1\u8db3\u72ec\u88c1\u6027\u7684\u6761\u4ef6\uff0c\u586b\u8865\u76f8\u5173\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u201c\u4e0e\u4e24\u4e2a\u4e0d\u540c\u90bb\u5c45\u76f8\u8fde\u201d\u7684\u6761\u4ef6\uff0c\u5206\u6790\u5176\u5728\u4e00\u81f4\u548c\u5c40\u90e8\u7b56\u7565\u8bc1\u660escf\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u9a8c\u8bc1\u5176\u5fc5\u8981\u6027\u548c\u90e8\u5206\u5145\u5206\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u6761\u4ef6\u5728\u4e00\u81f4\u548c\u5c40\u90e8\u7b56\u7565\u8bc1\u660escf\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5728\u7279\u5b9a\u57df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5145\u5206\u6027\u3002", "conclusion": "\u867d\u7136\u5b8c\u6574\u523b\u753b\u4ecd\u5f85\u89e3\u51b3\uff0c\u4f46\u8bba\u6587\u5728\u4e00\u81f4\u548c\u7b56\u7565\u8bc1\u660e\u6761\u4ef6\u4e0b\u5bf9\u72ec\u88c1\u6027\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.00214", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00214", "abs": "https://arxiv.org/abs/2507.00214", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u63a8\u7406\u589e\u5f3a\u6587\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u7387\u63d0\u53478.7%\u3002", "motivation": "\u6807\u51c6\u5206\u7c7b\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "1. \u5fae\u8c03Llama-3.2-1B-Instruct\u751f\u6210\u63a8\u7406\uff1b2. \u7528\u63a8\u7406\u589e\u5f3a\u4e0b\u6e38\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u751f\u6210\u6a21\u578b\uff08\u8f93\u51fa\u63a8\u7406\u548c\u60c5\u611f\uff09\u6bd4\u57fa\u7ebf\u6a21\u578b\uff08\u4ec5\u8f93\u51fa\u60c5\u611f\uff09\u51c6\u786e\u7387\u63d0\u9ad88.7%\u3002", "conclusion": "LLM\u751f\u6210\u7684\u63a8\u7406\u80fd\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u63d0\u4f9b\u663e\u5f0f\u89e3\u91ca\u3002"}}
{"id": "2507.00672", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.00672", "abs": "https://arxiv.org/abs/2507.00672", "authors": ["Haoxiang Luo", "Yinqiu Liu", "Ruichen Zhang", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Zehui Xiong", "Xianbin Wang", "Xuemin Shen"], "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration", "comment": null, "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u96c6\u6210\u591aLLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u4ee5\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u6280\u672f\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edfAI\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u52a8\u6001\u4efb\u52a1\u548c\u591a\u6a21\u6001\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u591aLLM\u534f\u4f5c\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u7f16\u6392\u3001\u8d44\u6e90\u8c03\u5ea6\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u8f6c\u79fb\u7b49\u6280\u672f\u5b9e\u73b0\u591aLLM\u7cfb\u7edf\uff0c\u5e76\u5173\u6ce8\u53ef\u4fe1\u8d56\u7684\u591aLLM\u67b6\u6784\u3002", "result": "\u591aLLM\u7cfb\u7edf\u80fd\u591f\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u3002", "conclusion": "\u591aLLM\u7cfb\u7edf\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u9700\u5173\u6ce8\u8d44\u6e90\u6548\u7387\u3001\u9690\u79c1\u548c\u4fe1\u4efb\u7b49\u95ee\u9898\u3002"}}
{"id": "2507.00044", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00044", "abs": "https://arxiv.org/abs/2507.00044", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "comment": "14 pages, 5 figures", "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u4e2d\u7684\u4f2a\u5f71\uff0c\u5e76\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002", "motivation": "WSI\u5728\u764c\u75c7\u8bca\u65ad\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f2a\u5f71\u4f1a\u5f71\u54cd\u56fe\u50cf\u5206\u6790\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684FMA\u3001\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684DLA\u548c\u57fa\u4e8e\u77e5\u8bc6\u7684KBA\uff0c\u7528\u4e8e\u68c0\u6d4b\u516d\u79cd\u5e38\u89c1\u4f2a\u5f71\u3002", "result": "FMA\u6027\u80fd\u6700\u4f73\uff08AUROC: 0.995\uff09\uff0c\u4f18\u4e8eDLA\uff080.977\uff09\u548cKBA\uff080.940\uff09\u3002", "conclusion": "FMA\u662f\u68c0\u6d4bWSI\u4f2a\u5f71\u7684\u6700\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u8d28\u91cf\u62a5\u544a\u5de5\u5177\u4ee5\u8f85\u52a9\u8bca\u65ad\u3002"}}
{"id": "2507.00550", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.00550", "abs": "https://arxiv.org/abs/2507.00550", "authors": ["Bruce Fang", "Danyi Gao"], "title": "Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling", "comment": null, "summary": "This paper addresses the challenges of rapid resource variation and highly\nuncertain task loads in cloud computing environments. It proposes an\noptimization method for elastic cloud resource scaling based on a multi-agent\nsystem. The method deploys multiple autonomous agents to perceive resource\nstates in parallel and make local decisions. While maintaining the distributed\nnature of the system, it introduces a collaborative value function to achieve\nglobal coordination. This improves the responsiveness of resource scheduling\nand enhances overall system performance. To strengthen system foresight, a\nlightweight state prediction model is designed. It assists agents in\nidentifying future workload trends and optimizes the selection of scaling\nactions. For policy training, the method adopts a centralized training and\ndecentralized execution reinforcement learning framework. This enables agents\nto learn effectively and coordinate strategies under conditions of incomplete\ninformation. The paper also constructs typical cloud scenarios, including\nmulti-tenancy and burst traffic, to evaluate the proposed method. The\nevaluation focuses on resource isolation, service quality assurance, and\nrobustness. Experimental results show that the proposed multi-agent scaling\nstrategy outperforms existing methods in resource utilization, SLA violation\ncontrol, and scheduling latency. The results demonstrate strong adaptability\nand intelligent regulation. This provides an efficient and reliable new\napproach to solving the problem of elastic resource scaling in complex cloud\nplatforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f39\u6027\u4e91\u8d44\u6e90\u6269\u5c55\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u51b3\u7b56\u4e0e\u5168\u5c40\u534f\u4f5c\u63d0\u5347\u8d44\u6e90\u8c03\u5ea6\u54cd\u5e94\u6027\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e91\u8ba1\u7b97\u73af\u5883\u4e2d\u8d44\u6e90\u5feb\u901f\u53d8\u5316\u548c\u4efb\u52a1\u8d1f\u8f7d\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u51b3\u7b56\u3001\u534f\u4f5c\u4ef7\u503c\u51fd\u6570\u548c\u8f7b\u91cf\u7ea7\u72b6\u6001\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u5229\u7528\u7387\u3001SLA\u8fdd\u89c4\u63a7\u5236\u548c\u8c03\u5ea6\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5f3a\u9002\u5e94\u6027\u548c\u667a\u80fd\u8c03\u8282\u80fd\u529b\u3002", "conclusion": "\u4e3a\u590d\u6742\u4e91\u5e73\u53f0\u4e2d\u7684\u5f39\u6027\u8d44\u6e90\u6269\u5c55\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.00795", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.00795", "abs": "https://arxiv.org/abs/2507.00795", "authors": ["Xinran Li", "Peizan Sheng", "Zeyang Yu"], "title": "Randomization Inference with Sample Attrition", "comment": null, "summary": "Although appealing, randomization inference for treatment effects can suffer\nfrom severe size distortion due to sample attrition. We propose new,\ncomputationally efficient methods for randomization inference that remain valid\nunder a range of potentially informative missingness mechanisms. We begin by\nconstructing valid p-values for testing sharp null hypotheses, using the\nworst-case p-value from the Fisher randomization test over all possible\nimputations of missing outcomes. Leveraging distribution-free test statistics,\nthis worst-case p-value admits a closed-form solution, connecting naturally to\nbounds in the partial identification literature. Our test statistics\nincorporate both potential outcomes and missingness indicators, allowing us to\nexploit structural assumptions-such as monotone missingness-for increased\npower. We further extend our framework to test non-sharp null hypotheses\nconcerning quantiles of individual treatment effects. The methods are\nillustrated through simulations and an empirical application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65b0\u7684\u968f\u673a\u5316\u63a8\u65ad\u65b9\u6cd5\uff0c\u89e3\u51b3\u6837\u672c\u6d41\u5931\u5bfc\u81f4\u7684\u4e25\u91cd\u5c3a\u5bf8\u5931\u771f\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7f3a\u5931\u673a\u5236\u3002", "motivation": "\u968f\u673a\u5316\u63a8\u65ad\u5728\u5904\u7406\u6837\u672c\u6d41\u5931\u65f6\u5b58\u5728\u5c3a\u5bf8\u5931\u771f\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u6709\u6548p\u503c\uff0c\u5229\u7528Fisher\u968f\u673a\u5316\u6d4b\u8bd5\u7684\u6700\u574f\u60c5\u51b5p\u503c\uff0c\u7ed3\u5408\u5206\u5e03\u65e0\u5173\u7684\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u5047\u8bbe\uff08\u5982\u5355\u8c03\u7f3a\u5931\uff09\u63d0\u9ad8\u529f\u6548\u3002", "result": "\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u5b9e\u8bc1\u5e94\u7528\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u9002\u7528\u4e8e\u975e\u5c16\u9510\u96f6\u5047\u8bbe\uff08\u5982\u4e2a\u4f53\u6cbb\u7597\u6548\u5e94\u7684\u5206\u4f4d\u6570\uff09\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u591a\u79cd\u7f3a\u5931\u673a\u5236\u4e0b\u6709\u6548\uff0c\u63d0\u9ad8\u4e86\u968f\u673a\u5316\u63a8\u65ad\u7684\u7a33\u5065\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.00421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00421", "abs": "https://arxiv.org/abs/2507.00421", "authors": ["Parthiv Katapara", "Anand Sharma"], "title": "Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development", "comment": "This paper present survey on DevOps practices which exists in\n  Embedded Software development", "summary": "The adoption of DevOps practices in embedded systems and firmware development\nis emerging as a response to the growing complexity of modern\nhardware--software co-designed products. Unlike cloud-native applications,\nembedded systems introduce challenges such as hardware dependency, real-time\nconstraints, and safety-critical requirements. This literature review\nsynthesizes findings from 20 academic and industrial sources to examine how\nDevOps principles--particularly continuous integration, continuous delivery,\nand automated testing--are adapted to embedded contexts. We categorize efforts\nacross tooling, testing strategies, pipeline automation, and security\npractices. The review highlights current limitations in deployment workflows\nand observability, proposing a roadmap for future research. This work offers\nresearchers and practitioners a consolidated understanding of Embedded DevOps,\nbridging fragmented literature with a structured perspective.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2dDevOps\u5b9e\u8df5\u7684\u9002\u5e94\u60c5\u51b5\uff0c\u5206\u6790\u4e86\u5de5\u5177\u3001\u6d4b\u8bd5\u3001\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u548c\u5b89\u5168\u5b9e\u8df5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u786c\u4ef6\u4f9d\u8d56\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u4f7f\u5f97DevOps\u5b9e\u8df5\u5728\u5d4c\u5165\u5f0f\u73af\u5883\u4e2d\u7684\u9002\u914d\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u679020\u7bc7\u5b66\u672f\u548c\u5de5\u4e1a\u6587\u732e\uff0c\u603b\u7ed3\u4e86\u5d4c\u5165\u5f0fDevOps\u7684\u5de5\u5177\u3001\u6d4b\u8bd5\u7b56\u7565\u3001\u6d41\u6c34\u7ebf\u81ea\u52a8\u5316\u548c\u5b89\u5168\u5b9e\u8df5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u90e8\u7f72\u5de5\u4f5c\u6d41\u548c\u53ef\u89c2\u6d4b\u6027\u5b58\u5728\u5c40\u9650\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u8def\u7ebf\u56fe\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5d4c\u5165\u5f0fDevOps\u7684\u7cfb\u7edf\u5316\u89c6\u89d2\uff0c\u586b\u8865\u4e86\u6587\u732e\u788e\u7247\u5316\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.00054", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00054", "abs": "https://arxiv.org/abs/2507.00054", "authors": ["Shreyansh Padarha"], "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "AI": {"tldr": "AdvDistill\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u7684\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u751f\u6210\u54cd\u5e94\u548c\u89c4\u5219\u9a8c\u8bc1\u5668\u5206\u914d\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u901a\u5e38\u5c40\u9650\u4e8e\u5b66\u751f\u6a21\u578b\u590d\u5236\u6559\u5e08\u6a21\u578b\u7684\u5206\u5e03\u5185\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "AdvDistill\u5229\u7528\u6559\u5e08\u6a21\u578b\u5bf9\u6bcf\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u5668\u5206\u914d\u5956\u52b1\uff0c\u5c06\u8fd9\u4e9b\u5956\u52b1\u4f5c\u4e3a\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u7684\u6743\u91cd\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u5728\u6570\u5b66\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6570\u636e\u96c6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5956\u52b1\u673a\u5236\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2507.00216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00216", "abs": "https://arxiv.org/abs/2507.00216", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "title": "Towards Style Alignment in Cross-Cultural Translation", "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u5728\u98ce\u683c\u7ffb\u8bd1\u4e2d\u7684\u5931\u8d25\uff0c\u5e76\u63d0\u51faRASTA\u65b9\u6cd5\u4ee5\u6539\u5584\u6587\u5316\u5dee\u5f02\u5bfc\u81f4\u7684\u98ce\u683c\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u6587\u5316\u5dee\u5f02\u5e38\u5bfc\u81f4\u8bf4\u8bdd\u8005\u610f\u56fe\u4e0e\u542c\u8005\u7406\u89e3\u7684\u98ce\u683c\u4e0d\u4e00\u81f4\uff0c\u4f8b\u5982\u793c\u8c8c\u5728\u7ffb\u8bd1\u4e2d\u4e22\u5931\u3002", "method": "\u63d0\u51faRASTA\u65b9\u6cd5\uff0c\u5229\u7528\u5b66\u4e60\u7684\u98ce\u683c\u6982\u5ff5\u6539\u8fdbLLM\u7ffb\u8bd1\uff0c\u4ee5\u66f4\u597d\u5730\u4f20\u8fbe\u6587\u5316\u6c9f\u901a\u89c4\u8303\u3002", "result": "LLM\u5728\u98ce\u683c\u7ffb\u8bd1\u4e2d\u5b58\u5728\u504f\u5411\u4e2d\u7acb\u548c\u5728\u975e\u897f\u65b9\u8bed\u8a00\u4e2d\u8868\u73b0\u8f83\u5dee\u7684\u95ee\u9898\u3002", "conclusion": "RASTA\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86LLM\u5728\u98ce\u683c\u7ffb\u8bd1\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2507.00856", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.00856", "abs": "https://arxiv.org/abs/2507.00856", "authors": ["Beining Wu", "Jun Huang", "Qiang Duan", "Liang Dong", "Zhipeng Cai"], "title": "Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework", "comment": "Under review at IEEE Transactions on Networking", "summary": "This paper aims to enhance the performance of Vehicular Platooning (VP)\nsystems integrated with Wireless Federated Learning (WFL). In highly dynamic\nenvironments, vehicular platoons experience frequent communication changes and\nresource constraints, which significantly affect information exchange and\nlearning model synchronization. To address these challenges, we first formulate\nWFL in VP as a joint optimization problem that simultaneously considers Age of\nInformation (AoI) and Federated Learning Model Drift (FLMD) to ensure timely\nand accurate control. Through theoretical analysis, we examine the impact of\nFLMD on convergence performance and develop a two-stage Resource-Aware Control\nframework (RACE). The first stage employs a Lagrangian dual decomposition\nmethod for resource configuration, while the second stage implements a\nmulti-agent deep reinforcement learning approach for vehicle selection. The\napproach integrates Multi-Head Self-Attention and Long Short-Term Memory\nnetworks to capture spatiotemporal correlations in communication states.\nExperimental results demonstrate that, compared to baseline methods, the\nproposed framework improves AoI optimization by up to 45%, accelerates learning\nconvergence, and adapts more effectively to dynamic VP environments on the\nAI4MARS dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\uff08WFL\uff09\u7684\u8f66\u961f\u534f\u540c\uff08VP\uff09\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u6846\u67b6RACE\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4fe1\u606f\u65f6\u6548\u6027\uff08AoI\uff09\u548c\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u6f02\u79fb\uff08FLMD\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u8f66\u961f\u534f\u540c\u7cfb\u7edf\u9762\u4e34\u9891\u7e41\u7684\u901a\u4fe1\u53d8\u5316\u548c\u8d44\u6e90\u9650\u5236\uff0c\u5f71\u54cd\u4fe1\u606f\u4ea4\u6362\u548c\u6a21\u578b\u540c\u6b65\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u65f6\u6548\u6027\u548c\u51c6\u786e\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8d44\u6e90\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff08RACE\uff09\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u5206\u89e3\u8fdb\u884c\u8d44\u6e90\u914d\u7f6e\uff0c\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8f66\u8f86\u9009\u62e9\uff0c\u7ed3\u5408\u591a\u5934\u81ea\u6ce8\u610f\u529b\u4e0eLSTM\u7f51\u7edc\u6355\u6349\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRACE\u5728AI4MARS\u6570\u636e\u96c6\u4e0a\u4f18\u5316AoI\u8fbe45%\uff0c\u52a0\u901f\u5b66\u4e60\u6536\u655b\uff0c\u5e76\u66f4\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "conclusion": "RACE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001VP\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u4e0e\u6a21\u578b\u540c\u6b65\u95ee\u9898\uff0c\u4e3aWFL\u5728VP\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.00045", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00045", "abs": "https://arxiv.org/abs/2507.00045", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u573a\u666f\uff08\u5982CaughtCheating\uff09\u4e2d\u8868\u73b0\u6781\u5dee\uff0c\u63ed\u793a\u4e86\u5176\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\uff08\u5982\u4fa6\u63a2\u63a8\u7406\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1CaughtCheating\u4efb\u52a1\uff08\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e2d\u68c0\u6d4b\u4f34\u4fa3\u53ef\u7591\u884c\u4e3a\u7684\u573a\u666f\uff09\uff0c\u5bf9MLLMs\u8fdb\u884c\u5b9e\u9a8c\u548c\u5206\u6790\u3002", "result": "\u53d1\u73b0MLLMs\u5728CaughtCheating\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u96f6\uff0c\u63ed\u793a\u4e86\u5176\u5728\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "conclusion": "CaughtCheating\u4efb\u52a1\u4e3aMLLMs\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u6d4b\u8bd5\uff0c\u6210\u529f\u89e3\u51b3\u6b64\u7c7b\u4efb\u52a1\u5c06\u63a8\u52a8\u5176\u8fbe\u5230\u4eba\u7c7b\u4fa6\u63a2\u6c34\u5e73\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.00576", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.00576", "abs": "https://arxiv.org/abs/2507.00576", "authors": ["Dante D. Sanchez-Gallegos", "J. L. Gonzalez-Compean", "Maxime Gonthier", "Valerie Hayot-Sasson", "J. Gregory Pauloski", "Haochen Pan", "Kyle Chard", "Jesus Carretero", "Ian Foster"], "title": "DynoStore: A wide-area distribution system for the management of data over heterogeneous storage", "comment": "10 pages. Conference: The 25th IEEE International Symposium on\n  Cluster, Cloud, and Internet Computing", "summary": "Data distribution across different facilities offers benefits such as\nenhanced resource utilization, increased resilience through replication, and\nimproved performance by processing data near its source. However, managing such\ndata is challenging due to heterogeneous access protocols, disparate\nauthentication models, and the lack of a unified coordination framework. This\npaper presents DynoStore, a system that manages data across heterogeneous\nstorage systems. At the core of DynoStore are data containers, an abstraction\nthat provides standardized interfaces for seamless data management,\nirrespective of the underlying storage systems. Multiple data container\nconnections create a cohesive wide-area storage network, ensuring resilience\nusing erasure coding policies. Furthermore, a load-balancing algorithm ensures\nequitable and efficient utilization of storage resources. We evaluate DynoStore\nusing benchmarks and real-world case studies, including the management of\nmedical and satellite data across geographically distributed environments. Our\nresults demonstrate a 10\\% performance improvement compared to centralized\ncloud-hosted systems while maintaining competitive performance with\nstate-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits\nsuperior fault tolerance, withstanding more failures than traditional systems.", "AI": {"tldr": "DynoStore\u662f\u4e00\u4e2a\u8de8\u5f02\u6784\u5b58\u50a8\u7cfb\u7edf\u7684\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u5bb9\u5668\u62bd\u8c61\u548c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\u4e0e\u5bb9\u9519\u80fd\u529b\u3002", "motivation": "\u5206\u5e03\u5f0f\u6570\u636e\u7ba1\u7406\u5b58\u5728\u5f02\u6784\u534f\u8bae\u3001\u8ba4\u8bc1\u6a21\u578b\u4e0d\u7edf\u4e00\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u534f\u8c03\u6846\u67b6\u3002", "method": "DynoStore\u91c7\u7528\u6570\u636e\u5bb9\u5668\u62bd\u8c61\u548c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\uff0c\u6784\u5efa\u5e7f\u57df\u5b58\u50a8\u7f51\u7edc\uff0c\u4f7f\u7528\u7ea0\u5220\u7801\u7b56\u7565\u786e\u4fdd\u5bb9\u9519\u3002", "result": "\u6027\u80fd\u63d0\u534710%\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u4e91\u7cfb\u7edf\uff0c\u5bb9\u9519\u80fd\u529b\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "DynoStore\u5728\u6027\u80fd\u548c\u5bb9\u9519\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5730\u7406\u5206\u5e03\u5f0f\u73af\u5883\u3002"}}
{"id": "2507.00481", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00481", "abs": "https://arxiv.org/abs/2507.00481", "authors": ["Philipp M. Z\u00e4hl", "Sabine Theis", "Martin R. Wolf"], "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f00\u53d1\u8005\u7684HEXACO\u4eba\u683c\u7279\u8d28\u5bf9\u8f6f\u4ef6\u56e2\u961f\u7684\u56e2\u961f\u5408\u4f5c\u8d28\u91cf\uff08TWQ\uff09\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u4eba\u683c\u56e0\u7d20\u6bd4\u6d41\u7a0b\u548c\u5de5\u5177\u66f4\u91cd\u8981\u3002\u521d\u6b65\u6570\u636e\uff08n=54\uff09\u663e\u793a\uff0c\u4eba\u683c\u7279\u8d28\u7ec4\u5408\u3001\u6027\u522b\u6bd4\u4f8b\u548c\u5e74\u9f84\u5206\u5e03\u5747\u5f71\u54cdTWQ\u3002", "motivation": "\u8ba4\u8bc6\u5230\u4eba\u683c\u7279\u8d28\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u5408\u4f5c\u7684\u5f71\u54cd\u53ef\u80fd\u8d85\u8fc7\u6d41\u7a0b\u548c\u5de5\u5177\uff0c\u7814\u7a76\u65e8\u5728\u91cf\u5316HEXACO\u4eba\u683c\u7279\u8d28\u5bf9TWQ\u7684\u4f5c\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u9879\u7814\u7a76\uff0c\u901a\u8fc7\u521d\u6b65\u6570\u636e\u6536\u96c6\uff08n=54\uff09\u5206\u6790HEXACO\u4eba\u683c\u7279\u8d28\u4e0eTWQ\u7684\u5173\u7cfb\uff0c\u5e76\u8003\u5bdf\u5176\u4ed6\u53d8\u91cf\uff08\u5982\u6027\u522b\u6bd4\u4f8b\u3001\u5e74\u9f84\u5206\u5e03\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u591a\u4e2aHEXACO\u4eba\u683c\u7279\u8d28\u53ca\u5176\u7ec4\u5408\u5bf9TWQ\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6027\u522b\u6bd4\u4f8b\u548c\u5e74\u9f84\u5206\u5e03\u4e5f\u6709\u4f5c\u7528\u3002\u521d\u6b65\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7814\u7a76\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aIT\u7ec4\u7ec7\u6539\u8fdb\u56e2\u961f\u5408\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5e76\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u9014\u5f84\u3002"}}
{"id": "2507.00079", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00079", "abs": "https://arxiv.org/abs/2507.00079", "authors": ["Ethan Smyth", "Alessandro Suglia"], "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVoyagerVision\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u5728Minecraft\u4e2d\u521b\u5efa\u7ed3\u6784\uff0c\u6269\u5c55\u4e86\u5f00\u653e\u6027\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u589e\u5f3a\u6a21\u578b\u5bf9\u7a7a\u95f4\u73af\u5883\u7684\u7406\u89e3\uff0c\u4ece\u800c\u6269\u5c55\u5176\u4efb\u52a1\u6267\u884c\u80fd\u529b\u548c\u5f00\u653e\u6027\u6f5c\u529b\u3002", "method": "\u63d0\u51faVoyagerVision\u6a21\u578b\uff0c\u5229\u7528\u622a\u56fe\u4f5c\u4e3a\u89c6\u89c9\u53cd\u9988\uff0c\u5728Minecraft\u4e2d\u521b\u5efa\u7ed3\u6784\u3002", "result": "VoyagerVision\u5e73\u5747\u572850\u6b21\u8fed\u4ee3\u4e2d\u521b\u5efa2.75\u4e2a\u72ec\u7279\u7ed3\u6784\uff0c\u5e76\u5728\u5e73\u5766\u4e16\u754c\u4e2d\u6210\u529f\u5b8c\u6210\u4e00\u534a\u7684\u6784\u5efa\u4efb\u52a1\u3002", "conclusion": "\u89c6\u89c9\u8f93\u5165\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u4e3a\u5f00\u653e\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.00239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00239", "abs": "https://arxiv.org/abs/2507.00239", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u5bf9\u9f50\uff0c\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u4ecd\u53ef\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u89e3\u7801\u88ab\u62d2\u7edd\u7684\u6709\u5bb3\u4fe1\u606f\uff0c\u8868\u660e\u8fd9\u4e9b\u4fe1\u606f\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u672a\u88ab\u6d88\u9664\u6216\u91cd\u5b9a\u4f4d\uff0c\u4ec5\u88ab\u76f4\u63a5\u8868\u8fbe\u6291\u5236\u3002", "motivation": "\u63a2\u8ba8\u6307\u4ee4\u5fae\u8c03\u548c\u5bf9\u9f50\u540e\u7684\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u6d88\u9664\u4e86\u6709\u5bb3\u4fe1\u606f\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u4fe1\u606f\u662f\u5426\u4ecd\u53ef\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u89e3\u7801\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u8bad\u7ec3\u5728LM\u9690\u85cf\u72b6\u6001\u4e0a\uff0c\u7814\u7a76\u88ab\u62d2\u7edd\u4fe1\u606f\u662f\u5426\u53ef\u89e3\u7801\uff0c\u5e76\u6d4b\u8bd5\u63a2\u9488\u5728\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u95f4\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u53ef\u89e3\u7801\u5927\u91cf\u88ab\u62d2\u7edd\u4fe1\u606f\uff08\u5982\u56fd\u5bb6\u5e73\u5747IQ\uff0cPearson\u76f8\u5173\u6027\u8d85\u8fc70.8\uff09\uff0c\u4e14\u57fa\u7840\u6a21\u578b\u7684\u63a2\u9488\u53ef\u8fc1\u79fb\u81f3\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff0c\u8868\u660e\u6709\u5bb3\u4fe1\u606f\u5728\u5185\u90e8\u8868\u793a\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u5e76\u672a\u5b8c\u5168\u6d88\u9664\u6216\u91cd\u5b9a\u4f4d\u6709\u5bb3\u4fe1\u606f\uff0c\u4ec5\u6291\u5236\u5176\u76f4\u63a5\u8868\u8fbe\uff0c\u8fd9\u4e9b\u4fe1\u606f\u4ecd\u53ef\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u8bbf\u95ee\uff0c\u5e76\u95f4\u63a5\u5f71\u54cd\u4e0b\u6e38\u884c\u4e3a\u3002"}}
{"id": "2507.00896", "categories": ["cs.NI", "C.2.2; C.2.1"], "pdf": "https://arxiv.org/pdf/2507.00896", "abs": "https://arxiv.org/abs/2507.00896", "authors": ["Saverio Mascolo", "Andrea Vittorio Balillo", "Gioacchino Manfredi", "Davide D'Agostino", "Luca De Cicco"], "title": "QUIC Delay Control: an implementation of congestion and delay control", "comment": "8 pages, 9 figures", "summary": "A new congestion and delay control algorithm named QUIC Delay Control\n(QUIC-DC) is proposed for controlling not only congestion but also the queuing\ndelay encountered along the forward communication path. The core idea is to\nestimate the one-way queuing delay of a connection to trigger an early reaction\nto congestion. This idea, along with the TCP Westwood+ congestion control\nalgorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2,\nNewReno, Westwood+. The results obtained in both emulated and real network\nconnections show that QUIC-DC can significantly reduce packet losses along with\nend-to-end communication delays, while preserving network utilization, features\nthat are both very useful for real-time applications.", "AI": {"tldr": "QUIC-DC\u662f\u4e00\u79cd\u65b0\u7684\u62e5\u585e\u548c\u5ef6\u8fdf\u63a7\u5236\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u5355\u5411\u6392\u961f\u5ef6\u8fdf\u6765\u63d0\u524d\u5e94\u5bf9\u62e5\u585e\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u5305\u4e22\u5931\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u5e94\u7528\u4e2d\u62e5\u585e\u548c\u6392\u961f\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u7ed3\u5408TCP Westwood+\u7b97\u6cd5\uff0c\u4f30\u8ba1\u5355\u5411\u6392\u961f\u5ef6\u8fdf\u5e76\u89e6\u53d1\u65e9\u671f\u62e5\u585e\u53cd\u5e94\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u7f51\u7edc\u4e2d\uff0cQUIC-DC\u663e\u8457\u51cf\u5c11\u6570\u636e\u5305\u4e22\u5931\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u5229\u7528\u7387\u3002", "conclusion": "QUIC-DC\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u80fd\u6709\u6548\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2507.00046", "categories": ["cs.CV", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.00046", "abs": "https://arxiv.org/abs/2507.00046", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "comment": "7 pages, 4 figures", "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u589e\u6750\u6469\u64e6\u6405\u62cc\u6c89\u79ef\uff08AFSD\uff09\u8fc7\u7a0b\u4e2d\u7684\u5b8c\u6574\u6027\u3002\u91c7\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u786e\u5b9a\u6700\u4f73\u5206\u5272\u9608\u503c\uff0c\u7ed3\u5408\u68af\u5ea6\u5206\u6790\u548c\u8ddd\u79bb\u53d8\u6362\u751f\u6210\u6ce8\u610f\u529b\u52a0\u6743\u53ef\u89c6\u5316\uff0c\u63ed\u793a\u4f20\u7edf\u6210\u50cf\u96be\u4ee5\u89c2\u5bdf\u5230\u7684\u7f3a\u9677\u548c\u6750\u6599\u8fc7\u6e21\u533a\u3002", "motivation": "\u4f20\u7edf\u6210\u50cf\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4bAFSD\u8fc7\u7a0b\u4e2d\u7684\u7f3a\u9677\u548c\u6750\u6599\u8fc7\u6e21\u533a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u81ea\u52a8\u5206\u5272\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u548c\u68af\u5ea6\u5206\u6790\u3001\u8ddd\u79bb\u53d8\u6362\uff0c\u751f\u6210\u6ce8\u610f\u529b\u52a0\u6743\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u591a\u901a\u9053\u6280\u672f\u91cf\u5316\u754c\u9762\u8d28\u91cf\u3002", "result": "PSO\u81ea\u52a8\u786e\u5b9a\u4e86\u6700\u4f73\u9608\u503c\uff08156-173\uff09\uff0c\u591a\u901a\u9053\u53ef\u89c6\u5316\u6280\u672f\u6210\u529f\u63ed\u793a\u4e86\u4e0d\u5b8c\u5168\u7ed3\u5408\u548c\u4e0d\u5747\u5300\u533a\u57df\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u6307\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522bAFSD\u4e2d\u7684\u7f3a\u9677\u548c\u4e0d\u5747\u5300\u6027\uff0c\u4e3a\u589e\u6750\u5236\u9020\u7ec4\u4ef6\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u5de5\u827a\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.00716", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.00716", "abs": "https://arxiv.org/abs/2507.00716", "authors": ["Mohsen Koohi Esfahani"], "title": "Accelerating Loading WebGraphs in ParaGrapher", "comment": null, "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.", "AI": {"tldr": "ParaGrapher\u662f\u4e00\u4e2a\u56fe\u52a0\u8f7dAPI\u548c\u5e93\uff0c\u7528\u4e8e\u9ad8\u6548\u52a0\u8f7d\u5927\u89c4\u6a21\u538b\u7f29\u56fe\u3002\u672c\u6587\u63d0\u51faPG-Fuse\u548cCompBin\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u5206\u522b\u89e3\u51b3\u5b58\u50a8\u5229\u7528\u7387\u548c\u89e3\u538b\u7f29\u5e26\u5bbd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "ParaGrapher\u5728\u9ad8\u6548\u52a0\u8f7d\u538b\u7f29\u56fe\u65b9\u9762\u5b58\u5728\u5b58\u50a8\u5229\u7528\u7387\u4f4e\u548c\u89e3\u538b\u7f29\u5e26\u5bbd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u5f15\u5165PG-Fuse\u4f18\u5316\u5b58\u50a8\u8bbf\u95ee\uff0c\u901a\u8fc7\u589e\u5927\u8bf7\u6c42\u5757\u5927\u5c0f\u548c\u7f13\u5b58\uff1b\u63d0\u51faCompBin\u6539\u8fdb\u538b\u7f29\u683c\u5f0f\uff0c\u652f\u6301\u76f4\u63a5\u8bbf\u95ee\u90bb\u5c45\u8282\u70b9\u3002", "result": "\u572812\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u56fe\u4e0a\u6d4b\u8bd5\uff0cPG-Fuse\u548cCompBin\u5206\u522b\u5b9e\u73b07.6\u500d\u548c21.8\u500d\u7684\u52a0\u901f\u3002", "conclusion": "PG-Fuse\u548cCompBin\u6709\u6548\u89e3\u51b3\u4e86ParaGrapher\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5904\u7406\u6027\u80fd\u3002"}}
{"id": "2507.00496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00496", "abs": "https://arxiv.org/abs/2507.00496", "authors": ["Hongjing Guo", "Chuanqi Tao", "Zhiqiu Huang", "Weiqin Zou"], "title": "Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey", "comment": null, "summary": "As Deep Learning (DL) models are increasingly applied in safety-critical\ndomains, ensuring their quality has emerged as a pressing challenge in modern\nsoftware engineering. Among emerging validation paradigms, coverage-guided\ntesting (CGT) has gained prominence as a systematic framework for identifying\nerroneous or unexpected model behaviors. Despite growing research attention,\nexisting CGT studies remain methodologically fragmented, limiting the\nunderstanding of current advances and emerging trends. This work addresses that\ngap through a comprehensive review of state-of-the-art CGT methods for DL\nmodels, including test coverage analysis, coverage-guided test input\ngeneration, and coverage-guided test input optimization. This work provides\ndetailed taxonomies to organize these methods based on methodological\ncharacteristics and application scenarios. We also investigate evaluation\npractices adopted in existing studies, including the use of benchmark datasets,\nmodel architectures, and evaluation aspects. Finally, open challenges and\nfuture directions are highlighted in terms of the correlation between\nstructural coverage and testing objectives, method generalizability across\ntasks and models, practical deployment concerns, and the need for standardized\nevaluation and tool support. This work aims to provide a roadmap for future\nacademic research and engineering practice in DL model quality assurance.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u8986\u76d6\u5f15\u5bfc\u6d4b\u8bd5\uff08CGT\uff09\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u6d4b\u8bd5\u8986\u76d6\u7387\u5206\u6790\u3001\u6d4b\u8bd5\u8f93\u5165\u751f\u6210\u548c\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u6a21\u578b\u8d28\u91cf\u6210\u4e3a\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709CGT\u7814\u7a76\u5206\u6563\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\u3002", "method": "\u901a\u8fc7\u5168\u9762\u56de\u987eCGT\u65b9\u6cd5\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u8bc4\u4f30\u5b9e\u8df5\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bc4\u4ef7\u7ef4\u5ea6\u3002", "result": "\u603b\u7ed3\u4e86CGT\u7684\u5f53\u524d\u8fdb\u5c55\u548c\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u7ed3\u6784\u8986\u76d6\u7387\u4e0e\u6d4b\u8bd5\u76ee\u6807\u7684\u5173\u8054\u6027\u3001\u65b9\u6cd5\u901a\u7528\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u6df1\u5ea6\u5b66\u4e60\u7684\u8d28\u91cf\u4fdd\u8bc1\u7814\u7a76\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u5de5\u5177\u652f\u6301\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.00092", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00092", "abs": "https://arxiv.org/abs/2507.00092", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9006\u5411\u63a8\u7406\u201d\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7SAGE-nano\u6a21\u578b\u5b9e\u73b0LLM\u7684\u81ea\u6211\u53cd\u601d\u548c\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5728Chain-of-Thought\u63d0\u793a\u4e0b\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u95ee\u9898\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u9006\u5411\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u53cd\u5411\u5206\u6790\u63a8\u7406\u94fe\uff0c\u751f\u6210\u89e3\u91ca\u3002", "result": "SAGE-nano\u5728AQUA-RAT\u7b49\u6d4b\u8bd5\u4e2d\u8fbe\u523074.6%\u7684\u63a8\u7406\u51c6\u786e\u7387\u548c92.1%\u7684\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\uff0c\u6027\u80fd\u63a5\u8fd1Claude-3.5\u548cGPT-4o\u3002", "conclusion": "\u9006\u5411\u63a8\u7406\u4e3a\u900f\u660eAI\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u586b\u8865\u4e86AI\u5b89\u5168\u3001\u6559\u80b2\u548c\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2507.00244", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "pdf": "https://arxiv.org/pdf/2507.00244", "abs": "https://arxiv.org/abs/2507.00244", "authors": ["Isabella Senturia", "Matilde Marcolli"], "title": "The Algebraic Structure of Morphosyntax", "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b66\u6a21\u578b\u7684\u5f62\u6001\u5b66-\u53e5\u6cd5\u63a5\u53e3\u7406\u8bba\uff0c\u7ed3\u5408\u4e86Merge\u548c\u5f3a\u6781\u7b80\u8bba\u9898\uff0c\u901a\u8fc7\u4ee3\u6570\u64cd\u4f5c\u548c\u5f62\u6001\u5b66\u6811\u7ed3\u6784\u63cf\u8ff0\u5f62\u6001\u53e5\u6cd5\u6811\u7684\u5f62\u6210\u3002", "motivation": "\u63a2\u7d22\u5f62\u6001\u5b66\u548c\u53e5\u6cd5\u4e4b\u95f4\u7684\u63a5\u53e3\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f3a\u6781\u7b80\u8bba\u9898\u7684\u6846\u67b6\u4e0b\uff0c\u5982\u4f55\u7528\u6570\u5b66\u6a21\u578b\u63cf\u8ff0\u4e24\u8005\u7684\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u5f62\u6001\u5b66\u6811\u7684\u4ee3\u6570\u7ed3\u6784\uff08magma\uff09\u548c\u64cd\u4f5c\u6570\uff08operad\uff09\u5bf9\u5e94\u5173\u7cfb\uff0c\u7ed3\u5408\u5f62\u6001\u5b66\u4f59\u79ef\u5206\u89e3\uff0c\u6784\u5efa\u5f62\u6001\u53e5\u6cd5\u6811\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\uff0c\u80fd\u591f\u7075\u6d3b\u8c03\u6574\u5f62\u6001\u5b66\u548c\u53e5\u6cd5\u4e4b\u95f4\u7684\u8fb9\u754c\uff0c\u5e76\u91cd\u65b0\u89e3\u91ca\u4e86\u5206\u5e03\u5f0f\u5f62\u6001\u5b66\u4e2d\u7684\u67d0\u4e9b\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5f62\u6001\u5b66-\u53e5\u6cd5\u63a5\u53e3\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u652f\u6301\u5f3a\u6781\u7b80\u8bba\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5f62\u6001\u5b66\u548c\u53e5\u6cd5\u5728\u7ed3\u6784\u5f62\u6210\u4e2d\u7684\u52a8\u6001\u4ea4\u4e92\u3002"}}
{"id": "2507.00049", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00049", "abs": "https://arxiv.org/abs/2507.00049", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "comment": "Preprint", "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "AI": {"tldr": "AdaDeDup\u662f\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5bc6\u5ea6\u526a\u679d\u548c\u6a21\u578b\u53cd\u9988\uff0c\u81ea\u9002\u5e94\u5730\u526a\u679d\u5197\u4f59\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5197\u4f59\u548c\u8ba1\u7b97\u8d1f\u62c5\u6311\u6218\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u65e0\u5173\u6027\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "AdaDeDup\u901a\u8fc7\u5bc6\u5ea6\u526a\u679d\u548c\u4ee3\u7406\u6a21\u578b\u53cd\u9988\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u526a\u679d\u9608\u503c\uff0c\u4fdd\u7559\u5173\u952e\u6570\u636e\u5e76\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaDeDup\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51cf\u5c11\u6027\u80fd\u4e0b\u964d54%\u4ee5\u4e0a\uff0c\u526a\u679d20%\u6570\u636e\u540e\u4ecd\u63a5\u8fd1\u539f\u59cb\u6027\u80fd\u3002", "conclusion": "AdaDeDup\u6709\u6548\u63d0\u5347\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u6570\u636e\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.00824", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.00824", "abs": "https://arxiv.org/abs/2507.00824", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Micha\u0142 Kr\u00f3l", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivi\u00e8re"], "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline.", "AI": {"tldr": "PANDAS\u662f\u4e00\u79cd\u5c06\u6570\u636e\u53ef\u7528\u6027\u91c7\u6837\uff08DAS\uff09\u4e0e\u4ee5\u592a\u574aDanksharding\u8981\u6c42\u96c6\u6210\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u652f\u6301\u57284\u79d2\u5185\u5b8c\u6210\u5c422\u6570\u636e\u4f20\u64ad\u548c\u91c7\u6837\u3002", "motivation": "\u89e3\u51b3\u4ee5\u592a\u574a\u5c422\u534f\u8bae\u5728\u5168\u5c40\u5e7f\u64ad\u6570\u636e\u65f6\u6269\u5c55\u6027\u53d7\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3Danksharding\u5bf9\u6570\u636e\u53ef\u7528\u6027\u91c7\u6837\u7684\u4e25\u683c\u65f6\u95f4\u8981\u6c42\u3002", "method": "PANDAS\u901a\u8fc7\u8f7b\u91cf\u7ea7\u76f4\u63a5\u4ea4\u6362\u4f20\u64ad\u5c422\u6570\u636e\u5e76\u91c7\u6837\u5176\u53ef\u7528\u6027\uff0c\u8bbe\u8ba1\u8003\u8651\u4e86\u6d88\u606f\u4e22\u5931\u3001\u8282\u70b9\u6545\u969c\u548c\u65e0\u54cd\u5e94\u53c2\u4e0e\u8005\u3002", "result": "\u57281000\u8282\u70b9\u96c6\u7fa4\u548c20000\u8282\u70b9\u7684\u6a21\u62df\u4e2d\uff0cPANDAS\u57284\u79d2\u5185\u5b9e\u73b0\u4e86\u884c\u661f\u7ea7\u5ef6\u8fdf\u4e0b\u7684\u6570\u636e\u4f20\u64ad\u548c\u91c7\u6837\u3002", "conclusion": "PANDAS\u4e3a\u4ee5\u592a\u574aDanksharding\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u5171\u8bc6\u548c\u8282\u70b9\u53d1\u73b0\u534f\u8bae\u7684\u5b9e\u7528DAS\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2507.00686", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00686", "abs": "https://arxiv.org/abs/2507.00686", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadiant\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u7528\u4e8e\u5c06\u7269\u8054\u7f51\uff08IoT\uff09\u4f20\u611f\u5668\u6570\u636e\u62bd\u8c61\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u7ea7\u522b\u7684\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u590d\u6742\u4e8b\u4ef6\u5904\u7406\uff08CEP\uff09\u5e94\u7528\u5b9e\u65f6\u68c0\u6d4b\u6d3b\u52a8\u6267\u884c\u3002", "motivation": "\u7269\u8054\u7f51\u4f20\u611f\u5668\u6570\u636e\u8fc7\u4e8e\u7ec6\u7c92\u5ea6\uff0c\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u4e1a\u52a1\u6d41\u7a0b\u5206\u6790\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u5176\u62bd\u8c61\u4e3a\u66f4\u9ad8\u5c42\u6b21\u7684\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86Radiant DSL\uff0c\u7528\u4e8e\u6307\u5b9a\u4f20\u611f\u5668\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3aCEP\u5e94\u7528\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u4e8b\u4ef6\u62bd\u8c61\u3002", "result": "\u5728\u667a\u80fd\u5236\u9020\u548c\u667a\u80fd\u533b\u7597\u9886\u57df\u8bc4\u4f30\u4e86CEP\u5e94\u7528\uff0c\u7ed3\u679c\u663e\u793a\u5176\u80fd\u6709\u6548\u76d1\u6d4b\u6d3b\u52a8\u6267\u884c\uff0c\u5e76\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "Radiant DSL\u548cCEP\u5e94\u7528\u4e3a\u7269\u8054\u7f51\u4f20\u611f\u5668\u6570\u636e\u7684\u4e1a\u52a1\u6d41\u7a0b\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e2e\u52a9\u9886\u57df\u4e13\u5bb6\u63d0\u5347\u6d3b\u52a8\u68c0\u6d4b\u8d28\u91cf\u3002"}}
{"id": "2507.00180", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00180", "abs": "https://arxiv.org/abs/2507.00180", "authors": ["Vidhi Rathore"], "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "comment": null, "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4ece\u9ed1\u76d2\u9057\u7559\u7cfb\u7edf\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u903b\u8f91\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u884c\u4e3a\u514b\u9686\uff09\u4ec5\u590d\u5236\u8f93\u5165\u8f93\u51fa\u884c\u4e3a\uff0c\u65e0\u6cd5\u6355\u6349\u5e95\u5c42\u610f\u56fe\uff0c\u800c\u9057\u7559\u7cfb\u7edf\u7f3a\u4e4f\u6587\u6863\u548c\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63a2\u7d22\u8f93\u5165\u7a7a\u95f4\uff0c\u8bc6\u522b\u5173\u952e\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7K-Means\u805a\u7c7b\u548c\u51b3\u7b56\u6811\u63d0\u53d6\u53ef\u8bfb\u89c4\u5219\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u9057\u7559\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u63d0\u53d6\u7684\u89c4\u5219\u51c6\u786e\u53cd\u6620\u4e86\u6838\u5fc3\u903b\u8f91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9057\u7559\u7cfb\u7edf\u8fc1\u79fb\u4e2d\u7684\u89c4\u8303\u548c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002"}}
{"id": "2507.00246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00246", "abs": "https://arxiv.org/abs/2507.00246", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u975e\u82f1\u8bed\u8bed\u8a00\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u66f4\u8282\u7701token\u4e14\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u652f\u6301\u591a\u8bed\u8a00\u63a8\u7406\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u8ba8\u82f1\u8bed\u662f\u5426\u662f\u6700\u9ad8\u6548\u7684\u63a8\u7406\u8bed\u8a00\uff0c\u5e76\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u5f00\u6e90\u8bed\u8a00\u63a8\u7406\u6a21\u578b\uff08DeepSeek R1\u3001Qwen 2.5\u3001Qwen 3\uff09\u5728\u56db\u79cd\u6570\u5b66\u6570\u636e\u96c6\u548c\u4e03\u79cd\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u975e\u82f1\u8bed\u8bed\u8a00\u63a8\u7406\u51cf\u5c11token\u4f7f\u7528\u4e14\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4e14\u6548\u679c\u4e0d\u53d7\u7ffb\u8bd1\u5f71\u54cd\u3002", "conclusion": "\u591a\u8bed\u8a00\u63a8\u7406\u5177\u6709\u6f5c\u529b\uff0c\u5f3a\u8c03\u591a\u8bed\u8a00\u57fa\u7840\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.00052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00052", "abs": "https://arxiv.org/abs/2507.00052", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "AI": {"tldr": "VSF--Med\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6f0f\u6d1e\u8bc4\u5206\u6846\u67b6\uff0c\u5305\u542b\u6587\u672c\u63d0\u793a\u653b\u51fb\u6a21\u677f\u3001\u89c6\u89c9\u6270\u52a8\u548c\u516b\u7ef4\u8bc4\u5206\u6807\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u7597VLM\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u533b\u7597VLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u4e0d\u8db3\uff0cVSF--Med\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u5b89\u5168\u8bc4\u4f30\u5de5\u5177\u3002", "method": "VSF--Med\u7ed3\u5408\u6587\u672c\u653b\u51fb\u6a21\u677f\u3001\u89c6\u89c9\u6270\u52a8\u548cLLM\u8bc4\u5206\u6807\u51c6\uff0c\u751f\u621030,000\u591a\u79cd\u5bf9\u6297\u6027\u53d8\u4f53\uff0c\u5e76\u901a\u8fc7z-score\u5f52\u4e00\u5316\u8ba1\u7b97\u7efc\u5408\u98ce\u9669\u6307\u6807\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u4e3b\u6d41VLM\u5728\u653b\u51fb\u6301\u7eed\u6027\u3001\u63d0\u793a\u6ce8\u5165\u6709\u6548\u6027\u548c\u5b89\u5168\u7ed5\u8fc7\u6210\u529f\u7387\u4e0a\u5747\u5b58\u5728\u663e\u8457\u6f0f\u6d1e\uff0cLlama-3.2-11B-Vision-Instruct\u548cGPT-4o\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "VSF--Med\u4e3a\u533b\u7597VLM\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2507.00855", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.00855", "abs": "https://arxiv.org/abs/2507.00855", "authors": ["Marta Navarro", "Josu\u00e9 Feliu", "Salvador Petit", "Mar\u00eda E. G\u00f3mez", "Julio Sahuquillo"], "title": "A New Family of Thread to Core Allocation Policies for an SMT ARM Processor", "comment": "13 pages", "summary": "Modern high-performance servers commonly integrate Simultaneous\nMultithreading (SMT) processors, which efficiently boosts throughput over\nsingle-threaded cores. Optimizing performance in SMT processors faces\nchallenges due to the inter-application interference within each SMT core. To\nmitigate the interference, thread-to-core (T2C) allocation policies play a\npivotal role. State-of-the-art T2C policies work in two steps: i) building a\nper-application performance stack using performance counters and ii) building\nperformance prediction models to identify the best pairs of applications to run\non each core.\n  This paper explores distinct ways to build the performance stack in ARM\nprocessors and introduces the Instructions and Stalls Cycles (ISC) stack, a\nnovel approach to overcome ARM PMU limitations. The ISC stacks are used as\ninputs for a performance prediction model to estimate the applications'\nperformance considering the inter-application interference. The accuracy of the\nprediction model (second step) depends on the accuracy of the performance stack\n(first step); thus, the higher the accuracy of the performance stack, the\nhigher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies.\nExperimental results show that $SYNPA4$, the best-performing SYNPA variant,\noutperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the\ngains achieved by the state-of-the-art policies for ARM processors.\nFurthermore, the multiple discussions and refinements presented throughout this\npaper can be applied to other SMT processors from distinct vendors and are\naimed at helping performance analysts build performance stacks for accurate\nperformance estimates in real processors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSYNPA\u7684\u7ebf\u7a0b\u5230\u6838\u5fc3\uff08T2C\uff09\u5206\u914d\u7b56\u7565\u5bb6\u65cf\uff0c\u901a\u8fc7\u6539\u8fdb\u6027\u80fd\u5806\u6808\u6784\u5efa\u65b9\u6cd5\uff08ISC\u5806\u6808\uff09\u548c\u6027\u80fd\u9884\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86ARM\u5904\u7406\u5668\u4e0a\u591a\u7ebf\u7a0b\u5e94\u7528\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u9ad8\u6027\u80fd\u670d\u52a1\u5668\u5e7f\u6cdb\u4f7f\u7528SMT\u5904\u7406\u5668\uff0c\u4f46\u591a\u7ebf\u7a0b\u5e94\u7528\u95f4\u7684\u5e72\u6270\u95ee\u9898\u9650\u5236\u4e86\u6027\u80fd\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6027\u80fd\u5806\u6808\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u5347T2C\u5206\u914d\u7b56\u7565\u7684\u51c6\u786e\u6027\u3002", "method": "1. \u63d0\u51faISC\u5806\u6808\u65b9\u6cd5\uff0c\u514b\u670dARM PMU\u7684\u9650\u5236\uff1b2. \u4f7f\u7528ISC\u5806\u6808\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u6027\u80fd\u9884\u6d4b\u6a21\u578b\uff1b3. \u63d0\u51faSYNPA\u7cfb\u5217T2C\u5206\u914d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u4f73\u53d8\u4f53SYNPA4\u7684\u5468\u8f6c\u65f6\u95f4\u6bd4Linux\u63d0\u5347\u4e8638%\uff0c\u662f\u73b0\u6709ARM\u5904\u7406\u5668\u7b56\u7565\u76843\u500d\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8eARM\u5904\u7406\u5668\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6SMT\u5904\u7406\u5668\uff0c\u4e3a\u6027\u80fd\u5206\u6790\u5e08\u63d0\u4f9b\u4e86\u6784\u5efa\u9ad8\u6027\u80fd\u5806\u6808\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.00699", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00699", "abs": "https://arxiv.org/abs/2507.00699", "authors": ["Guoliang Duan", "Mingwei Liu", "Yanlin Wang", "Chong Wang", "Xin Peng", "Zibin Zheng"], "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback", "comment": null, "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF.", "AI": {"tldr": "MultiCodeIF\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u4e2d\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u7ef4\u5ea6\u7ea6\u675f\uff0c\u63ed\u793aLLMs\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u5f00\u53d1\u4e2d\u7684\u590d\u6742\u7ea6\u675f\uff0c\u9700\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faMultiCodeIF\u57fa\u51c6\uff0c\u57fa\u4e8e9\u7c7b27\u79cd\u7ea6\u675f\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210\u4efb\u52a1\uff0c\u8bc4\u4f30LLMs\u5728\u591a\u8f6e\u53cd\u9988\u4e2d\u7684\u8868\u73b0\u3002", "result": "Claude-3-7-Sonnet\u8868\u73b0\u6700\u4f73\uff0863.0%\uff09\uff0c\u4f46\u591a\u5c42\u7ea7\u7ea6\u675f\u4e0b\u6a21\u578b\u6210\u529f\u7387\u964d\u81f318.8%\uff1b\u53cd\u9988\u53ef\u63d0\u5347\u81f383.4%\u3002", "conclusion": "MultiCodeIF\u4e3aLLMs\u5728\u771f\u5b9e\u4ee3\u7801\u751f\u6210\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53cd\u9988\u654f\u611f\u7684\u6846\u67b6\u3002"}}
{"id": "2507.00181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00181", "abs": "https://arxiv.org/abs/2507.00181", "authors": ["Georgios P. Georgiou"], "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline", "comment": null, "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cChatGPT\u7b49\u751f\u6210\u5f0fAI\u5de5\u5177\u53ef\u80fd\u964d\u4f4e\u5b66\u751f\u5728\u5b66\u672f\u5199\u4f5c\u4efb\u52a1\u4e2d\u7684\u8ba4\u77e5\u6295\u5165\uff0c\u5bfc\u81f4\u8ba4\u77e5\u5378\u8f7d\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5de5\u5177\uff08\u5982ChatGPT\uff09\u5bf9\u5b66\u751f\u8ba4\u77e5\u6295\u5165\u7684\u5f71\u54cd\uff0c\u4ee5\u5e94\u5bf9\u6559\u80b2\u4e2d\u5bf9AI\u53ef\u80fd\u524a\u5f31\u6df1\u5ea6\u601d\u8003\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u62c5\u5fe7\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u968f\u673a\u5206\u914d\u5b66\u751f\u81f3AI\u8f85\u52a9\uff08ChatGPT\uff09\u6216\u975e\u8f85\u52a9\uff08\u5bf9\u7167\u7ec4\uff09\u6761\u4ef6\uff0c\u5b8c\u6210\u8bae\u8bba\u6587\u5199\u4f5c\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528\u8ba4\u77e5\u6295\u5165\u91cf\u8868\uff08CES-AI\uff09\u8bc4\u4f30\u3002", "result": "ChatGPT\u7ec4\u7684\u8ba4\u77e5\u6295\u5165\u5206\u6570\u663e\u8457\u4f4e\u4e8e\u5bf9\u7167\u7ec4\uff0c\u8868\u660eAI\u8f85\u52a9\u53ef\u80fd\u5bfc\u81f4\u8ba4\u77e5\u5378\u8f7d\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5f00\u53d1\u6559\u5b66\u7b56\u7565\uff0c\u4fc3\u8fdb\u5b66\u751f\u4e0eAI\u751f\u6210\u5185\u5bb9\u7684\u4e3b\u52a8\u53cd\u601d\u6027\u4e92\u52a8\uff0c\u4ee5\u907f\u514d\u635f\u5bb3\u81ea\u4e3b\u5b66\u4e60\u548c\u6df1\u5ea6\u8ba4\u77e5\u6295\u5165\u3002"}}
{"id": "2507.00258", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00258", "abs": "https://arxiv.org/abs/2507.00258", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u57fa\u4e8e\u63d0\u793a\u7684\u5fae\u8c03\u5728\u6027\u80fd\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u6bd4\u57fa\u4e8e\u53c2\u6570\u7684\u5fae\u8c03\u66f4\u9690\u79c1\u5b89\u5168\u3002", "motivation": "\u968f\u7740\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5fae\u8c03\u65b9\u6cd5\u591a\u6837\u5316\uff0c\u4f46\u5176\u9690\u79c1\u98ce\u9669\uff08\u5982\u8bb0\u5fc6\u5316\uff09\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002", "method": "\u901a\u8fc7\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u8bc4\u4f30\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5bf9\u8bb0\u5fc6\u5316\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u4e8e\u63d0\u793a\u7684\u5fae\u8c03\u6027\u80fd\u4e0e\u57fa\u4e8e\u53c2\u6570\u7684\u5fae\u8c03\u76f8\u5f53\uff0c\u4f46\u5bf9MIAs\u7684\u8106\u5f31\u6027\u66f4\u4f4e\uff0c\u4e14\u4e0d\u53d7\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u3002", "conclusion": "\u57fa\u4e8e\u53c2\u6570\u7684\u5fae\u8c03\u66f4\u5bb9\u6613\u6cc4\u9732\u9690\u79c1\u4fe1\u606f\uff0c\u800c\u57fa\u4e8e\u63d0\u793a\u7684\u5fae\u8c03\u662f\u66f4\u9690\u79c1\u5b89\u5168\u7684\u9009\u62e9\u3002"}}
{"id": "2507.00068", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00068", "abs": "https://arxiv.org/abs/2507.00068", "authors": ["Ziqi Zhong", "Daniel Tang"], "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "AI": {"tldr": "MANTA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7edf\u4e00\u89c6\u89c9\u548c\u542c\u89c9\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u3001\u65f6\u95f4\u540c\u6b65\u3001\u591a\u5c3a\u5ea6\u8868\u793a\u548c\u7a00\u758f\u4fe1\u606f\u68c0\u7d22\u7b49\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u95ee\u7b54\u548c\u8de8\u6a21\u6001\u7406\u89e3\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u4e0d\u540c\u6a21\u6001\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u8868\u793a\u548c\u63a8\u7406\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u591a\u6a21\u6001\u6570\u636e\u7684\u5904\u7406\u3002", "method": "MANTA\u901a\u8fc7\u4fe1\u606f\u8bba\u4f18\u5316\u5b9e\u73b0\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u81ea\u9002\u5e94\u65f6\u95f4\u540c\u6b65\uff0c\u5206\u5c42\u5185\u5bb9\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\uff0c\u5e76\u91c7\u7528\u6570\u5b66\u6846\u67b6\u8bc1\u660e\u5176\u5728\u4ee4\u724c\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u6027\u3002", "result": "\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cMANTA\u5c06\u6700\u5148\u8fdb\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8622.6%\uff0c\u5728\u8d85\u8fc730\u5206\u949f\u7684\u89c6\u9891\u4e2d\u63d0\u534727.3%\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u63a8\u7406\u548c\u8de8\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u534723.8%\u548c25.1%\u3002", "conclusion": "MANTA\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u7edf\u4e00\u591a\u6a21\u6001\u8868\u793a\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.00909", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.00909", "abs": "https://arxiv.org/abs/2507.00909", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "comment": "10 pages, 6 figures, 1 table", "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEmerald Conductor\u7684\u7eaf\u8f6f\u4ef6\u65b9\u6cd5\uff0c\u5c06AI\u6570\u636e\u4e2d\u5fc3\u8f6c\u53d8\u4e3a\u7075\u6d3b\u7684\u7535\u7f51\u8d44\u6e90\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\u6539\u9020\u5373\u53ef\u9ad8\u6548\u5229\u7528\u73b0\u6709\u7535\u529b\u7cfb\u7edf\u3002", "motivation": "AI\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u7535\u529b\u9700\u6c42\u6fc0\u589e\uff0c\u5a01\u80c1\u7535\u7f51\u53ef\u9760\u6027\uff0c\u589e\u52a0\u793e\u533a\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\u6210\u672c\uff0c\u5e76\u963b\u788dAI\u521b\u65b0\u3002", "method": "\u901a\u8fc7\u5b9e\u65f6\u7535\u7f51\u4fe1\u53f7\u534f\u8c03AI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u6216\u50a8\u80fd\uff0c\u5728\u51e4\u51f0\u57ce\u7684\u4e00\u4e2a256-GPU\u96c6\u7fa4\u4e2d\u8fdb\u884c\u4e86\u8bd5\u9a8c\u3002", "result": "\u8bd5\u9a8c\u671f\u95f4\uff0c\u5728\u7535\u7f51\u9ad8\u5cf0\u65f6\u6bb5\u5b9e\u73b0\u4e8625%\u7684\u96c6\u7fa4\u529f\u8017\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301AI\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u6570\u636e\u4e2d\u5fc3\u91cd\u65b0\u6784\u60f3\u4e3a\u589e\u5f3a\u7535\u7f51\u53ef\u9760\u6027\u3001\u964d\u4f4e\u6210\u672c\u548c\u52a0\u901fAI\u53d1\u5c55\u7684\u4ea4\u4e92\u5f0f\u7535\u7f51\u8d44\u4ea7\u3002"}}
{"id": "2507.00786", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00786", "abs": "https://arxiv.org/abs/2507.00786", "authors": ["Jukka Ruohonen", "Qusai Ramadan"], "title": "Snaps: Bloated and Outdated?", "comment": "Submitted as a \"poster paper\" to APSEC", "summary": "Snap is an alternative software packaging system developed by Canonical and\nprovided by default in the Ubuntu Linux distribution. Given the heterogeneity\nof various Linux distributions and their various releases, Snap allows an\ninteroperable delivery of software directly to users. However, concerns and\ncriticism have also been frequently expressed. Regarding this criticism, the\npaper shows that currently distributed snap packages are indeed on average\nbloated in terms of their sizes and outdated in terms updating frequencies.\nWith these empirical observations, this short paper contributes to the research\ndomain of software packaging, software packages, and package managers.", "AI": {"tldr": "Snap\u8f6f\u4ef6\u5305\u7cfb\u7edf\u5728\u5c3a\u5bf8\u548c\u66f4\u65b0\u9891\u7387\u4e0a\u5b58\u5728\u81a8\u80c0\u548c\u6ede\u540e\u95ee\u9898\u3002", "motivation": "\u7814\u7a76Snap\u8f6f\u4ef6\u5305\u7cfb\u7edf\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u4ee5\u9a8c\u8bc1\u5bf9\u5176\u7684\u6279\u8bc4\u662f\u5426\u6210\u7acb\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u89c2\u5bdf\u5206\u6790\u5f53\u524dSnap\u8f6f\u4ef6\u5305\u7684\u5c3a\u5bf8\u548c\u66f4\u65b0\u9891\u7387\u3002", "result": "\u53d1\u73b0Snap\u8f6f\u4ef6\u5305\u5e73\u5747\u5c3a\u5bf8\u8fc7\u5927\u4e14\u66f4\u65b0\u9891\u7387\u6ede\u540e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f6f\u4ef6\u5305\u7ba1\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u652f\u6301\u4e86\u5bf9Snap\u7684\u6279\u8bc4\u3002"}}
{"id": "2507.00205", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00205", "abs": "https://arxiv.org/abs/2507.00205", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "comment": "Submitted to npj Digital Medicine", "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "AI": {"tldr": "xHAIM\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u76f8\u5173\u6570\u636e\u8bc6\u522b\u3001\u60a3\u8005\u6458\u8981\u751f\u6210\u3001\u9884\u6d4b\u5efa\u6a21\u548c\u4e34\u5e8a\u89e3\u91ca\uff0c\u63d0\u5347\u4e86HAIM\u6846\u67b6\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3HAIM\u6846\u67b6\u5728\u4efb\u52a1\u65e0\u5173\u6027\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "xHAIM\u901a\u8fc7\u56db\u4e2a\u6b65\u9aa4\u5b9e\u73b0\uff1a\u4efb\u52a1\u76f8\u5173\u6570\u636e\u8bc6\u522b\u3001\u60a3\u8005\u6458\u8981\u751f\u6210\u3001\u9884\u6d4b\u5efa\u6a21\u548c\u4e34\u5e8a\u89e3\u91ca\u3002", "result": "\u5728HAIM-MIMIC-MM\u6570\u636e\u96c6\u4e0a\uff0cxHAIM\u5c06\u5e73\u5747AUC\u4ece79.9%\u63d0\u5347\u81f390.3%\u3002", "conclusion": "xHAIM\u5c06AI\u4ece\u9ed1\u76d2\u9884\u6d4b\u5668\u8f6c\u53d8\u4e3a\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.00297", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00297", "abs": "https://arxiv.org/abs/2507.00297", "authors": ["David Ifeoluwa Adelani"], "title": "Natural language processing for African languages", "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u7279\u522b\u662f\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u8bed\u8a00\uff09\u5728NLP\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u5347\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e86\u6807\u6ce8\u6570\u636e\u96c6\u4ee5\u652f\u6301\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728NLP\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u8bed\u8a00\uff0c\u56e0\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u548c\u9ad8\u8d28\u91cf\u8bed\u6599\u800c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5206\u6790\u516c\u5f00\u8bed\u6599\u5e93\u7684\u566a\u58f0\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\uff1b\u7814\u7a76\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u9002\u5e94\u6027\uff1b\u5f00\u53d121\u79cd\u975e\u6d32\u8bed\u8a00\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u8868\u660e\u8bed\u4e49\u8868\u793a\u8d28\u91cf\u4f9d\u8d56\u6570\u636e\u8d28\u91cf\u800c\u975e\u6570\u91cf\uff1b\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u6f5c\u529b\uff1b\u6807\u6ce8\u6570\u636e\u96c6\u652f\u6301\u4e86\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u6027\u80fd\u7684\u5173\u952e\uff0c\u975e\u6d32\u8bed\u8a00\u7684\u6807\u6ce8\u6570\u636e\u96c6\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.00070", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00070", "abs": "https://arxiv.org/abs/2507.00070", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zu\u00f1iga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "title": "An efficient plant disease detection using transfer learning approach", "comment": "15 pages , 4 figures. Scientific Reports 2025", "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv7\u548cYOLOv8\u7684\u8fc1\u79fb\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u68c0\u6d4b\u690d\u7269\u75c5\u5bb3\uff0c\u7ed3\u679c\u663e\u793aYOLOv8\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u519c\u4e1a\u3002", "motivation": "\u690d\u7269\u75c5\u5bb3\u5bf9\u519c\u4e1a\u9020\u6210\u91cd\u5927\u5f71\u54cd\uff0c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u6280\u672f\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u4f7f\u7528YOLOv7\u548cYOLOv8\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u690d\u7269\u53f6\u7247\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u68c0\u6d4b\u7ec6\u83cc\u3001\u771f\u83cc\u548c\u75c5\u6bd2\u75c5\u5bb3\u3002", "result": "\u6a21\u578b\u6027\u80fd\u6307\u6807\u4f18\u5f02\uff0cmAP\u4e3a91.05\uff0cF1-score\u4e3a89.40\uff0cYOLOv8\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u690d\u7269\u75c5\u5bb3\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u3002"}}
{"id": "2507.00949", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.00949", "abs": "https://arxiv.org/abs/2507.00949", "authors": ["Yuqing Wang", "Charles Colley", "Brian Wheatman", "Jiya Su", "David F. Gleich", "Andrew A. Chien"], "title": "How Fast Can Graph Computations Go on Fine-grained Parallel Architectures", "comment": "13 pages, 11 figures, 6 tables", "summary": "Large-scale graph problems are of critical and growing importance and\nhistorically parallel architectures have provided little support. In the spirit\nof co-design, we explore the question, How fast can graph computing go on a\nfine-grained architecture? We explore the possibilities of an architecture\noptimized for fine-grained parallelism, natural programming, and the\nirregularity and skew found in real-world graphs. Using two graph benchmarks,\nPageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph\narchitecture, UpDown, to explore what performance codesign can achieve. To\ndemonstrate programmability, we wrote five variants of these algorithms.\nSimulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes\n(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS\non RMAT, exceeding the best prior results by 5x and 100x respectively.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7ec6\u7c92\u5ea6\u5e76\u884c\u67b6\u6784\u4e0a\u5b9e\u73b0\u9ad8\u6548\u56fe\u8ba1\u7b97\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8bbe\u8ba1UpDown\u67b6\u6784\uff0c\u5728PageRank\u548cBFS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u89c4\u6a21\u56fe\u8ba1\u7b97\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5e76\u884c\u67b6\u6784\u652f\u6301\u4e0d\u8db3\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7ec6\u7c92\u5ea6\u67b6\u6784\u7684\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86UpDown\u67b6\u6784\uff0c\u9488\u5bf9\u7ec6\u7c92\u5ea6\u5e76\u884c\u6027\u548c\u771f\u5b9e\u56fe\u7684\u4e0d\u89c4\u5219\u6027\u4f18\u5316\uff0c\u5e76\u901a\u8fc7PageRank\u548cBFS\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5728256\u8282\u70b9\u6a21\u62df\u548c16,384\u8282\u70b9\u6295\u5f71\u4e2d\uff0cUpDown\u7cfb\u7edf\u5206\u522b\u5b9e\u73b0\u4e86637K GTEPS PR\u548c989K GTEPS BFS\uff0c\u6027\u80fd\u8fdc\u8d85\u4ee5\u5f80\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "UpDown\u67b6\u6784\u901a\u8fc7\u4ee3\u7801\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8ba1\u7b97\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7ec6\u7c92\u5ea6\u5e76\u884c\u67b6\u6784\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.00788", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma S\u00f6derberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u52a9\u624b\u5bf9\u8f6f\u4ef6\u53ef\u7ef4\u62a4\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u8f85\u52a9\u5f00\u53d1\u80fd\u8f7b\u5fae\u63d0\u5347\u540e\u7eed\u4ee3\u7801\u6f14\u5316\u7684\u901f\u5ea6\u548c\u4ee3\u7801\u5065\u5eb7\u5ea6\uff0c\u4f46\u6574\u4f53\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "motivation": "AI\u52a9\u624b\u5982GitHub Copilot\u548cCursor\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4f46\u5176\u5bf9\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u5b9e\u9a8c\uff0c151\u540d\u53c2\u4e0e\u8005\uff0895%\u4e3a\u4e13\u4e1a\u5f00\u53d1\u8005\uff09\u5206\u522b\u5728\u6709/\u65e0AI\u8f85\u52a9\u4e0b\u5f00\u53d1\u529f\u80fd\uff0c\u968f\u540e\u7531\u65b0\u53c2\u4e0e\u8005\u6f14\u5316\u4ee3\u7801\u3002", "result": "AI\u8f85\u52a9\u5f00\u53d1\u663e\u8457\u63d0\u5347\u5f00\u53d1\u901f\u5ea6\uff08\u4e2d\u4f4d\u6570\u51cf\u5c1130.7%\u4efb\u52a1\u65f6\u95f4\uff09\uff0c\u4f46\u5bf9\u53ef\u7ef4\u62a4\u6027\u5f71\u54cd\u8f83\u5c0f\uff0c\u4ec5\u4e60\u60ef\u6027\u7528\u6237\u4ee3\u7801\u5065\u5eb7\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AI\u52a9\u624b\u80fd\u6709\u6548\u52a0\u901f\u5f00\u53d1\u4e14\u672a\u635f\u5bb3\u53ef\u7ef4\u62a4\u6027\uff0c\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u4ee3\u7801\u81a8\u80c0\u548c\u8ba4\u77e5\u503a\u52a1\u98ce\u9669\u3002"}}
{"id": "2507.00218", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.00218", "abs": "https://arxiv.org/abs/2507.00218", "authors": ["Fangting Zhou", "Attila Lischka", "Balazs Kulcsar", "Jiaming Wu", "Morteza Haghir Chehreghani", "Gilbert Laporte"], "title": "Learning for routing: A guided review of recent developments and future directions", "comment": "Accepted for publication in Transportation Research Part E: Logistics\n  and Transportation Review", "summary": "This paper reviews the current progress in applying machine learning (ML)\ntools to solve NP-hard combinatorial optimization problems, with a focus on\nrouting problems such as the traveling salesman problem (TSP) and the vehicle\nrouting problem (VRP). Due to the inherent complexity of these problems, exact\nalgorithms often require excessive computational time to find optimal\nsolutions, while heuristics can only provide approximate solutions without\nguaranteeing optimality. With the recent success of machine learning models,\nthere is a growing trend in proposing and implementing diverse ML techniques to\nenhance the resolution of these challenging routing problems. We propose a\ntaxonomy categorizing ML-based routing methods into construction-based and\nimprovement-based approaches, highlighting their applicability to various\nproblem characteristics. This review aims to integrate traditional OR methods\nwith state-of-the-art ML techniques, providing a structured framework to guide\nfuture research and address emerging VRP variants.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u5728\u89e3\u51b3NP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff08\u5982TSP\u548cVRP\uff09\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0e\u4f20\u7edf\u8fd0\u7b79\u5b66\u65b9\u6cd5\u7684\u7ed3\u5408\u3002", "motivation": "\u7531\u4e8eNP\u96be\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u6c42\u89e3\uff0c\u673a\u5668\u5b66\u4e60\u4e3a\u8fd9\u4e9b\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5c06\u57fa\u4e8eML\u7684\u8def\u7531\u65b9\u6cd5\u5206\u4e3a\u6784\u9020\u578b\u548c\u6539\u8fdb\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u9002\u7528\u6027\u3002", "result": "\u7efc\u8ff0\u4e86ML\u5728\u8def\u7531\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "conclusion": "ML\u4e0e\u4f20\u7edf\u8fd0\u7b79\u5b66\u65b9\u6cd5\u7684\u7ed3\u5408\u4e3a\u89e3\u51b3\u65b0\u5174VRP\u53d8\u4f53\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.00322", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5e73\u8861\u62ec\u53f7\u7b49\u7b80\u5355\u8bed\u6cd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7814\u7a76\u53d1\u73b0\u9519\u8bef\u6e90\u4e8e\u4e0d\u53ef\u9760\u7ec4\u4ef6\uff08\u5982\u6ce8\u610f\u529b\u5934\u548cFF\u795e\u7ecf\u5143\uff09\u7684\u5e72\u6270\u3002\u63d0\u51faRASteer\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u53ef\u9760\u7ec4\u4ef6\u7684\u4f5c\u7528\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u8bed\u6cd5\u4efb\u52a1\u4e2d\u6301\u7eed\u51fa\u9519\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u5206\u6790\u6a21\u578b\u7ec4\u4ef6\uff08\u6ce8\u610f\u529b\u5934\u548cFF\u795e\u7ecf\u5143\uff09\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u53ef\u9760\u4e0e\u4e0d\u53ef\u9760\u7ec4\u4ef6\u7684\u5dee\u5f02\uff0c\u63d0\u51faRASteer\u65b9\u6cd5\u589e\u5f3a\u53ef\u9760\u7ec4\u4ef6\u3002", "result": "RASteer\u5c06\u5e73\u8861\u62ec\u53f7\u4efb\u52a1\u7684\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f3\u8fd1100%\uff0c\u5e76\u5728\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ea620%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u548c\u589e\u5f3a\u53ef\u9760\u7ec4\u4ef6\uff0cRASteer\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u901a\u7528\u80fd\u529b\u3002"}}
{"id": "2507.00153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00153", "abs": "https://arxiv.org/abs/2507.00153", "authors": ["Peter Mortimer", "Mirko Maehlisch"], "title": "Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "The performance of leaning-based perception algorithms suffer when deployed\nin out-of-distribution and underrepresented environments. Outdoor robots are\nparticularly susceptible to rapid changes in visual scene appearance due to\ndynamic lighting, seasonality and weather effects that lead to scenes\nunderrepresented in the training data of the learning-based perception system.\nIn this conceptual paper, we focus on preparing our autonomous vehicle for\ndeployment in snow-filled environments. We propose a novel method for\ndiffusion-based image augmentation to more closely represent the deployment\nenvironment in our training data. Diffusion-based image augmentations rely on\nthe public availability of vision foundation models learned on internet-scale\ndatasets. The diffusion-based image augmentations allow us to take control over\nthe semantic distribution of the ground surfaces in the training data and to\nfine-tune our model for its deployment environment. We employ open vocabulary\nsemantic segmentation models to filter out augmentation candidates that contain\nhallucinations. We believe that diffusion-based image augmentations can be\nextended to many other environments apart from snow surfaces, like sandy\nenvironments and volcanic terrains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u96ea\u5730\u73af\u5883\u4e2d\u7684\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5b66\u4e60\u7684\u611f\u77e5\u7b97\u6cd5\u5728\u5206\u5e03\u5916\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u6237\u5916\u673a\u5668\u4eba\u56e0\u52a8\u6001\u5149\u7167\u3001\u5b63\u8282\u548c\u5929\u6c14\u53d8\u5316\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6a21\u578b\u8fc7\u6ee4\u5e7b\u89c9\u5019\u9009\u56fe\u50cf\uff0c\u4ee5\u66f4\u63a5\u8fd1\u90e8\u7f72\u73af\u5883\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u4e2d\u5730\u9762\u8bed\u4e49\u5206\u5e03\uff0c\u5e76\u9488\u5bf9\u90e8\u7f72\u73af\u5883\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "\u6269\u6563\u56fe\u50cf\u589e\u5f3a\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u73af\u5883\uff08\u5982\u6c99\u5730\u548c\u706b\u5c71\u5730\u5f62\uff09\uff0c\u63d0\u5347\u611f\u77e5\u7b97\u6cd5\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2507.00803", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00803", "abs": "https://arxiv.org/abs/2507.00803", "authors": ["Gillian Daniel", "Chris Hall", "Per Hammer", "Alec-Angus Macdonald", "Hollie Marwick-Best", "Emma McKenzie", "George Popa", "Derek Somerville", "Tim Storer"], "title": "Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses", "comment": null, "summary": "Over more than two decades, The University of Glasgow has co-designed and\ndelivered numerous software engineering focused courses with industry partners,\ncovering both technical and discipline specific professional skills. Such\ncollaborations are not unique and many of the benefits are well recognised in\nthe literature. These include enhancing the real-world relevance of curricula,\ndeveloping student professional networks ahead of graduation and easing\nrecruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of\nindustry practitioners who participate in course design and delivery. This gap\nis significant, since the effort invested by practitioners is often substantial\nand may require ongoing support from both the industry partner and academic\ninstitution. Understanding the motivations, expectations and experiences of\npractitioners who engage in course delivery can guide the formation of future\npartnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective\nconducted amongst the practitioner coauthors of this paper, with the academic\ncoauthors acting as facilitators. All coauthors have participated in the recent\nco-design and delivery of software engineering courses, but we choose to focus\nexplicitly on the perspectives of the practitioners. We report on the themes\nthat emerged from the discussions and our resulting recommendations for future\ncollaborations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u884c\u4e1a\u4ece\u4e1a\u8005\u53c2\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u8bbe\u8ba1\u4e0e\u4ea4\u4ed8\u7684\u89c6\u89d2\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5408\u4f5c\u7684\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u884c\u4e1a\u4ece\u4e1a\u8005\u5728\u8bfe\u7a0b\u8bbe\u8ba1\u4e0e\u4ea4\u4ed8\u4e2d\u7684\u89c6\u89d2\uff0c\u800c\u4ed6\u4eec\u7684\u6295\u5165\u548c\u652f\u6301\u5bf9\u5408\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u56de\u987e\u6027\u7814\u7a76\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4e86\u4ece\u4e1a\u8005\u7684\u89c2\u70b9\uff0c\u5b66\u672f\u4f5c\u8005\u4f5c\u4e3a\u534f\u8c03\u8005\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4ece\u4e1a\u8005\u7684\u52a8\u673a\u3001\u671f\u671b\u548c\u7ecf\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5408\u4f5c\u7684\u5efa\u8bae\u3002", "conclusion": "\u7406\u89e3\u4ece\u4e1a\u8005\u7684\u89c6\u89d2\u6709\u52a9\u4e8e\u5f62\u6210\u53ef\u6301\u7eed\u7684\u884c\u4e1a-\u5b66\u672f\u5408\u4f5c\u5173\u7cfb\u3002"}}
{"id": "2507.00417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00417", "abs": "https://arxiv.org/abs/2507.00417", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "AI": {"tldr": "ASTRO\u6846\u67b6\u901a\u8fc7\u81ea\u56de\u5f52\u641c\u7d22\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u81ea\u6211\u53cd\u601d\u3001\u56de\u6eaf\u548c\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u63a8\u7406\u6a21\u578b\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u975e\u63a8\u7406\u6a21\u578b\uff08\u5982Llama 3\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u884c\u4e3a\u8bad\u7ec3\u589e\u5f3a\u5176\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u5c06\u641c\u7d22\u8f68\u8ff9\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u94fe\u5f0f\u601d\u8003\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5728MATH-500\u3001AMC 2023\u548cAIME 2024\u4e0a\u5206\u522b\u5b9e\u73b016.0%\u300126.9%\u548c20.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u9700\u8981\u8fed\u4ee3\u4fee\u6b63\u7684\u96be\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u641c\u7d22\u542f\u53d1\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.00330", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00330", "abs": "https://arxiv.org/abs/2507.00330", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "COLDSELECT\u662f\u4e00\u79cd\u8054\u5408\u9009\u62e9verbalizer\u548c\u5b9e\u4f8b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u6570\u636e\u591a\u6837\u6027\uff0c\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5bf9\u6a21\u677f\u3001verbalizer\u548c\u5c11\u6837\u672c\u5b9e\u4f8b\u9009\u62e9\u654f\u611f\uff0c\u5c24\u5176\u5728\u65e0\u6807\u7b7e\u6570\u636e\u7684\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u3002\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5b9e\u4f8b\u4e0everbalizer\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "COLDSELECT\u5c06PLM\u8bcd\u6c47\u548c$h_{[MASK]}$\u5d4c\u5165\u6620\u5c04\u5230\u5171\u4eab\u7a7a\u95f4\uff0c\u5e94\u7528\u964d\u7ef4\u548c\u805a\u7c7b\uff0c\u4ee5\u6700\u5c0f\u5316\u4e0d\u786e\u5b9a\u6027\u548c\u6700\u5927\u5316\u591a\u6837\u6027\u4e3a\u76ee\u6807\u8fdb\u884c\u9009\u62e9\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOLDSELECT\u5728\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u548c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "COLDSELECT\u901a\u8fc7\u8054\u5408\u4f18\u5316verbalizer\u548c\u5b9e\u4f8b\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.00162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00162", "abs": "https://arxiv.org/abs/2507.00162", "authors": ["Yu Lu", "Yi Yang"], "title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion", "comment": "under review", "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.", "AI": {"tldr": "FreeLong\u548cFreeLong++\u662f\u65e0\u9700\u8bad\u7ec3\u7684\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u8861\u9891\u7387\u5206\u5e03\u548c\u591a\u5206\u652f\u67b6\u6784\u63d0\u5347\u89c6\u9891\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u77ed\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u5b58\u5728\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9ad8\u9891\u5931\u771f\u73b0\u8c61\u3002", "method": "FreeLong\u901a\u8fc7\u5168\u5c40\u4f4e\u9891\u548c\u5c40\u90e8\u9ad8\u9891\u7279\u5f81\u878d\u5408\uff1bFreeLong++\u6269\u5c55\u4e3a\u591a\u5206\u652f\u67b6\u6784\uff0c\u5b9e\u73b0\u591a\u9891\u6bb5\u878d\u5408\u3002", "result": "\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u957f\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff08\u59824\u500d\u548c8\u500d\u957f\u5ea6\uff09\uff0c\u652f\u6301\u591a\u63d0\u793a\u751f\u6210\u548c\u53ef\u63a7\u89c6\u9891\u751f\u6210\u3002", "conclusion": "FreeLong++\u4e3a\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5bb9\u73b0\u6709\u6a21\u578b\u5e76\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002"}}
{"id": "2507.00432", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00432", "abs": "https://arxiv.org/abs/2507.00432", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5f3a\u7684LLMs\u5728\u5176\u4ed6\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0cRL\u8c03\u4f18\u6a21\u578b\u6bd4SFT\u8c03\u4f18\u6a21\u578b\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u662f\u5426\u53cd\u6620\u66f4\u5e7f\u6cdb\u7684\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u8fc7\u5ea6\u62df\u5408\u3002", "method": "\u8bc4\u4f3020\u591a\u4e2a\u5f00\u653e\u6743\u91cd\u7684\u63a8\u7406\u8c03\u4f18\u6a21\u578b\uff0c\u6db5\u76d6\u6570\u5b66\u3001\u79d1\u5b66QA\u3001\u4ee3\u7406\u89c4\u5212\u3001\u7f16\u7801\u548c\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\uff0c\u5e76\u5bf9Qwen3-14B\u6a21\u578b\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\u3002", "result": "\u5927\u591a\u6570\u6570\u5b66\u8868\u73b0\u597d\u7684\u6a21\u578b\u5728\u5176\u4ed6\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff1bRL\u8c03\u4f18\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f3a\uff0cSFT\u8c03\u4f18\u6a21\u578b\u6613\u9057\u5fd8\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u9700\u91cd\u65b0\u601d\u8003\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9SFT\u84b8\u998f\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.00355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00355", "abs": "https://arxiv.org/abs/2507.00355", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "title": "Question Decomposition for Retrieval-Augmented Generation", "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u7684RAG\u6d41\u7a0b\uff0c\u901a\u8fc7\u5206\u89e3\u591a\u8df3\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\u5e76\u91cd\u65b0\u6392\u5e8f\u68c0\u7d22\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6RAG\u5728\u591a\u8df3\u95ee\u9898\u4e2d\u56e0\u4fe1\u606f\u5206\u6563\u800c\u96be\u4ee5\u68c0\u7d22\u8db3\u591f\u8bc1\u636e\u7684\u6311\u6218\u3002", "method": "1. \u4f7f\u7528LLM\u5206\u89e3\u539f\u59cb\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\uff1b2. \u4e3a\u6bcf\u4e2a\u5b50\u95ee\u9898\u68c0\u7d22\u76f8\u5173\u6bb5\u843d\uff1b3. \u5408\u5e76\u5e76\u91cd\u65b0\u6392\u5e8f\u5019\u9009\u6bb5\u843d\u4ee5\u63d0\u9ad8\u8986\u76d6\u7387\u548c\u7cbe\u786e\u5ea6\u3002", "result": "\u5728MultiHop-RAG\u548cHotpotQA\u4e0a\uff0c\u68c0\u7d22\u6027\u80fd\uff08MRR@10\uff09\u63d0\u534736.7%\uff0c\u7b54\u6848\u51c6\u786e\u7387\uff08F1\uff09\u63d0\u534711.6%\u3002", "conclusion": "\u95ee\u9898\u5206\u89e3\u4e0e\u91cd\u65b0\u6392\u5e8f\u7ed3\u5408\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u7d22\u5f15\u7684\u5b9e\u7528\u6539\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u9898\u7684\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2507.00170", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.00170", "abs": "https://arxiv.org/abs/2507.00170", "authors": ["Hugo Baudchon", "Arthur Ouaknine", "Martin Weiss", "M\u00e9lisande Teng", "Thomas R. Walla", "Antoine Caron-Guay", "Christopher Pal", "Etienne Lalibert\u00e9"], "title": "SelvaBox: A high-resolution dataset for tropical tree crown detection", "comment": null, "summary": "Detecting individual tree crowns in tropical forests is essential to study\nthese complex and crucial ecosystems impacted by human interventions and\nclimate change. However, tropical crowns vary widely in size, structure, and\npattern and are largely overlapping and intertwined, requiring advanced remote\nsensing methods applied to high-resolution imagery. Despite growing interest in\ntropical tree crown detection, annotated datasets remain scarce, hindering\nrobust model development. We introduce SelvaBox, the largest open-access\ndataset for tropical tree crown detection in high-resolution drone imagery. It\nspans three countries and contains more than 83,000 manually labeled crowns -\nan order of magnitude larger than all previous tropical forest datasets\ncombined. Extensive benchmarks on SelvaBox reveal two key findings: (1)\nhigher-resolution inputs consistently boost detection accuracy; and (2) models\ntrained exclusively on SelvaBox achieve competitive zero-shot detection\nperformance on unseen tropical tree crown datasets, matching or exceeding\ncompeting methods. Furthermore, jointly training on SelvaBox and three other\ndatasets at resolutions from 3 to 10 cm per pixel within a unified\nmulti-resolution pipeline yields a detector ranking first or second across all\nevaluated datasets. Our dataset, code, and pre-trained weights are made public.", "AI": {"tldr": "SelvaBox\u662f\u4e00\u4e2a\u7528\u4e8e\u70ed\u5e26\u6811\u51a0\u68c0\u6d4b\u7684\u6700\u5927\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5305\u542b83,000\u591a\u4e2a\u6807\u8bb0\u6811\u51a0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u70ed\u5e26\u6811\u51a0\u68c0\u6d4b\u5bf9\u7814\u7a76\u590d\u6742\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u5f15\u5165SelvaBox\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u548c\u591a\u5206\u8fa8\u7387\u7ba1\u9053\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff1bSelvaBox\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SelvaBox\u4e3a\u70ed\u5e26\u6811\u51a0\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.00557", "categories": ["cs.AI", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.00557", "abs": "https://arxiv.org/abs/2507.00557", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "comment": null, "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\uff082d-LS\uff09\u7528\u4e8eSMT-NRA\uff0c\u7ed3\u5408\u4e86MCSAT\u6846\u67b6\u548c\u6837\u672c\u5355\u5143\u6295\u5f71\u7b97\u5b50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u63d0\u5347SMT-NRA\u95ee\u9898\u7684\u5c40\u90e8\u641c\u7d22\u6548\u7387\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u64cd\u4f5c\u548c\u6846\u67b6\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\u3002", "method": "1. \u63d0\u51fa2d-cell-jump\u64cd\u4f5c\uff1b2. \u6269\u5c55\u5c40\u90e8\u641c\u7d22\u6846\u67b62d-LS\uff0c\u6574\u5408MCSAT\uff1b3. \u5b9e\u73b0\u6837\u672c\u5355\u5143\u6295\u5f71\u7b97\u5b50\uff1b4. \u8bbe\u8ba1\u6df7\u5408\u6846\u67b6\u7ed3\u5408MCSAT\u30012d-LS\u548cOpenCAD\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5c40\u90e8\u641c\u7d22\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SMT-NRA\u95ee\u9898\u7684\u641c\u7d22\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.00380", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00380", "abs": "https://arxiv.org/abs/2507.00380", "authors": ["Vojt\u011bch Lanz", "Jan Haji\u010d jr"], "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u683c\u91cc\u9ad8\u5229\u5723\u6b4c\u65cb\u5f8b\u7684\u5206\u6bb5\u7406\u8bba\uff08centonisation\uff09\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5206\u5c42Pitman-Yor\u8bed\u8a00\u6a21\u578b\u5bfb\u627e\u6700\u4f18\u5206\u6bb5\uff0c\u53d1\u73b0\u5176\u5728\u8c03\u5f0f\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7ed3\u679c\u4e0e\u4f20\u7edfcentonisation\u7406\u8bba\u4e0d\u7b26\u3002", "motivation": "\u7814\u7a76\u683c\u91cc\u9ad8\u5229\u5723\u6b4c\u65cb\u5f8b\u7684\u5206\u6bb5\u7406\u8bba\uff08centonisation\uff09\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u8bb0\u5fc6\u6548\u7387\u548c\u8c03\u5f0f\u5206\u7c7b\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u65e0\u76d1\u7763\u5206\u5c42Pitman-Yor\u8bed\u8a00\u6a21\u578b\u5bf9\u5723\u6b4c\u65cb\u5f8b\u8fdb\u884c\u6700\u4f18\u5206\u6bb5\u3002", "result": "\u6700\u4f18\u5206\u6bb5\u5728\u8c03\u5f0f\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e0e\u4f20\u7edfcentonisation\u7406\u8bba\u4e0d\u7b26\uff1b\u65cb\u5f8b\u7684\u5f00\u5934\u548c\u7ed3\u5c3e\u90e8\u5206\u66f4\u5177\u516c\u5f0f\u5316\u7279\u5f81\u3002", "conclusion": "\u5c3d\u7ba1\u6700\u4f18\u5206\u6bb5\u5728\u8c03\u5f0f\u5206\u7c7b\u4e2d\u6709\u6548\uff0c\u4f46\u5b83\u5e76\u672a\u652f\u6301\u4f20\u7edf\u7684centonisation\u7406\u8bba\uff0c\u8868\u660e\u8bb0\u5fc6\u6548\u7387\u4e0e\u8c03\u5f0f\u5206\u7c7b\u5b58\u5728\u5173\u8054\u3002"}}
{"id": "2507.00182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00182", "abs": "https://arxiv.org/abs/2507.00182", "authors": ["J. I. Ru\u00edz", "A. M\u00e9ndez", "E. Rodr\u00edguez"], "title": "Graph-Based Deep Learning for Component Segmentation of Maize Plants", "comment": null, "summary": "In precision agriculture, one of the most important tasks when exploring crop\nproduction is identifying individual plant components. There are several\nattempts to accomplish this task by the use of traditional 2D imaging, 3D\nreconstructions, and Convolutional Neural Networks (CNN). However, they have\nseveral drawbacks when processing 3D data and identifying individual plant\ncomponents. Therefore, in this work, we propose a novel Deep Learning\narchitecture to detect components of individual plants on Light Detection and\nRanging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on\nthe concept of Graph Neural Networks (GNN), and feature enhancing with\nPrincipal Component Analysis (PCA). For this, each point is taken as a vertex\nand by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,\nthus representing the 3D PC data set. Subsequently, Edge-Conv layers are used\nto further increase the features of each point. Finally, Graph Attention\nNetworks (GAT) are applied to classify visible phenotypic components of the\nplant, such as the leaf, stem, and soil. This study demonstrates that our\ngraph-based deep learning approach enhances segmentation accuracy for\nidentifying individual plant components, achieving percentages above 80% in the\nIoU average, thus outperforming other existing models based on point clouds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u4eceLiDAR 3D\u70b9\u4e91\u6570\u636e\u4e2d\u8bc6\u522b\u690d\u7269\u7ec4\u4ef6\uff0c\u51c6\u786e\u7387\u8d85\u8fc780%\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u57283D\u6570\u636e\u5904\u7406\u548c\u690d\u7269\u7ec4\u4ef6\u8bc6\u522b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408GNN\u3001PCA\u548cKNN\u6784\u5efa\u56fe\u7ed3\u6784\uff0c\u5229\u7528Edge-Conv\u548cGAT\u5c42\u589e\u5f3a\u7279\u5f81\u5e76\u5206\u7c7b\u690d\u7269\u7ec4\u4ef6\u3002", "result": "\u6a21\u578b\u5728IoU\u5e73\u5747\u51c6\u786e\u7387\u4e0a\u8d85\u8fc780%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u56fe\u795e\u7ecf\u7f51\u7edc\u663e\u8457\u63d0\u5347\u4e86\u690d\u7269\u7ec4\u4ef6\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002"}}
{"id": "2507.00726", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00726", "abs": "https://arxiv.org/abs/2507.00726", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "comment": "27 pages", "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8c61\u68cb\u4e2d\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u5bc6\u96c6\u5956\u52b1\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\uff0c\u4f46\u6a21\u578b\u8868\u73b0\u4ecd\u8fdc\u4f4e\u4e8e\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u53ef\u4ee5\u901a\u8fc7RL\u5728\u8c61\u68cb\u4e2d\u53d1\u5c55\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u8c61\u68cb\u52a8\u4f5c\u4ef7\u503c\u7f51\u7edc\u4e3aLLM\u7684\u8f93\u51fa\u52a8\u4f5c\u8d28\u91cf\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1\uff08\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u5e76\u4e0e\u7a00\u758f\u4e8c\u8fdb\u5236\u5956\u52b1\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5bc6\u96c6\u5956\u52b1\u901a\u5e38\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\uff0c\u4f46\u6240\u6709\u6a21\u578b\u8868\u73b0\u5747\u8fdc\u4f4e\u4e8e\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u8c61\u68cb\u7684\u5185\u90e8\u7406\u89e3\u4e0d\u8db3\u53ef\u80fd\u662f\u6027\u80fd\u74f6\u9888\uff0c\u4ec5\u9760RL\u96be\u4ee5\u5b8c\u5168\u514b\u670d\u3002"}}
{"id": "2507.00389", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00389", "abs": "https://arxiv.org/abs/2507.00389", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "CAPITAL\u662f\u4e00\u79cd\u56e0\u679c\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u524d\u95e8\u8c03\u6574\u878d\u5165\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u9690\u5f0f\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9690\u5f0f\u60c5\u611f\u5206\u6790\u4e2d\u4f9d\u8d56\u591a\u6570\u6295\u7968\u800c\u5ffd\u7565\u56e0\u679c\u6709\u6548\u6027\u7684\u95ee\u9898\u3002", "method": "CAPITAL\u5c06\u603b\u4f53\u56e0\u679c\u6548\u5e94\u5206\u89e3\u4e3a\u8f93\u5165\u63d0\u793a\u5bf9\u63a8\u7406\u94fe\u7684\u5f71\u54cd\u548c\u63a8\u7406\u94fe\u5bf9\u6700\u7ec8\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u5229\u7528\u7f16\u7801\u5668\u805a\u7c7b\u548cNWGM\u8fd1\u4f3c\u8fdb\u884c\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u7f16\u7801\u5668\u8868\u793a\u4e0eLLM\u63a8\u7406\u7a7a\u95f4\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u79cdLLM\u4e0a\uff0cCAPITAL\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CAPITAL\u4e3a\u5c06\u56e0\u679c\u63a8\u7406\u878d\u5165LLM\u63d0\u793a\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u504f\u6ce8\u610f\u8bc6\u60c5\u611f\u63a8\u7406\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.00224", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00224", "abs": "https://arxiv.org/abs/2507.00224", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFiboSB\u6570\u636e\u96c6\uff0c\u7528\u4e8e6D\u59ff\u6001\u4f30\u8ba1\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6539\u8fdbYOLO11-x\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5b66\u751f\u4e0e\u7269\u7406\u5bf9\u8c61\u7684\u4ea4\u4e92\uff0c6D\u59ff\u6001\u4f30\u8ba1\u53ef\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165FiboSB\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cd6D\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u6539\u8fdbYOLO11-x\u3002", "result": "\u6539\u8fdb\u540e\u7684YOLO11-x\u5728FiboSB\u4e0a\u8fbe\u5230mAP_50\u4e3a0.898\u3002", "conclusion": "FiboSB\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7b97\u6cd5\u4e3a\u534f\u4f5c\u573a\u666f\u4e2d\u76846D\u59ff\u6001\u4f30\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.00810", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.00810", "abs": "https://arxiv.org/abs/2507.00810", "authors": ["Qing Xu", "Xiaohua Xuan"], "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis", "comment": null, "summary": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6570\u503c\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u975e\u5149\u6ed1\u4f18\u5316\u3001\u4e8c\u6b21\u89c4\u5212\u548c\u8fed\u4ee3\u8fc7\u7a0b\u7684\u6781\u5c0f\u6781\u5927\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u8bc1\u660e\u3002", "motivation": "\u6781\u5c0f\u6781\u5927\u95ee\u9898\u5728\u9c81\u68d2\u4f18\u5316\u548c\u4e0d\u5e73\u8861\u5b66\u4e60\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u5728\u6536\u655b\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u975e\u5149\u6ed1\u4f18\u5316\u3001\u4e8c\u6b21\u89c4\u5212\u548c\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u63d0\u51fa\u6539\u8fdb\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u68af\u5ea6\u8fde\u7eed\u6027\u548c\u6709\u754c\u6027\u7b49\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u662f\u6536\u655b\u7684\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u6781\u5c0f\u6781\u5927\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.00439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00439", "abs": "https://arxiv.org/abs/2507.00439", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u76d1\u7763\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u6837\u4eba\u7fa4\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\u548c\u5f00\u6e90\u57fa\u51c6\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e0d\u540c\u4eba\u7fa4\u5bf9\u4e3b\u89c2\u95ee\u9898\u7684\u56de\u7b54\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u76f8\u5bf9\u7b80\u5355\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u8bc4\u4f30\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u6837\u4eba\u7fa4\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5177\u4f53\u7fa4\u4f53\u7684\u5bf9\u9f50\u5dee\u5f02\u3002", "conclusion": "\u65b9\u6cd5\u7b80\u5355\u901a\u7528\uff0c\u6613\u4e8e\u91c7\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.00243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00243", "abs": "https://arxiv.org/abs/2507.00243", "authors": ["Chi-Yao Huang", "Zeel Bhatt", "Yezhou Yang"], "title": "VOCAL: Visual Odometry via ContrAstive Learning", "comment": null, "summary": "Breakthroughs in visual odometry (VO) have fundamentally reshaped the\nlandscape of robotics, enabling ultra-precise camera state estimation that is\ncrucial for modern autonomous systems. Despite these advances, many\nlearning-based VO techniques rely on rigid geometric assumptions, which often\nfall short in interpretability and lack a solid theoretical basis within fully\ndata-driven frameworks. To overcome these limitations, we introduce VOCAL\n(Visual Odometry via ContrAstive Learning), a novel framework that reimagines\nVO as a label ranking challenge. By integrating Bayesian inference with a\nrepresentation learning framework, VOCAL organizes visual features to mirror\ncamera states. The ranking mechanism compels similar camera states to converge\ninto consistent and spatially coherent representations within the latent space.\nThis strategic alignment not only bolsters the interpretability of the learned\nfeatures but also ensures compatibility with multimodal data sources. Extensive\nevaluations on the KITTI dataset highlight VOCAL's enhanced interpretability\nand flexibility, pushing VO toward more general and explainable spatial\nintelligence.", "AI": {"tldr": "VOCAL\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u8868\u793a\u5b66\u4e60\u63d0\u5347\u7279\u5f81\u53ef\u89e3\u91ca\u6027\u548c\u591a\u6a21\u6001\u6570\u636e\u517c\u5bb9\u6027\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u578b\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u521a\u6027\u51e0\u4f55\u5047\u8bbe\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u89c6\u89c9\u91cc\u7a0b\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6807\u7b7e\u6392\u5e8f\u95ee\u9898\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u8868\u793a\u5b66\u4e60\uff0c\u7ec4\u7ec7\u89c6\u89c9\u7279\u5f81\u4ee5\u53cd\u6620\u76f8\u673a\u72b6\u6001\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86VOCAL\u7684\u4f18\u8d8a\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "VOCAL\u63a8\u52a8\u4e86\u89c6\u89c9\u91cc\u7a0b\u8ba1\u5411\u66f4\u901a\u7528\u548c\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u667a\u80fd\u53d1\u5c55\u3002"}}
{"id": "2507.00841", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.00841", "abs": "https://arxiv.org/abs/2507.00841", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "comment": "12 pages", "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u5e8f\u5217\u4fe1\u606f\u7684\u98ce\u9669\u8bc6\u522b\u673a\u5236\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6848\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7cfb\u7edf\u9762\u4e34\u6f5c\u5728\u7684\u8d8a\u72f1\u98ce\u9669\uff0c\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u5728\u590d\u6742\u4ea4\u4e92\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u9ad8\u6548\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u5e8f\u5217\u4fe1\u606f\u6784\u5efa\u98ce\u9669\u8bc6\u522b\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8f85\u52a9\u8bc4\u4f30\u65b9\u6848\u3002", "result": "\u521d\u6b65\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u98ce\u9669\u884c\u4e3a\u8bc6\u522b\u80fd\u529b\uff0c\u964d\u4f4e\u4ee3\u7406\u88ab\u8d8a\u72f1\u7684\u6982\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\u5efa\u6a21\u4e0e\u9632\u62a4\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2507.00460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00460", "abs": "https://arxiv.org/abs/2507.00460", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f00\u653e\u7684LLM\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982HELM\uff09\u5b58\u5728\u6f0f\u6d1e\uff0c\u901a\u8fc7\u5fae\u8c03\u5c0f\u578b\u6a21\u578b\u5728\u516c\u5f00\u6d4b\u8bd5\u96c6\u4e0a\u201c\u4f5c\u5f0a\u201d\u53ef\u4ee5\u53d6\u5f97\u9ad8\u5206\uff0c\u4f46\u5b9e\u9645\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "motivation": "\u63ed\u793a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u5f3a\u8c03\u5176\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5fae\u8c03BART\u3001T5\u548cGPT-2\u7684\u5c0f\u578b\u53d8\u4f53\u5728\u516c\u5f00\u6d4b\u8bd5\u96c6\u4e0a\u6784\u9020\u201c\u4f5c\u5f0a\u201d\u6a21\u578b\u3002", "result": "\u8fd9\u4e9b\u6a21\u578b\u5728HELM\u57fa\u51c6\u4e0a\u6392\u540d\u9760\u524d\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5b9e\u7528\u6027\u6709\u9650\u3002", "conclusion": "\u9700\u7ed3\u5408\u79c1\u6709\u6216\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u91cd\u65b0\u8bc4\u4f30\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\uff0c\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.00248", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00248", "abs": "https://arxiv.org/abs/2507.00248", "authors": ["Nikita Nikitin", "Eugene Fomin"], "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7DNN\u7684\u5b9e\u65f6\u624b\u8bed\u8bc6\u522b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5e27\u7387\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u624b\u8bed\u8bc6\u522b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u4e0e\u63a8\u7406\u73af\u5883\u5e27\u7387\u5dee\u5f02\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7f16\u7801\u624b\u8bed\u7279\u5b9a\u53c2\u6570\uff08\u5982\u624b\u5f62\u3001\u624b\u638c\u65b9\u5411\u3001\u52a8\u4f5c\u548c\u4f4d\u7f6e\uff09\u4e3a\u5411\u91cf\u5316\u8f93\u5165\uff0c\u5e76\u5229\u7528MediaPipe\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u8bbe\u8ba1\u4f18\u5316\u7684DNN\u67b6\u6784\uff08<10MB\uff09\u3002", "result": "\u5728343\u4e2a\u624b\u8bed\u5206\u7c7b\u4e2d\u5b9e\u73b092%\u7684\u51c6\u786e\u7387\uff0c\u5ef6\u8fdf\u4f4e\u4e8e10ms\uff0c\u5e76\u96c6\u6210\u5230'slait ai'\u5e94\u7528\u4e2d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u4e3a\u5b9e\u65f6\u624b\u8bed\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00951", "abs": "https://arxiv.org/abs/2507.00951", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "comment": null, "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u601d\u8003\u3001\u63a8\u7406\u548c\u884c\u52a8\uff0c\u5206\u6790\u4e86\u5f53\u524dAI\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8de8\u5b66\u79d1\u7684AGI\u53d1\u5c55\u6846\u67b6\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u63a8\u7406\u3001\u8bb0\u5fc6\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b9e\u73b0\u901a\u7528\u667a\u80fd\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u89e3\u51b3\u5f53\u524dAI\u6a21\u578b\u5728\u63a8\u7406\u548c\u884c\u52a8\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u7efc\u5408\u7814\u7a76\uff0c\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u3001\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u3001\u5fc3\u7406\u5b66\u3001\u751f\u6210\u6a21\u578b\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5206\u6790\u901a\u7528\u667a\u80fd\u7684\u67b6\u6784\u548c\u8ba4\u77e5\u57fa\u7840\u3002", "result": "\u63d0\u51fa\u4e86Agentic RAG\u6846\u67b6\u548c\u591a\u79cd\u6cdb\u5316\u7b56\u7565\uff0c\u5f3a\u8c03\u8bb0\u5fc6\u4e0e\u63a8\u7406\u7684\u6574\u5408\u662f\u5b9e\u73b0\u771f\u6b63\u667a\u80fd\u7684\u5173\u952e\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u7edf\u8ba1\u5b66\u4e60\u548c\u76ee\u6807\u5bfc\u5411\u8ba4\u77e5\u4e4b\u95f4\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46AGI\u7684\u53d1\u5c55\u4ecd\u9762\u4e34\u79d1\u5b66\u3001\u6280\u672f\u548c\u4f26\u7406\u6311\u6218\u3002"}}
{"id": "2507.00509", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00509", "abs": "https://arxiv.org/abs/2507.00509", "authors": ["To Eun Kim", "Jo\u00e3o Coelho", "Gbemileke Onilude", "Jai Singh"], "title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u5e7f\u544a\u7ba1\u7406\u6d41\u7a0b\uff0c\u7528\u4e8eRAG\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5305\u62ec\u5e7f\u544a\u91cd\u5199\u5668\u548c\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u4f18\u5316\u5e7f\u544a\u63d2\u5165\u7b56\u7565\u3002", "motivation": "\u751f\u6210\u5f0f\u641c\u7d22\u7cfb\u7edf\u6a21\u7cca\u4e86\u4fe1\u606f\u4e0e\u5e7f\u544a\u7684\u754c\u9650\uff0c\u5f71\u54cd\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\uff0c\u9700\u89e3\u51b3\u5e7f\u544a\u63d2\u5165\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5e7f\u544a\u5206\u7c7b\u5668\uff0c\u6307\u5bfc\u5e7f\u544a\u91cd\u5199\u5668\u7684\u5fae\u8c03\u548c\u6700\u4f73\u5019\u9009\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5206\u7c7b\u5668\u68c0\u6d4b\u6027\u80fd\u5f3a\uff0c\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u5e7f\u544a\u9690\u853d\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5e7f\u544a\u611f\u77e5\u751f\u6210\u7cfb\u7edf\u548c\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u5bf9\u6297\u6027\u5171\u8fdb\u5316\u6846\u67b6\u3002"}}
{"id": "2507.00253", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00253", "abs": "https://arxiv.org/abs/2507.00253", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGazeTarget360\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1360\u5ea6\u89c6\u7ebf\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u80cc\u666f\u4fe1\u606f\u5438\u6536\u548c\u89c6\u7ebf\u76ee\u6807\u9884\u6d4b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5728\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u4eba\u7406\u89e3\u4eba\u7c7b\u89c6\u7ebf\u76ee\u6807\u662f\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982OpenFace\uff09\u65e0\u6cd5\u6709\u6548\u9884\u6d4b\u89c6\u7ebf\u76ee\u6807\uff0c\u5c24\u5176\u662f\u5728\u89c6\u7ebf\u79bb\u5f00\u76f8\u673a\u65f6\u3002", "method": "GazeTarget360\u6574\u5408\u4e86\u6761\u4ef6\u63a8\u7406\u5f15\u64ce\uff0c\u5305\u62ec\u773c\u63a5\u89e6\u68c0\u6d4b\u5668\u3001\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u89e3\u7801\u5668\u3002", "result": "\u4ea4\u53c9\u9a8c\u8bc1\u8868\u660e\uff0cGazeTarget360\u80fd\u5728\u672a\u89c1\u573a\u666f\u4e2d\u51c6\u786e\u9884\u6d4b\u89c6\u7ebf\u76ee\u6807\uff0c\u662f\u9996\u4e2a\u9ad8\u6548\u4e14\u53ef\u90e8\u7f72\u7684\u7cfb\u7edf\u3002", "conclusion": "GazeTarget360\u4e3a\u89c6\u7ebf\u76ee\u6807\u9884\u6d4b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.00979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCIP\u7684\u65b0\u6280\u672f\uff0c\u5229\u7528\u56e0\u679c\u5f71\u54cd\u56fe\uff08CIDs\uff09\u6765\u8bc6\u522b\u548c\u51cf\u8f7b\u81ea\u4e3b\u4ee3\u7406\u51b3\u7b56\u4e2d\u7684\u98ce\u9669\uff0c\u4ece\u800c\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u4ee3\u7406\u5728\u8f85\u52a9\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u5b89\u5168\u53ef\u9760\u4ee5\u9632\u6b62\u610f\u5916\u540e\u679c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u57fa\u4e8e\u4efb\u52a1\u89c4\u8303\u521d\u59cb\u5316CID\u4ee5\u63cf\u8ff0\u51b3\u7b56\u8fc7\u7a0b\uff1b2\uff09\u4f7f\u7528CID\u6307\u5bfc\u4ee3\u7406\u4e0e\u73af\u5883\u4ea4\u4e92\uff1b3\uff09\u6839\u636e\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u548c\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316CID\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u6267\u884c\u548c\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "CIP\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u5173\u7cfb\u8868\u793a\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.00534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00534", "abs": "https://arxiv.org/abs/2507.00534", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "Nirantar\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u548c\u591a\u9886\u57dfASR\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u6db5\u76d622\u79cd\u8bed\u8a00\u548c208\u4e2a\u5370\u5ea6\u5730\u533a\uff0c\u652f\u6301\u591a\u79cd\u589e\u91cf\u5b66\u4e60\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u771f\u5b9e\u3001\u52a8\u6001\u7684\u8bed\u8a00\u548c\u9886\u57df\u53d8\u5316\u6570\u636e\u3002", "method": "\u5229\u7528\u589e\u91cf\u6536\u96c6\u76843250\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\uff0c\u5305\u62ec1720\u5c0f\u65f6\u65b0\u6570\u636e\uff0c\u8bc4\u4f30\u8bed\u8a00\u589e\u91cf\u3001\u9886\u57df\u589e\u91cf\u53ca\u6df7\u5408\u589e\u91cf\u5b66\u4e60\u573a\u666f\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728Nirantar\u6846\u67b6\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u8868\u660e\u9700\u8981\u66f4\u9c81\u68d2\u7684\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "Nirantar\u4e3a\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.00261", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "AI": {"tldr": "VirtualFencer\u7cfb\u7edf\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u89c6\u9891\u4e2d\u63d0\u53d63D\u51fb\u5251\u52a8\u4f5c\u548c\u7b56\u7565\uff0c\u5e76\u751f\u6210\u903c\u771f\u7684\u51fb\u5251\u884c\u4e3a\u3002", "motivation": "\u51fb\u5251\u52a8\u4f5c\u591a\u6837\u4e14\u53d7\u7b56\u7565\u9a71\u52a8\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u6765\u6355\u6349\u5176\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faVirtualFencer\u7cfb\u7edf\uff0c\u4ece\u89c6\u9891\u4e2d\u65e0\u76d1\u7763\u63d0\u53d63D\u52a8\u4f5c\u548c\u7b56\u7565\uff0c\u5e76\u751f\u6210\u884c\u4e3a\u3002", "result": "\u7cfb\u7edf\u80fd\u81ea\u6211\u5bf9\u6297\u3001\u4e0e\u771f\u5b9e\u51fb\u5251\u624b\u52a8\u4f5c\u5bf9\u6297\uff0c\u5e76\u4e0e\u4e13\u4e1a\u51fb\u5251\u624b\u4e92\u52a8\u3002", "conclusion": "VirtualFencer\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u5728\u51fb\u5251\u8fd0\u52a8\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.00540", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00540", "abs": "https://arxiv.org/abs/2507.00540", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80f6\u56ca\u7f51\u7edc\u7684\u7528\u6237\u8bed\u4e49\u610f\u56fe\u5efa\u6a21\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u548c\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u63d0\u5347\u610f\u56fe\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u5c42\u6b21\u5173\u7cfb\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u610f\u56fe\u8bc6\u522b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5411\u91cf\u5316\u80f6\u56ca\u7ed3\u6784\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u7ed3\u5408\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u57fa\u4e8e\u8fb9\u754c\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u5efa\u6a21\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u590d\u6742\u8bed\u4e49\u6761\u4ef6\u4e0b\u7684\u610f\u56fe\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2507.00263", "categories": ["cs.CV", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.00263", "abs": "https://arxiv.org/abs/2507.00263", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "comment": null, "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u89e3\u51b3\u5ea6\u5047\u79df\u8d41\u5e73\u53f0\u4e2d\u623f\u95f4\u573a\u666f\u53d1\u73b0\u548c\u5206\u7ec4\u95ee\u9898\uff0c\u4ee5\u53ca\u8bc6\u522b\u5367\u5ba4\u5e8a\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5ea6\u5047\u79df\u8d41\u5e73\u53f0\u4e2d\u5927\u91cf\u672a\u5206\u7c7b\u7684\u623f\u4ea7\u56fe\u7247\u7ed9\u65c5\u884c\u8005\u7406\u89e3\u7a7a\u95f4\u5e03\u5c40\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u7684\u623f\u95f4\u7c7b\u578b\u68c0\u6d4b\u3001\u91cd\u53e0\u68c0\u6d4b\u6a21\u578b\u548c\u805a\u7c7b\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6620\u5c04\u5367\u5ba4\u5e8a\u578b\u3002", "result": "\u6574\u4f53\u6d41\u7a0b\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u5d4c\u5165\u805a\u7c7b\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u65f6\u548c\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u9ad8\u6548\uff0c\u4e3a\u65c5\u884c\u8005\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7a7a\u95f4\u5e03\u5c40\u7406\u89e3\u3002"}}
{"id": "2507.00547", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00547", "abs": "https://arxiv.org/abs/2507.00547", "authors": ["Malmi Amadoru"], "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9ad8\u7ea7\u8ba1\u7b97\u7b97\u6cd5\u5728\u7406\u8bba\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u65b9\u6cd5\u5b66\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u786e\u4fdd\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u4e25\u8c28\u6027\u7684\u6307\u5357\u3002", "motivation": "\u7531\u4e8e\u9ad8\u7ea7\u8ba1\u7b97\u7b97\u6cd5\u7684\u4e0d\u900f\u660e\u6027\u548c\u5e94\u7528\u4e2d\u7684\u900f\u660e\u5ea6\u4e0d\u8db3\uff0c\u7814\u7a76\u4fe1\u4efb\u5ea6\u53ef\u80fd\u53d7\u635f\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u8ba8\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u4e3b\u9898\u5efa\u6a21\u7b97\u6cd5\u7684\u5e94\u7528\u793a\u4f8b\u548c\u4e00\u5957\u6307\u5357\uff0c\u8ba8\u8bba\u5982\u4f55\u786e\u4fdd\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u7684\u4e25\u8c28\u6027\u3002", "result": "\u63d0\u51fa\u7684\u6307\u5357\u4e0d\u4ec5\u9002\u7528\u4e8e\u4e3b\u9898\u5efa\u6a21\u7b97\u6cd5\uff0c\u4e5f\u53ef\u901a\u8fc7\u8c03\u6574\u5e94\u7528\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u5bf9\u65b0\u624b\u7814\u7a76\u8005\u3001\u7f16\u8f91\u548c\u5ba1\u7a3f\u4eba\u6709\u5e2e\u52a9\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4e3b\u9898\u5efa\u6a21\u6587\u732e\u548c\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\u8ba8\u8bba\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2507.00287", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00287", "abs": "https://arxiv.org/abs/2507.00287", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "title": "Self-Supervised Multiview Xray Matching", "comment": "MICCAI 2025", "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7ba1\u9053\uff0c\u901a\u8fc7\u5408\u6210X\u5c04\u7ebf\u89c6\u56fe\u81ea\u52a8\u751f\u6210\u591a\u5bf9\u591a\u5bf9\u5e94\u77e9\u9635\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\uff0c\u63d0\u5347\u4e86\u591a\u89c6\u56fe\u9aa8\u6298\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u65b9\u6cd5\u5728\u591a\u89c6\u56feX\u5c04\u7ebf\u5206\u6790\u4e2d\u96be\u4ee5\u5efa\u7acb\u7a33\u5065\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fd9\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u6570\u5b57\u91cd\u5efa\u653e\u5c04\u5f71\u50cf\uff08DRR\uff09\u4ece\u65e0\u6807\u6ce8CT\u4f53\u79ef\u81ea\u52a8\u751f\u6210\u5408\u6210X\u5c04\u7ebf\u89c6\u56fe\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eTransformer\u7684\u8bad\u7ec3\u9884\u6d4b\u591a\u89c6\u56fe\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eX\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5f15\u5165\u5bf9\u5e94\u5173\u7cfb\u63d0\u5347\u4e86\u591a\u89c6\u56fe\u9aa8\u6298\u5206\u7c7b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u89c6\u56feX\u5c04\u7ebf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u6298\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.00579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00579", "abs": "https://arxiv.org/abs/2507.00579", "authors": ["Miriam Ansch\u00fctz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u5e7b\u89c9\u8bc6\u522b\u7cfb\u7edf\uff0c\u7ed3\u5408\u68c0\u7d22\u5f0f\u4e8b\u5b9e\u9a8c\u8bc1\u548cBERT\u6a21\u578b\uff0c\u5728SemEval-2025\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u963b\u788d\u4e86\u5176\u53ef\u4fe1\u5ea6\u548c\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u82f1\u8bed\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u591a\u8bed\u8a00\u573a\u666f\u3002", "method": "\u91c7\u7528\u4e24\u90e8\u5206\u7ba1\u9053\uff1a\u57fa\u4e8eWikipedia\u7684\u68c0\u7d22\u5f0f\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u9488\u5bf9\u5e38\u89c1\u5e7b\u89c9\u6a21\u5f0f\u5fae\u8c03\u7684BERT\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u5728\u516b\u79cd\u8bed\u8a00\u4e2d\u8fdb\u5165\u524d10\u540d\uff0c\u652f\u6301\u8d85\u8fc7\u4efb\u52a1\u6db5\u76d6\u768414\u79cd\u8bed\u8a00\u3002", "conclusion": "\u591a\u8bed\u8a00\u5e7b\u89c9\u8bc6\u522b\u5668\u6709\u52a9\u4e8e\u63d0\u5347LLM\u8f93\u51fa\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.00292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00292", "abs": "https://arxiv.org/abs/2507.00292", "authors": ["Ali Mammadov", "Lo\u00efc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "comment": "MICCAI 2025", "summary": "Digital pathology has revolutionized the field by enabling the digitization\nof tissue samples into whole slide images (WSIs). However, the high resolution\nand large size of WSIs present significant challenges when it comes to applying\nDeep Learning models. As a solution, WSIs are often divided into smaller\npatches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a\n(too) costly pixel-wise annotation. By treating each slide as a bag of patches,\nMultiple Instance Learning (MIL) methods have emerged as a suitable solution\nfor WSI classification. A major drawback of MIL methods is their high\nvariability in performance across different runs, which can reach up to 10-15\nAUC points on the test set, making it difficult to compare different MIL\nmethods reliably. This variability mainly comes from three factors: i) weight\ninitialization, ii) batch (shuffling) ordering, iii) and learning rate. To\naddress that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL\nmethods. We first train multiple models for a few epochs and average the most\nstable and promising ones based on validation scores. This approach can be\napplied to any existing MIL model to reduce performance variability. It also\nsimplifies hyperparameter tuning and improves reproducibility while maintaining\ncomputational efficiency. We extensively validate our approach on WSI\nclassification tasks using 2 different datasets, 3 initialization strategies\nand 5 MIL methods, for a total of more than 2000 experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u4fdd\u771f\u5ea6\u6a21\u578b\u878d\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u51cf\u5c11\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u65b9\u6cd5\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u6027\u80fd\u6ce2\u52a8\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u56e0\u9ad8\u5206\u8fa8\u7387\u548c\u5c3a\u5bf8\u5927\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002MIL\u65b9\u6cd5\u867d\u6709\u6548\uff0c\u4f46\u6027\u80fd\u6ce2\u52a8\u5927\uff0c\u5f71\u54cd\u53ef\u9760\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\u5e76\u57fa\u4e8e\u9a8c\u8bc1\u5206\u6570\u5e73\u5747\u6700\u7a33\u5b9a\u548c\u6709\u524d\u666f\u7684\u6a21\u578b\uff0c\u51cf\u5c11\u6027\u80fd\u6ce2\u52a8\u3002\u9002\u7528\u4e8e\u4efb\u4f55\u73b0\u6709MIL\u6a21\u578b\u3002", "result": "\u57282\u4e2a\u6570\u636e\u96c6\u30013\u79cd\u521d\u59cb\u5316\u7b56\u7565\u548c5\u79cdMIL\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\uff0c\u51712000\u591a\u6b21\u5b9e\u9a8c\uff0c\u663e\u8457\u964d\u4f4e\u6027\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.00601", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00601", "abs": "https://arxiv.org/abs/2507.00601", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u8fc1\u79fb\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\u8fc1\u79fb\u548c\u9002\u5e94\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u77e5\u8bc6\u5bf9\u9f50\u635f\u5931\u548c\u8f6f\u63d0\u793a\u8c03\u4f18\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u5e94\u6a21\u5757\u3001\u51bb\u7ed3\u7b56\u7565\u548c\u63d0\u793a\u6ce8\u5165\uff0c\u4ee5\u6700\u5c0f\u6807\u6ce8\u5b9e\u73b0\u9ad8\u6548\u8fc1\u79fb\u3002", "result": "\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\uff08\u5982MLQA\u3001XQuAD\u548cPAWS-X\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5f3a\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u65e2\u589e\u5f3a\u4e86\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u53c8\u4fdd\u7559\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002"}}
{"id": "2507.00327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00327", "abs": "https://arxiv.org/abs/2507.00327", "authors": ["Chuyan Zhang", "Kefan Wang", "Yun Gu"], "title": "Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes", "comment": "Accepted by ICCV 2025", "summary": "Low-Rank Adaptation (LoRA) has proven effective in reducing computational\ncosts while maintaining performance comparable to fully fine-tuned foundation\nmodels across various tasks. However, its fixed low-rank structure restricts\nits adaptability in scenarios with substantial domain gaps, where higher ranks\nare often required to capture domain-specific complexities. Current adaptive\nLoRA methods attempt to overcome this limitation by dynamically expanding or\nselectively allocating ranks, but these approaches frequently depend on\ncomputationally intensive techniques such as iterative pruning, rank searches,\nor additional regularization. To address these challenges, we introduce Stable\nRank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the\nstable rank of pre-trained weight matrices as a natural prior for layer-wise\nrank allocation. By leveraging the stable rank, which reflects the intrinsic\ndimensionality of the weights, SR-LoRA enables a principled and efficient\nredistribution of ranks across layers, enhancing adaptability without incurring\nadditional search costs. Empirical evaluations on few-shot tasks with\nsignificant domain gaps show that SR-LoRA consistently outperforms recent\nadaptive LoRA variants, achieving a superior trade-off between performance and\nefficiency. Our code is available at\nhttps://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.", "AI": {"tldr": "SR-LoRA\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u7684\u7a33\u5b9a\u79e9\u4f5c\u4e3a\u5c42\u95f4\u79e9\u5206\u914d\u7684\u5148\u9a8c\uff0c\u63d0\u5347\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u5728\u9886\u57df\u5dee\u8ddd\u5927\u7684\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u65e0\u9700\u989d\u5916\u641c\u7d22\u6210\u672c\u3002", "motivation": "\u56fa\u5b9a\u4f4e\u79e9\u7ed3\u6784\u7684LoRA\u5728\u9886\u57df\u5dee\u8ddd\u5927\u7684\u4efb\u52a1\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u6280\u672f\u3002", "method": "\u63d0\u51faSR-LoRA\u6846\u67b6\uff0c\u5229\u7528\u7a33\u5b9a\u79e9\u6307\u5bfc\u5c42\u95f4\u79e9\u5206\u914d\uff0c\u9ad8\u6548\u63d0\u5347\u9002\u5e94\u6027\u3002", "result": "\u5728\u9886\u57df\u5dee\u8ddd\u5927\u7684\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\uff0cSR-LoRA\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u81ea\u9002\u5e94LoRA\u65b9\u6cd5\u3002", "conclusion": "SR-LoRA\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5e73\u8861\u3002"}}
{"id": "2507.00606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00606", "abs": "https://arxiv.org/abs/2507.00606", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "AI": {"tldr": "MoR\u6846\u67b6\u901a\u8fc7\u5d4c\u5165\u591a\u6837\u63a8\u7406\u7b56\u7565\uff0c\u4f7fLLMs\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u63d0\u793a\u5373\u53ef\u81ea\u9002\u5e94\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLMs\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "MoR\u5206\u4e3a\u4e24\u9636\u6bb5\uff1aThought Generation\u751f\u6210\u63a8\u7406\u94fe\u6a21\u677f\uff0cSFT Dataset Construction\u7528\u6a21\u677f\u548c\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "MoR150\u5728CoT\u63d0\u793a\u4e0b\u6027\u80fd\u63d0\u53472.2%\uff0c\u76f8\u6bd4\u57fa\u7ebf\u63d0\u534713.5%\u3002", "conclusion": "MoR\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u7684\u9c81\u68d2\u63a8\u7406\u3002"}}
{"id": "2507.00328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00328", "abs": "https://arxiv.org/abs/2507.00328", "authors": ["Xuan Liu", "Yinhao Ren", "Marc D. Ryser", "Lars J. Grimm", "Joseph Y. Lo"], "title": "MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms", "comment": null, "summary": "Accurate lesion tracking in temporal mammograms is essential for monitoring\nbreast cancer progression and facilitating early diagnosis. However, automated\nlesion correspondence across exams remains a challenges in computer-aided\ndiagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,\na mask-guided lesion tracking framework that automates lesion localization\nacross consecutively exams. Our approach follows a coarse-to-fine strategy\nincorporating three key modules: global search, local search, and score\nrefinement. To support large-scale training and evaluation, we introduce a new\ndataset with curated prior-exam annotations for 730 mass and calcification\ncases from the public EMBED mammogram dataset, yielding over 20000 lesion\npairs, making it the largest known resource for temporal lesion tracking in\nmammograms. Experimental results demonstrate that MammoTracker achieves 0.455\naverage overlap and 0.509 accuracy, surpassing baseline models by 8%,\nhighlighting its potential to enhance CAD-based lesion progression analysis.\nOur dataset will be available at\nhttps://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.", "AI": {"tldr": "MammoTracker\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a9\u6a21\u5f15\u5bfc\u7684\u75c5\u7076\u8ffd\u8e2a\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4e73\u817aX\u5149\u7247\u4e2d\u75c5\u7076\u7684\u5b9a\u4f4d\uff0c\u901a\u8fc7\u5168\u5c40\u641c\u7d22\u3001\u5c40\u90e8\u641c\u7d22\u548c\u5206\u6570\u7ec6\u5316\u6a21\u5757\u5b9e\u73b0\u7c97\u5230\u7ec6\u7684\u7b56\u7565\u3002", "motivation": "\u4e73\u817aX\u5149\u7247\u4e2d\u75c5\u7076\u7684\u51c6\u786e\u8ffd\u8e2a\u5bf9\u4e73\u817a\u764c\u8fdb\u5c55\u76d1\u6d4b\u548c\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u81ea\u52a8\u5316\u75c5\u7076\u5bf9\u5e94\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faMammoTracker\u6846\u67b6\uff0c\u91c7\u7528\u5168\u5c40\u641c\u7d22\u3001\u5c40\u90e8\u641c\u7d22\u548c\u5206\u6570\u7ec6\u5316\u4e09\u6a21\u5757\u7684\u7c97\u5230\u7ec6\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMammoTracker\u7684\u5e73\u5747\u91cd\u53e0\u7387\u4e3a0.455\uff0c\u51c6\u786e\u7387\u4e3a0.509\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b8%\u3002", "conclusion": "MammoTracker\u5728\u75c5\u7076\u8ffd\u8e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u63d0\u5347\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u7684\u75c5\u7076\u8fdb\u5c55\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2507.00665", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00665", "abs": "https://arxiv.org/abs/2507.00665", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "AI": {"tldr": "SAFER\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u89e3\u91ca\u548c\u6539\u8fdb\u5956\u52b1\u6a21\u578b\uff0c\u63ed\u793a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7279\u5f81\uff0c\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u8bbe\u8ba1\u6570\u636e\u6bd2\u5316\u548c\u53bb\u566a\u7b56\u7565\uff0c\u63d0\u5347LLM\u5b89\u5168\u5bf9\u9f50\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5728RLHF\u4e2d\u6838\u5fc3\u4f46\u4e0d\u900f\u660e\uff0c\u9700\u6539\u8fdb\u4ee5\u589e\u5f3aLLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u6790\u5956\u52b1\u6a21\u578b\u6fc0\u6d3b\uff0c\u8bbe\u8ba1\u6570\u636e\u6bd2\u5316\u548c\u53bb\u566a\u7b56\u7565\u3002", "result": "SAFER\u80fd\u7cbe\u786e\u8c03\u6574\u5b89\u5168\u5bf9\u9f50\uff0c\u4e0d\u5f71\u54cd\u901a\u7528\u804a\u5929\u6027\u80fd\u3002", "conclusion": "SAFER\u4e3a\u9ad8\u98ce\u9669LLM\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u89e3\u91ca\u3001\u5ba1\u8ba1\u548c\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.00334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00334", "abs": "https://arxiv.org/abs/2507.00334", "authors": ["Mengyi Shan", "Zecheng He", "Haoyu Ma", "Felix Juefei-Xu", "Peizhao Zhang", "Tingbo Hou", "Ching-Yao Chuang"], "title": "Populate-A-Scene: Affordance-Aware Human Video Generation", "comment": "Project page: https://shanmy.github.io/Populate-A-Scene", "summary": "Can a video generation model be repurposed as an interactive world simulator?\nWe explore the affordance perception potential of text-to-video models by\nteaching them to predict human-environment interaction. Given a scene image and\na prompt describing human actions, we fine-tune the model to insert a person\ninto the scene, while ensuring coherent behavior, appearance, harmonization,\nand scene affordance. Unlike prior work, we infer human affordance for video\ngeneration (i.e., where to insert a person and how they should behave) from a\nsingle scene image, without explicit conditions like bounding boxes or body\nposes. An in-depth study of cross-attention heatmaps demonstrates that we can\nuncover the inherent affordance perception of a pre-trained video model without\nlabeled affordance datasets.", "AI": {"tldr": "\u63a2\u7d22\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u662f\u5426\u53ef\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5355\u5f20\u573a\u666f\u56fe\u50cf\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\uff0c\u65e0\u9700\u663e\u5f0f\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u89c6\u9891\u751f\u6210\u6a21\u578b\u662f\u5426\u5177\u5907\u611f\u77e5\u573a\u666f\u529f\u80fd\uff08affordance perception\uff09\u7684\u6f5c\u529b\uff0c\u4ee5\u9884\u6d4b\u4eba\u7c7b\u4e0e\u73af\u5883\u4ea4\u4e92\u3002", "method": "\u57fa\u4e8e\u573a\u666f\u56fe\u50cf\u548c\u52a8\u4f5c\u63d0\u793a\uff0c\u5fae\u8c03\u6a21\u578b\u4ee5\u63d2\u5165\u4eba\u7269\uff0c\u786e\u4fdd\u884c\u4e3a\u3001\u5916\u89c2\u3001\u534f\u8c03\u6027\u548c\u573a\u666f\u529f\u80fd\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u70ed\u56fe\u5206\u6790\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u5177\u5907\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u5185\u5728\u529f\u80fd\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u88ab\u91cd\u65b0\u7528\u4f5c\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u62df\u5668\uff0c\u5177\u5907\u611f\u77e5\u573a\u666f\u529f\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.00700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00700", "abs": "https://arxiv.org/abs/2507.00700", "authors": ["Ahmed Sabir", "Azinovi\u010d Gasper", "Mengsay Loem", "Rajesh Sharma"], "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bad\u7ec3\u8bed\u8a00\u4e0d\u540c\uff08\u5982\u65e5\u8bed\u548c\u82f1\u8bed\uff09\u65f6\uff0c\u4f1a\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u6587\u5316\u8ba4\u77e5\u76f8\u4f3c\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u6587\u5316\u80cc\u666f\uff08\u4e1c\u4e9a\u4e0e\u897f\u65b9\uff09\u5bf9\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u7684\u5f71\u54cd\u662f\u5426\u4e5f\u4f1a\u4f53\u73b0\u5728VLMs\u4e2d\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u56fe\u50cf\u63cf\u8ff0\uff0c\u7814\u7a76VLMs\u662f\u5426\u8868\u73b0\u51fa\u6574\u4f53\u6027\u4e0e\u5206\u6790\u6027\u503e\u5411\u7684\u5dee\u5f02\u3002", "result": "VLMs\u4e0d\u4ec5\u5185\u5316\u4e86\u8bed\u8a00\u7ed3\u6784\u7279\u6027\uff0c\u8fd8\u518d\u73b0\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6587\u5316\u884c\u4e3a\uff0c\u8868\u660e\u6587\u5316\u8ba4\u77e5\u53ef\u80fd\u9690\u5f0f\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u3002", "conclusion": "\u6587\u5316\u8ba4\u77e5\u53ef\u80fd\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u5f71\u54cdVLMs\u7684\u8f93\u51fa\uff0c\u4f7f\u5176\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u6587\u5316\u884c\u4e3a\u3002"}}
{"id": "2507.00339", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.2.6; I.4.6"], "pdf": "https://arxiv.org/pdf/2507.00339", "abs": "https://arxiv.org/abs/2507.00339", "authors": ["Alexander Moore", "Amar Saini", "Kylie Cancilla", "Doug Poland", "Carmen Carrano"], "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video", "comment": "9 pages, 2 figures", "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,", "AI": {"tldr": "MOVi-MC-AC\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6444\u50cf\u5934\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6a21\u6001\u5206\u5272\u548c\u5185\u5bb9\u5b8c\u6210\uff0c\u63d0\u4f9b\u591a\u89c6\u89d2\u5bf9\u8c61\u8ddf\u8e2a\u548c\u6a21\u6001\u5185\u5bb9\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6444\u50cf\u5934\u89c6\u89d2\u548c\u6a21\u6001\u5185\u5bb9\u6807\u7b7e\uff0c\u9650\u5236\u4e86\u6a21\u6001\u5206\u5272\u548c\u5185\u5bb9\u5b8c\u6210\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u591a\u6444\u50cf\u5934\u89c6\u9891\u573a\u666f\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u5bf9\u8c61ID\u548c\u6a21\u6001\u5185\u5bb9\u6807\u7b7e\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b580\u4e07\u4e2a\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u662f\u6a21\u6001\u5206\u5272\u9886\u57df\u6700\u5927\u7684\u6570\u636e\u96c6\uff0c\u5e76\u9996\u6b21\u63d0\u4f9b\u771f\u5b9e\u6a21\u6001\u5185\u5bb9\u6807\u7b7e\u3002", "conclusion": "MOVi-MC-AC\u586b\u8865\u4e86\u591a\u6444\u50cf\u5934\u89c6\u89d2\u548c\u6a21\u6001\u5185\u5bb9\u6807\u7b7e\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2507.00718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00718", "abs": "https://arxiv.org/abs/2507.00718", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u751f\u6210\u8d22\u52a1\u62a5\u544a\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u548c\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u5316\u9ad8\u4eae\u7cfb\u7edf\u5206\u7c7b\u4fe1\u606f\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u8d22\u52a1\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u8bc4\u4f30\u5176\u4e8b\u5b9e\u57fa\u7840\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6846\u67b6\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u548c\u8bc4\u4f30\uff0c\u5f15\u5165\u81ea\u52a8\u5316\u9ad8\u4eae\u7cfb\u7edf\u5206\u7c7b\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u80fd\u751f\u6210\u8fde\u8d2f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u8d22\u52a1\u62a5\u544a\u3002", "conclusion": "LLMs\u5728\u8d22\u52a1\u62a5\u544a\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u6846\u67b6\u548c\u8bc4\u4f30\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u5176\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2507.00356", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00356", "abs": "https://arxiv.org/abs/2507.00356", "authors": ["Zhiwei Yi", "Xin Cheng", "Jingyu Ma", "Ruifei Zhu", "Junwei Tian", "Yuanxiu Zhou", "Xinge Zhao", "Hongzhe Li"], "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation", "comment": "A Remote Sensing Fundation Model for Very High Resolution Images", "summary": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CGEarthEye\u6846\u67b6\uff0c\u4e13\u4e3a\u5409\u6797\u4e00\u53f7\u536b\u661f\u8bbe\u8ba1\uff0c\u5305\u542b\u4e94\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u603b\u53c2\u6570\u8fbe21\u4ebf\u3002\u901a\u8fc7\u6784\u5efa\u5168\u7403\u8986\u76d6\u7684\u591a\u65f6\u76f8\u81ea\u76d1\u7763\u5b66\u4e60\u6570\u636e\u96c6JLSSD\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u5bf9\u6bd4\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\uff0cCGEarthEye\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u5353\u8d8a\u7684\u7279\u5f81\u53ef\u89c6\u5316\u3001\u6a21\u578b\u6536\u655b\u6027\u548c\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u8d85\u9ad8\u5206\u8fa8\u7387\u5149\u5b66\u9065\u611f\u5f71\u50cf\u83b7\u53d6\u6e20\u9053\u6709\u9650\uff0c\u9650\u5236\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08RSVFM\uff09\u7684\u53d1\u5c55\u3002\u5409\u6797\u4e00\u53f7\u536b\u661f\u4f5c\u4e3a\u5168\u7403\u6700\u5927\u7684\u4e9a\u7c73\u7ea7\u5546\u4e1a\u9065\u611f\u536b\u661f\u661f\u5ea7\uff0c\u62e5\u6709\u4e30\u5bcc\u7684\u4e9a\u7c73\u7ea7\u5f71\u50cf\u8d44\u6e90\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u63a8\u52a8RSVFM\u7684\u8fdb\u6b65\u3002", "method": "\u63d0\u51faCGEarthEye\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u9aa8\u5e72\u7f51\u7edc\uff1b\u6784\u5efaJLSSD\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u7ea7\u8868\u793a\u805a\u7c7b\u548c\u91c7\u6837\u7b56\u7565\uff1b\u7ed3\u5408\u5b63\u8282\u6027\u5bf9\u6bd4\u3001\u589e\u5f3a\u5bf9\u6bd4\u548c\u63a9\u7801\u8865\u4e01\u6807\u8bb0\u5bf9\u6bd4\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCGEarthEye\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u7279\u5f81\u53ef\u89c6\u5316\u3001\u6a21\u578b\u6536\u655b\u6027\u3001\u53c2\u6570\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CGEarthEye\u7684\u5353\u8d8a\u8868\u5f81\u80fd\u529b\u6709\u671b\u63a8\u52a8\u5409\u6797\u4e00\u53f7\u6570\u636e\u5728\u4f20\u7edf\u5730\u7403\u89c2\u6d4b\u5e94\u7528\u4e2d\u7684\u66f4\u5e7f\u6cdb\u548c\u9ad8\u6548\u4f7f\u7528\u3002"}}
{"id": "2507.00769", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00769", "abs": "https://arxiv.org/abs/2507.00769", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "AI": {"tldr": "LitBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u521b\u610f\u5199\u4f5c\u7684\u6807\u51c6\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u6807\u7b7e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u53d1\u73b0Claude-3.7-Sonnet\u662f\u6700\u5f3a\u7684\u96f6\u6837\u672c\u8bc4\u59d4\uff0c\u5956\u52b1\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u73b0\u6210\u8bc4\u59d4\u3002", "motivation": "\u521b\u610f\u5199\u4f5c\u7f3a\u4e4f\u660e\u786e\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u73b0\u6210\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u59d4\u7684\u53ef\u9760\u6027\u5b58\u7591\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165LitBench\u6570\u636e\u96c6\uff0c\u8bad\u7ec3Bradley Terry\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5728\u7ebf\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "Claude-3.7-Sonnet\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u8fbe73%\uff0c\u5956\u52b1\u6a21\u578b\u51c6\u786e\u7387\u8fbe78%\uff0c\u4f18\u4e8e\u73b0\u6210\u8bc4\u59d4\u3002", "conclusion": "LitBench\u4e3a\u521b\u610f\u5199\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u4f18\u5316\u8d44\u6e90\u3002"}}
{"id": "2507.00363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00363", "abs": "https://arxiv.org/abs/2507.00363", "authors": ["Xingjun Wang", "Lianlei Shan"], "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control", "comment": null, "summary": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023},\naddressing challenges in initialization, optimization, and density control.\nGaussian Splatting is an alternative for rendering realistic images while\nsupporting real-time performance, and it has gained popularity due to its\nexplicit 3D Gaussian representation. However, 3DGS heavily depends on accurate\ninitialization and faces difficulties in optimizing unstructured Gaussian\ndistributions into ordered surfaces, with limited adaptive density control\nmechanism proposed so far. Our first key contribution is a geometry-guided\ninitialization to predict Gaussian parameters, ensuring precise placement and\nfaster convergence. We then introduce a surface-aligned optimization strategy\nto refine Gaussian placement, improving geometric accuracy and aligning with\nthe surface normals of the scene. Finally, we present a dynamic adaptive\ndensity control mechanism that adjusts Gaussian density based on regional\ncomplexity, for visual fidelity. These innovations enable our method to achieve\nhigh-fidelity real-time rendering and significant improvements in visual\nquality, even in complex scenes. Our method demonstrates comparable or superior\nresults to state-of-the-art methods, rendering high-fidelity images in real\ntime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u3001\u4f18\u5316\u548c\u5bc6\u5ea6\u63a7\u5236\u7684\u6311\u6218\u3002", "motivation": "3DGS\u56e0\u5176\u663e\u5f0f3D\u9ad8\u65af\u8868\u793a\u800c\u6d41\u884c\uff0c\u4f46\u5176\u4f9d\u8d56\u51c6\u786e\u521d\u59cb\u5316\uff0c\u4e14\u5728\u4f18\u5316\u65e0\u5e8f\u9ad8\u65af\u5206\u5e03\u4e3a\u6709\u5e8f\u8868\u9762\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\u3002", "method": "1. \u51e0\u4f55\u5f15\u5bfc\u7684\u521d\u59cb\u5316\u9884\u6d4b\u9ad8\u65af\u53c2\u6570\uff1b2. \u8868\u9762\u5bf9\u9f50\u7684\u4f18\u5316\u7b56\u7565\uff1b3. \u52a8\u6001\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5b9e\u65f6\u6e32\u67d3\uff0c\u89c6\u89c9\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u6e32\u67d3\u9ad8\u4fdd\u771f\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002"}}
{"id": "2507.00782", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "pdf": "https://arxiv.org/pdf/2507.00782", "abs": "https://arxiv.org/abs/2507.00782", "authors": ["Matthieu Pierre Boyer"], "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u529f\u80fd\u6027\u7f16\u7a0b\u65b9\u6cd5\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\uff0c\u901a\u8fc7\u57fa\u4e8e\u7c7b\u522b\u7684\u7c7b\u578b\u548c\u6548\u679c\u7cfb\u7edf\u53ca\u56fe\u89e3\u6f14\u7b97\uff0c\u63d0\u9ad8\u4e86\u4f20\u7edf\u6307\u79f0\u98ce\u683c\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6307\u79f0\u98ce\u683c\u5728\u8868\u8fbe\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u548c\u8868\u8fbe\u6027\u66f4\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7c7b\u522b\u7684\u7c7b\u578b\u548c\u6548\u679c\u7cfb\u7edf\uff0c\u5e76\u6784\u5efa\u56fe\u89e3\u6f14\u7b97\u6a21\u578b\u4ee5\u89e3\u6790\u548c\u5904\u7406\u6548\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u8ba1\u7b97\u53e5\u5b50\u7684\u6307\u79f0\u3002", "conclusion": "\u529f\u80fd\u6027\u7f16\u7a0b\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.00365", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00365", "abs": "https://arxiv.org/abs/2507.00365", "authors": ["Wanghui Xiao"], "title": "An Improved U-Net Model for Offline handwriting signature denoising", "comment": null, "summary": "Handwriting signatures, as an important means of identity recognition, are\nwidely used in multiple fields such as financial transactions, commercial\ncontracts and personal affairs due to their legal effect and uniqueness. In\nforensic science appraisals, the analysis of offline handwriting signatures\nrequires the appraiser to provide a certain number of signature samples, which\nare usually derived from various historical contracts or archival materials.\nHowever, the provided handwriting samples are often mixed with a large amount\nof interfering information, which brings severe challenges to handwriting\nidentification work. This study proposes a signature handwriting denoising\nmodel based on the improved U-net structure, aiming to enhance the robustness\nof the signature recognition system. By introducing discrete wavelet transform\nand PCA transform, the model's ability to suppress noise has been enhanced. The\nexperimental results show that this modelis significantly superior to the\ntraditional methods in denoising effect, can effectively improve the clarity\nand readability of the signed images, and provide more reliable technical\nsupport for signature analysis and recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-net\u7ed3\u6784\u7684\u7b7e\u540d\u624b\u5199\u53bb\u566a\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548cPCA\u53d8\u6362\u589e\u5f3a\u53bb\u566a\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u624b\u5199\u7b7e\u540d\u5728\u8eab\u4efd\u8bc6\u522b\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5386\u53f2\u6837\u672c\u5e38\u542b\u5e72\u6270\u4fe1\u606f\uff0c\u7ed9\u8bc6\u522b\u5e26\u6765\u6311\u6218\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684U-net\u7ed3\u6784\uff0c\u7ed3\u5408\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548cPCA\u53d8\u6362\uff0c\u63d0\u5347\u53bb\u566a\u6548\u679c\u3002", "result": "\u6a21\u578b\u53bb\u566a\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7b7e\u540d\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\u548c\u53ef\u8bfb\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7b7e\u540d\u5206\u6790\u548c\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.00783", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.00783", "abs": "https://arxiv.org/abs/2507.00783", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "title": "Generative AI and the future of scientometrics: current topics and future questions", "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86GenAI\u5728\u79d1\u5b66\u8ba1\u91cf\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5bf9\u9886\u57df\u7684\u5e7f\u6cdb\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8GenAI\u5728\u79d1\u5b66\u8ba1\u91cf\u5b66\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u5bf9\u4eba\u7c7b\u63a8\u7406\u7684\u6a21\u4eff\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u79d1\u5b66\u6d4b\u91cf\u6587\u672c\u7279\u5f81\u7684\u53ef\u80fd\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4ecb\u7ecdGenAI\u7684\u751f\u6210\u548c\u6982\u7387\u7279\u6027\uff0c\u7ed3\u5408\u5176\u5728\u79d1\u5b66\u8ba1\u91cf\u5b66\u4e2d\u7684\u5b9e\u9a8c\u5e94\u7528\uff08\u5982\u4e3b\u9898\u6807\u6ce8\u3001\u5f15\u6587\u5206\u6790\u7b49\uff09\uff0c\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\u3002", "result": "GenAI\u5728\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7a33\u5b9a\u8bed\u4e49\u6216\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u3002\u5efa\u8bae\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u9700\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u7406\u8bba\u53cd\u601d\u6765\u5e94\u5bf9GenAI\u5bf9\u79d1\u5b66\u77e5\u8bc6\u751f\u4ea7\u6a21\u5f0f\u7684\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2507.00368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00368", "abs": "https://arxiv.org/abs/2507.00368", "authors": ["Hikaru Shijo", "Yutaka Yoshihama", "Kenichi Yadani", "Norifumi Murata"], "title": "Out-of-Distribution Detection with Adaptive Top-K Logits Integration", "comment": null, "summary": "Neural networks often make overconfident predictions from out-of-distribution\n(OOD) samples. Detection of OOD data is therefore crucial to improve the safety\nof machine learning. The simplest and most powerful method for OOD detection is\nMaxLogit, which uses the model's maximum logit to provide an OOD score. We have\ndiscovered that, in addition to the maximum logit, some other logits are also\nuseful for OOD detection. Based on this finding, we propose a new method called\nATLI (Adaptive Top-k Logits Integration), which adaptively determines effective\ntop-k logits that are specific to each model and combines the maximum logit\nwith the other top-k logits. In this study we evaluate our proposed method\nusing ImageNet-1K benchmark. Extensive experiments showed our proposed method\nto reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit\napproach, and decreased FPR95 by an additional 2.67% compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aATLI\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u6709\u6548\u7684top-k logits\u5e76\u7ed3\u5408\u6700\u5927logit\uff0c\u663e\u8457\u63d0\u9ad8\u4e86OOD\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u6837\u672c\u7684\u9884\u6d4b\u5f80\u5f80\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u56e0\u6b64OOD\u68c0\u6d4b\u5bf9\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faATLI\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6a21\u578b\u7279\u5b9a\u7684top-k logits\uff0c\u5e76\u5c06\u5176\u4e0e\u6700\u5927logit\u7ed3\u5408\u3002", "result": "\u5728ImageNet-1K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cATLI\u5c06FPR95\u964d\u4f4e\u4e866.73%\uff08\u76f8\u6bd4MaxLogit\uff09\uff0c\u5e76\u6bd4\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e862.67%\u3002", "conclusion": "ATLI\u65b9\u6cd5\u5728OOD\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.00814", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.00814", "abs": "https://arxiv.org/abs/2507.00814", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "title": "Many LLMs Are More Utilitarian Than One", "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u7fa4\u4f53\u8ba8\u8bba\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u529f\u5229\u4e3b\u4e49\u503e\u5411\uff0c\u4f46\u5176\u9a71\u52a8\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "motivation": "\u63a2\u8ba8\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u9053\u5fb7\u5224\u65ad\u4e2d\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5e76\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u884c\u4e3a\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u6d4b\u8bd5\u516d\u79cd\u6a21\u578b\u5728\u72ec\u7acb\u548c\u7fa4\u4f53\u8ba8\u8bba\u6761\u4ef6\u4e0b\u7684\u9053\u5fb7\u56f0\u5883\u8868\u73b0\u3002", "result": "LLM\u7fa4\u4f53\u66f4\u503e\u5411\u4e8e\u63a5\u53d7\u9053\u5fb7\u8fdd\u89c4\u884c\u4e3a\uff0c\u4f46\u9a71\u52a8\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "conclusion": "LLM\u96c6\u4f53\u884c\u4e3a\u7684\u8868\u9762\u6a21\u4eff\u4eba\u7c7b\uff0c\u4f46\u5185\u5728\u673a\u5236\u4e0d\u540c\uff0c\u5bf9AI\u5bf9\u9f50\u548c\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u6709\u542f\u793a\u3002"}}
{"id": "2507.00371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00371", "abs": "https://arxiv.org/abs/2507.00371", "authors": ["Xin Yang", "Ruiming Du", "Hanyang Huang", "Jiayang Xie", "Pengyao Xie", "Leisen Fang", "Ziyue Guo", "Nanjun Jiang", "Yu Jiang", "Haiyan Cen"], "title": "PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching", "comment": null, "summary": "Organ segmentation of plant point clouds is a prerequisite for the\nhigh-resolution and accurate extraction of organ-level phenotypic traits.\nAlthough the fast development of deep learning has boosted much research on\nsegmentation of plant point clouds, the existing techniques for organ\nsegmentation still face limitations in resolution, segmentation accuracy, and\ngeneralizability across various plant species. In this study, we proposed a\nnovel approach called plant segmentation neural radiance fields (PlantSegNeRF),\naiming to directly generate high-precision instance point clouds from\nmulti-view RGB image sequences for a wide range of plant species. PlantSegNeRF\nperformed 2D instance segmentation on the multi-view images to generate\ninstance masks for each organ with a corresponding ID. The multi-view instance\nIDs corresponding to the same plant organ were then matched and refined using a\nspecially designed instance matching module. The instance NeRF was developed to\nrender an implicit scene, containing color, density, semantic and instance\ninformation. The implicit scene was ultimately converted into high-precision\nplant instance point clouds based on the volume density. The results proved\nthat in semantic segmentation of point clouds, PlantSegNeRF outperformed the\ncommonly used methods, demonstrating an average improvement of 16.1%, 18.3%,\n17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the\nsecond-best results on structurally complex datasets. More importantly,\nPlantSegNeRF exhibited significant advantages in plant point cloud instance\nsegmentation tasks. Across all plant datasets, it achieved average improvements\nof 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.\nThis study extends the organ-level plant phenotyping and provides a\nhigh-throughput way to supply high-quality 3D data for the development of\nlarge-scale models in plant science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPlantSegNeRF\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d2RGB\u56fe\u50cf\u5e8f\u5217\u76f4\u63a5\u751f\u6210\u9ad8\u7cbe\u5ea6\u690d\u7269\u5668\u5b98\u70b9\u4e91\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u690d\u7269\u70b9\u4e91\u5668\u5b98\u5206\u5272\u6280\u672f\u5728\u5206\u8fa8\u7387\u3001\u7cbe\u5ea6\u548c\u8de8\u7269\u79cd\u901a\u7528\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "PlantSegNeRF\u7ed3\u54082D\u5b9e\u4f8b\u5206\u5272\u3001\u5b9e\u4f8b\u5339\u914d\u6a21\u5757\u548c\u5b9e\u4f8bNeRF\uff0c\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u5305\u542b\u989c\u8272\u3001\u5bc6\u5ea6\u3001\u8bed\u4e49\u548c\u5b9e\u4f8b\u4fe1\u606f\u7684\u9690\u5f0f\u573a\u666f\uff0c\u6700\u7ec8\u8f6c\u6362\u4e3a\u9ad8\u7cbe\u5ea6\u70b9\u4e91\u3002", "result": "\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0cPlantSegNeRF\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cIoU\u4e0a\u5e73\u5747\u63d0\u534716.1%-24.2%\uff1b\u5728\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\uff0cmPrec\u3001mRec\u3001mCov\u548cmWCov\u5e73\u5747\u63d0\u534711.7%-38.2%\u3002", "conclusion": "PlantSegNeRF\u4e3a\u690d\u7269\u5668\u5b98\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u901a\u91cf\u65b9\u6cd5\uff0c\u5e76\u4e3a\u690d\u7269\u79d1\u5b66\u5927\u89c4\u6a21\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf3D\u6570\u636e\u3002"}}
{"id": "2507.00828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00828", "abs": "https://arxiv.org/abs/2507.00828", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolom\u00e9", "Jordan Boyd-Graber", "Philip Resnik"], "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\u53ca\u5176\u81ea\u52a8\u5316\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e3b\u9898\u6a21\u578b\u548c\u6587\u6863\u805a\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u6807\u7b7e\u6216\u81ea\u52a8\u5316\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u96be\u4ee5\u6269\u5c55\u7684\u4e13\u5bb6\u6807\u7b7e\uff0c\u8981\u4e48\u4f7f\u7528\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u7684\u81ea\u52a8\u5316\u6307\u6807\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bc4\u4f30\u534f\u8bae\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u6216\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u63a8\u65ad\u7c7b\u522b\u5e76\u5e94\u7528\u4e8e\u5176\u4ed6\u6587\u6863\uff0c\u6536\u96c6\u4e86\u5927\u91cf\u6807\u6ce8\u6570\u636e\u9a8c\u8bc1\u81ea\u52a8\u5316\u4ee3\u7406\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u4f73\u7684LLM\u4ee3\u7406\u5728\u7edf\u8ba1\u4e0a\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u53ef\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u5408\u7406\u66ff\u4ee3\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u8bae\u548c\u81ea\u52a8\u5316\u4ee3\u7406\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e3b\u9898\u6a21\u578b\u548c\u6587\u6863\u805a\u7c7b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00372", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00372", "abs": "https://arxiv.org/abs/2507.00372", "authors": ["Xinge Yang", "Chuong Nguyen", "Wenbin Wang", "Kaizhang Kang", "Wolfgang Heidrich", "Xiaoxing Li"], "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur", "comment": null, "summary": "Modern cameras with large apertures often suffer from a shallow depth of\nfield, resulting in blurry images of objects outside the focal plane. This\nlimitation is particularly problematic for fixed-focus cameras, such as those\nused in smart glasses, where adding autofocus mechanisms is challenging due to\nform factor and power constraints. Due to unmatched optical aberrations and\ndefocus properties unique to each camera system, deep learning models trained\non existing open-source datasets often face domain gaps and do not perform well\nin real-world settings. In this paper, we propose an efficient and scalable\ndataset synthesis approach that does not rely on fine-tuning with real-world\ndata. Our method simultaneously models depth-dependent defocus and spatially\nvarying optical aberrations, addressing both computational complexity and the\nscarcity of high-quality RGB-D datasets. Experimental results demonstrate that\na network trained on our low resolution synthetic images generalizes\neffectively to high resolution (12MP) real-world images across diverse scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u5408\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u5927\u5149\u5708\u76f8\u673a\u56e0\u6d45\u666f\u6df1\u5bfc\u81f4\u7684\u6a21\u7cca\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u3002", "motivation": "\u5927\u5149\u5708\u76f8\u673a\u56e0\u6d45\u666f\u6df1\u5bfc\u81f4\u56fe\u50cf\u6a21\u7cca\uff0c\u56fa\u5b9a\u5bf9\u7126\u76f8\u673a\uff08\u5982\u667a\u80fd\u773c\u955c\uff09\u96be\u4ee5\u6dfb\u52a0\u81ea\u52a8\u5bf9\u7126\u673a\u5236\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u5149\u5b66\u50cf\u5dee\u548c\u6563\u7126\u7279\u6027\u4e0d\u5339\u914d\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u540c\u65f6\u5efa\u6a21\u6df1\u5ea6\u76f8\u5173\u6563\u7126\u548c\u7a7a\u95f4\u53d8\u5316\u5149\u5b66\u50cf\u5dee\uff0c\u89e3\u51b3\u8ba1\u7b97\u590d\u6742\u6027\u548c\u9ad8\u8d28\u91cfRGB-D\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u4f4e\u5206\u8fa8\u7387\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u7684\u7f51\u7edc\u80fd\u6709\u6548\u6cdb\u5316\u5230\u9ad8\u5206\u8fa8\u7387\uff0812MP\uff09\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u9ad8\u6548\u5408\u6210\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u7cca\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u3002"}}
{"id": "2507.00838", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00838", "abs": "https://arxiv.org/abs/2507.00838", "authors": ["Karol Przystalski", "Jan K. Argasi\u0144ski", "Iwona Grabska-Gradzi\u0144ska", "Jeremi K. Ochab"], "title": "Stylometry recognizes human and LLM-generated texts in short samples", "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u6587\u4f53\u6d4b\u91cf\u5b66\u533a\u5206\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4eba\u7c7b\u751f\u6210\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u6d89\u53ca\u6a21\u578b\u5f52\u5c5e\u3001\u77e5\u8bc6\u4ea7\u6743\u548cAI\u4f26\u7406\u95ee\u9898\u3002\u901a\u8fc7\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u5e94\u7528\u6811\u6a21\u578b\u5206\u7c7b\uff0c\u7ed3\u679c\u663e\u793a\u5728\u591a\u7c7b\u548c\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3LLM\u751f\u6210\u6587\u672c\u7684\u5f52\u5c5e\u95ee\u9898\u53ca\u5176\u5bf9\u77e5\u8bc6\u4ea7\u6743\u548cAI\u4f26\u7406\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u6587\u4f53\u6d4b\u91cf\u5b66\u5728\u6b64\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u57fa\u4e8eWikipedia\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4eba\u7c7b\u548c\u591a\u79cdLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u4f7f\u7528\u6811\u6a21\u578b\uff08\u5982LightGBM\uff09\u7ed3\u5408\u6587\u4f53\u7279\u5f81\uff08StyloMetrix\u548cn-gram\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7ed3\u679c\u663e\u793a\u591a\u5206\u7c7b\u4efb\u52a1\u4e2dMatthews\u76f8\u5173\u7cfb\u6570\u8fbe0.87\uff0c\u4e8c\u5206\u7c7b\u51c6\u786e\u7387\u57280.79\u81f31\u4e4b\u95f4\uff0cWikipedia\u4e0eGPT-4\u7684\u5e73\u8861\u6570\u636e\u96c6\u51c6\u786e\u7387\u9ad8\u8fbe0.98\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u5bf9\u4e8e\u7279\u5b9a\u6587\u672c\u7c7b\u578b\uff0c\u6587\u4f53\u6d4b\u91cf\u5b66\u53ef\u4ee5\u6709\u6548\u533a\u5206\u673a\u5668\u4e0e\u4eba\u7c7b\u751f\u6210\u6587\u672c\uff0c\u5c24\u5176\u5728LLM\u65e5\u76ca\u590d\u6742\u7684\u80cc\u666f\u4e0b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.00373", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00373", "abs": "https://arxiv.org/abs/2507.00373", "authors": ["Ian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "title": "Customizable ROI-Based Deep Image Compression", "comment": null, "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u81ea\u5b9a\u4e49ROI\u7684\u6df1\u5ea6\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u6587\u672c\u5b9a\u4e49ROI\u5e76\u8c03\u6574ROI\u4e0e\u975eROI\u7684\u8d28\u91cf\u6743\u8861\u3002", "motivation": "\u73b0\u6709ROI\u56fe\u50cf\u538b\u7f29\u65b9\u6848\u56fa\u5b9aROI\u4e14\u65e0\u6cd5\u5e73\u8861ROI\u4e0e\u975eROI\u7684\u8d28\u91cf\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u6587\u672c\u63a7\u5236\u63a9\u7801\u83b7\u53d6\u6a21\u5757\uff08TMA\uff09\u3001\u81ea\u5b9a\u4e49\u503c\u5206\u914d\u673a\u5236\uff08CVA\uff09\u548c\u6f5c\u5728\u63a9\u7801\u6ce8\u610f\u529b\u6a21\u5757\uff08LMA\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u652f\u6301ROI\u81ea\u5b9a\u4e49\u53ca\u8d28\u91cf\u6743\u8861\u7ba1\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aROI\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u7528\u6237\u5b9a\u5236\u80fd\u529b\u3002"}}
{"id": "2507.00875", "categories": ["cs.CL", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.00875", "abs": "https://arxiv.org/abs/2507.00875", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "TransLaw\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u7ffb\u8bd1\u9999\u6e2f\u6cd5\u5f8b\u5224\u51b3\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u4ee3\u7406\uff08\u7ffb\u8bd1\u3001\u6ce8\u91ca\u3001\u6821\u5bf9\uff09\u534f\u4f5c\uff0c\u63d0\u9ad8\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6cd5\u5f8b\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u6cd5\u5f8b\u672f\u8bed\u590d\u6742\u3001\u6587\u5316\u5dee\u5f02\u548c\u8bed\u8a00\u7ed3\u6784\u4e25\u683c\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u4ee3\u7406\uff08Translator\u3001Annotator\u3001Proofreader\uff09\u534f\u4f5c\u7684\u6846\u67b6\uff0c\u652f\u6301\u81ea\u5b9a\u4e49LLM\u914d\u7f6e\u3002", "result": "TransLaw\u5728\u8bed\u4e49\u51c6\u786e\u6027\u3001\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u98ce\u683c\u5fe0\u5b9e\u5ea6\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u4f46\u5728\u590d\u6742\u672f\u8bed\u548c\u98ce\u683c\u81ea\u7136\u5ea6\u4e0a\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "TransLaw\u5c55\u793a\u4e86LLM\u5728\u6cd5\u5f8b\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u3002"}}
{"id": "2507.00377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00377", "abs": "https://arxiv.org/abs/2507.00377", "authors": ["Jianhao Xie", "Ziang Zhang", "Zhenyu Weng", "Yuesheng Zhu", "Guibo Luo"], "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis", "comment": "11 pages,3 figures", "summary": "Recent advancements in deep learning for medical image segmentation are often\nlimited by the scarcity of high-quality training data.While diffusion models\nprovide a potential solution by generating synthetic images, their\neffectiveness in medical imaging remains constrained due to their reliance on\nlarge-scale medical datasets and the need for higher image quality. To address\nthese challenges, we present MedDiff-FT, a controllable medical image\ngeneration method that fine-tunes a diffusion foundation model to produce\nmedical images with structural dependency and domain specificity in a\ndata-efficient manner. During inference, a dynamic adaptive guiding mask\nenforces spatial constraints to ensure anatomically coherent synthesis, while a\nlightweight stochastic mask generator enhances diversity through hierarchical\nrandomness injection. Additionally, an automated quality assessment protocol\nfilters suboptimal outputs using feature-space metrics, followed by mask\ncorrosion to refine fidelity. Evaluated on five medical segmentation\ndatasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's\nsegmentation performance by an average of 1% in Dice score. The framework\neffectively balances generation quality, diversity, and computational\nefficiency, offering a practical solution for medical data augmentation. The\ncode is available at https://github.com/JianhaoXie1/MedDiff-FT.", "AI": {"tldr": "MedDiff-FT\u662f\u4e00\u79cd\u53ef\u63a7\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u6570\u636e\u9ad8\u6548\u7684\u65b9\u5f0f\u751f\u6210\u5177\u6709\u7ed3\u6784\u4f9d\u8d56\u6027\u548c\u9886\u57df\u7279\u5f02\u6027\u7684\u533b\u5b66\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u81ea\u9002\u5e94\u5f15\u5bfc\u63a9\u7801\u548c\u8f7b\u91cf\u7ea7\u968f\u673a\u63a9\u7801\u751f\u6210\u5668\uff0c\u7ed3\u5408\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u534f\u8bae\u548c\u63a9\u7801\u8150\u8680\u6280\u672f\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0cMedDiff-FT\u7684\u5408\u6210\u56fe\u50cf-\u63a9\u7801\u5bf9\u5c06SOTA\u65b9\u6cd5\u7684Dice\u5206\u6570\u5e73\u5747\u63d0\u9ad8\u4e861%\u3002", "conclusion": "MedDiff-FT\u5728\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u533b\u5b66\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00883", "abs": "https://arxiv.org/abs/2507.00883", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6570\u5b66\u95ee\u9898\u7684\u5448\u73b0\u65b9\u5f0f\u5e26\u6709\u6587\u5316\u80cc\u666f\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u9002\u5e94\u7248\u672c\u4e0a\u7684\u8868\u73b0\u666e\u904d\u4e0d\u5982\u539f\u7248\u7f8e\u56fd\u6570\u636e\u96c6\uff0c\u4f46\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u8868\u73b0\u66f4\u7a33\u5065\u3002", "motivation": "\u6570\u5b66\u95ee\u9898\u5e38\u88ab\u8ba4\u4e3a\u6587\u5316\u4e2d\u7acb\uff0c\u4f46\u5b9e\u9645\u5448\u73b0\u65b9\u5f0f\u9690\u542b\u6587\u5316\u80cc\u666f\u3002\u73b0\u6709\u57fa\u51c6\uff08\u5982GSM8K\uff09\u4ee5\u897f\u65b9\u6587\u5316\u4e3a\u4e3b\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u8f6c\u6362\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4e3a\u975e\u6d32\u3001\u5370\u5ea6\u3001\u4e2d\u56fd\u3001\u97e9\u56fd\u548c\u65e5\u672c\u521b\u5efa\u6587\u5316\u9002\u5e94\u7248\u672c\u7684GSM8K\u6d4b\u8bd5\u96c6\uff0c\u5e76\u8bc4\u4f30\u516d\u79cd\u4e0d\u540c\u89c4\u6a21\u7684LLM\u5728\u4e94\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u539f\u7248\u7f8e\u56fd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6587\u5316\u9002\u5e94\u7248\u672c\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5bf9\u6587\u5316\u5dee\u5f02\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u6df1\u5c42\u63a8\u7406\u80fd\u529b\u6709\u52a9\u4e8e\u7f29\u5c0f\u6570\u5b66\u4efb\u52a1\u4e2d\u6587\u5316\u5448\u73b0\u5dee\u5f02\u5e26\u6765\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2507.00392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00392", "abs": "https://arxiv.org/abs/2507.00392", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space", "comment": null, "summary": "Feature matching plays a fundamental role in many computer vision tasks, yet\nexisting methods heavily rely on scarce and clean multi-view image collections,\nwhich constrains their generalization to diverse and challenging scenarios.\nMoreover, conventional feature encoders are typically trained on single-view 2D\nimages, limiting their capacity to capture 3D-aware correspondences. In this\npaper, we propose a novel two-stage framework that lifts 2D images to 3D space,\nnamed as \\textbf{Lift to Match (L2M)}, taking full advantage of large-scale and\ndiverse single-view images. To be specific, in the first stage, we learn a\n3D-aware feature encoder using a combination of multi-view image synthesis and\n3D feature Gaussian representation, which injects 3D geometry knowledge into\nthe encoder. In the second stage, a novel-view rendering strategy, combined\nwith large-scale synthetic data generation from single-view images, is employed\nto learn a feature decoder for robust feature matching, thus achieving\ngeneralization across diverse domains. Extensive experiments demonstrate that\nour method achieves superior generalization across zero-shot evaluation\nbenchmarks, highlighting the effectiveness of the proposed framework for robust\nfeature matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLift to Match (L2M)\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5c062D\u56fe\u50cf\u63d0\u5347\u52303D\u7a7a\u95f4\uff0c\u5229\u7528\u5927\u89c4\u6a21\u5355\u89c6\u56fe\u56fe\u50cf\u5b9e\u73b0\u9c81\u68d2\u7279\u5f81\u5339\u914d\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u4f9d\u8d56\u7a00\u7f3a\u4e14\u5e72\u51c0\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f20\u7edf\u7279\u5f81\u7f16\u7801\u5668\u57fa\u4e8e\u5355\u89c6\u56fe2D\u56fe\u50cf\u8bad\u7ec3\uff0c\u96be\u4ee5\u6355\u63493D\u611f\u77e5\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u591a\u89c6\u56fe\u56fe\u50cf\u5408\u6210\u548c3D\u7279\u5f81\u9ad8\u65af\u8868\u793a\u5b66\u4e603D\u611f\u77e5\u7279\u5f81\u7f16\u7801\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u65b0\u89c6\u56fe\u6e32\u67d3\u7b56\u7565\u548c\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u5b66\u4e60\u7279\u5f81\u89e3\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "L2M\u6846\u67b6\u901a\u8fc73D\u611f\u77e5\u7279\u5f81\u7f16\u7801\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u7684\u9c81\u68d2\u7279\u5f81\u5339\u914d\u3002"}}
{"id": "2507.00885", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00885", "abs": "https://arxiv.org/abs/2507.00885", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0b\u6e38\u6269\u5c55\u5b9a\u5f8b\u4ec5\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\uff0839%\uff09\u7b26\u5408\u7ebf\u6027\u8d8b\u52bf\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5fae\u5c0f\u53d8\u5316\u53ef\u80fd\u5b8c\u5168\u6539\u53d8\u6269\u5c55\u8d8b\u52bf\u3002", "motivation": "\u63a2\u8ba8\u4e0b\u6e38\u6269\u5c55\u5b9a\u5f8b\u662f\u5426\u80fd\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\uff0c\u4ee5\u53ca\u5176\u9002\u7528\u6761\u4ef6\u548c\u5c40\u9650\u6027\u3002", "method": "\u5bf9\u73b0\u6709\u4e0b\u6e38\u6269\u5c55\u5b9a\u5f8b\u6570\u636e\u8fdb\u884c\u5143\u5206\u6790\uff0c\u8bc4\u4f30\u7ebf\u6027\u6269\u5c55\u8d8b\u52bf\u7684\u666e\u904d\u6027\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u4ec539%\u7684\u60c5\u51b5\u4e0b\u7b26\u5408\u7ebf\u6027\u6269\u5c55\u8d8b\u52bf\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5fae\u5c0f\u53d8\u5316\u53ef\u80fd\u663e\u8457\u6539\u53d8\u6269\u5c55\u884c\u4e3a\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6269\u5c55\u5b9a\u5f8b\u7684\u6210\u529f\u6761\u4ef6\uff0c\u5e76\u5173\u6ce8\u504f\u79bb\u7ebf\u6027\u8d8b\u52bf\u7684\u60c5\u51b5\u3002"}}
{"id": "2507.00401", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00401", "abs": "https://arxiv.org/abs/2507.00401", "authors": ["Xin Xu", "Eibe Frank", "Geoffrey Holmes"], "title": "Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains", "comment": null, "summary": "We investigate cross-domain few-shot learning under the constraint that\nfine-tuning of backbones (i.e., feature extractors) is impossible or infeasible\n-- a scenario that is increasingly common in practical use cases. Handling the\nlow-quality and static embeddings produced by frozen, \"black-box\" backbones\nleads to a problem representation of few-shot classification as a series of\nmultiple instance verification (MIV) tasks. Inspired by this representation, we\nintroduce a novel approach to few-shot domain adaptation, named the \"MIV-head\",\nakin to a classification head that is agnostic to any pretrained backbone and\ncomputationally efficient. The core components designed for the MIV-head, when\ntrained on few-shot data from a target domain, collectively yield strong\nperformance on test data from that domain. Importantly, it does so without\nfine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting\nunder various settings and on an extension of the Meta-dataset benchmark for\ncross-domain few-shot image classification, using representative off-the-shelf\nconvolutional neural network and vision transformer backbones pretrained on\nImageNet1K, we show that the MIV-head achieves highly competitive accuracy when\ncompared to state-of-the-art \"adapter\" (or partially fine-tuning) methods\napplied to the same backbones, while incurring substantially lower adaptation\ncost. We also find well-known \"classification head\" approaches lag far behind\nin terms of accuracy. Ablation study empirically justifies the core components\nof our approach. We share our code at https://github.com/xxweka/MIV-head.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIV-head\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u65e0\u9700\u5fae\u8c03\u4e3b\u5e72\u7f51\u7edc\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u6cd5\u5fae\u8c03\u4e3b\u5e72\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\uff0c\u5904\u7406\u4f4e\u8d28\u91cf\u9759\u6001\u5d4c\u5165\u7684\u5c11\u6837\u672c\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u5c06\u5c11\u6837\u672c\u5206\u7c7b\u8868\u793a\u4e3a\u591a\u5b9e\u4f8b\u9a8c\u8bc1\u4efb\u52a1\uff0c\u8bbe\u8ba1MIV-head\uff0c\u65e0\u9700\u5fae\u8c03\u4e3b\u5e72\u7f51\u7edc\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002", "result": "\u5728\u8de8\u57df\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cMIV-head\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9002\u5e94\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "MIV-head\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.00891", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00891", "abs": "https://arxiv.org/abs/2507.00891", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MemeCMD\uff0c\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684\u4e2d\u6587\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684\u8868\u60c5\u5305\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6a21\u6001\u8868\u8fbe\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u96c6\u591a\u4e3a\u7eaf\u6587\u672c\u6216\u624b\u52a8\u6807\u6ce8\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u8868\u8fbe\u529b\u548c\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21MLLM\u6807\u6ce8\u7684\u8868\u60c5\u5305\u5e93\u548c\u53cc\u4ee3\u7406\u81ea\u52a8\u751f\u6210\u7684\u5bf9\u8bdd\uff0c\u5f15\u5165\u68c0\u7d22\u6846\u67b6\u548c\u81ea\u9002\u5e94\u9608\u503c\u786e\u4fdd\u8868\u60c5\u5305\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u81ea\u7136\u95f4\u9694\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u4e0a\u4e0b\u6587\u6070\u5f53\u4e14\u591a\u6837\u5316\u7684\u8868\u60c5\u5305\u5bf9\u8bdd\uff0c\u4e3a\u591a\u6a21\u6001\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u8d44\u6e90\u3002", "conclusion": "MemeCMD\u4e3a\u591a\u6a21\u6001\u5bf9\u8bdd\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2507.00429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00429", "abs": "https://arxiv.org/abs/2507.00429", "authors": ["Jingyi Pan", "Dan Xu", "Qiong Luo"], "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting", "comment": "ICCV 2025, Project page: https://rorisis.github.io/DiGA3D/", "summary": "Developing a unified pipeline that enables users to remove, re-texture, or\nreplace objects in a versatile manner is crucial for text-guided 3D inpainting.\nHowever, there are still challenges in performing multiple 3D inpainting tasks\nwithin a unified framework: 1) Single reference inpainting methods lack\nrobustness when dealing with views that are far from the reference view. 2)\nAppearance inconsistency arises when independently inpainting multi-view images\nwith 2D diffusion priors; 3) Geometry inconsistency limits performance when\nthere are significant geometric changes in the inpainting regions. To tackle\nthese challenges, we introduce DiGA3D, a novel and versatile 3D inpainting\npipeline that leverages diffusion models to propagate consistent appearance and\ngeometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy\nfor selecting multiple reference views to reduce errors during propagation.\nNext, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that\npropagates attention features from the selected reference views to other views\nvia diffusion models to maintain appearance consistency. Furthermore, DiGA3D\nintroduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to\nfurther improve the geometric consistency of inpainted 3D scenes. Extensive\nexperiments on multiple 3D inpainting tasks demonstrate the effectiveness of\nour method. The project page is available at https://rorisis.github.io/DiGA3D/.", "AI": {"tldr": "DiGA3D\u662f\u4e00\u4e2a\u7edf\u4e00\u76843D\u4fee\u590d\u7ba1\u9053\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5728\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u4e2d\u4f20\u64ad\u4e00\u81f4\u7684\u5916\u89c2\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u4fee\u590d\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u8fdc\u79bb\u53c2\u8003\u89c6\u56fe\u7684\u89c6\u89d2\u3001\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u5916\u89c2\u4e0d\u4e00\u81f4\u4ee5\u53ca\u51e0\u4f55\u53d8\u5316\u65f6\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "DiGA3D\u91c7\u7528\u591a\u53c2\u8003\u89c6\u56fe\u9009\u62e9\u7b56\u7565\u3001\u6ce8\u610f\u529b\u7279\u5f81\u4f20\u64ad\u673a\u5236\uff08AFP\uff09\u548c\u7eb9\u7406-\u51e0\u4f55\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08TG-SDS\uff09\u635f\u5931\u6765\u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiGA3D\u5728\u591a\u79cd3D\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiGA3D\u901a\u8fc7\u521b\u65b0\u7684\u7b56\u7565\u548c\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u4fee\u590d\u7684\u4e00\u81f4\u6027\u548c\u6548\u679c\u3002"}}
{"id": "2507.00911", "categories": ["cs.CL", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2507.00911", "abs": "https://arxiv.org/abs/2507.00911", "authors": ["Luise H\u00e4user", "Alexandros Stamatakis"], "title": "The Cognate Data Bottleneck in Language Phylogenetics", "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u8ba1\u7b97\u7cfb\u7edf\u53d1\u80b2\u65b9\u6cd5\u5904\u7406\u540c\u6e90\u8bcd\u6570\u636e\u7684\u6311\u6218\uff0c\u6307\u51fa\u5f53\u524d\u7f3a\u4e4f\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u540c\u6e90\u8bcd\u6570\u636e\u96c6\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4eceBabelNet\u63d0\u53d6\u7684\u6570\u636e\u96c6\u5728\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u8ba1\u7b97\u7cfb\u7edf\u53d1\u80b2\u65b9\u6cd5\u5728\u540c\u6e90\u8bcd\u6570\u636e\u5e94\u7528\u4e2d\u7684\u74f6\u9888\uff0c\u5373\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u4eceBabelNet\u81ea\u52a8\u63d0\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u5176\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u7ed3\u679c\u4e0e\u6807\u51c6\u9ec4\u91d1\u6811\u7684\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4eceBabelNet\u63d0\u53d6\u7684\u6570\u636e\u96c6\u751f\u6210\u7684\u7cfb\u7edf\u53d1\u80b2\u6811\u4e0e\u6807\u51c6\u9ec4\u91d1\u6811\u4e0d\u4e00\u81f4\uff0c\u4e14\u5176\u4ed6\u591a\u8bed\u8a00\u8d44\u6e90\u4e5f\u96be\u4ee5\u63d0\u4f9b\u66f4\u5408\u9002\u7684\u7279\u5f81\u77e9\u9635\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u5f53\u524d\u65e0\u6cd5\u4e3a\u540c\u6e90\u8bcd\u6570\u636e\u751f\u6210\u8db3\u591f\u5927\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u7cfb\u7edf\u53d1\u80b2\u65b9\u6cd5\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.00430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00430", "abs": "https://arxiv.org/abs/2507.00430", "authors": ["Huanxin Yang", "Qiwen Wang"], "title": "MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition", "comment": null, "summary": "Handwritten mathematical expression recognition (HMER) suffers from complex\nformula structures and character layouts in sequence prediction. In this paper,\nwe incorporate frequency domain analysis into HMER and propose a method that\nmarries frequency domain with HMER (MFH), leveraging the discrete cosine\ntransform (DCT). We emphasize the structural analysis assistance of frequency\ninformation for recognizing mathematical formulas. When implemented on various\nbaseline models, our network exhibits a consistent performance enhancement,\ndemonstrating the efficacy of frequency domain information. Experiments show\nthat our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on\nthe CROHME 2014/2016/2019 test sets. The source code is available at\nhttps://github.com/Hryxyhe/MFH.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9891\u57df\u5206\u6790\u7684\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u65b9\u6cd5\uff08MFH\uff09\uff0c\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u56e0\u590d\u6742\u7684\u516c\u5f0f\u7ed3\u6784\u548c\u5b57\u7b26\u5e03\u5c40\u800c\u9762\u4e34\u6311\u6218\uff0c\u9891\u57df\u4fe1\u606f\u53ef\u80fd\u63d0\u4f9b\u7ed3\u6784\u5206\u6790\u8f85\u52a9\u3002", "method": "\u63d0\u51faMFH\u65b9\u6cd5\uff0c\u5c06\u9891\u57df\u5206\u6790\uff08DCT\uff09\u4e0eHMER\u7ed3\u5408\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u516c\u5f0f\u7ed3\u6784\u7684\u7406\u89e3\u3002", "result": "\u5728CROHME 2014/2016/2019\u6d4b\u8bd5\u96c6\u4e0a\uff0cMFH-CoMER\u5206\u522b\u8fbe\u523061.66%\u300162.07%\u300163.72%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u9891\u57df\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347HMER\u6027\u80fd\uff0cMFH\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.00985", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00985", "abs": "https://arxiv.org/abs/2507.00985", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u7684\u9053\u5fb7\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u8868\u9762\u6027\u548c\u8bca\u65ad\u4e0d\u8db3\u7684\u6096\u8bba\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u542f\u53d1\u5f0f\u6570\u636e\u96c6\u7684\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLMs\u5728\u9053\u5fb7\u81ea\u6211\u4fee\u6b63\u4e2d\u7684\u8868\u9762\u6027\u548c\u8bca\u65ad\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u5206\u6790\u5fae\u8c03\u8bed\u6599\u5e93\u4e2d\u7684\u8bdd\u8bed\u7ed3\u6784\uff0c\u63ed\u793a\u542f\u53d1\u5f0f\u6377\u5f84\u7684\u5b58\u5728\u3002", "result": "\u53d1\u73b0\u9053\u5fb7\u81ea\u6211\u4fee\u6b63\u4f9d\u8d56\u542f\u53d1\u5f0f\u6377\u5f84\uff0c\u5bfc\u81f4\u4fee\u6b63\u4e0e\u8bca\u65ad\u80fd\u529b\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u5229\u7528\u542f\u53d1\u5f0f\u6570\u636e\u96c6\u6539\u8fdb\u9053\u5fb7\u81ea\u6211\u4fee\u6b63\uff0c\u5e76\u6307\u51fa\u5176\u6cdb\u5316\u6311\u6218\u3002"}}
{"id": "2507.00447", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00447", "abs": "https://arxiv.org/abs/2507.00447", "authors": ["Xin Luo", "Menglin Zhang", "Yunwei Lan", "Tianyu Zhang", "Rui Li", "Chang Liu", "Dong Liu"], "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration", "comment": "Code and Models will be publicly available at\n  https://github.com/Luciennnnnnn/Latent-PMRF", "summary": "The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face\nrestoration algorithms must balance perceptual quality and fidelity. To achieve\nminimal distortion while maintaining perfect perceptual quality, Posterior-Mean\nRectified Flow (PMRF) proposes a flow based approach where source distribution\nis minimum distortion estimations. Although PMRF is shown to be effective, its\npixel-space modeling approach limits its ability to align with human\nperception, where human perception is defined as how humans distinguish between\ntwo image distributions. In this work, we propose Latent-PMRF, which\nreformulates PMRF in the latent space of a variational autoencoder (VAE),\nfacilitating better alignment with human perception during optimization. By\ndefining the source distribution on latent representations of minimum\ndistortion estimation, we bound the minimum distortion by the VAE's\nreconstruction error. Moreover, we reveal the design of VAE is crucial, and our\nproposed VAE significantly outperforms existing VAEs in both reconstruction and\nrestoration. Extensive experiments on blind face restoration demonstrate the\nsuperiority of Latent-PMRF, offering an improved PD-tradeoff compared to\nexisting methods, along with remarkable convergence efficiency, achieving a\n5.79X speedup over PMRF in terms of FID. Our code will be available as\nopen-source.", "AI": {"tldr": "Latent-PMRF\u901a\u8fc7\u5c06PMRF\u65b9\u6cd5\u6539\u8fdb\u5230VAE\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f2\u8138\u6062\u590d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "PMRF\u65b9\u6cd5\u5728\u50cf\u7d20\u7a7a\u95f4\u4e2d\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u5176\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5e73\u8861\u611f\u77e5\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faLatent-PMRF\uff0c\u5c06PMRF\u91cd\u65b0\u5b9a\u4e49\u5728VAE\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5229\u7528\u6f5c\u5728\u8868\u793a\u7684\u6700\u5c0f\u5931\u771f\u4f30\u8ba1\u6765\u4f18\u5316PD\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLatent-PMRF\u5728\u76f2\u8138\u6062\u590d\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e865.79\u500d\u7684FID\u52a0\u901f\u3002", "conclusion": "Latent-PMRF\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86PD\u6743\u8861\u548c\u6536\u655b\u6548\u7387\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2507.00994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00994", "abs": "https://arxiv.org/abs/2507.00994", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "Andr\u00e9 F. T. Martins", "C\u00e9line Hudelot", "Pierre Colombo"], "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86CLM\u548cMLM\u5728\u6587\u672c\u8868\u793a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u7684\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u7814\u7a76CLM\u548cMLM\u5728\u6587\u672c\u8868\u793a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u660e\u786e\u5176\u4f18\u52bf\u662f\u5426\u6e90\u4e8e\u76ee\u6807\u51fd\u6570\u672c\u8eab\u8fd8\u662f\u5176\u4ed6\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5bf9\u7167\u5b9e\u9a8c\uff0c\u8bad\u7ec330\u4e2a\u6a21\u578b\uff08\u53c2\u6570\u4ece2.1\u4ebf\u523010\u4ebf\uff09\uff0c\u5e76\u8fdb\u884c\u8d85\u8fc715,000\u6b21\u5fae\u8c03\u548c\u8bc4\u4f30\u3002", "result": "MLM\u5728\u6587\u672c\u8868\u793a\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46CLM\u6a21\u578b\u66f4\u5177\u6570\u636e\u6548\u7387\u548c\u5fae\u8c03\u7a33\u5b9a\u6027\uff1b\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u5148CLM\u540eMLM\uff09\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408CLM\u548cMLM\u7684\u4f18\u52bf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4ece\u73b0\u6709LLM\u751f\u6001\u7cfb\u7edf\u521d\u59cb\u5316\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002"}}
{"id": "2507.00454", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00454", "abs": "https://arxiv.org/abs/2507.00454", "authors": ["Yihao Zhen", "Qiang Wang", "Yu Qiao", "Liangqiong Qu", "Huijie Fan"], "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales", "comment": null, "summary": "A main challenge of Visual-Language Tracking (VLT) is the misalignment\nbetween visual inputs and language descriptions caused by target movement.\nPrevious trackers have explored many effective feature modification methods to\npreserve more aligned features. However, an important yet unexplored factor\nultimately hinders their capability, which is the inherent differences in the\ntemporal and spatial scale of information between visual and language inputs.\nTo address this issue, we propose a novel visual-language tracker that enhances\nthe effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and\n\\textbf{S}patial scale of different input components, named as\n\\textbf{ATSTrack}. Specifically, we decompose each language description into\nphrases with different attributes based on their temporal and spatial\ncorrespondence with visual inputs, and modify their features in a fine-grained\nmanner. Moreover, we introduce a Visual-Language token that comprises modified\nlinguistic information from the previous frame to guide the model to extract\nvisual features that are more relevant to language description, thereby\nreducing the impact caused by the differences in spatial scale. Experimental\nresults show that our proposed ATSTrack achieves performance comparable to\nexisting methods. Our code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aATSTrack\u7684\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u5c3a\u5ea6\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u4e2d\uff0c\u89c6\u89c9\u8f93\u5165\u4e0e\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u963b\u788d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u65f6\u95f4\u548c\u7a7a\u95f4\u5c3a\u5ea6\u7684\u5dee\u5f02\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5c06\u8bed\u8a00\u63cf\u8ff0\u5206\u89e3\u4e3a\u5177\u6709\u4e0d\u540c\u5c5e\u6027\u7684\u77ed\u8bed\uff0c\u5e76\u6839\u636e\u5176\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7279\u5f81\u4fee\u6539\uff1b\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u4ee4\u724c\u4ee5\u5229\u7528\u524d\u4e00\u5e27\u7684\u8bed\u8a00\u4fe1\u606f\u6307\u5bfc\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cATSTrack\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u65f6\u95f4\u548c\u7a7a\u95f4\u5c3a\u5ea6\u7684\u5bf9\u9f50\uff0cATSTrack\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u4e2d\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2507.00999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00999", "abs": "https://arxiv.org/abs/2507.00999", "authors": ["Mar\u00eda Grandury", "Javier Aula-Blasco", "J\u00falia Falc\u00e3o", "Cl\u00e9mentine Fourrier", "Miguel Gonz\u00e1lez", "Gonzalo Mart\u00ednez", "Gonzalo Santamar\u00eda", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena G\u00f3mez", "Marta Guerrero", "Guido Ivetta", "Natalia L\u00f3pez", "Flor Miriam Plaza-del-Arco", "Mar\u00eda Teresa Mart\u00edn-Valdivia", "Helena Montoro", "Carmen Mu\u00f1oz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "Mar\u00eda Estrella Vallecillo-Rodr\u00edguez", "Jorge Vallego", "Irune Zubiaga"], "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "AI": {"tldr": "La Leaderboard\u662f\u9996\u4e2a\u5f00\u6e90\u6392\u884c\u699c\uff0c\u7528\u4e8e\u8bc4\u4f30\u897f\u73ed\u7259\u548c\u62c9\u4e01\u7f8e\u6d32\u8bed\u8a00\u53ca\u65b9\u8a00\u7684\u751f\u6210\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u65e8\u5728\u63a8\u52a8\u897f\u73ed\u7259\u8bed\u793e\u533a\u7684LLM\u53d1\u5c55\u3002", "motivation": "\u6fc0\u52b1\u5f00\u53d1\u80fd\u591f\u4ee3\u8868\u897f\u73ed\u7259\u8bed\u793e\u533a\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u7684LLMs\u3002", "method": "\u7ed3\u540866\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u5df4\u65af\u514b\u8bed\u3001\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u3001\u52a0\u5229\u897f\u4e9a\u8bed\u53ca\u897f\u73ed\u7259\u8bed\u65b9\u8a00\uff09\uff0c\u8bc4\u4f3050\u4e2a\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002", "result": "\u5c55\u793a\u4e8650\u4e2a\u6a21\u578b\u7684\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u63d0\u5021\u51cf\u5c11few-shot\u793a\u4f8b\u4ee5\u964d\u4f4e\u73af\u5883\u5f71\u54cd\u3002", "conclusion": "La Leaderboard\u4e3a\u897f\u73ed\u7259\u8bed\u793e\u533a\u7684LLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u9f13\u52b1\u5176\u4ed6\u8bed\u8a00\u7684\u793e\u533a\u9a71\u52a8\u6392\u884c\u699c\u53d1\u5c55\u3002"}}
{"id": "2507.00462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00462", "abs": "https://arxiv.org/abs/2507.00462", "authors": ["Jizhou Han", "Chenhao Ding", "SongLin Dong", "Yuhang He", "Xinyuan Gao", "Yihong Gong"], "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation", "comment": null, "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.", "AI": {"tldr": "MS-TTA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7kNN Mean-Shift\u589e\u5f3aCLIP\u7684\u7279\u5f81\u8868\u793a\uff0c\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u9ad8\u7f6e\u4fe1\u5ea6\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u7684\u6f5c\u529b\uff0c\u5bfc\u81f4\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9002\u5e94\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMS-TTA\uff0c\u5229\u7528\u5355\u6b65kNN Mean-Shift\u4f18\u5316\u6240\u6709\u6d4b\u8bd5\u6837\u672c\u7684\u7279\u5f81\u8868\u793a\uff0c\u589e\u5f3a\u7279\u5f81\u7d27\u51d1\u6027\u548c\u7c7b\u522b\u53ef\u5206\u6027\u3002", "result": "\u5728OOD\u548c\u8de8\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMS-TTA\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7a33\u5b9a\u9002\u5e94\u3002", "conclusion": "MS-TTA\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347CLIP\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01001", "abs": "https://arxiv.org/abs/2507.01001", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "AI": {"tldr": "SciArena\u662f\u4e00\u4e2a\u5f00\u653e\u534f\u4f5c\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u79d1\u5b66\u6587\u732e\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u793e\u533a\u6295\u7968\u548c\u96c6\u4f53\u667a\u6167\u9a71\u52a8\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u6587\u732e\u7406\u89e3\u4e0e\u5408\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u793e\u533a\u53c2\u4e0e\uff0cSciArena\u65e8\u5728\u901a\u8fc7\u793e\u533a\u6295\u7968\u548c\u96c6\u4f53\u667a\u6167\u63d0\u4f9b\u66f4\u5f00\u653e\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cChatbot Arena\u7684\u793e\u533a\u6295\u7968\u65b9\u6cd5\uff0c\u652f\u630123\u4e2a\u5f00\u6e90\u548c\u4e13\u6709\u57fa\u7840\u6a21\u578b\uff0c\u6536\u96c6\u4e8613,000\u591a\u4efd\u7814\u7a76\u8005\u6295\u7968\u3002", "result": "\u6570\u636e\u663e\u793a\u95ee\u9898\u591a\u6837\u4e14\u7b26\u5408\u5b9e\u9645\u6587\u732e\u9700\u6c42\uff0c\u7814\u7a76\u8005\u8bc4\u4f30\u5177\u6709\u4e00\u81f4\u6027\u548c\u9ad8\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u3002\u53d1\u5e03\u4e86SciArena-Eval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "conclusion": "SciArena\u4e3a\u79d1\u5b66\u6587\u732e\u4efb\u52a1\u63d0\u4f9b\u4e86\u793e\u533a\u9a71\u52a8\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u4f46\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.00469", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00469", "abs": "https://arxiv.org/abs/2507.00469", "authors": ["Yue Tan", "Xiaoqian Hu", "Hao Xue", "Celso De Melo", "Flora D. Salim"], "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding", "comment": "23 pages, 12 figures, 10 tables", "summary": "Frontier vision-language models (VLMs) have made remarkable improvements in\nvideo understanding tasks. However, real-world videos typically exist as\ncontinuously evolving data streams (e.g., dynamic scenes captured by wearable\nglasses), necessitating models to continually adapt to shifting data\ndistributions and novel scenarios. Considering the prohibitive computational\ncosts of fine-tuning models on new tasks, usually, a small subset of parameters\nis updated while the bulk of the model remains frozen. This poses new\nchallenges to existing continual learning frameworks in the context of large\nmultimodal foundation models, i.e., catastrophic forgetting and update\nconflict. While the foundation models struggle with parameter-efficient\ncontinual learning, the hippocampus in the human brain has evolved highly\nefficient mechanisms for memory formation and consolidation. Inspired by the\nrapid Binding and pattern separation mechanisms in the hippocampus, in this\nwork, we propose Bisecle for video-language continual learning, where a\nmulti-directional supervision module is used to capture more cross-modal\nrelationships and a contrastive prompt learning scheme is designed to isolate\ntask-specific knowledge to facilitate efficient memory storage. Binding and\nseparation processes further strengthen the ability of VLMs to retain complex\nexperiences, enabling robust and efficient continual learning in video\nunderstanding tasks. We perform a thorough evaluation of the proposed Bisecle,\ndemonstrating its ability to mitigate forgetting and enhance cross-task\ngeneralization on several VideoQA benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBisecle\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u76d1\u7763\u6a21\u5757\u548c\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u65b9\u6848\uff0c\u89e3\u51b3\u89c6\u9891\u8bed\u8a00\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u66f4\u65b0\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u662f\u6301\u7eed\u6f14\u5316\u7684\u6570\u636e\u6d41\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u548c\u66f4\u65b0\u51b2\u7a81\u7684\u6311\u6218\u3002", "method": "\u53d7\u6d77\u9a6c\u4f53\u5feb\u901f\u7ed1\u5b9a\u548c\u6a21\u5f0f\u5206\u79bb\u673a\u5236\u542f\u53d1\uff0c\u8bbe\u8ba1\u591a\u65b9\u5411\u76d1\u7763\u6a21\u5757\u548c\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2aVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBisecle\u6709\u6548\u51cf\u8f7b\u9057\u5fd8\u5e76\u63d0\u5347\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Bisecle\u901a\u8fc7\u6a21\u4eff\u6d77\u9a6c\u4f53\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u8bed\u8a00\u6301\u7eed\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.00472", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00472", "abs": "https://arxiv.org/abs/2507.00472", "authors": ["Ying Guo", "Xi Liu", "Cheng Zhen", "Pengfei Yan", "Xiaoming Wei"], "title": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations", "comment": "ICCV 2025. Homepage: https://jinyugy21.github.io/ARIG/", "summary": "Face-to-face communication, as a common human activity, motivates the\nresearch on interactive head generation. A virtual agent can generate motion\nresponses with both listening and speaking capabilities based on the audio or\nmotion signals of the other user and itself. However, previous clip-wise\ngeneration paradigm or explicit listener/speaker generator-switching methods\nhave limitations in future signal acquisition, contextual behavioral\nunderstanding, and switching smoothness, making it challenging to be real-time\nand realistic. In this paper, we propose an autoregressive (AR) based\nframe-wise framework called ARIG to realize the real-time generation with\nbetter interaction realism. To achieve real-time generation, we model motion\nprediction as a non-vector-quantized AR process. Unlike discrete codebook-index\nprediction, we represent motion distribution using diffusion procedure,\nachieving more accurate predictions in continuous space. To improve interaction\nrealism, we emphasize interactive behavior understanding (IBU) and detailed\nconversational state understanding (CSU). In IBU, based on dual-track\ndual-modal signals, we summarize short-range behaviors through\nbidirectional-integrated learning and perform contextual understanding over\nlong ranges. In CSU, we use voice activity signals and context features of IBU\nto understand the various states (interruption, feedback, pause, etc.) that\nexist in actual conversations. These serve as conditions for the final\nprogressive motion prediction. Extensive experiments have verified the\neffectiveness of our model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u7684\u5e27\u7ea7\u6846\u67b6ARIG\uff0c\u7528\u4e8e\u5b9e\u65f6\u751f\u6210\u66f4\u771f\u5b9e\u7684\u4ea4\u4e92\u5f0f\u5934\u90e8\u8fd0\u52a8\u3002", "motivation": "\u9762\u5bf9\u9762\u5bf9\u8bdd\u662f\u5e38\u89c1\u7684\u4eba\u7c7b\u6d3b\u52a8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u771f\u5b9e\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528\u975e\u5411\u91cf\u91cf\u5316\u7684\u81ea\u56de\u5f52\u8fc7\u7a0b\u5efa\u6a21\u8fd0\u52a8\u9884\u6d4b\uff0c\u7ed3\u5408\u6269\u6563\u8fc7\u7a0b\u8868\u793a\u8fd0\u52a8\u5206\u5e03\uff0c\u5e76\u5f3a\u8c03\u4ea4\u4e92\u884c\u4e3a\u7406\u89e3\u548c\u5bf9\u8bdd\u72b6\u6001\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "ARIG\u6846\u67b6\u5728\u5b9e\u65f6\u6027\u548c\u4ea4\u4e92\u771f\u5b9e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.00474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00474", "abs": "https://arxiv.org/abs/2507.00474", "authors": ["Yaofei Duan", "Yuhao Huang", "Xin Yang", "Luyi Han", "Xinyu Xie", "Zhiyuan Zhu", "Ping He", "Ka-Hou Chan", "Ligang Cui", "Sio-Kei Im", "Dong Ni", "Tao Tan"], "title": "ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis", "comment": "11 pages, 4 figures, 4 tables. Accepted by conference MICCAI2025", "summary": "Deep learning-based diagnostic models often suffer performance drops due to\ndistribution shifts between training (source) and test (target) domains.\nCollecting and labeling sufficient target domain data for model retraining\nrepresents an optimal solution, yet is limited by time and scarce resources.\nActive learning (AL) offers an efficient approach to reduce annotation costs\nwhile maintaining performance, but struggles to handle the challenge posed by\ndistribution variations across different datasets. In this study, we propose a\nnovel unsupervised Active learning framework for Domain Adaptation, named\nADAptation, which efficiently selects informative samples from multi-domain\ndata pools under limited annotation budget. As a fundamental step, our method\nfirst utilizes the distribution homogenization capabilities of diffusion models\nto bridge cross-dataset gaps by translating target images into source-domain\nstyle. We then introduce two key innovations: (a) a hypersphere-constrained\ncontrastive learning network for compact feature clustering, and (b) a\ndual-scoring mechanism that quantifies and balances sample uncertainty and\nrepresentativeness. Extensive experiments on four breast ultrasound datasets\n(three public and one in-house/multi-center) across five common deep\nclassifiers demonstrate that our method surpasses existing strong AL-based\ncompetitors, validating its effectiveness and generalization for clinical\ndomain adaptation. The code is available at the anonymized link:\nhttps://github.com/miccai25-966/ADAptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADAptation\u7684\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u53cc\u8bc4\u5206\u673a\u5236\u63d0\u5347\u6837\u672c\u9009\u62e9\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u57df\u5206\u5e03\u504f\u79fb\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5206\u5e03\u5bf9\u9f50\uff0c\u7ed3\u5408\u8d85\u7403\u7ea6\u675f\u5bf9\u6bd4\u5b66\u4e60\u7f51\u7edc\u548c\u53cc\u8bc4\u5206\u673a\u5236\uff08\u4e0d\u786e\u5b9a\u6027\u548c\u4ee3\u8868\u6027\uff09\u9009\u62e9\u6837\u672c\u3002", "result": "\u5728\u56db\u4e2a\u4e73\u817a\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "ADAptation\u6846\u67b6\u5728\u4e34\u5e8a\u9886\u57df\u9002\u5e94\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.00490", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00490", "abs": "https://arxiv.org/abs/2507.00490", "authors": ["Zijian Chen", "Yuan Tian", "Yuze Sun", "Wei Sun", "Zicheng Zhang", "Weisi Lin", "Guangtao Zhai", "Wenjun Zhang"], "title": "Just Noticeable Difference for Large Multimodal Models", "comment": "19 pages, 19 figures", "summary": "Just noticeable difference (JND), the minimum change that the human visual\nsystem (HVS) can perceive, has been studied for decades. Although recent work\nhas extended this line of research into machine vision, there has been a\nscarcity of studies systematically exploring its perceptual boundaries across\nmultiple tasks and stimulus types, particularly in the current era of rapidly\nadvancing large multimodal models (LMMs), where studying the multifaceted\ncapabilities of models has become a mainstream focus. Moreover, the perceptual\ndefects of LMMs are not investigated thoroughly, resulting in potential\nsecurity issues and suboptimal response efficiency. In this paper, we take an\ninitial attempt and demonstrate that there exist significant visual blind spots\nin current LMMs. To systemically quantify this characteristic, we propose a new\nconcept, {\\bf LMM-JND}, together with its determination pipeline. Targeting\nuncovering the behavior commonalities in HVS-aligned visual perception tasks,\nwe delve into several LMM families and construct a large-scale dataset, named\nVPA-JND, which contains 21.5k reference images with over 489k stimuli across 12\ndistortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where\nstate-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle\nwith basic comparison queries and fall significantly short of human-level\nvisual performance. We further explore the effects of vision and language\nbackbones and find a notable correlation between their design philosophy that\nmay instruct the future refinement of LMMs for their visual acuity. Together,\nour research underscores the significance of LMM-JND as a unique perspective\nfor studying LMMs, and predictable LMM-JND is crucial for security concerns.\nThis work will be available at https://github.com/zijianchen98/LMM-JND.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LMM-JND\u6982\u5ff5\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u89c6\u89c9\u76f2\u533a\uff0c\u5e76\u6784\u5efa\u4e86VPA-JND\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLMMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76LMMs\u5728\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u7f3a\u9677\uff0c\u4ee5\u89e3\u51b3\u6f5c\u5728\u7684\u5b89\u5168\u95ee\u9898\u548c\u54cd\u5e94\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLMM-JND\u6982\u5ff5\u53ca\u5176\u786e\u5b9a\u6d41\u7a0b\uff0c\u6784\u5efa\u5305\u542b21.5k\u53c2\u8003\u56fe\u50cf\u548c489k\u523a\u6fc0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6VPA-JND\uff0c\u5206\u6790\u591a\u79cdLMM\u5bb6\u65cf\u7684\u89c6\u89c9\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u5148\u8fdbLMMs\uff08\u5982GPT-4o\u548cInternVL2.5\u7cfb\u5217\uff09\u5728\u57fa\u672c\u89c6\u89c9\u6bd4\u8f83\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "LMM-JND\u4e3a\u7814\u7a76LMMs\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5176\u53ef\u9884\u6d4b\u6027\u5bf9\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.00493", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00493", "abs": "https://arxiv.org/abs/2507.00493", "authors": ["Fenil R. Doshi", "Thomas Fel", "Talia Konkle", "George Alvarez"], "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models", "comment": "Project page: https://www.fenildoshi.com/configural-shape/", "summary": "Humans are able to recognize objects based on both local texture cues and the\nconfiguration of object parts, yet contemporary vision models primarily harvest\nlocal texture cues, yielding brittle, non-compositional features. Work on\nshape-vs-texture bias has pitted shape and texture representations in\nopposition, measuring shape relative to texture, ignoring the possibility that\nmodels (and humans) can simultaneously rely on both types of cues, and\nobscuring the absolute quality of both types of representation. We therefore\nrecast shape evaluation as a matter of absolute configural competence,\noperationalized by the Configural Shape Score (CSS), which (i) measures the\nability to recognize both images in Object-Anagram pairs that preserve local\ntexture while permuting global part arrangement to depict different object\ncategories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)\nuncovers a broad spectrum of configural sensitivity with fully self-supervised\nand language-aligned transformers -- exemplified by DINOv2, SigLIP2 and\nEVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes\nreveal that (iii) high-CSS networks depend on long-range interactions:\nradius-controlled attention masks abolish performance showing a distinctive\nU-shaped integration profile, and representational-similarity analyses expose a\nmid-depth transition from local to global coding. A BagNet control remains at\nchance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that\nconfigural shape score also predicts other shape-dependent evals. Overall, we\npropose that the path toward truly robust, generalizable, and human-like vision\nsystems may not lie in forcing an artificial choice between shape and texture,\nbut rather in architectural and learning frameworks that seamlessly integrate\nboth local-texture and global configural shape.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u72b6\u8bc4\u4f30\u65b9\u6cd5\uff08CSS\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u5bf9\u5168\u5c40\u914d\u7f6e\u5f62\u72b6\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u9ad8CSS\u6a21\u578b\u4f9d\u8d56\u957f\u7a0b\u4ea4\u4e92\uff0c\u5e76\u5c55\u793a\u4e86\u5f62\u72b6\u548c\u7eb9\u7406\u7684\u6574\u5408\u5bf9\u7a33\u5065\u89c6\u89c9\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u7eb9\u7406\u7ebf\u7d22\uff0c\u5ffd\u7565\u4e86\u5f62\u72b6\u914d\u7f6e\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u7279\u5f81\u8106\u5f31\u4e14\u975e\u7ec4\u5408\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5bf9\u5f62\u72b6\u914d\u7f6e\u7684\u7edd\u5bf9\u80fd\u529b\uff0c\u800c\u975e\u4e0e\u7eb9\u7406\u5bf9\u7acb\u7684\u76f8\u5bf9\u80fd\u529b\u3002", "method": "\u63d0\u51faConfigural Shape Score (CSS)\uff0c\u901a\u8fc7Object-Anagram\u5bf9\u6d4b\u91cf\u6a21\u578b\u5bf9\u5168\u5c40\u5f62\u72b6\u914d\u7f6e\u7684\u654f\u611f\u6027\uff0c\u5e76\u5206\u679086\u79cd\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u9ad8CSS\u6a21\u578b\uff08\u5982DINOv2\u3001SigLIP2\u3001EVA-CLIP\uff09\u4f9d\u8d56\u957f\u7a0b\u4ea4\u4e92\uff0c\u8868\u73b0\u51fa\u5c40\u90e8\u5230\u5168\u5c40\u7f16\u7801\u7684\u8fc7\u6e21\uff0c\u4e14CSS\u80fd\u9884\u6d4b\u5176\u4ed6\u5f62\u72b6\u4f9d\u8d56\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "\u771f\u6b63\u7a33\u5065\u4e14\u7c7b\u4eba\u7684\u89c6\u89c9\u7cfb\u7edf\u9700\u6574\u5408\u5c40\u90e8\u7eb9\u7406\u548c\u5168\u5c40\u5f62\u72b6\u914d\u7f6e\uff0c\u800c\u975e\u5728\u4e8c\u8005\u95f4\u505a\u4eba\u4e3a\u9009\u62e9\u3002"}}
{"id": "2507.00501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00501", "abs": "https://arxiv.org/abs/2507.00501", "authors": ["Yongzhen Wang", "Liangliang Chen", "Bingwen Hu", "Heng Liu", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing", "comment": "12 pages, 11 figures, 6 tables", "summary": "Recent progress in image restoration has underscored Spatial State Models\n(SSMs) as powerful tools for modeling long-range dependencies, owing to their\nappealing linear complexity and computational efficiency. However, SSM-based\napproaches exhibit limitations in reconstructing localized structures and tend\nto be less effective when handling high-dimensional data, frequently resulting\nin suboptimal recovery of fine image features. To tackle these challenges, we\nintroduce Laplace-Mamba, a novel framework that integrates Laplace frequency\nprior with a hybrid Mamba-CNN architecture for efficient image dehazing.\nLeveraging the Laplace decomposition, the image is disentangled into\nlow-frequency components capturing global texture and high-frequency components\nrepresenting edges and fine details. This decomposition enables specialized\nprocessing via dual parallel pathways: the low-frequency branch employs SSMs\nfor global context modeling, while the high-frequency branch utilizes CNNs to\nrefine local structural details, effectively addressing diverse haze scenarios.\nNotably, the Laplace transformation facilitates information-preserving\ndownsampling of low-frequency components in accordance with the Nyquist theory,\nthereby significantly improving computational efficiency. Extensive evaluations\nacross multiple benchmarks demonstrate that our method outperforms\nstate-of-the-art approaches in both restoration quality and efficiency. The\nsource code and pretrained models are available at\nhttps://github.com/yz-wang/Laplace-Mamba.", "AI": {"tldr": "Laplace-Mamba\u6846\u67b6\u7ed3\u5408Laplace\u9891\u7387\u5148\u9a8c\u4e0e\u6df7\u5408Mamba-CNN\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u5e76\u884c\u8def\u5f84\u5206\u522b\u5904\u7406\u9ad8\u4f4e\u9891\u6210\u5206\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u53bb\u96fe\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eSSM\u7684\u65b9\u6cd5\u5728\u91cd\u5efa\u5c40\u90e8\u7ed3\u6784\u548c\u9ad8\u7ef4\u6570\u636e\u5904\u7406\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7279\u5f81\u6062\u590d\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51faLaplace-Mamba\u6846\u67b6\uff0c\u5229\u7528Laplace\u5206\u89e3\u5c06\u56fe\u50cf\u5206\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u6210\u5206\uff0c\u5206\u522b\u901a\u8fc7SSM\u548cCNN\u5904\u7406\uff0c\u7ed3\u5408Nyquist\u7406\u8bba\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLaplace-Mamba\u5728\u6062\u590d\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Laplace-Mamba\u901a\u8fc7\u7ed3\u5408Laplace\u5206\u89e3\u4e0e\u6df7\u5408\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u53bb\u96fe\u4e2d\u7684\u5c40\u90e8\u7ed3\u6784\u548c\u9ad8\u7ef4\u6570\u636e\u95ee\u9898\u3002"}}
{"id": "2507.00525", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00525", "abs": "https://arxiv.org/abs/2507.00525", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "comment": null, "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "AI": {"tldr": "Box-QAymo\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u7528\u6237\u6307\u5b9a\u5bf9\u8c61\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u6355\u6349\u7528\u6237\u610f\u56fe\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5168\u573a\u666f\u63cf\u8ff0\u6216\u8def\u5f84\u9884\u6d4b\uff0c\u65e0\u6cd5\u8bc4\u4f30VLMs\u5bf9\u5c40\u90e8\u7528\u6237\u9a71\u52a8\u67e5\u8be2\u7684\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51faBox-QAymo\u6570\u636e\u96c6\uff0c\u7528\u6237\u901a\u8fc7\u7ed8\u5236\u8fb9\u754c\u6846\u8868\u8fbe\u610f\u56fe\uff0c\u652f\u6301\u5c42\u6b21\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u62ec\u5c5e\u6027\u9884\u6d4b\u3001\u8fd0\u52a8\u7406\u89e3\u548c\u65f6\u7a7a\u63a8\u7406\u3002", "result": "\u5f53\u524dVLMs\u5728\u611f\u77e5\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u663e\u8457\u53d7\u9650\uff0c\u63ed\u793a\u4e86\u4e0e\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u652f\u6301\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u7528\u6237\u6c9f\u901a\u3002"}}
{"id": "2507.00502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00502", "abs": "https://arxiv.org/abs/2507.00502", "authors": ["JianChao Zhao", "Songlin Dong"], "title": "ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation", "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to enable models to adapt\non-the-fly to a stream of unlabeled data under evolving distribution shifts.\nHowever, existing CTTA methods typically rely on shared model parameters across\nall domains, making them vulnerable to feature entanglement and catastrophic\nforgetting in the presence of large or non-stationary domain shifts. To address\nthis limitation, we propose \\textbf{ExPaMoE}, a novel framework based on an\n\\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples\ndomain-general and domain-specific knowledge via a dual-branch expert design\nwith token-guided feature separation, and dynamically expands its expert pool\nbased on a \\emph{Spectral-Aware Online Domain Discriminator} (SODD) that\ndetects distribution changes in real-time using frequency-domain cues.\nExtensive experiments demonstrate the superiority of ExPaMoE across diverse\nCTTA scenarios. We evaluate our method on standard benchmarks including\nCIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic\nsegmentation. Additionally, we introduce \\textbf{ImageNet++}, a large-scale and\nrealistic CTTA benchmark built from multiple ImageNet-derived datasets, to\nbetter reflect long-term adaptation under complex domain evolution. ExPaMoE\nconsistently outperforms prior arts, showing strong robustness, scalability,\nand resistance to forgetting.", "AI": {"tldr": "ExPaMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u6269\u5c55\u5e76\u884c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6301\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709CTTA\u65b9\u6cd5\u4f9d\u8d56\u5171\u4eab\u6a21\u578b\u53c2\u6570\uff0c\u6613\u53d7\u5927\u6216\u975e\u5e73\u7a33\u57df\u504f\u79fb\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u7279\u5f81\u7ea0\u7f20\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "ExPaMoE\u901a\u8fc7\u53cc\u5206\u652f\u4e13\u5bb6\u8bbe\u8ba1\u548c\u57fa\u4e8e\u9891\u7387\u57df\u7ebf\u7d22\u7684\u5b9e\u65f6\u5206\u5e03\u53d8\u5316\u68c0\u6d4b\u5668\uff08SODD\uff09\uff0c\u52a8\u6001\u6269\u5c55\u4e13\u5bb6\u6c60\uff0c\u89e3\u8026\u9886\u57df\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cExPaMoE\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6297\u9057\u5fd8\u80fd\u529b\u3002", "conclusion": "ExPaMoE\u4e3aCTTA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u57df\u6f14\u5316\u7684\u957f\u671f\u9002\u5e94\u3002"}}
{"id": "2507.00537", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00537", "abs": "https://arxiv.org/abs/2507.00537", "authors": ["Feng Lin", "Marco Chen", "Haokui Zhang", "Xiaotian Yu", "Guangming Lu", "Rong Xiao"], "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation", "comment": "21 pages, 7 figures", "summary": "This paper studies the role of attention heads in CLIP's image encoder. While\nCLIP has exhibited robust performance across diverse applications, we\nhypothesize that certain attention heads negatively affect final\nrepresentations and that ablating them can improve performance in downstream\ntasks. To capitalize on this insight, we propose a simple yet effective method,\ncalled Attention Ablation Technique (AAT), to suppress the contribution of\nspecific heads by manipulating attention weights. By integrating two\nalternative strategies tailored for different application scenarios, AAT\nsystematically identifies and ablates detrimental attention heads to enhance\nrepresentation quality. Experiments demonstrate that AAT consistently improves\ndownstream task performance across various domains, boosting recall rate by up\nto 11.1% on CLIP-family models for cross-modal retrieval. The results highlight\nthe potential of AAT to effectively refine large-scale vision-language models\nwith virtually no increase in inference cost.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86CLIP\u56fe\u50cf\u7f16\u7801\u5668\u4e2d\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6ce8\u610f\u529b\u6d88\u878d\u6280\u672f\uff08AAT\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6291\u5236\u7279\u5b9a\u5934\u7684\u8d21\u732e\u6765\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "CLIP\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u53ef\u80fd\u5bf9\u6700\u7ec8\u8868\u793a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u6d88\u878d\u8fd9\u4e9b\u5934\u53ef\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faAAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u64cd\u7eb5\u6ce8\u610f\u529b\u6743\u91cd\u6291\u5236\u7279\u5b9a\u5934\u7684\u8d21\u732e\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u6574\u5408\u4e24\u79cd\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAAT\u5728\u591a\u4e2a\u9886\u57df\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u8de8\u6a21\u6001\u68c0\u7d22\u53ec\u56de\u7387\u6700\u9ad8\u63d0\u534711.1%\u3002", "conclusion": "AAT\u80fd\u6709\u6548\u4f18\u5316\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e14\u51e0\u4e4e\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2507.00505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00505", "abs": "https://arxiv.org/abs/2507.00505", "authors": ["Haoran Lou", "Chunxiao Fan", "Ziyan Liu", "Yuexin Wu", "Xinxiang Wang"], "title": "LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs", "comment": "ICCV", "summary": "The architecture of multimodal large language models (MLLMs) commonly\nconnects a vision encoder, often based on CLIP-ViT, to a large language model.\nWhile CLIP-ViT works well for capturing global image features, it struggles to\nmodel local relationships between adjacent patches, leading to weaker visual\nrepresentation, which in turn affects the detailed understanding ability of\nMLLMs. To solve this, we propose LLaVA-SP, which \\textbf{ only adds six spatial\nvisual tokens} to the original visual tokens to enhance the visual\nrepresentation. Our approach offers three key advantages: 1)We propose a novel\nProjector, which uses convolutional kernels to derive visual spatial tokens\nfrom ViT patch features, simulating two visual spatial ordering approaches:\n``from central region to global\" and ``from abstract to specific\". Then, a\ncross-attention mechanism is applied to fuse fine-grained visual information,\nenriching the overall visual representation. 2) We present two model variants:\nLLaVA-SP-Cropping, which focuses on detail features through progressive\ncropping, and LLaVA-SP-Pooling, which captures global semantics through\nadaptive pooling, enabling the model to handle diverse visual understanding\ntasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,\nachieves significant performance improvements across various multimodal\nbenchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple\ntasks with nearly identical inference latency. The code and models are\navailable at\n\\href{https://github.com/CnFaker/LLaVA-SP}{\\texttt{https://github.com/CnFaker/LLaVA-SP}}.", "AI": {"tldr": "LLaVA-SP\u901a\u8fc7\u6dfb\u52a0\u516d\u4e2a\u7a7a\u95f4\u89c6\u89c9\u6807\u8bb0\u589e\u5f3a\u89c6\u89c9\u8868\u793a\uff0c\u63d0\u51fa\u65b0\u578b\u6295\u5f71\u5668\u548c\u4e24\u79cd\u53d8\u4f53\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "CLIP-ViT\u5728\u6355\u6349\u5c40\u90e8\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cdMLLMs\u7684\u7ec6\u8282\u7406\u89e3\u80fd\u529b\uff0c\u9700\u6539\u8fdb\u89c6\u89c9\u8868\u793a\u3002", "method": "\u63d0\u51faLLaVA-SP\uff0c\u6dfb\u52a0\u7a7a\u95f4\u89c6\u89c9\u6807\u8bb0\uff0c\u4f7f\u7528\u5377\u79ef\u6838\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f00\u53d1\u4e24\u79cd\u53d8\u4f53\uff08Cropping\u548cPooling\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLaVA-SP\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eLLaVA-1.5\uff0c\u63a8\u7406\u5ef6\u8fdf\u51e0\u4e4e\u76f8\u540c\u3002", "conclusion": "LLaVA-SP\u901a\u8fc7\u7b80\u5355\u6539\u8fdb\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8868\u793a\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.00506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00506", "abs": "https://arxiv.org/abs/2507.00506", "authors": ["Yunfei Xie", "Yuxuan Cheng", "Juncheng Wu", "Haoyu Zhang", "Yuyin Zhou", "Shoudong Han"], "title": "SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning", "comment": null, "summary": "Recent advancements in adapting vision-language pre-training models like CLIP\nfor person re-identification (ReID) tasks often rely on complex adapter design\nor modality-specific tuning while neglecting cross-modal interaction, leading\nto high computational costs or suboptimal alignment. To address these\nlimitations, we propose a simple yet effective framework named Selective\nCross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and\nrobustness against real-world perturbations. Our method introduces two key\ninnovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a\nlightweight module that dynamically injects discriminative visual features into\ntext prompts via a cross-modal gating mechanism. Moreover, the proposed\nPerturbation-Driven Consistency Alignment (PDCA) is a dual-path training\nstrategy that enforces invariant feature alignment under random image\nperturbations by regularizing consistency between original and augmented\ncross-modal embeddings. Extensive experiments are conducted on several popular\nbenchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,\nand P-DukeMTMC, which demonstrate the impressive performance of the proposed\nmethod. Notably, our framework eliminates heavy adapters while maintaining\nefficient inference, achieving an optimal trade-off between performance and\ncomputational overhead. The code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCING\u7684\u7b80\u5355\u6709\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u89c6\u89c9\u63d0\u793a\u878d\u5408\u548c\u6270\u52a8\u9a71\u52a8\u4e00\u81f4\u6027\u5bf9\u9f50\uff0c\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\uff0c\u907f\u514d\u4e86\u590d\u6742\u9002\u914d\u5668\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u9002\u914d\u5668\u6216\u6a21\u6001\u7279\u5b9a\u8c03\u4f18\uff0c\u5ffd\u7565\u4e86\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u6216\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u89c6\u89c9\u63d0\u793a\u878d\u5408\uff08SVIP\uff09\u548c\u6270\u52a8\u9a71\u52a8\u4e00\u81f4\u6027\u5bf9\u9f50\uff08PDCA\uff09\uff0c\u52a8\u6001\u6ce8\u5165\u89c6\u89c9\u7279\u5f81\u5e76\u589e\u5f3a\u7279\u5f81\u5bf9\u9f50\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u5f00\u9500\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "SCING\u6846\u67b6\u7b80\u5316\u4e86\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u6548\u679c\uff0c\u5177\u6709\u9ad8\u6548\u63a8\u7406\u4f18\u52bf\u3002"}}
{"id": "2507.00583", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00583", "abs": "https://arxiv.org/abs/2507.00583", "authors": ["Christian Intern\u00f2", "Robert Geirhos", "Markus Olhofer", "Sunny Liu", "Barbara Hammer", "David Klindt"], "title": "AI-Generated Video Detection via Perceptual Straightening", "comment": null, "summary": "The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.", "AI": {"tldr": "ReStraV\u5229\u7528\u795e\u7ecf\u8868\u793a\u51e0\u4f55\u5b66\u533a\u5206AI\u751f\u6210\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\uff0c\u901a\u8fc7\u5206\u6790\u65f6\u95f4\u66f2\u7387\u548c\u9010\u6b65\u8ddd\u79bb\u5dee\u5f02\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u5408\u6210\u89c6\u9891\u9ad8\u5ea6\u903c\u771f\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u4e14\u96be\u4ee5\u6355\u6349\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u57fa\u4e8e\u201c\u611f\u77e5\u62c9\u76f4\u201d\u5047\u8bbe\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u53d8\u6362\u5668\uff08DINOv2\uff09\u91cf\u5316\u89c6\u9891\u7684\u795e\u7ecf\u8868\u793a\u57df\u4e2d\u7684\u65f6\u95f4\u66f2\u7387\u548c\u9010\u6b65\u8ddd\u79bb\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u3002", "result": "\u5728VidProM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8fbe\u523097.17%\u51c6\u786e\u7387\u548c98.63% AUROC\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReStraV\u4e3aAI\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u795e\u7ecf\u8868\u793a\u51e0\u4f55\u5b66\u7684\u65b0\u5e94\u7528\u3002"}}
{"id": "2507.00898", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00898", "abs": "https://arxiv.org/abs/2507.00898", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "AI": {"tldr": "ONLY\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u67e5\u8be2\u548c\u5355\u5c42\u5e72\u9884\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "LVLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u67e5\u8be2\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faONLY\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5230\u89c6\u89c9\u71b5\u6bd4\u9009\u62e9\u6027\u653e\u5927\u5173\u952e\u6587\u672c\u4fe1\u606f\uff0c\u4ec5\u9700\u5355\u6b21\u67e5\u8be2\u548c\u5355\u5c42\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cONLY\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5b9e\u73b0\u7b80\u5355\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "ONLY\u4e3aLVLM\u7684\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2507.00519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00519", "abs": "https://arxiv.org/abs/2507.00519", "authors": ["Ruize Cui", "Jiaan Zhang", "Jialun Pei", "Kai Wang", "Pheng-Ann Heng", "Jing Qin"], "title": "Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection", "comment": "This paper has been accepted by MICCAI 2025", "summary": "Liver landmarks provide crucial anatomical guidance to the surgeon during\nlaparoscopic liver surgery to minimize surgical risk. However, the tubular\nstructural properties of landmarks and dynamic intraoperative deformations pose\nsignificant challenges for automatic landmark detection. In this study, we\nintroduce TopoNet, a novel topology-constrained learning framework for\nlaparoscopic liver landmark detection. Our framework adopts a snake-CNN\ndual-path encoder to simultaneously capture detailed RGB texture information\nand depth-informed topological structures. Meanwhile, we propose a\nboundary-aware topology fusion (BTF) module, which adaptively merges RGB-D\nfeatures to enhance edge perception while preserving global topology.\nAdditionally, a topological constraint loss function is embedded, which\ncontains a center-line constraint loss and a topological persistence loss to\nensure homotopy equivalence between predictions and labels. Extensive\nexperiments on L3D and P2ILF datasets demonstrate that TopoNet achieves\noutstanding accuracy and computational complexity, highlighting the potential\nfor clinical applications in laparoscopic liver surgery. Our code will be\navailable at https://github.com/cuiruize/TopoNet.", "AI": {"tldr": "TopoNet\u662f\u4e00\u79cd\u7528\u4e8e\u8179\u8154\u955c\u809d\u810f\u6807\u5fd7\u7269\u68c0\u6d4b\u7684\u65b0\u578b\u62d3\u6251\u7ea6\u675f\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RGB-D\u7279\u5f81\u548c\u62d3\u6251\u7ea6\u675f\u635f\u5931\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u809d\u810f\u6807\u5fd7\u7269\u5728\u8179\u8154\u955c\u624b\u672f\u4e2d\u4e3a\u5916\u79d1\u533b\u751f\u63d0\u4f9b\u5173\u952e\u89e3\u5256\u5f15\u5bfc\uff0c\u4f46\u6807\u5fd7\u7269\u7684\u7ba1\u72b6\u7ed3\u6784\u548c\u672f\u4e2d\u52a8\u6001\u53d8\u5f62\u589e\u52a0\u4e86\u81ea\u52a8\u68c0\u6d4b\u7684\u96be\u5ea6\u3002", "method": "\u91c7\u7528snake-CNN\u53cc\u8def\u5f84\u7f16\u7801\u5668\u6355\u6349RGB\u7eb9\u7406\u548c\u6df1\u5ea6\u62d3\u6251\u7ed3\u6784\uff0c\u63d0\u51fa\u8fb9\u754c\u611f\u77e5\u62d3\u6251\u878d\u5408\u6a21\u5757\uff08BTF\uff09\u548c\u62d3\u6251\u7ea6\u675f\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728L3D\u548cP2ILF\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTopoNet\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TopoNet\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.00709", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00709", "abs": "https://arxiv.org/abs/2507.00709", "authors": ["Yiming Yang", "Yueru Luo", "Bingkun He", "Hongbin Lin", "Suzhong Fu", "Chao Yan", "Kun Tang", "Xinrui Yan", "Chao Zheng", "Shuguang Cui", "Zhen Li"], "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving", "comment": null, "summary": "Lane segment topology reasoning constructs a comprehensive road network by\ncapturing the topological relationships between lane segments and their\nsemantic types. This enables end-to-end autonomous driving systems to perform\nroad-dependent maneuvers such as turning and lane changing. However, the\nlimitations in consistent positional embedding and temporal multiple attribute\nlearning in existing methods hinder accurate roadnet reconstruction. To address\nthese issues, we propose TopoStreamer, an end-to-end temporal perception model\nfor lane segment topology reasoning. Specifically, TopoStreamer introduces\nthree key improvements: streaming attribute constraints, dynamic lane boundary\npositional encoding, and lane segment denoising. The streaming attribute\nconstraints enforce temporal consistency in both centerline and boundary\ncoordinates, along with their classifications. Meanwhile, dynamic lane boundary\npositional encoding enhances the learning of up-to-date positional information\nwithin queries, while lane segment denoising helps capture diverse lane segment\npatterns, ultimately improving model performance. Additionally, we assess the\naccuracy of existing models using a lane boundary classification metric, which\nserves as a crucial measure for lane-changing scenarios in autonomous driving.\nOn the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements\nover state-of-the-art methods, achieving substantial performance gains of +3.4%\nmAP in lane segment perception and +2.1% OLS in centerline perception tasks.", "AI": {"tldr": "TopoStreamer\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u65f6\u5e8f\u611f\u77e5\u6a21\u578b\uff0c\u7528\u4e8e\u8f66\u9053\u6bb5\u62d3\u6251\u63a8\u7406\uff0c\u901a\u8fc7\u6d41\u5f0f\u5c5e\u6027\u7ea6\u675f\u3001\u52a8\u6001\u8f66\u9053\u8fb9\u754c\u4f4d\u7f6e\u7f16\u7801\u548c\u8f66\u9053\u6bb5\u53bb\u566a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f4d\u7f6e\u5d4c\u5165\u548c\u65f6\u5e8f\u591a\u5c5e\u6027\u5b66\u4e60\u4e0a\u7684\u5c40\u9650\u6027\u963b\u788d\u4e86\u9053\u8def\u7f51\u7edc\u91cd\u5efa\u7684\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "TopoStreamer\u5f15\u5165\u4e86\u6d41\u5f0f\u5c5e\u6027\u7ea6\u675f\u3001\u52a8\u6001\u8f66\u9053\u8fb9\u754c\u4f4d\u7f6e\u7f16\u7801\u548c\u8f66\u9053\u6bb5\u53bb\u566a\u4e09\u9879\u5173\u952e\u6280\u672f\u3002", "result": "\u5728OpenLane-V2\u6570\u636e\u96c6\u4e0a\uff0cTopoStreamer\u5728\u8f66\u9053\u6bb5\u611f\u77e5\u548c\u4e2d\u5fc3\u7ebf\u611f\u77e5\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u5347\u4e863.4% mAP\u548c2.1% OLS\u3002", "conclusion": "TopoStreamer\u901a\u8fc7\u6539\u8fdb\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u4f4d\u7f6e\u4fe1\u606f\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u9053\u6bb5\u62d3\u6251\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.00554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00554", "abs": "https://arxiv.org/abs/2507.00554", "authors": ["Zhenya Yang", "Bingchen Gong", "Kai Chen", "Qi Dou"], "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing", "comment": null, "summary": "Despite the advancements in quality and efficiency achieved by 3D Gaussian\nSplatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent\nchallenge. Existing approaches primarily rely on low-pass filtering to mitigate\naliasing. However, these methods are not sensitive to the sampling rate, often\nresulting in under-filtering and over-smoothing renderings. To address this\nlimitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework\nfor Gaussian Splatting, which dynamically predicts the optimal filtering\nstrength for each 3D Gaussian primitive. Specifically, we introduce a set of\nbasis functions to each Gaussian, which take the sampling rate as input to\nmodel appearance variations, enabling sampling-rate-sensitive filtering. These\nbasis function parameters are jointly optimized with the 3D Gaussian in an\nend-to-end manner. The sampling rate is influenced by both focal length and\ncamera distance. However, existing methods and datasets rely solely on\ndown-sampling to simulate focal length changes for anti-aliasing evaluation,\noverlooking the impact of camera distance. To enable a more comprehensive\nassessment, we introduce a new synthetic dataset featuring objects rendered at\nvarying camera distances. Extensive experiments on both public datasets and our\nnewly collected dataset demonstrate that our method achieves SOTA rendering\nquality while effectively eliminating aliasing. The code and dataset have been\nopen-sourced.", "AI": {"tldr": "LOD-GS\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9884\u6d4b3D\u9ad8\u65af\u539f\u8bed\u6700\u4f18\u6ee4\u6ce2\u5f3a\u5ea6\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e32\u67d3\u4e2d\u7684\u952f\u9f7f\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u91c7\u6837\u7387\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u6e32\u67d3\u8d28\u91cf\u4e0d\u8db3\u6216\u8fc7\u5ea6\u5e73\u6ed1\uff0cLOD-GS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e00\u7ec4\u57fa\u51fd\u6570\uff0c\u4ee5\u91c7\u6837\u7387\u4e3a\u8f93\u5165\u5efa\u6a21\u5916\u89c2\u53d8\u5316\uff0c\u5b9e\u73b0\u91c7\u6837\u7387\u654f\u611f\u6ee4\u6ce2\uff0c\u5e76\u4e0e3D\u9ad8\u65af\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u65b0\u6570\u636e\u96c6\u4e0a\uff0cLOD-GS\u5b9e\u73b0\u4e86SOTA\u6e32\u67d3\u8d28\u91cf\uff0c\u6709\u6548\u6d88\u9664\u952f\u9f7f\u3002", "conclusion": "LOD-GS\u901a\u8fc7\u52a8\u6001\u6ee4\u6ce2\u548c\u5168\u9762\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6e32\u67d3\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.00724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00724", "abs": "https://arxiv.org/abs/2507.00724", "authors": ["Linghui Zhu", "Yiming Li", "Haiqin Weng", "Yan Liu", "Tianwei Zhang", "Shu-Tao Xia", "Zhi Wang"], "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features", "comment": null, "summary": "Large vision models achieve remarkable performance in various downstream\ntasks, primarily by personalizing pre-trained models through fine-tuning with\nprivate and valuable local data, which makes the personalized model a valuable\nintellectual property for its owner. Similar to the era of traditional DNNs,\nmodel stealing attacks also pose significant risks to these personalized\nmodels. However, in this paper, we reveal that most existing defense methods\n(developed for traditional DNNs), typically designed for models trained from\nscratch, either introduce additional security risks, are prone to misjudgment,\nor are even ineffective for fine-tuned models. To alleviate these problems,\nthis paper proposes a harmless model ownership verification method for\npersonalized models by decoupling similar common features. In general, our\nmethod consists of three main stages. In the first stage, we create shadow\nmodels that retain common features of the victim model while disrupting\ndataset-specific features. We represent the dataset-specific features of the\nvictim model by the output differences between the shadow and victim models.\nAfter that, a meta-classifier is trained to identify stolen models by\ndetermining whether suspicious models contain the dataset-specific features of\nthe victim. In the third stage, we conduct model ownership verification by\nhypothesis test to mitigate randomness and enhance robustness. Extensive\nexperiments on benchmark datasets verify the effectiveness of the proposed\nmethod in detecting different types of model stealing simultaneously.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e2a\u6027\u5316\u6a21\u578b\u7684\u65e0\u5bb3\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u76f8\u4f3c\u7279\u5f81\u6765\u9632\u5fa1\u6a21\u578b\u7a83\u53d6\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u5fae\u8c03\u6a21\u578b\u6548\u679c\u4e0d\u4f73\u6216\u5f15\u5165\u989d\u5916\u98ce\u9669\uff0c\u9700\u5f00\u53d1\u66f4\u5b89\u5168\u6709\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a\u521b\u5efa\u4fdd\u7559\u5171\u540c\u7279\u5f81\u7684\u5f71\u5b50\u6a21\u578b\uff0c\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u8bc6\u522b\u7a83\u53d6\u6a21\u578b\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u9a8c\u8bc1\u6240\u6709\u6743\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u591a\u79cd\u6a21\u578b\u7a83\u53d6\u7c7b\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e2a\u6027\u5316\u6a21\u578b\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u9ad8\u6548\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6848\u3002"}}
{"id": "2507.00566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00566", "abs": "https://arxiv.org/abs/2507.00566", "authors": ["Kai Zhou", "Shuhai Zhang", "Zeng You", "Jinwu Hu", "Mingkui Tan", "Fei Liu"], "title": "Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment", "comment": "This paper is accepted by IEEE TIP 2025. Code is publicly available\n  at https://github.com/kaai520/PGFA", "summary": "Zero-shot skeleton-based action recognition aims to classify unseen\nskeleton-based human actions without prior exposure to such categories during\ntraining. This task is extremely challenging due to the difficulty in\ngeneralizing from known to unknown actions. Previous studies typically use\ntwo-stage training: pre-training skeleton encoders on seen action categories\nusing cross-entropy loss and then aligning pre-extracted skeleton and text\nfeatures, enabling knowledge transfer to unseen classes through skeleton-text\nalignment and language models' generalization. However, their efficacy is\nhindered by 1) insufficient discrimination for skeleton features, as the fixed\nskeleton encoder fails to capture necessary alignment information for effective\nskeleton-text alignment; 2) the neglect of alignment bias between skeleton and\nunseen text features during testing. To this end, we propose a prototype-guided\nfeature alignment paradigm for zero-shot skeleton-based action recognition,\ntermed PGFA. Specifically, we develop an end-to-end cross-modal contrastive\ntraining framework to improve skeleton-text alignment, ensuring sufficient\ndiscrimination for skeleton features. Additionally, we introduce a\nprototype-guided text feature alignment strategy to mitigate the adverse impact\nof the distribution discrepancy during testing. We provide a theoretical\nanalysis to support our prototype-guided text feature alignment strategy and\nempirically evaluate our overall PGFA on three well-known datasets. Compared\nwith the top competitor SMIE method, our PGFA achieves absolute accuracy\nimprovements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD\ndatasets, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u578b\u5f15\u5bfc\u7684\u7279\u5f81\u5bf9\u9f50\u8303\u5f0f\uff08PGFA\uff09\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8de8\u6a21\u6001\u5bf9\u6bd4\u8bad\u7ec3\u6846\u67b6\u548c\u539f\u578b\u5f15\u5bfc\u7684\u6587\u672c\u7279\u5f81\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u9aa8\u67b6\u7279\u5f81\u5224\u522b\u6027\u548c\u9aa8\u67b6-\u6587\u672c\u5bf9\u9f50\u504f\u5dee\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faPGFA\u65b9\u6cd5\uff0c\u5305\u62ec\u7aef\u5230\u7aef\u8de8\u6a21\u6001\u5bf9\u6bd4\u8bad\u7ec3\u6846\u67b6\u548c\u539f\u578b\u5f15\u5bfc\u7684\u6587\u672c\u7279\u5f81\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u5728NTU-60\u3001NTU-120\u548cPKU-MMD\u6570\u636e\u96c6\u4e0a\uff0cPGFA\u6bd4SMIE\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u4e8622.96%\u300112.53%\u548c18.54%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "PGFA\u901a\u8fc7\u6539\u8fdb\u9aa8\u67b6\u7279\u5f81\u5224\u522b\u6027\u548c\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.00570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00570", "abs": "https://arxiv.org/abs/2507.00570", "authors": ["Zizhao Li", "Xueyang Kang", "Joseph West", "Kourosh Khoshelham"], "title": "Out-of-distribution detection in 3D applications: a review", "comment": null, "summary": "The ability to detect objects that are not prevalent in the training set is a\ncritical capability in many 3D applications, including autonomous driving.\nMachine learning methods for object recognition often assume that all object\ncategories encountered during inference belong to a closed set of classes\npresent in the training data. This assumption limits generalization to the real\nworld, as objects not seen during training may be misclassified or entirely\nignored. As part of reliable AI, OOD detection identifies inputs that deviate\nsignificantly from the training distribution. This paper provides a\ncomprehensive overview of OOD detection within the broader scope of trustworthy\nand uncertain AI. We begin with key use cases across diverse domains, introduce\nbenchmark datasets spanning multiple modalities, and discuss evaluation\nmetrics. Next, we present a comparative analysis of OOD detection\nmethodologies, exploring model structures, uncertainty indicators, and\ndistributional distance taxonomies, alongside uncertainty calibration\ntechniques. Finally, we highlight promising research directions, including\nadversarially robust OOD detection and failure identification, particularly\nrelevant to 3D applications. The paper offers both theoretical and practical\ninsights into OOD detection, showcasing emerging research opportunities such as\n3D vision integration. These insights help new researchers navigate the field\nmore effectively, contributing to the development of reliable, safe, and robust\nAI systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86OOD\u68c0\u6d4b\u5728\u53ef\u4fe1\u8d56AI\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u6db5\u76d6\u7528\u4f8b\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u3001\u65b9\u6cd5\u6bd4\u8f83\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u7684\u5bf9\u8c61\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5206\u6790\u6a21\u578b\u7ed3\u6784\u3001\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u548c\u5206\u5e03\u8ddd\u79bb\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6280\u672f\u3002", "result": "\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u89c1\u89e3\uff0c\u5c55\u793a\u4e863D\u89c6\u89c9\u96c6\u6210\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "OOD\u68c0\u6d4b\u662f\u5f00\u53d1\u53ef\u9760AI\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u548c\u5931\u8d25\u8bc6\u522b\u3002"}}
{"id": "2507.00790", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00790", "abs": "https://arxiv.org/abs/2507.00790", "authors": ["Huaqiu Li", "Yong Wang", "Tongwen Huang", "Hailang Huang", "Haoqian Wang", "Xiangxiang Chu"], "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling", "comment": null, "summary": "Unified image restoration is a significantly challenging task in low-level\nvision. Existing methods either make tailored designs for specific tasks,\nlimiting their generalizability across various types of degradation, or rely on\ntraining with paired datasets, thereby suffering from closed-set constraints.\nTo address these issues, we propose a novel, dataset-free, and unified approach\nthrough recurrent posterior sampling utilizing a pretrained latent diffusion\nmodel. Our method incorporates the multimodal understanding model to provide\nsematic priors for the generative model under a task-blind condition.\nFurthermore, it utilizes a lightweight module to align the degraded input with\nthe generated preference of the diffusion model, and employs recurrent\nrefinement for posterior sampling. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods, validating its effectiveness and\nrobustness. Our code and data will be available at\nhttps://github.com/AMAP-ML/LD-RPS.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u65e0\u6570\u636e\u96c6\u7edf\u4e00\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5faa\u73af\u540e\u9a8c\u91c7\u6837\u548c\u591a\u6a21\u6001\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u8bed\u4e49\u5148\u9a8c\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u6cdb\u5316\u6027\u5dee\uff0c\u8981\u4e48\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u53d7\u9650\u4e8e\u95ed\u96c6\u7ea6\u675f\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u8bed\u4e49\u5148\u9a8c\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5bf9\u9f50\u9000\u5316\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u5faa\u73af\u7ec6\u5316\u8fdb\u884c\u540e\u9a8c\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u6570\u636e\u96c6\u7edf\u4e00\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00585", "abs": "https://arxiv.org/abs/2507.00585", "authors": ["Tang Hao", "Guo ZhiQing", "Wang LieJun", "Liu Chao"], "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation", "comment": null, "summary": "In recent years, it has been found that \"grandmother cells\" in the primary\nvisual cortex (V1) of macaques can directly recognize visual input with complex\nshapes. This inspires us to examine the value of these cells in promoting the\nresearch of medical image segmentation. In this paper, we design a Similarity\nMemory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,\nwe propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and\nremembers the category features of specific lesions or organs in medical images\nthrough the similarity memory prior in the prototype memory bank, thus helping\nthe network to learn subtle texture changes between categories. DMW-LA also\ndynamically updates the similarity memory prior in reverse through Weight-Loss\nDynamic (W-LD) update strategy, effectively assisting the network directly\nextract category features. In addition, we propose the Double-Similarity Global\nInternal Enhancement Module (DS-GIM) to deeply explore the internal differences\nin the feature distribution of input data through cosine similarity and\neuclidean distance. Extensive experiments on four public datasets show that\nSim-MPNet has better segmentation performance than other state-of-the-art\nmethods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSim-MPNet\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u7ed3\u5408\u52a8\u6001\u8bb0\u5fc6\u6743\u91cd\u635f\u5931\u6ce8\u610f\u529b\u548c\u53cc\u76f8\u4f3c\u6027\u5168\u5c40\u5185\u90e8\u589e\u5f3a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u53d7\u7315\u7334\u521d\u7ea7\u89c6\u89c9\u76ae\u5c42\u4e2d\u201c\u7956\u6bcd\u7ec6\u80de\u201d\u542f\u53d1\uff0c\u63a2\u7d22\u5176\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u8bbe\u8ba1Sim-MPNet\uff0c\u5305\u62ec\u52a8\u6001\u8bb0\u5fc6\u6743\u91cd\u635f\u5931\u6ce8\u610f\u529b\uff08DMW-LA\uff09\u548c\u53cc\u76f8\u4f3c\u6027\u5168\u5c40\u5185\u90e8\u589e\u5f3a\u6a21\u5757\uff08DS-GIM\uff09\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Sim-MPNet\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u548c\u76f8\u4f3c\u6027\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2507.00586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00586", "abs": "https://arxiv.org/abs/2507.00586", "authors": ["Luming Zhao", "Jingwen Xuan", "Jiamin Lou", "Yonghui Yu", "Wenwu Yang"], "title": "Context-Aware Academic Emotion Dataset and Benchmark", "comment": "Accepted to ICCV 2025", "summary": "Academic emotion analysis plays a crucial role in evaluating students'\nengagement and cognitive states during the learning process. This paper\naddresses the challenge of automatically recognizing academic emotions through\nfacial expressions in real-world learning environments. While significant\nprogress has been made in facial expression recognition for basic emotions,\nacademic emotion recognition remains underexplored, largely due to the scarcity\nof publicly available datasets. To bridge this gap, we introduce RAER, a novel\ndataset comprising approximately 2,700 video clips collected from around 140\nstudents in diverse, natural learning contexts such as classrooms, libraries,\nlaboratories, and dormitories, covering both classroom sessions and individual\nstudy. Each clip was annotated independently by approximately ten annotators\nusing two distinct sets of academic emotion labels with varying granularity,\nenhancing annotation consistency and reliability. To our knowledge, RAER is the\nfirst dataset capturing diverse natural learning scenarios. Observing that\nannotators naturally consider context cues-such as whether a student is looking\nat a phone or reading a book-alongside facial expressions, we propose CLIP-CAER\n(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes\nlearnable text prompts within the vision-language model CLIP to effectively\nintegrate facial expression and context cues from videos. Experimental results\ndemonstrate that CLIP-CAER substantially outperforms state-of-the-art\nvideo-based facial expression recognition methods, which are primarily designed\nfor basic emotions, emphasizing the crucial role of context in accurately\nrecognizing academic emotions. Project page: https://zgsfer.github.io/CAER", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u672f\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff08CLIP-CAER\uff09\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u6db5\u76d6\u591a\u6837\u5316\u81ea\u7136\u5b66\u4e60\u573a\u666f\u7684\u5b66\u672f\u60c5\u611f\u6570\u636e\u96c6RAER\u3002", "motivation": "\u5b66\u672f\u60c5\u611f\u5206\u6790\u5bf9\u8bc4\u4f30\u5b66\u751f\u5b66\u4e60\u72b6\u6001\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u57fa\u7840\u60c5\u611f\u8bc6\u522b\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5b66\u672f\u60c5\u611f\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7RAER\u6570\u636e\u96c6\u548cCLIP-CAER\u65b9\u6cd5\uff0c\u7ed3\u5408\u9762\u90e8\u8868\u60c5\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff08\u5982\u5b66\u4e60\u573a\u666f\uff09\u8fdb\u884c\u5b66\u672f\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCLIP-CAER\u5728\u5b66\u672f\u60c5\u611f\u8bc6\u522b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u89c6\u9891\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u5b66\u672f\u60c5\u611f\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0cCLIP-CAER\u4e3a\u5b66\u672f\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.00817", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00817", "abs": "https://arxiv.org/abs/2507.00817", "authors": ["Jiaming Zhang", "Rui Hu", "Qing Guo", "Wei Yang Bryan Lim"], "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs", "comment": null, "summary": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems.", "AI": {"tldr": "CAVALRY-V\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08V-MLLMs\uff09\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u76ee\u6807\u8bed\u4e49-\u89c6\u89c9\u635f\u5931\u51fd\u6570\u548c\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u751f\u6210\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u63a2\u7d22V-MLLMs\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u89e3\u51b3\u8de8\u6a21\u6001\u63a8\u7406\u3001\u65f6\u95f4\u4f9d\u8d56\u548c\u8ba1\u7b97\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53cc\u76ee\u6807\u8bed\u4e49-\u89c6\u89c9\u635f\u5931\u51fd\u6570\u548c\u4e24\u9636\u6bb5\u751f\u6210\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAVALRY-V\u5e73\u5747\u63d0\u534722.8%\u7684\u653b\u51fb\u6548\u679c\uff0c\u5e76\u5728\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e2d\u63d0\u534734.4%\u3002", "conclusion": "CAVALRY-V\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5bf9\u6297\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u65b9\u6cd5\u3002"}}
{"id": "2507.00593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00593", "abs": "https://arxiv.org/abs/2507.00593", "authors": ["Fernando Alonso-Fernandez", "Talha Hanif Butt", "Prayag Tiwari"], "title": "Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods", "comment": "Under review at ESWA", "summary": "Safe overtaking manoeuvres in trucks are vital for preventing accidents and\nensuring efficient traffic flow. Accurate prediction of such manoeuvres is\nessential for Advanced Driver Assistance Systems (ADAS) to make timely and\ninformed decisions. In this study, we focus on overtake detection using\nController Area Network (CAN) bus data collected from five in-service trucks\nprovided by the Volvo Group. We evaluate three common classifiers for vehicle\nmanoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and\nSupport Vector Machines (SVM), and analyse how different preprocessing\nconfigurations affect performance. We find that variability in traffic\nconditions strongly influences the signal patterns, particularly in the\nno-overtake class, affecting classification performance if training data lacks\nadequate diversity. Since the data were collected under unconstrained,\nreal-world conditions, class diversity cannot be guaranteed a priori. However,\ntraining with data from multiple vehicles improves generalisation and reduces\ncondition-specific bias. Our pertruck analysis also reveals that classification\naccuracy, especially for overtakes, depends on the amount of training data per\nvehicle. To address this, we apply a score-level fusion strategy, which yields\nthe best per-truck performance across most cases. Overall, we achieve an\naccuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True\nPositive Rate). This research has been part of the BIG FUN project, which\nexplores how Artificial Intelligence can be applied to logged vehicle data to\nunderstand and predict driver behaviour, particularly in relation to Camera\nMonitor Systems (CMS), being introduced as digital replacements for traditional\nexterior mirrors.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5361\u8f66CAN\u603b\u7ebf\u6570\u636e\uff0c\u8bc4\u4f30\u4e09\u79cd\u5206\u7c7b\u5668\uff08ANN\u3001RF\u3001SVM\uff09\u7528\u4e8e\u8d85\u8f66\u68c0\u6d4b\uff0c\u53d1\u73b0\u591a\u8f66\u6570\u636e\u8bad\u7ec3\u548c\u5206\u6570\u7ea7\u878d\u5408\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5361\u8f66\u5b89\u5168\u8d85\u8f66\u5bf9\u9632\u6b62\u4e8b\u6545\u548c\u4ea4\u901a\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u9700\u901a\u8fc7ADAS\u7cfb\u7edf\u51c6\u786e\u9884\u6d4b\u8d85\u8f66\u884c\u4e3a\u3002", "method": "\u4f7f\u7528Volvo\u63d0\u4f9b\u7684\u4e94\u8f86\u5361\u8f66CAN\u603b\u7ebf\u6570\u636e\uff0c\u8bc4\u4f30ANN\u3001RF\u548cSVM\u4e09\u79cd\u5206\u7c7b\u5668\uff0c\u5206\u6790\u4e0d\u540c\u9884\u5904\u7406\u914d\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u591a\u8f66\u6570\u636e\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5206\u6570\u7ea7\u878d\u5408\u7b56\u7565\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u6700\u7ec8\u5b9e\u73b0TNR=93%\u548cTPR=86.5%\u3002", "conclusion": "\u6570\u636e\u591a\u6837\u6027\u548c\u591a\u8f66\u8bad\u7ec3\u5bf9\u8d85\u8f66\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5206\u6570\u7ea7\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.00603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00603", "abs": "https://arxiv.org/abs/2507.00603", "authors": ["Yupeng Zheng", "Pengxuan Yang", "Zebin Xing", "Qichao Zhang", "Yuhang Zheng", "Yinfeng Gao", "Pengfei Li", "Teng Zhang", "Zhongpu Xia", "Peng Jia", "Dongbin Zhao"], "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", "comment": "ICCV 2025, first version", "summary": "End-to-end autonomous driving directly generates planning trajectories from\nraw sensor data, yet it typically relies on costly perception supervision to\nextract scene information. A critical research challenge arises: constructing\nan informative driving world model to enable perception annotation-free,\nend-to-end planning via self-supervised learning. In this paper, we present\nWorld4Drive, an end-to-end autonomous driving framework that employs vision\nfoundation models to build latent world models for generating and evaluating\nmulti-modal planning trajectories. Specifically, World4Drive first extracts\nscene features, including driving intention and world latent representations\nenriched with spatial-semantic priors provided by vision foundation models. It\nthen generates multi-modal planning trajectories based on current scene\nfeatures and driving intentions and predicts multiple intention-driven future\nstates within the latent space. Finally, it introduces a world model selector\nmodule to evaluate and select the best trajectory. We achieve perception\nannotation-free, end-to-end planning through self-supervised alignment between\nactual future observations and predicted observations reconstructed from the\nlatent space. World4Drive achieves state-of-the-art performance without manual\nperception annotations on both the open-loop nuScenes and closed-loop NavSim\nbenchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower\ncollision rate, and 3.75 faster training convergence. Codes will be accessed at\nhttps://github.com/ucaszyp/World4Drive.", "AI": {"tldr": "World4Drive\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6784\u5efa\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\uff0c\u65e0\u9700\u611f\u77e5\u6807\u6ce8\u5373\u53ef\u751f\u6210\u591a\u6a21\u6001\u89c4\u5212\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u4f9d\u8d56\u6602\u8d35\u611f\u77e5\u76d1\u7763\u63d0\u53d6\u573a\u666f\u4fe1\u606f\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u65e0\u611f\u77e5\u6807\u6ce8\u7684\u7aef\u5230\u7aef\u89c4\u5212\u3002", "method": "\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u573a\u666f\u7279\u5f81\uff0c\u751f\u6210\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u9009\u62e9\u5668\u8bc4\u4f30\u6700\u4f73\u8f68\u8ff9\u3002", "result": "\u5728nuScenes\u548cNavSim\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cL2\u8bef\u5dee\u964d\u4f4e18.1%\uff0c\u78b0\u649e\u7387\u51cf\u5c1146.7%\uff0c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u5feb3.75\u500d\u3002", "conclusion": "World4Drive\u5c55\u793a\u4e86\u65e0\u611f\u77e5\u6807\u6ce8\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u884c\u6027\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.00608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00608", "abs": "https://arxiv.org/abs/2507.00608", "authors": ["Zehua Fu", "Chenguang Liu", "Yuyu Chen", "Jiaqi Zhou", "Qingjie Liu", "Yunhong Wang"], "title": "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection", "comment": "Accepted by IEEE Transactions on Intelligent Transportation Systems.\n  15 pages, 10 figures", "summary": "Despite its significant success, object detection in traffic and\ntransportation scenarios requires time-consuming and laborious efforts in\nacquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation\n(UDA) for object detection has recently gained increasing research attention.\nUDA for object detection has been dominated by domain alignment methods, which\nachieve top performance. Recently, self-labeling methods have gained popularity\ndue to their simplicity and efficiency. In this paper, we investigate the\nlimitations that prevent self-labeling detectors from achieving commensurate\nperformance with domain alignment methods. Specifically, we identify the high\nproportion of simple samples during training, i.e., the simple-label bias, as\nthe central cause. We propose a novel approach called De-Simplifying Pseudo\nLabels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level\nmemory bank to implement an innovative pseudo label updating strategy. Then,\nadversarial samples are introduced during training to enhance the proportion.\nFurthermore, we propose an adaptive weighted loss to avoid the model suffering\nfrom an abundance of false positive pseudo labels in the late training period.\nExperimental results demonstrate that DeSimPL effectively reduces the\nproportion of simple samples during training, leading to a significant\nperformance improvement for self-labeling detectors. Extensive experiments\nconducted on four benchmarks validate our analysis and conclusions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDeSimPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u4e2d\u7684\u7b80\u5355\u6837\u672c\u6bd4\u4f8b\uff0c\u63d0\u5347\u81ea\u6807\u8bb0\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u6807\u8bb0\u68c0\u6d4b\u5668\u56e0\u7b80\u5355\u6837\u672c\u6bd4\u4f8b\u8fc7\u9ad8\u800c\u6027\u80fd\u4e0d\u53ca\u9886\u57df\u5bf9\u9f50\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDeSimPL\u65b9\u6cd5\uff0c\u5305\u62ec\u5b9e\u4f8b\u7ea7\u8bb0\u5fc6\u5e93\u3001\u4f2a\u6807\u7b7e\u66f4\u65b0\u7b56\u7565\u3001\u5bf9\u6297\u6837\u672c\u548c\u81ea\u9002\u5e94\u52a0\u6743\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDeSimPL\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DeSimPL\u6210\u529f\u89e3\u51b3\u4e86\u7b80\u5355\u6837\u672c\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u81ea\u6807\u8bb0\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u6027\u80fd\u63d0\u5347\u7684\u9014\u5f84\u3002"}}
{"id": "2507.00648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00648", "abs": "https://arxiv.org/abs/2507.00648", "authors": ["Siyuan Yao", "Rui Zhu", "Ziqi Wang", "Wenqi Ren", "Yanyang Yan", "Xiaochun Cao"], "title": "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions", "comment": "Accepted to ICCV 2025", "summary": "Visual object tracking has gained promising progress in past decades. Most of\nthe existing approaches focus on learning target representation in\nwell-conditioned daytime data, while for the unconstrained real-world scenarios\nwith adverse weather conditions, e.g. nighttime or foggy environment, the\ntremendous domain shift leads to significant performance degradation. In this\npaper, we propose UMDATrack, which is capable of maintaining high-quality\ntarget state prediction under various adverse weather conditions within a\nunified domain adaptation framework. Specifically, we first use a controllable\nscenario generator to synthesize a small amount of unlabeled videos (less than\n2% frames in source daytime datasets) in multiple weather conditions under the\nguidance of different text prompts. Afterwards, we design a simple yet\neffective domain-customized adapter (DCA), allowing the target objects'\nrepresentation to rapidly adapt to various weather conditions without redundant\nmodel updating. Furthermore, to enhance the localization consistency between\nsource and target domains, we propose a target-aware confidence alignment\nmodule (TCA) following optimal transport theorem. Extensive experiments\ndemonstrate that UMDATrack can surpass existing advanced visual trackers and\nlead new state-of-the-art performance by a significant margin. Our code is\navailable at https://github.com/Z-Z188/UMDATrack.", "AI": {"tldr": "UMDATrack\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u57df\u9002\u5e94\u6846\u67b6\uff0c\u80fd\u591f\u5728\u591a\u79cd\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u76ee\u6807\u72b6\u6001\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5728\u767d\u5929\u6570\u636e\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u53ef\u63a7\u573a\u666f\u751f\u6210\u5668\u5408\u6210\u5c11\u91cf\u672a\u6807\u8bb0\u89c6\u9891\uff0c\u8bbe\u8ba1\u57df\u5b9a\u5236\u9002\u914d\u5668\uff08DCA\uff09\u548c\u76ee\u6807\u611f\u77e5\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u6a21\u5757\uff08TCA\uff09\u3002", "result": "UMDATrack\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u89c6\u89c9\u8ddf\u8e2a\u5668\uff0c\u8fbe\u5230\u65b0\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "UMDATrack\u901a\u8fc7\u7edf\u4e00\u57df\u9002\u5e94\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u76ee\u6807\u8ddf\u8e2a\u95ee\u9898\u3002"}}
{"id": "2507.00969", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00969", "abs": "https://arxiv.org/abs/2507.00969", "authors": ["Alberto Neri", "Maximilan Fehrentz", "Veronica Penza", "Leonardo S. Mattos", "Nazim Haouchine"], "title": "Surgical Neural Radiance Fields from One Image", "comment": null, "summary": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D\nreconstruction and view synthesis, yet their reliance on extensive multi-view\ndata limits their application in surgical intraoperative settings where only\nlimited data is available. In particular, collecting such extensive data\nintraoperatively is impractical due to time constraints. This work addresses\nthis challenge by leveraging a single intraoperative image and preoperative\ndata to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera\nviewpoints and images needed for robust and unobstructed training.\nIntraoperatively, the appearance of the surgical image is transferred to the\npre-constructed training set through neural style transfer, specifically\ncombining WTC2 and STROTSS to prevent over-stylization. This process enables\nthe creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases.\nQuantitative comparisons to NeRF models trained on real surgical microscope\nimages demonstrate strong synthesis agreement, with similarity metrics\nindicating high reconstruction fidelity and stylistic alignment. When compared\nwith ground truth, our method demonstrates high structural similarity,\nconfirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF\ntraining in surgical settings, overcoming the limitations of traditional\nmulti-view methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5355\u5f20\u672f\u4e2d\u56fe\u50cf\u548c\u672f\u524d\u6570\u636e\u9ad8\u6548\u8bad\u7ec3NeRF\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u624b\u672f\u573a\u666f\u4e2d\u591a\u89c6\u56fe\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "NeRF\u57283D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u624b\u672f\u4e2d\u56e0\u6570\u636e\u6709\u9650\u800c\u96be\u4ee5\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5355\u5f20\u672f\u4e2d\u56fe\u50cf\u548c\u672f\u524d\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u5229\u7528\u672f\u524dMRI\u6570\u636e\u5b9a\u4e49\u8bad\u7ec3\u96c6\uff0c\u901a\u8fc7\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u5c06\u672f\u4e2d\u56fe\u50cf\u5916\u89c2\u8f6c\u79fb\u5230\u8bad\u7ec3\u96c6\uff0c\u7ed3\u5408WTC2\u548cSTROTSS\u907f\u514d\u8fc7\u5ea6\u98ce\u683c\u5316\u3002", "result": "\u5728\u56db\u4e2a\u795e\u7ecf\u5916\u79d1\u6848\u4f8b\u4e2d\u9a8c\u8bc1\uff0c\u4e0e\u771f\u5b9e\u624b\u672f\u663e\u5fae\u955c\u56fe\u50cf\u8bad\u7ec3\u7684NeRF\u6a21\u578b\u76f8\u6bd4\uff0c\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u5bf9\u9f50\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5355\u56fe\u50cfNeRF\u8bad\u7ec3\u5728\u624b\u672f\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u591a\u89c6\u56fe\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2507.00659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00659", "abs": "https://arxiv.org/abs/2507.00659", "authors": ["Juelin Zhu", "Shuaibang Peng", "Long Wang", "Hanlin Tan", "Yu Liu", "Maojun Zhang", "Shen Yan"], "title": "LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment", "comment": "Accepted by ICCV 2025", "summary": "We propose a novel method for aerial visual localization over low\nLevel-of-Detail (LoD) city models. Previous wireframe-alignment-based method\nLoD-Loc has shown promising localization results leveraging LoD models.\nHowever, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the\nmajority of available models and those many countries plan to construct\nnationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD\ncity models could unlock drones' potential for global urban localization. To\naddress these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine\nstrategy using explicit silhouette alignment to achieve accurate localization\nover low-LoD city models in the air. Specifically, given a query image, LoD-Loc\nv2 first applies a building segmentation network to shape building silhouettes.\nThen, in the coarse pose selection stage, we construct a pose cost volume by\nuniformly sampling pose hypotheses around a prior pose to represent the pose\nprobability distribution. Each cost of the volume measures the degree of\nalignment between the projected and predicted silhouettes. We select the pose\nwith maximum value as the coarse pose. In the fine pose estimation stage, a\nparticle filtering method incorporating a multi-beam tracking approach is used\nto efficiently explore the hypothesis space and obtain the final pose\nestimation. To further facilitate research in this field, we release two\ndatasets with LoD1 city models covering 10.7 km , along with real RGB queries\nand ground-truth pose annotations. Experimental results show that LoD-Loc v2\nimproves estimation accuracy with high-LoD models and enables localization with\nlow-LoD models for the first time. Moreover, it outperforms state-of-the-art\nbaselines by large margins, even surpassing texture-model-based methods, and\nbroadens the convergence basin to accommodate larger prior errors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u7ec6\u8282\u5c42\u6b21\uff08LoD\uff09\u57ce\u5e02\u6a21\u578b\u7684\u7a7a\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u65b0\u65b9\u6cd5LoD-Loc v2\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u7b56\u7565\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9ad8LoD\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u53ef\u7528\u7684\u591a\u4e3a\u4f4eLoD\u6a21\u578b\uff0c\u9650\u5236\u4e86\u65e0\u4eba\u673a\u5168\u7403\u5b9a\u4f4d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\uff0c\u5305\u62ec\u5efa\u7b51\u8f6e\u5ed3\u5206\u5272\u3001\u7c97\u59ff\u6001\u9009\u62e9\u548c\u7ec6\u7c92\u5ea6\u7c92\u5b50\u6ee4\u6ce2\u4f18\u5316\u3002", "result": "LoD-Loc v2\u9996\u6b21\u5b9e\u73b0\u4f4eLoD\u6a21\u578b\u5b9a\u4f4d\uff0c\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6269\u5927\u4e86\u6536\u655b\u8303\u56f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2507.00676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00676", "abs": "https://arxiv.org/abs/2507.00676", "authors": ["Edward Effendy", "Kuan-Wei Tseng", "Rei Kawakami"], "title": "A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation", "comment": null, "summary": "Accepted in the ICIP 2025\n  We present a novel transformer-based framework for whole-body grasping that\naddresses both pose generation and motion infilling, enabling realistic and\nstable object interactions. Our pipeline comprises three stages: Grasp Pose\nGeneration for full-body grasp generation, Temporal Infilling for smooth motion\ncontinuity, and a LiftUp Transformer that refines downsampled joints back to\nhigh-resolution markers. To overcome the scarcity of hand-object interaction\ndata, we introduce a data-efficient Generalized Pretraining stage on large,\ndiverse motion datasets, yielding robust spatio-temporal representations\ntransferable to grasping tasks. Experiments on the GRAB dataset show that our\nmethod outperforms state-of-the-art baselines in terms of coherence, stability,\nand visual realism. The modular design also supports easy adaptation to other\nhuman-motion applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5168\u8eab\u6293\u63e1\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u59ff\u52bf\u751f\u6210\u548c\u8fd0\u52a8\u586b\u5145\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u7a33\u5b9a\u7684\u7269\u4f53\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u5168\u8eab\u6293\u63e1\u4e2d\u59ff\u52bf\u751f\u6210\u548c\u8fd0\u52a8\u8fde\u7eed\u6027\u7684\u6311\u6218\uff0c\u5e76\u514b\u670d\u624b-\u7269\u4f53\u4ea4\u4e92\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u6293\u63e1\u59ff\u52bf\u751f\u6210\u3001\u65f6\u95f4\u586b\u5145\u548cLiftUp Transformer\uff0c\u5e76\u901a\u8fc7\u5e7f\u4e49\u9884\u8bad\u7ec3\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "result": "\u5728GRAB\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fde\u8d2f\u6027\u3001\u7a33\u5b9a\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u6a21\u5757\u5316\u8bbe\u8ba1\u6613\u4e8e\u9002\u914d\u5176\u4ed6\u4eba\u4f53\u8fd0\u52a8\u5e94\u7528\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.01006", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01006", "abs": "https://arxiv.org/abs/2507.01006", "authors": ["Wenyi Hong", "Wenmeng Yu", "Xiaotao Gu", "Guo Wang", "Guobing Gan", "Haomiao Tang", "Jiale Cheng", "Ji Qi", "Junhui Ji", "Lihang Pan", "Shuaiqi Duan", "Weihan Wang", "Yan Wang", "Yean Cheng", "Zehai He", "Zhe Su", "Zhen Yang", "Ziyang Pan", "Aohan Zeng", "Baoxu Wang", "Boyan Shi", "Changyu Pang", "Chenhui Zhang", "Da Yin", "Fan Yang", "Guoqing Chen", "Jiazheng Xu", "Jiali Chen", "Jing Chen", "Jinhao Chen", "Jinghao Lin", "Jinjiang Wang", "Junjie Chen", "Leqi Lei", "Leyi Pan", "Mingzhi Zhang", "Qinkai Zheng", "Sheng Yang", "Shi Zhong", "Shiyu Huang", "Shuyuan Zhao", "Siyan Xue", "Shangqin Tu", "Shengbiao Meng", "Tianshu Zhang", "Tianwei Luo", "Tianxiang Hao", "Tianle Gong", "Wenkai Li", "Wei Jia", "Xin Lyu", "Xuancheng Huang", "Yanling Wang", "Yadong Xue", "Yanfeng Wang", "Yifan An", "Yifan Du", "Yiming Shi", "Yiheng Huang", "Yilin Niu", "Yuan Wang", "Yuanchang Yue", "Yuchen Li", "Yutao Zhang", "Yuxuan Zhang", "Zhanxiao Du", "Zhenyu Hou", "Zhao Xue", "Zhengxiao Du", "Zihan Wang", "Peng Zhang", "Debing Liu", "Bin Xu", "Juanzi Li", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "comment": null, "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "AI": {"tldr": "GLM-4.1V-Thinking\u662f\u4e00\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8bfe\u7a0b\u91c7\u6837\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a8\u52a8\u901a\u7528\u591a\u6a21\u6001\u63a8\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6784\u5efa\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bfe\u7a0b\u91c7\u6837\uff08RLCS\uff09\u4f18\u5316\u6027\u80fd\u3002", "result": "\u572828\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\uff0c\u751a\u81f3\u4e0e\u66f4\u5927\u6a21\u578b\u6216\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\u7ade\u4e89\u3002", "conclusion": "GLM-4.1V-Thinking\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u7814\u7a76\u9886\u57df\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\u3002"}}
{"id": "2507.00690", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.00690", "abs": "https://arxiv.org/abs/2507.00690", "authors": ["Keke Tang", "Ziyong Du", "Weilong Peng", "Xiaofei Wang", "Peican Zhu", "Ligang Liu", "Zhihong Tian"], "title": "Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack", "comment": null, "summary": "Adversarial attacks on point clouds often impose strict geometric constraints\nto preserve plausibility; however, such constraints inherently limit\ntransferability and undefendability. While deformation offers an alternative,\nexisting unstructured approaches may introduce unnatural distortions, making\nadversarial point clouds conspicuous and undermining their plausibility. In\nthis paper, we propose CageAttack, a cage-based deformation framework that\nproduces natural adversarial point clouds. It first constructs a cage around\nthe target object, providing a structured basis for smooth, natural-looking\ndeformation. Perturbations are then applied to the cage vertices, which\nseamlessly propagate to the point cloud, ensuring that the resulting\ndeformations remain intrinsic to the object and preserve plausibility.\nExtensive experiments on seven 3D deep neural network classifiers across three\ndatasets show that CageAttack achieves a superior balance among\ntransferability, undefendability, and plausibility, outperforming\nstate-of-the-art methods. Codes will be made public upon acceptance.", "AI": {"tldr": "CageAttack\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b3c\u5f62\u53d8\u5f62\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u81ea\u7136\u7684\u5bf9\u6297\u6027\u70b9\u4e91\uff0c\u5e73\u8861\u4e86\u53ef\u8f6c\u79fb\u6027\u3001\u4e0d\u53ef\u9632\u5fa1\u6027\u548c\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u6027\u70b9\u4e91\u653b\u51fb\u65b9\u6cd5\u53d7\u9650\u4e8e\u51e0\u4f55\u7ea6\u675f\uff0c\u5bfc\u81f4\u53ef\u8f6c\u79fb\u6027\u548c\u4e0d\u53ef\u9632\u5fa1\u6027\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u975e\u7ed3\u6784\u5316\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u4e0d\u81ea\u7136\u7684\u53d8\u5f62\u3002", "method": "CageAttack\u901a\u8fc7\u6784\u5efa\u76ee\u6807\u5bf9\u8c61\u7684\u7b3c\u5f62\u7ed3\u6784\uff0c\u5bf9\u7b3c\u9876\u70b9\u65bd\u52a0\u6270\u52a8\uff0c\u5b9e\u73b0\u5e73\u6ed1\u81ea\u7136\u7684\u70b9\u4e91\u53d8\u5f62\u3002", "result": "\u5728\u4e03\u4e2a3D\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCageAttack\u5728\u53ef\u8f6c\u79fb\u6027\u3001\u4e0d\u53ef\u9632\u5fa1\u6027\u548c\u5408\u7406\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CageAttack\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u6297\u6027\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u591a\u4e2a\u5173\u952e\u6027\u80fd\u6307\u6807\u3002"}}
{"id": "2507.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00698", "abs": "https://arxiv.org/abs/2507.00698", "authors": ["Qihang Fan", "Huaibo Huang", "Yuang Ai", "ran He"], "title": "Rectifying Magnitude Neglect in Linear Attention", "comment": "Accepted by ICCV2025", "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\uff08Linear Attention\uff09\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5MALA\uff0c\u901a\u8fc7\u5f15\u5165\u67e5\u8be2\uff08Query\uff09\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u4f7f\u5176\u6027\u80fd\u63a5\u8fd1\u6807\u51c6Softmax Attention\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u7136\u590d\u6742\u5ea6\u4f4e\uff0c\u4f46\u6027\u80fd\u663e\u8457\u4f4e\u4e8eSoftmax Attention\uff0c\u539f\u56e0\u662f\u5176\u5ffd\u7565\u4e86\u67e5\u8be2\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u5e03\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u3002", "method": "\u63d0\u51faMALA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5f15\u5165\u67e5\u8be2\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u4f7f\u5176\u6ce8\u610f\u529b\u5206\u5e03\u66f4\u63a5\u8fd1Softmax Attention\u3002", "result": "MALA\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u8bed\u4e49\u5206\u5272\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MALA\u901a\u8fc7\u5f15\u5165\u67e5\u8be2\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u4f4e\u590d\u6742\u5ea6\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.00707", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00707", "abs": "https://arxiv.org/abs/2507.00707", "authors": ["Zeming Chen", "Hang Zhao"], "title": "BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving", "comment": null, "summary": "Multi-view image generation in autonomous driving demands consistent 3D scene\nunderstanding across camera views. Most existing methods treat this problem as\na 2D image set generation task, lacking explicit 3D modeling. However, we argue\nthat a structured representation is crucial for scene generation, especially\nfor autonomous driving applications. This paper proposes BEV-VAE for consistent\nand controllable view synthesis. BEV-VAE first trains a multi-view image\nvariational autoencoder for a compact and unified BEV latent space and then\ngenerates the scene with a latent diffusion transformer. BEV-VAE supports\narbitrary view generation given camera configurations, and optionally 3D\nlayouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance\nin both 3D consistent reconstruction and generation. The code is available at:\nhttps://github.com/Czm369/bev-vae.", "AI": {"tldr": "BEV-VAE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u5efa\u6a21\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684BEV\u6f5c\u5728\u7a7a\u95f4\u548c\u6f5c\u5728\u6269\u6563\u53d8\u6362\u5668\u5b9e\u73b0\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u89c6\u56fe\u5408\u6210\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u9700\u8981\u8de8\u89c6\u56fe\u76843D\u573a\u666f\u4e00\u81f4\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f3D\u5efa\u6a21\uff0c\u56e0\u6b64\u63d0\u51fa\u7ed3\u6784\u5316\u8868\u793a\u7684\u91cd\u8981\u6027\u3002", "method": "BEV-VAE\u8bad\u7ec3\u591a\u89c6\u89d2\u56fe\u50cf\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ee5\u6784\u5efa\u7edf\u4e00\u7684BEV\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u53d8\u6362\u5668\u751f\u6210\u573a\u666f\u3002", "result": "\u5728nuScenes\u548cArgoverse 2\u6570\u636e\u96c6\u4e0a\uff0cBEV-VAE\u57283D\u4e00\u81f4\u6027\u91cd\u5efa\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BEV-VAE\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u5b9e\u73b0\u4e86\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u9ad8\u6548\u751f\u6210\uff0c\u652f\u6301\u4efb\u610f\u89c6\u89d2\u548c3D\u5e03\u5c40\u8f93\u5165\u3002"}}
{"id": "2507.00721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00721", "abs": "https://arxiv.org/abs/2507.00721", "authors": ["Xiao Zhang", "Fei Wei", "Yong Wang", "Wenda Zhao", "Feiyi Li", "Xiangxiang Chu"], "title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement", "comment": "ICCV2025", "summary": "Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the\nlack of images in the target domain. Previous approaches leverage\nVision-Language Models (VLMs) to tackle this challenge, exploiting their\nzero-shot learning capabilities. However, these methods primarily address\ndomain distribution shifts and overlook the misalignment between the detection\ntask and VLMs, which rely on manually crafted prompts. To overcome these\nlimitations, we propose the unified prompt and representation enhancement\n(UPRE) framework, which jointly optimizes both textual prompts and visual\nrepresentations. Specifically, our approach introduces a multi-view domain\nprompt that combines linguistic domain priors with detection-specific\nknowledge, and a visual representation enhancement module that produces domain\nstyle variations. Furthermore, we introduce multi-level enhancement strategies,\nincluding relative domain distance and positive-negative separation, which\nalign multi-modal representations at the image level and capture diverse visual\nrepresentations at the instance level, respectively. Extensive experiments\nconducted on nine benchmark datasets demonstrate the superior performance of\nour framework in ZSDA detection scenarios. Code is available at\nhttps://github.com/AMAP-ML/UPRE.", "AI": {"tldr": "UPRE\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6587\u672c\u63d0\u793a\u548c\u89c6\u89c9\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u57df\u9002\u5e94\u4e2d\u7684\u4efb\u52a1\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u96f6\u6837\u672c\u57df\u9002\u5e94\uff08ZSDA\uff09\u56e0\u76ee\u6807\u57df\u7f3a\u4e4f\u56fe\u50cf\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57df\u5206\u5e03\u504f\u79fb\uff0c\u5ffd\u89c6\u4e86\u68c0\u6d4b\u4efb\u52a1\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faUPRE\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u57df\u63d0\u793a\u548c\u89c6\u89c9\u8868\u793a\u589e\u5f3a\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u591a\u7ea7\u589e\u5f3a\u7b56\u7565\uff08\u5982\u76f8\u5bf9\u57df\u8ddd\u79bb\u548c\u6b63\u8d1f\u5206\u79bb\uff09\u3002", "result": "\u5728\u4e5d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUPRE\u5728ZSDA\u68c0\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UPRE\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u548c\u89c6\u89c9\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u4e0e\u6a21\u578b\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3aZSDA\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.00739", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.00739", "abs": "https://arxiv.org/abs/2507.00739", "authors": ["An Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Nguyen"], "title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "comment": null, "summary": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u5347\u65b9\u6848\u7684\u53cc\u6b63\u4ea4\u53ef\u8c03\u5c0f\u6ce2\u5355\u5143\uff0c\u653e\u5bbd\u4e86\u6b63\u4ea4\u6027\u548c\u6ee4\u6ce2\u5668\u957f\u5ea6\u9650\u5236\uff0c\u63d0\u5347\u4e86CNN\u4e2d\u7684\u5377\u79ef\u3001\u6c60\u5316\u548c\u4e0b\u91c7\u6837\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5c0f\u6ce2\u5355\u5143\u7684\u6b63\u4ea4\u6027\u548c\u6ee4\u6ce2\u5668\u957f\u5ea6\u9650\u5236\u9650\u5236\u4e86\u7075\u6d3b\u6027\uff0c\u5f71\u54cd\u4e86CNN\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u63d0\u5347\u65b9\u6848\u6784\u5efa\u53cc\u6b63\u4ea4\u53ef\u8c03\u5c0f\u6ce2\u5355\u5143\uff0c\u653e\u5bbd\u6b63\u4ea4\u6027\u548c\u6ee4\u6ce2\u5668\u957f\u5ea6\u7ea6\u675f\uff0c\u4f18\u5316\u5377\u79ef\u3001\u6c60\u5316\u548c\u4e0b\u91c7\u6837\u64cd\u4f5c\u3002", "result": "\u5728ResNet-18\u4e2d\uff0cCIFAR-10\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53472.12%\uff0cDTD\u63d0\u53479.73%\uff1b\u5728MVTec\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.00748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00748", "abs": "https://arxiv.org/abs/2507.00748", "authors": ["Bob Zhang", "Haoran Li", "Tao Zhang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yanbin Hao"], "title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning", "comment": "11 pages", "summary": "Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications involving complex multi-image\ncompositions and multimodal instructions, which reveals limitations in\ncross-image reasoning and generalization. To address these challenges, we adopt\na Reinforcement Learning (RL) based post-training strategy to improve the\nreasoning performance of MLLMs in multi-image grounding tasks. Our approach\nbegins with synthesizing high-quality chain-of-thought (CoT) data for\ncold-start initialization, followed by supervised fine-tuning (SFT) using\nlow-rank adaptation (LoRA). The cold-start training stage enables the model to\nidentify correct solutions. Subsequently, we perform rejection sampling using\nthe merged SFT model to curate high-quality RL data and leverage rule-based RL\nto guide the model toward optimal reasoning paths. Extensive experimental\nresults demonstrate the effectiveness of our approach, achieving +9.04\\%\nimprovements on MIG-Bench and +4.98\\% improvements on several out-of-domain\nreasoning grounding benchmarks over the SFT baseline. Furthermore, our approach\nexhibits strong generalization in multi-image perception, with gains of +3.1\\%\nand +2.4\\% over the base model on subsets of the BLINK and MMIU benchmarks,\nrespectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6027\u80fd\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u6570\u636e\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u7ed3\u5408\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u56fe\u50cf\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u56fe\u50cf\u7ec4\u5408\u548c\u591a\u6a21\u6001\u6307\u4ee4\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u6539\u8fdb\u5176\u8de8\u56fe\u50cf\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u5408\u6210\u601d\u7ef4\u94fe\u6570\u636e\u521d\u59cb\u5316\u3001\u76d1\u7763\u5fae\u8c03\uff08LoRA\uff09\uff0c\u5e76\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u548c\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728MIG-Bench\u4e0a\u63d0\u53479.04%\uff0c\u5728\u591a\u4e2a\u57df\u5916\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u53474.98%\uff0c\u5728BLINK\u548cMMIU\u5b50\u96c6\u4e0a\u5206\u522b\u63d0\u53473.1%\u548c2.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.00752", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00752", "abs": "https://arxiv.org/abs/2507.00752", "authors": ["Hao Xing", "Kai Zhe Boey", "Yuankai Wu", "Darius Burschka", "Gordon Cheng"], "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation", "comment": "7 pages, 4 figures, accepted in IROS25, Hangzhou, China", "summary": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u56fe\u5377\u79ef\u7f51\u7edc\uff08MMGCN\uff09\uff0c\u901a\u8fc7\u6574\u5408\u4f4e\u5e27\u7387\u89c6\u89c9\u6570\u636e\u548c\u9ad8\u5e27\u7387\u8fd0\u52a8\u6570\u636e\uff0c\u51cf\u5c11\u52a8\u4f5c\u5206\u5272\u4e2d\u7684\u8fc7\u5206\u5272\u9519\u8bef\uff0c\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u534f\u4f5c\u673a\u5668\u4eba\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u7684\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u5bf9\u7406\u89e3\u5b50\u6d3b\u52a8\u6807\u7b7e\u53ca\u5176\u65f6\u95f4\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u566a\u58f0\u95ee\u9898\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5206\u5272\u9519\u8bef\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u56fe\u5377\u79ef\u7f51\u7edc\uff08MMGCN\uff09\uff0c\u7ed3\u5408\u6b63\u5f26\u7f16\u7801\u7b56\u7565\u3001\u65f6\u95f4\u56fe\u878d\u5408\u6a21\u5757\u548c\u6570\u636e\u589e\u5f3a\u6280\u672fSmoothLabelMix\uff0c\u63d0\u5347\u52a8\u4f5c\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728Bimanual Actions Dataset\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cF1@10\u4e3a94.5%\uff0cF1@25\u4e3a92.8%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MMGCN\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.00754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00754", "abs": "https://arxiv.org/abs/2507.00754", "authors": ["Selim Kuzucu", "Muhammad Ferjad Naeem", "Anna Kukleva", "Federico Tombari", "Bernt Schiele"], "title": "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs", "comment": "26 pages, 6 figures", "summary": "The integration of Large Language Model (LLMs) blocks with Vision\nTransformers (ViTs) holds immense promise for vision-only tasks by leveraging\nthe rich semantic knowledge and reasoning capabilities of LLMs. However, a\nfundamental challenge lies in the inherent modality mismatch between\ntext-centric pretraining of LLMs and vision-centric training of ViTs. Direct\nfusion often fails to fully exploit the LLM's potential and suffers from\nunstable finetuning. As a result, LLM blocks are kept frozen while only the\nvision components are learned. As a remedy to these challenges, we introduce\nLanguage-Unlocked Vision Transformers (LUViT), a novel approach that bridges\nthis modality mismatch through a synergistic pre-training strategy. LUViT\nco-adapts a ViT backbone and an LLM fusion block by (1) employing Masked\nAuto-Encoding (MAE) to pre-train the ViT for richer visual representations, and\n(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM\nblock using the MAE objective. This joint optimization guides the ViT to\nproduce LLM-aligned features and the LLM to effectively interpret visual\ninformation. We demonstrate through extensive experiments that LUViT\nsignificantly improves performance on various downstream vision tasks,\nshowcasing a more effective and efficient pathway to harness LLM knowledge for\nvisual understanding.", "AI": {"tldr": "LUViT\u901a\u8fc7\u8054\u5408\u9884\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3LLM\u4e0eViT\u7684\u6a21\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u4e0eViT\u6a21\u6001\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u76f4\u63a5\u878d\u5408\u6548\u679c\u4e0d\u4f73\u95ee\u9898\u3002", "method": "\u91c7\u7528MAE\u9884\u8bad\u7ec3ViT\uff0c\u540c\u65f6\u7528LoRA\u5c42\u8bad\u7ec3LLM\u5757\uff0c\u5b9e\u73b0\u8054\u5408\u4f18\u5316\u3002", "result": "LUViT\u5728\u591a\u79cd\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LUViT\u4e3a\u5229\u7528LLM\u77e5\u8bc6\u8fdb\u884c\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.00756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00756", "abs": "https://arxiv.org/abs/2507.00756", "authors": ["Hao Xing", "Kai Zhe Boey", "Gordon Cheng"], "title": "Towards Open-World Human Action Segmentation Using Graph Convolutional Networks", "comment": "8 pages, 3 figures, accepted in IROS25, Hangzhou, China", "summary": "Human-object interaction segmentation is a fundamental task of daily activity\nunderstanding, which plays a crucial role in applications such as assistive\nrobotics, healthcare, and autonomous systems. Most existing learning-based\nmethods excel in closed-world action segmentation, they struggle to generalize\nto open-world scenarios where novel actions emerge. Collecting exhaustive\naction categories for training is impractical due to the dynamic diversity of\nhuman activities, necessitating models that detect and segment\nout-of-distribution actions without manual annotation. To address this issue,\nwe formally define the open-world action segmentation problem and propose a\nstructured framework for detecting and segmenting unseen actions. Our framework\nintroduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional\nNetwork (EPGCN) with a novel decoder module for robust spatiotemporal feature\nupsampling. 2) Mixup-based training to synthesize out-of-distribution data,\neliminating reliance on manual annotations. 3) A novel Temporal Clustering loss\nthat groups in-distribution actions while distancing out-of-distribution\nsamples.\n  We evaluate our framework on two challenging human-object interaction\nrecognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.\nExperimental results demonstrate significant improvements over state-of-the-art\naction segmentation models across multiple open-set evaluation metrics,\nachieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and\nout-of-distribution detection performances (AUROC), respectively. Additionally,\nwe conduct an in-depth ablation study to assess the impact of each proposed\ncomponent, identifying the optimal framework configuration for open-world\naction segmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u4e16\u754c\u52a8\u4f5c\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u91d1\u5b57\u5854\u56fe\u5377\u79ef\u7f51\u7edc\u3001Mixup\u8bad\u7ec3\u548c\u65f6\u5e8f\u805a\u7c7b\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c01\u95ed\u4e16\u754c\u52a8\u4f5c\u5206\u5272\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u5f00\u653e\u4e16\u754c\u573a\u666f\uff0c\u9700\u8981\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u6a21\u578b\u6765\u5904\u7406\u65b0\u52a8\u4f5c\u3002", "method": "\u63d0\u51faEPGCN\u7f51\u7edc\u3001Mixup\u8bad\u7ec3\u5408\u6210\u5206\u5e03\u5916\u6570\u636e\u3001\u65f6\u5e8f\u805a\u7c7b\u635f\u5931\u4f18\u5316\u52a8\u4f5c\u5206\u7ec4\u3002", "result": "\u5728Bimanual Actions\u548cH2O\u6570\u636e\u96c6\u4e0a\uff0c\u5f00\u653e\u96c6\u5206\u5272\u6027\u80fd\u63d0\u534716.9%\uff0c\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\u63d0\u534734.6%\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u52a8\u4f5c\u5206\u5272\u95ee\u9898\uff0c\u5404\u7ec4\u4ef6\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u8d21\u732e\u3002"}}
{"id": "2507.00789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00789", "abs": "https://arxiv.org/abs/2507.00789", "authors": ["Ziji Lu"], "title": "OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection", "comment": null, "summary": "Text-to-image diffusion models often struggle to achieve accurate semantic\nalignment between generated images and text prompts while maintaining\nefficiency for deployment on resource-constrained hardware. Existing approaches\neither incur substantial computational overhead through noise optimization or\ncompromise semantic fidelity by aggressively pruning tokens. In this work, we\npropose OptiPrune, a unified framework that combines distribution-aware initial\nnoise optimization with similarity-based token pruning to address both\nchallenges simultaneously. Specifically, (1) we introduce a distribution-aware\nnoise optimization module guided by attention scores to steer the initial\nlatent noise toward semantically meaningful regions, mitigating issues such as\nsubject neglect and feature entanglement; (2) we design a hardware-efficient\ntoken pruning strategy that selects representative base tokens via patch-wise\nsimilarity, injects randomness to enhance generalization, and recovers pruned\ntokens using maximum similarity copying before attention operations. Our method\npreserves the Gaussian prior during noise optimization and enables efficient\ninference without sacrificing alignment quality. Experiments on benchmark\ndatasets, including Animal-Animal, demonstrate that OptiPrune achieves\nstate-of-the-art prompt-image consistency with significantly reduced\ncomputational cost.", "AI": {"tldr": "OptiPrune\u7ed3\u5408\u4e86\u5206\u5e03\u611f\u77e5\u7684\u521d\u59cb\u566a\u58f0\u4f18\u5316\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684token\u526a\u679d\uff0c\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "1. \u5206\u5e03\u611f\u77e5\u566a\u58f0\u4f18\u5316\u6a21\u5757\uff1b2. \u786c\u4ef6\u9ad8\u6548\u7684token\u526a\u679d\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u63d0\u793a-\u56fe\u50cf\u4e00\u81f4\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "OptiPrune\u5728\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u3002"}}
{"id": "2507.00792", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00792", "abs": "https://arxiv.org/abs/2507.00792", "authors": ["Hendric Voss", "Stefan Kopp"], "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "comment": null, "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTensorFlow\u7684\u5b9e\u65f6\u9006\u5411\u8fd0\u52a8\u5b66\uff08IK\uff09\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u591a\u7ea6\u675f\u95ee\u9898\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u5173\u8282\u9650\u5236\u7b49\u6311\u6218\u3002", "motivation": "\u5b9e\u65f6\u751f\u6210\u903c\u771f\u7684\u865a\u62df\u4eba\u4f53\u8fd0\u52a8\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u3001\u865a\u62df\u73af\u5883\u3001\u673a\u5668\u4eba\u548c\u751f\u7269\u529b\u5b66\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5229\u7528TensorFlow\u7684\u81ea\u52a8\u5fae\u5206\u548c\u5373\u65f6\u7f16\u8bd1\u6280\u672f\uff0c\u5c06\u6b63\u5411\u548c\u9006\u5411\u8fd0\u52a8\u5b66\u89c6\u4e3a\u53ef\u5fae\u5206\u64cd\u4f5c\uff0c\u5904\u7406\u9ad8\u81ea\u7531\u5ea6\u7684\u4eba\u4f53\u9aa8\u9abc\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6c42\u89e3\u5668\u5728SMPLX\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5feb\u901f\u6536\u655b\u3001\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u903c\u771f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00802", "abs": "https://arxiv.org/abs/2507.00802", "authors": ["Minye Shao", "Xingyu Miao", "Haoran Duan", "Zeyu Wang", "Jingkun Chen", "Yawen Huang", "Xian Wu", "Jingjing Deng", "Yang Long", "Yefeng Zheng"], "title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency", "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  preprint version). MICCAI proceedings DOI will appear here", "summary": "3D medical image generation is essential for data augmentation and patient\nprivacy, calling for reliable and efficient models suited for clinical\npractice. However, current methods suffer from limited anatomical fidelity,\nrestricted axial length, and substantial computational cost, placing them\nbeyond reach for regions with limited resources and infrastructure. We\nintroduce TRACE, a framework that generates 3D medical images with\nspatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.\nTRACE models sequential 2D slices as video frame pairs, combining segmentation\npriors and radiology reports for anatomical alignment, incorporating optical\nflow to sustain temporal coherence. During inference, an overlapping-frame\nstrategy links frame pairs into a flexible length sequence, reconstructed into\na spatiotemporally and anatomically aligned 3D volume. Experimental results\ndemonstrate that TRACE effectively balances computational efficiency with\npreserving anatomical fidelity and spatiotemporal consistency. Code is\navailable at: https://github.com/VinyehShaw/TRACE.", "AI": {"tldr": "TRACE\u662f\u4e00\u4e2a\u57fa\u4e8e2D\u591a\u6a21\u6001\u6761\u4ef6\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u65f6\u7a7a\u5bf9\u9f50\u76843D\u533b\u5b66\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u5256\u4fdd\u771f\u5ea6\u3001\u8f74\u5411\u957f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u5f53\u524d3D\u533b\u5b66\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u89e3\u5256\u4fdd\u771f\u5ea6\u4f4e\u3001\u8f74\u5411\u957f\u5ea6\u53d7\u9650\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u5e94\u7528\u3002", "method": "TRACE\u901a\u8fc7\u5c062D\u5207\u7247\u5efa\u6a21\u4e3a\u89c6\u9891\u5e27\u5bf9\uff0c\u7ed3\u5408\u5206\u5272\u5148\u9a8c\u548c\u653e\u5c04\u5b66\u62a5\u544a\u5b9e\u73b0\u89e3\u5256\u5bf9\u9f50\uff0c\u5e76\u5229\u7528\u5149\u6d41\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u63a8\u7406\u65f6\u91c7\u7528\u91cd\u53e0\u5e27\u7b56\u7565\u751f\u6210\u7075\u6d3b\u957f\u5ea6\u76843D\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTRACE\u5728\u8ba1\u7b97\u6548\u7387\u548c\u4fdd\u6301\u89e3\u5256\u4fdd\u771f\u5ea6\u4e0e\u65f6\u7a7a\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "conclusion": "TRACE\u4e3a3D\u533b\u5b66\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u3002"}}
{"id": "2507.00822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00822", "abs": "https://arxiv.org/abs/2507.00822", "authors": ["Yasser El Jarida", "Youssef Iraqi", "Loubna Mekouar"], "title": "Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data", "comment": "Accepted at the Synthetic Data for Computer Vision Workshop @ CVPR\n  2025. 10 pages, 5 figures. Code available at\n  https://github.com/YasserElj/Synthetic-Granular-Gen", "summary": "Accurate particle size distribution (PSD) measurement is important in\nindustries such as mining, pharmaceuticals, and fertilizer manufacturing,\nsignificantly influencing product quality and operational efficiency.\nTraditional PSD methods like sieve analysis and laser diffraction are manual,\ntime-consuming, and limited by particle overlap. Recent developments in\nconvolutional neural networks (CNNs) enable automated, real-time PSD estimation\ndirectly from particle images. In this work, we present a CNN-based methodology\ntrained on realistic synthetic particle imagery generated using Blender's\nadvanced rendering capabilities. Synthetic data sets using this method can\nreplicate various industrial scenarios by systematically varying particle\nshapes, textures, lighting, and spatial arrangements that closely resemble the\nactual configurations. We evaluated three CNN-based architectures, ResNet-50,\nInceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,\nd50, d90). Results demonstrated comparable accuracy across models, with\nEfficientNet-B0 achieving the best computational efficiency suitable for\nreal-time industrial deployment. This approach shows the effectiveness of\nrealistic synthetic data for robust CNN training, which offers significant\npotential for automated industrial PSD monitoring. The code is released at :\nhttps://github.com/YasserElj/Synthetic-Granular-Gen", "AI": {"tldr": "\u4f7f\u7528CNN\u548cBlender\u751f\u6210\u7684\u5408\u6210\u7c92\u5b50\u56fe\u50cf\u5b9e\u73b0\u5b9e\u65f6PSD\u6d4b\u91cf\uff0cEfficientNet-B0\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edfPSD\u6d4b\u91cf\u65b9\u6cd5\u8017\u65f6\u4e14\u53d7\u9650\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528Blender\u751f\u6210\u5408\u6210\u7c92\u5b50\u56fe\u50cf\u8bad\u7ec3CNN\uff08ResNet-50\u3001InceptionV3\u3001EfficientNet-B0\uff09\u9884\u6d4bPSD\u53c2\u6570\u3002", "result": "EfficientNet-B0\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u9002\u5408\u5de5\u4e1a\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u8bad\u7ec3CNN\u6709\u6548\uff0c\u4e3a\u5de5\u4e1aPSD\u76d1\u6d4b\u63d0\u4f9b\u81ea\u52a8\u5316\u6f5c\u529b\u3002"}}
{"id": "2507.00825", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.00825", "abs": "https://arxiv.org/abs/2507.00825", "authors": ["Hongxing Peng", "Lide Chen", "Hui Zhu", "Yan Chen"], "title": "High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery", "comment": "14 pages, 9 figures, to appear in KBS", "summary": "Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial\nchallenges, including small target sizes, high-density distributions, and\ncluttered backgrounds in UAV imagery. Current algorithms often depend on\nhand-crafted components like anchor boxes, which demand fine-tuning and exhibit\nlimited generalization, and Non-Maximum Suppression (NMS), which is\nthreshold-sensitive and prone to misclassifying dense objects. These generic\narchitectures thus struggle to adapt to aerial imaging characteristics,\nresulting in performance limitations. Moreover, emerging end-to-end frameworks\nhave yet to effectively mitigate these aerial-specific challenges.To address\nthese issues, we propose HEGS-DETR, a comprehensively enhanced, real-time\nDetection Transformer framework tailored for UAVs. First, we introduce the\nHigh-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.\nHFESNet preserves critical high-frequency spatial details to extract robust\nsemantic features, thereby improving discriminative capability for small and\noccluded targets in complex backgrounds. Second, our Efficient Small Object\nPyramid (ESOP) strategy strategically fuses high-resolution feature maps with\nminimal computational overhead, significantly boosting small object detection.\nFinally, the proposed Selective Query Recollection (SQR) and Geometry-Aware\nPositional Encoding (GAPE) modules enhance the detector's decoder stability and\nlocalization accuracy, effectively optimizing bounding boxes and providing\nexplicit spatial priors for dense scenes. Experiments on the VisDrone dataset\ndemonstrate that HEGS-DETR achieves a 5.1\\% AP$_{50}$ and 3.8\\% AP increase\nover the baseline, while maintaining real-time speed and reducing parameter\ncount by 4M.", "AI": {"tldr": "HEGS-DETR\u662f\u4e00\u79cd\u9488\u5bf9\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u7684\u6539\u8fdbTransformer\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u9891\u589e\u5f3a\u8bed\u4e49\u7f51\u7edc\u3001\u9ad8\u6548\u5c0f\u76ee\u6807\u91d1\u5b57\u5854\u548c\u9009\u62e9\u6027\u67e5\u8be2\u91cd\u6536\u96c6\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u548c\u5bc6\u96c6\u573a\u666f\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c0f\u76ee\u6807\u3001\u9ad8\u5bc6\u5ea6\u5206\u5e03\u548c\u590d\u6742\u80cc\u666f\u7684\u6311\u6218\uff0c\u73b0\u6709\u7b97\u6cd5\u4f9d\u8d56\u624b\u5de5\u7ec4\u4ef6\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHEGS-DETR\u6846\u67b6\uff0c\u5305\u62ec\u9ad8\u9891\u589e\u5f3a\u8bed\u4e49\u7f51\u7edc\uff08HFESNet\uff09\u3001\u9ad8\u6548\u5c0f\u76ee\u6807\u91d1\u5b57\u5854\uff08ESOP\uff09\u3001\u9009\u62e9\u6027\u67e5\u8be2\u91cd\u6536\u96c6\uff08SQR\uff09\u548c\u51e0\u4f55\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\uff08GAPE\uff09\u3002", "result": "\u5728VisDrone\u6570\u636e\u96c6\u4e0a\uff0cHEGS-DETR\u6bd4\u57fa\u7ebf\u6a21\u578bAP$_{50}$\u63d0\u53475.1%\uff0cAP\u63d0\u53473.8%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u901f\u5ea6\u548c\u51cf\u5c11\u53c2\u6570\u3002", "conclusion": "HEGS-DETR\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2507.00845", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00845", "abs": "https://arxiv.org/abs/2507.00845", "authors": ["Peter Pavl\u00edk", "Marc Schleiss", "Anna Bou Ezzeddine", "Viera Rozinajov\u00e1"], "title": "Do Echo Top Heights Improve Deep Learning Nowcasts?", "comment": "Pre-review version of an article accepted at Transactions on\n  Large-Scale Data and Knowledge-Centered Systems", "summary": "Precipitation nowcasting -- the short-term prediction of rainfall using\nrecent radar observations -- is critical for weather-sensitive sectors such as\ntransportation, agriculture, and disaster mitigation. While recent deep\nlearning models have shown promise in improving nowcasting skill, most\napproaches rely solely on 2D radar reflectivity fields, discarding valuable\nvertical information available in the full 3D radar volume. In this work, we\nexplore the use of Echo Top Height (ETH), a 2D projection indicating the\nmaximum altitude of radar reflectivity above a given threshold, as an auxiliary\ninput variable for deep learning-based nowcasting. We examine the relationship\nbetween ETH and radar reflectivity, confirming its relevance for predicting\nrainfall intensity. We implement a single-pass 3D U-Net that processes both the\nradar reflectivity and ETH as separate input channels. While our models are\nable to leverage ETH to improve skill at low rain-rate thresholds, results are\ninconsistent at higher intensities and the models with ETH systematically\nunderestimate precipitation intensity. Three case studies are used to\nillustrate how ETH can help in some cases, but also confuse the models and\nincrease the error variance. Nonetheless, the study serves as a foundation for\ncritically assessing the potential contribution of additional variables to\nnowcasting performance.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\uff0c\u5229\u7528Echo Top Height\uff08ETH\uff09\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8f85\u52a9\u8f93\u5165\u53d8\u91cf\uff0c\u4ee5\u63d0\u5347\u9884\u62a5\u6280\u80fd\u3002\u867d\u7136ETH\u5728\u4f4e\u96e8\u5f3a\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u96e8\u5f3a\u4e0b\u6548\u679c\u4e0d\u4e00\u81f4\u4e14\u53ef\u80fd\u589e\u52a0\u8bef\u5dee\u3002", "motivation": "\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u5bf9\u5929\u6c14\u654f\u611f\u884c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u591a\u5ffd\u75653D\u96f7\u8fbe\u6570\u636e\u7684\u5782\u76f4\u4fe1\u606f\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22ETH\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u75283D U-Net\u6a21\u578b\uff0c\u5c06\u96f7\u8fbe\u53cd\u5c04\u7387\u548cETH\u4f5c\u4e3a\u72ec\u7acb\u8f93\u5165\u901a\u9053\uff0c\u5206\u6790ETH\u4e0e\u964d\u96e8\u5f3a\u5ea6\u7684\u5173\u7cfb\u3002", "result": "ETH\u5728\u4f4e\u96e8\u5f3a\u4e0b\u80fd\u63d0\u5347\u9884\u62a5\u6280\u80fd\uff0c\u4f46\u5728\u9ad8\u96e8\u5f3a\u4e0b\u6548\u679c\u4e0d\u7a33\u5b9a\u4e14\u53ef\u80fd\u5bfc\u81f4\u4f4e\u4f30\u964d\u6c34\u5f3a\u5ea6\u3002", "conclusion": "ETH\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u5728\u4e34\u8fd1\u9884\u62a5\u4e2d\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5728\u9ad8\u96e8\u5f3a\u4e0b\u7684\u8868\u73b0\u548c\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.00849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00849", "abs": "https://arxiv.org/abs/2507.00849", "authors": ["Wei Li", "Jiaman Tang", "Yang Li", "Beihao Xia", "Ligang Tan", "Hongmao Qin"], "title": "UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection", "comment": "The paper was accepted by the 36th IEEE Intelligent Vehicles\n  Symposium (IEEE IV 2025)", "summary": "Unmanned Aerial Vehicle (UAV) object detection has been widely used in\ntraffic management, agriculture, emergency rescue, etc. However, it faces\nsignificant challenges, including occlusions, small object sizes, and irregular\nshapes. These challenges highlight the necessity for a robust and efficient\nmultimodal UAV object detection method. Mamba has demonstrated considerable\npotential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a\nmultimodal UAV object detection framework based on Mamba architectures. To\nimprove geometric adaptability, we propose the Deformable Token Mamba Block\n(DTMB) to generate deformable tokens by incorporating adaptive patches from\ndeformable convolutions alongside normal patches from normal convolutions,\nwhich serve as the inputs to the Mamba Block. To optimize the multimodal\nfeature complementarity, we design two separate DTMBs for the RGB and infrared\n(IR) modalities, with the outputs from both DTMBs integrated into the Mamba\nBlock for feature extraction and into the Fusion Mamba Block for feature\nfusion. Additionally, to improve multiscale object detection, especially for\nsmall objects, we stack four DTMBs at different scales to produce multiscale\nfeature representations, which are then sent to the Detection Neck for Mamba\n(DNM). The DNM module, inspired by the YOLO series, includes modifications to\nthe SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In\nparticular, we employ cross-enhanced spatial attention before the DTMB and\ncross-channel attention after the Fusion Mamba Block to extract more\ndiscriminative features. Experimental results on the DroneVehicle dataset show\nthat our method outperforms the baseline OAFA method by 3.6% in the mAP metric.\nCodes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684\u591a\u6a21\u6001\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u6846\u67b6UAVD-Mamba\uff0c\u901a\u8fc7\u6539\u8fdb\u51e0\u4f55\u9002\u5e94\u6027\u548c\u591a\u6a21\u6001\u7279\u5f81\u4e92\u8865\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u5728\u4ea4\u901a\u7ba1\u7406\u3001\u519c\u4e1a\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9762\u4e34\u906e\u6321\u3001\u5c0f\u76ee\u6807\u548c\u5f62\u72b6\u4e0d\u89c4\u5219\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86Deformable Token Mamba Block\uff08DTMB\uff09\u63d0\u5347\u51e0\u4f55\u9002\u5e94\u6027\uff0c\u5e76\u5206\u522b\u5904\u7406RGB\u548c\u7ea2\u5916\u6a21\u6001\uff1b\u901a\u8fc7\u5806\u53e0DTMB\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff1b\u6539\u8fdb\u4e86YOLOv11\u7684SPPF\u548cC3K2\u6a21\u5757\u3002", "result": "\u5728DroneVehicle\u6570\u636e\u96c6\u4e0a\uff0cmAP\u6307\u6807\u6bd4\u57fa\u7ebfOAFA\u65b9\u6cd5\u63d0\u9ad8\u4e863.6%\u3002", "conclusion": "UAVD-Mamba\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.00852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00852", "abs": "https://arxiv.org/abs/2507.00852", "authors": ["Fatemeh Sadat Daneshmand"], "title": "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting", "comment": null, "summary": "Flexible manufacturing systems in Industry 4.0 require robots capable of\nhandling objects in unstructured environments without rigid positioning\nconstraints. This paper presents a computer vision system that enables\nindustrial robots to detect and grasp pen components in arbitrary orientations\nwithout requiring structured trays, while maintaining robust performance under\nvarying lighting conditions. We implement and evaluate a Mask R-CNN-based\napproach on a complete pen manufacturing line at ZHAW, addressing three\ncritical challenges: object detection without positional constraints,\nrobustness to extreme lighting variations, and reliable performance with\ncost-effective cameras. Our system achieves 95% detection accuracy across\ndiverse lighting conditions while eliminating the need for structured component\nplacement, demonstrating a 30% reduction in setup time and significant\nimprovement in manufacturing flexibility. The approach is validated through\nextensive testing under four distinct lighting scenarios, showing practical\napplicability for real-world industrial deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMask R-CNN\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u4f7f\u5de5\u4e1a\u673a\u5668\u4eba\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u68c0\u6d4b\u548c\u6293\u53d6\u7b14\u96f6\u4ef6\uff0c\u65e0\u9700\u56fa\u5b9a\u4f4d\u7f6e\u7ea6\u675f\uff0c\u5e76\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a4.0\u4e2d\u7684\u67d4\u6027\u5236\u9020\u7cfb\u7edf\u9700\u8981\u673a\u5668\u4eba\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5904\u7406\u7269\u4f53\uff0c\u800c\u65e0\u9700\u4e25\u683c\u7684\u5b9a\u4f4d\u7ea6\u675f\u3002", "method": "\u91c7\u7528Mask R-CNN\u65b9\u6cd5\uff0c\u5728ZHAW\u7684\u5b8c\u6574\u7b14\u5236\u9020\u7ebf\u4e0a\u5b9e\u73b0\u5e76\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u65e0\u4f4d\u7f6e\u7ea6\u675f\u7684\u7269\u4f53\u68c0\u6d4b\u3001\u6781\u7aef\u5149\u7167\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u4f4e\u6210\u672c\u6444\u50cf\u5934\u7684\u53ef\u9760\u6027\u80fd\u4e09\u5927\u6311\u6218\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u8fbe\u523095%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u6d88\u9664\u4e86\u7ed3\u6784\u5316\u96f6\u4ef6\u653e\u7f6e\u7684\u9700\u6c42\uff0c\u51cf\u5c11\u4e8630%\u7684\u5b89\u88c5\u65f6\u95f4\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5236\u9020\u7075\u6d3b\u6027\u3002", "conclusion": "\u901a\u8fc7\u56db\u79cd\u4e0d\u540c\u5149\u7167\u573a\u666f\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2507.00861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00861", "abs": "https://arxiv.org/abs/2507.00861", "authors": ["Xiaoshuai Hao", "Lingdong Kong", "Rong Yin", "Pengwei Wang", "Jing Zhang", "Yunfeng Diao", "Shu Zhao"], "title": "SafeMap: Robust HD Map Construction from Incomplete Observations", "comment": "Accepted by ICML 2025", "summary": "Robust high-definition (HD) map construction is vital for autonomous driving,\nyet existing methods often struggle with incomplete multi-view camera data.\nThis paper presents SafeMap, a novel framework specifically designed to secure\naccuracy even when certain camera views are missing. SafeMap integrates two key\ncomponents: the Gaussian-based Perspective View Reconstruction (G-PVR) module\nand the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.\nG-PVR leverages prior knowledge of view importance to dynamically prioritize\nthe most informative regions based on the relationships among available camera\nviews. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV\nrepresentations derived from incomplete observations. Together, these\ncomponents facilitate the end-to-end map reconstruction and robust HD map\ngeneration. SafeMap is easy to implement and integrates seamlessly into\nexisting systems, offering a plug-and-play solution for enhanced robustness.\nExperimental results demonstrate that SafeMap significantly outperforms\nprevious methods in both complete and incomplete scenarios, highlighting its\nsuperior performance and reliability.", "AI": {"tldr": "SafeMap\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7G-PVR\u548cD-BEVC\u6a21\u5757\u89e3\u51b3\u591a\u89c6\u89d2\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u76f8\u673a\u6570\u636e\u4e0d\u5b8c\u6574\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cSafeMap\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u786e\u4fdd\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u7684\u51c6\u786e\u6027\u3002", "method": "SafeMap\u6574\u5408\u4e86G-PVR\u6a21\u5757\uff08\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u52a8\u6001\u4f18\u5148\u5904\u7406\u4fe1\u606f\u533a\u57df\uff09\u548cD-BEVC\u6a21\u5757\uff08\u5229\u7528\u5168\u666fBEV\u7279\u5f81\u6821\u6b63\u4e0d\u5b8c\u6574\u89c2\u6d4b\u7684BEV\u8868\u793a\uff09\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5730\u56fe\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeMap\u5728\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u573a\u666f\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "SafeMap\u662f\u4e00\u79cd\u6613\u4e8e\u5b9e\u73b0\u4e14\u517c\u5bb9\u73b0\u6709\u7cfb\u7edf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u589e\u5f3a\u7684\u5373\u63d2\u5373\u7528\u65b9\u6848\u3002"}}
{"id": "2507.00868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00868", "abs": "https://arxiv.org/abs/2507.00868", "authors": ["Simon Rei\u00df", "Zdravko Marinov", "Alexander Jaus", "Constantin Seibold", "M. Saquib Sarfraz", "Erik Rodner", "Rainer Stiefelhagen"], "title": "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?", "comment": "Accepted to ICCV 2025", "summary": "In this paper, we explore the potential of visual in-context learning to\nenable a single model to handle multiple tasks and adapt to new tasks during\ntest time without re-training. Unlike previous approaches, our focus is on\ntraining in-context learners to adapt to sequences of tasks, rather than\nindividual tasks. Our goal is to solve complex tasks that involve multiple\nintermediate steps using a single model, allowing users to define entire vision\npipelines flexibly at test time. To achieve this, we first examine the\nproperties and limitations of visual in-context learning architectures, with a\nparticular focus on the role of codebooks. We then introduce a novel method for\ntraining in-context learners using a synthetic compositional task generation\nengine. This engine bootstraps task sequences from arbitrary segmentation\ndatasets, enabling the training of visual in-context learners for compositional\ntasks. Additionally, we investigate different masking-based training objectives\nto gather insights into how to train models better for solving complex,\ncompositional tasks. Our exploration not only provides important insights\nespecially for multi-modal medical task sequences but also highlights\nchallenges that need to be addressed.", "AI": {"tldr": "\u63a2\u7d22\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4f7f\u5355\u4e00\u6a21\u578b\u80fd\u5904\u7406\u591a\u4efb\u52a1\u5e76\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u76ee\u6807\u662f\u8ba9\u6a21\u578b\u9002\u5e94\u4efb\u52a1\u5e8f\u5217\u800c\u975e\u5355\u4e2a\u4efb\u52a1\uff0c\u89e3\u51b3\u6d89\u53ca\u591a\u6b65\u9aa4\u7684\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u7075\u6d3b\u5b9a\u4e49\u89c6\u89c9\u6d41\u7a0b\u3002", "method": "\u7814\u7a76\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u67b6\u6784\u7684\u7279\u6027\u4e0e\u9650\u5236\uff0c\u63d0\u51fa\u57fa\u4e8e\u5408\u6210\u7ec4\u5408\u4efb\u52a1\u751f\u6210\u5f15\u64ce\u7684\u65b0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u63a9\u7801\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u4e3a\u591a\u6a21\u6001\u533b\u7597\u4efb\u52a1\u5e8f\u5217\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u9700\u89e3\u51b3\u7684\u6311\u6218\u3002", "conclusion": "\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u591a\u4efb\u52a1\u9002\u5e94\u548c\u590d\u6742\u4efb\u52a1\u89e3\u51b3\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.00886", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00886", "abs": "https://arxiv.org/abs/2507.00886", "authors": ["Anna-Maria Halacheva", "Jan-Nico Zaech", "Xi Wang", "Danda Pani Paudel", "Luc Van Gool"], "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond", "comment": null, "summary": "As multimodal language models advance, their application to 3D scene\nunderstanding is a fast-growing frontier, driving the development of 3D\nVision-Language Models (VLMs). Current methods show strong dependence on object\ndetectors, introducing processing bottlenecks and limitations in taxonomic\nflexibility. To address these limitations, we propose a scene-centric 3D VLM\nfor 3D Gaussian splat scenes that employs language- and task-aware scene\nrepresentations. Our approach directly embeds rich linguistic features into the\n3D scene representation by associating language with each Gaussian primitive,\nachieving early modality alignment. To process the resulting dense\nrepresentations, we introduce a dual sparsifier that distills them into\ncompact, task-relevant tokens via task-guided and location-guided pathways,\nproducing sparse, task-aware global and local scene tokens. Notably, we present\nthe first Gaussian splatting-based VLM, leveraging photorealistic 3D\nrepresentations derived from standard RGB images, demonstrating strong\ngeneralization: it improves performance of prior 3D VLM five folds, in\nout-of-the-domain settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u573a\u666f\u4e2d\u5fc33D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u901a\u8fc7\u8bed\u8a00\u548c\u4efb\u52a1\u611f\u77e5\u7684\u573a\u666f\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4f9d\u8d56\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u5bfc\u81f4\u5904\u7406\u74f6\u9888\u548c\u5206\u7c7b\u7075\u6d3b\u6027\u53d7\u9650\u3002", "method": "\u76f4\u63a5\u5d4c\u5165\u8bed\u8a00\u7279\u5f81\u52303D\u573a\u666f\u8868\u793a\u4e2d\uff0c\u901a\u8fc7\u4efb\u52a1\u548c\u4f4d\u7f6e\u5f15\u5bfc\u7684\u53cc\u91cd\u7a00\u758f\u5316\u5668\u751f\u6210\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u4ee4\u724c\u3002", "result": "\u5728\u57df\u5916\u8bbe\u7f6e\u4e2d\uff0c\u6027\u80fd\u6bd4\u73b0\u67093D VLM\u63d0\u5347\u4e94\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5229\u7528\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u76843D\u573a\u666f\u7406\u89e3\u548c\u8bed\u8a00\u5bf9\u9f50\u3002"}}
{"id": "2507.00916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00916", "abs": "https://arxiv.org/abs/2507.00916", "authors": ["Tianshi Cao", "Marie-Julie Rakotosaona", "Ben Poole", "Federico Tombari", "Michael Niemeyer"], "title": "Masks make discriminative models great again!", "comment": null, "summary": "We present Image2GS, a novel approach that addresses the challenging problem\nof reconstructing photorealistic 3D scenes from a single image by focusing\nspecifically on the image-to-3D lifting component of the reconstruction\nprocess. By decoupling the lifting problem (converting an image to a 3D model\nrepresenting what is visible) from the completion problem (hallucinating\ncontent not present in the input), we create a more deterministic task suitable\nfor discriminative models. Our method employs visibility masks derived from\noptimized 3D Gaussian splats to exclude areas not visible from the source view\nduring training. This masked training strategy significantly improves\nreconstruction quality in visible regions compared to strong baselines.\nNotably, despite being trained only on masked regions, Image2GS remains\ncompetitive with state-of-the-art discriminative models trained on full target\nimages when evaluated on complete scenes. Our findings highlight the\nfundamental struggle discriminative models face when fitting unseen regions and\ndemonstrate the advantages of addressing image-to-3D lifting as a distinct\nproblem with specialized techniques.", "AI": {"tldr": "Image2GS\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u56fe\u50cf\u91cd\u5efa\u903c\u771f3D\u573a\u666f\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u56fe\u50cf\u52303D\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u53ef\u89c1\u533a\u57df\u91cd\u5efa\u548c\u672a\u53ef\u89c1\u533a\u57df\u8865\u5168\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u56fe\u50cf\u91cd\u5efa3D\u573a\u666f\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u56fe\u50cf\u52303D\u8f6c\u6362\u7684\u786e\u5b9a\u6027\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u76843D\u9ad8\u65af\u6591\u70b9\u751f\u6210\u7684\u53ef\u89c1\u6027\u63a9\u7801\uff0c\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u4e0d\u53ef\u89c1\u533a\u57df\uff0c\u4e13\u6ce8\u4e8e\u53ef\u89c1\u533a\u57df\u7684\u91cd\u5efa\u3002", "result": "\u5728\u53ef\u89c1\u533a\u57df\u7684\u91cd\u5efa\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u5b8c\u6574\u573a\u666f\u8bc4\u4f30\u4e2d\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u56fe\u50cf\u52303D\u8f6c\u6362\u4f5c\u4e3a\u72ec\u7acb\u95ee\u9898\u5904\u7406\uff0c\u5e76\u91c7\u7528\u4e13\u95e8\u6280\u672f\uff0c\u80fd\u6709\u6548\u63d0\u5347\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2507.00950", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.00950", "abs": "https://arxiv.org/abs/2507.00950", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "MVP: Winning Solution to SMP Challenge 2025 Video Track", "comment": null, "summary": "Social media platforms serve as central hubs for content dissemination,\nopinion expression, and public engagement across diverse modalities. Accurately\npredicting the popularity of social media videos enables valuable applications\nin content recommendation, trend detection, and audience engagement. In this\npaper, we present Multimodal Video Predictor (MVP), our winning solution to the\nVideo Track of the SMP Challenge 2025. MVP constructs expressive post\nrepresentations by integrating deep video features extracted from pretrained\nmodels with user metadata and contextual information. The framework applies\nsystematic preprocessing techniques, including log-transformations and outlier\nremoval, to improve model robustness. A gradient-boosted regression model is\ntrained to capture complex patterns across modalities. Our approach ranked\nfirst in the official evaluation of the Video Track, demonstrating its\neffectiveness and reliability for multimodal video popularity prediction on\nsocial platforms. The source code is available at\nhttps://anonymous.4open.science/r/SMPDVideo.", "AI": {"tldr": "MVP\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u89c6\u9891\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u89c6\u9891\u7279\u5f81\u3001\u7528\u6237\u5143\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u89c6\u9891\u6d41\u884c\u5ea6\u9884\u6d4b\uff0c\u5e76\u5728SMP Challenge 2025\u4e2d\u83b7\u80dc\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u89c6\u9891\u7684\u6d41\u884c\u5ea6\u9884\u6d4b\u5bf9\u5185\u5bb9\u63a8\u8350\u3001\u8d8b\u52bf\u68c0\u6d4b\u548c\u7528\u6237\u53c2\u4e0e\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "MVP\u6574\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6df1\u5ea6\u89c6\u9891\u7279\u5f81\u3001\u7528\u6237\u5143\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u7cfb\u7edf\u9884\u5904\u7406\uff08\u5982\u5bf9\u6570\u53d8\u6362\u548c\u5f02\u5e38\u503c\u53bb\u9664\uff09\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u6a21\u578b\u6355\u6349\u591a\u6a21\u6001\u590d\u6742\u6a21\u5f0f\u3002", "result": "MVP\u5728SMP Challenge 2025\u7684Video Track\u5b98\u65b9\u8bc4\u4f30\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "MVP\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u9760\u7684\u591a\u6a21\u6001\u89c6\u9891\u6d41\u884c\u5ea6\u9884\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2507.00980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00980", "abs": "https://arxiv.org/abs/2507.00980", "authors": ["Yuheng Du", "Sheng Yang", "Lingxuan Wang", "Zhenghua Hou", "Chengying Cai", "Zhitao Tan", "Mingxia Chen", "Shi-Sheng Huang", "Qiang Li"], "title": "RTMap: Real-Time Recursive Mapping with Change Detection and Localization", "comment": null, "summary": "While recent online HD mapping methods relieve burdened offline pipelines and\nsolve map freshness, they remain limited by perceptual inaccuracies, occlusion\nin dense traffic, and an inability to fuse multi-agent observations. We propose\nRTMap to enhance these single-traversal methods by persistently crowdsourcing a\nmulti-traversal HD map as a self-evolutional memory. On onboard agents, RTMap\nsimultaneously addresses three core challenges in an end-to-end fashion: (1)\nUncertainty-aware positional modeling for HD map elements, (2)\nprobabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)\nreal-time detection for possible road structural changes. Experiments on\nseveral public autonomous driving datasets demonstrate our solid performance on\nboth the prior-aided map quality and the localization accuracy, demonstrating\nour effectiveness of robustly serving downstream prediction and planning\nmodules while gradually improving the accuracy and freshness of the\ncrowdsourced prior-map asynchronously. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RTMap (Camera ready version\nincorporating reviewer suggestions will be updated soon).", "AI": {"tldr": "RTMap\u901a\u8fc7\u591a\u8f68\u8ff9\u4f17\u5305\u9ad8\u7cbe\u5730\u56fe\uff0c\u89e3\u51b3\u5355\u6b21\u904d\u5386\u65b9\u6cd5\u7684\u611f\u77e5\u4e0d\u51c6\u786e\u3001\u906e\u6321\u548c\u591a\u667a\u80fd\u4f53\u89c2\u6d4b\u878d\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u65b9\u6cd5\u5b58\u5728\u611f\u77e5\u4e0d\u51c6\u786e\u3001\u906e\u6321\u548c\u65e0\u6cd5\u878d\u5408\u591a\u667a\u80fd\u4f53\u89c2\u6d4b\u7684\u5c40\u9650\u6027\u3002", "method": "RTMap\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4f4d\u7f6e\u5efa\u6a21\u3001\u6982\u7387\u611f\u77e5\u7684\u5b9a\u4f4d\u548c\u5b9e\u65f6\u9053\u8def\u7ed3\u6784\u53d8\u5316\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRTMap\u5728\u4f17\u5305\u5730\u56fe\u8d28\u91cf\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u4e0b\u6e38\u9884\u6d4b\u548c\u89c4\u5212\u6a21\u5757\u3002", "conclusion": "RTMap\u80fd\u5f02\u6b65\u63d0\u5347\u4f17\u5305\u5730\u56fe\u7684\u51c6\u786e\u6027\u548c\u65b0\u9c9c\u5ea6\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.00981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00981", "abs": "https://arxiv.org/abs/2507.00981", "authors": ["Jack Nugent", "Siyang Wu", "Zeyu Ma", "Beining Han", "Meenal Parakh", "Abhishek Joshi", "Lingjie Mei", "Alexander Raistrick", "Xinyuan Li", "Jia Deng"], "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations", "comment": null, "summary": "Recent years have witnessed substantial progress on monocular depth\nestimation, particularly as measured by the success of large models on standard\nbenchmarks. However, performance on standard benchmarks does not offer a\ncomplete assessment, because most evaluate accuracy but not robustness. In this\nwork, we introduce PDE (Procedural Depth Evaluation), a new benchmark which\nenables systematic robustness evaluation. PDE uses procedural generation to\ncreate 3D scenes that test robustness to various controlled perturbations,\nincluding object, camera, material and lighting changes. Our analysis yields\ninteresting findings on what perturbations are challenging for state-of-the-art\ndepth models, which we hope will inform further research. Code and data are\navailable at https://github.com/princeton-vl/proc-depth-eval.", "AI": {"tldr": "PDE\uff08Procedural Depth Evaluation\uff09\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u7a0b\u5e8f\u751f\u62103D\u573a\u666f\u6d4b\u8bd5\u591a\u79cd\u6270\u52a8\u3002", "motivation": "\u73b0\u6709\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u9c81\u68d2\u6027\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u591a\u79cd\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u7a0b\u5e8f\u751f\u6210\u6280\u672f\u521b\u5efa3D\u573a\u666f\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u7269\u4f53\u3001\u76f8\u673a\u3001\u6750\u8d28\u548c\u5149\u7167\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u5148\u8fdb\u6df1\u5ea6\u6a21\u578b\u5728\u7279\u5b9a\u6270\u52a8\u4e0b\u7684\u6311\u6218\u6027\u8868\u73b0\u3002", "conclusion": "PDE\u4e3a\u6df1\u5ea6\u4f30\u8ba1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.00992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00992", "abs": "https://arxiv.org/abs/2507.00992", "authors": ["Yuanrui Wang", "Cong Han", "YafeiLi", "Zhipeng Jin", "Xiawei Li", "SiNan Du", "Wen Tao", "Yi Yang", "shuanglong li", "Chun Yuan", "Liu Lin"], "title": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis", "comment": "Accepted by ICCV 2025", "summary": "Text-to-image generation has greatly advanced content creation, yet\naccurately rendering visual text remains a key challenge due to blurred glyphs,\nsemantic drift, and limited style control. Existing methods often rely on\npre-rendered glyph images as conditions, but these struggle to retain original\nfont styles and color cues, necessitating complex multi-branch designs that\nincrease model overhead and reduce flexibility. To address these issues, we\npropose a segmentation-guided framework that uses pixel-level visual text masks\n-- rich in glyph shape, color, and spatial detail -- as unified conditional\ninputs. Our method introduces two core components: (1) a fine-tuned bilingual\nsegmentation model for precise text mask extraction, and (2) a streamlined\ndiffusion model augmented with adaptive glyph conditioning and a\nregion-specific loss to preserve textual fidelity in both content and style.\nOur approach achieves state-of-the-art performance on the AnyText benchmark,\nsignificantly surpassing prior methods in both Chinese and English settings. To\nenable more rigorous evaluation, we also introduce two new benchmarks:\nGlyphMM-benchmark for testing layout and glyph consistency in complex\ntypesetting, and MiniText-benchmark for assessing generation quality in\nsmall-scale text regions. Experimental results show that our model outperforms\nexisting methods by a large margin in both scenarios, particularly excelling at\nsmall text rendering and complex layout preservation, validating its strong\ngeneralization and deployment readiness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u5f15\u5bfc\u7684\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u89c6\u89c9\u6587\u672c\u63a9\u7801\u4f5c\u4e3a\u7edf\u4e00\u6761\u4ef6\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b57\u4f53\u98ce\u683c\u548c\u989c\u8272\u4fdd\u7559\u4e0a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u751f\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6e32\u67d3\u89c6\u89c9\u6587\u672c\u65f6\u5b58\u5728\u5b57\u4f53\u6a21\u7cca\u3001\u8bed\u4e49\u6f02\u79fb\u548c\u98ce\u683c\u63a7\u5236\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u9884\u6e32\u67d3\u5b57\u5f62\u56fe\u50cf\u5bfc\u81f4\u6a21\u578b\u590d\u6742\u6027\u548c\u7075\u6d3b\u6027\u964d\u4f4e\u3002", "method": "\u91c7\u7528\u5206\u5272\u5f15\u5bfc\u6846\u67b6\uff0c\u5305\u62ec\u5fae\u8c03\u7684\u53cc\u8bed\u5206\u5272\u6a21\u578b\u7528\u4e8e\u7cbe\u786e\u63d0\u53d6\u6587\u672c\u63a9\u7801\uff0c\u4ee5\u53ca\u4f18\u5316\u7684\u6269\u6563\u6a21\u578b\u7ed3\u5408\u81ea\u9002\u5e94\u5b57\u5f62\u6761\u4ef6\u548c\u533a\u57df\u7279\u5b9a\u635f\u5931\u3002", "result": "\u5728AnyText\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u65b0\u5f15\u5165\u7684GlyphMM\u548cMiniText\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u9886\u5148\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u590d\u6742\u5e03\u5c40\u548c\u5c0f\u6587\u672c\u6e32\u67d3\u65b9\u9762\uff0c\u9a8c\u8bc1\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u90e8\u7f72\u51c6\u5907\u6027\u3002"}}
{"id": "2507.01009", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01009", "abs": "https://arxiv.org/abs/2507.01009", "authors": ["Anna Foix Romero", "Craig Russell", "Alexander Krull", "Virginie Uhlmann"], "title": "ShapeEmbed: a self-supervised learning framework for 2D contour quantification", "comment": null, "summary": "The shape of objects is an important source of visual information in a wide\nrange of applications. One of the core challenges of shape quantification is to\nensure that the extracted measurements remain invariant to transformations that\npreserve an object's intrinsic geometry, such as changing its size,\norientation, and position in the image. In this work, we introduce ShapeEmbed,\na self-supervised representation learning framework designed to encode the\ncontour of objects in 2D images, represented as a Euclidean distance matrix,\ninto a shape descriptor that is invariant to translation, scaling, rotation,\nreflection, and point indexing. Our approach overcomes the limitations of\ntraditional shape descriptors while improving upon existing state-of-the-art\nautoencoder-based approaches. We demonstrate that the descriptors learned by\nour framework outperform their competitors in shape classification tasks on\nnatural and biological images. We envision our approach to be of particular\nrelevance to biological imaging applications.", "AI": {"tldr": "ShapeEmbed\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7f16\u78012D\u56fe\u50cf\u4e2d\u7269\u4f53\u7684\u8f6e\u5ed3\uff0c\u751f\u6210\u5bf9\u5e73\u79fb\u3001\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u53cd\u5c04\u548c\u70b9\u7d22\u5f15\u4e0d\u53d8\u7684\u5f62\u72b6\u63cf\u8ff0\u7b26\u3002", "motivation": "\u89e3\u51b3\u5f62\u72b6\u91cf\u5316\u4e2d\u63d0\u53d6\u7684\u6d4b\u91cf\u503c\u5bf9\u4fdd\u6301\u7269\u4f53\u56fa\u6709\u51e0\u4f55\u7684\u53d8\u6362\uff08\u5982\u5927\u5c0f\u3001\u65b9\u5411\u548c\u4f4d\u7f6e\u53d8\u5316\uff09\u4e0d\u53d8\u6027\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u77e9\u9635\u8868\u793a\u7269\u4f53\u8f6e\u5ed3\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6ShapeEmbed\u751f\u6210\u4e0d\u53d8\u5f62\u72b6\u63cf\u8ff0\u7b26\u3002", "result": "ShapeEmbed\u751f\u6210\u7684\u63cf\u8ff0\u7b26\u5728\u81ea\u7136\u548c\u751f\u7269\u56fe\u50cf\u7684\u5f62\u72b6\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ShapeEmbed\u5728\u751f\u7269\u6210\u50cf\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.01012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01012", "abs": "https://arxiv.org/abs/2507.01012", "authors": ["Zhe Kong", "Le Li", "Yong Zhang", "Feng Gao", "Shaoshu Yang", "Tao Wang", "Kaihao Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Guanying Chen", "Wenhan Luo"], "title": "DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution", "comment": "Accepted by ACM SIGGRAPH 2025, Homepage:\n  https://kongzhecn.github.io/projects/dam-vsr/ Github:\n  https://github.com/kongzhecn/DAM-VSR", "summary": "Real-world video super-resolution (VSR) presents significant challenges due\nto complex and unpredictable degradations. Although some recent methods utilize\nimage diffusion models for VSR and have shown improved detail generation\ncapabilities, they still struggle to produce temporally consistent frames. We\nattempt to use Stable Video Diffusion (SVD) combined with ControlNet to address\nthis issue. However, due to the intrinsic image-animation characteristics of\nSVD, it is challenging to generate fine details using only low-quality videos.\nTo tackle this problem, we propose DAM-VSR, an appearance and motion\ndisentanglement framework for VSR. This framework disentangles VSR into\nappearance enhancement and motion control problems. Specifically, appearance\nenhancement is achieved through reference image super-resolution, while motion\ncontrol is achieved through video ControlNet. This disentanglement fully\nleverages the generative prior of video diffusion models and the detail\ngeneration capabilities of image super-resolution models. Furthermore, equipped\nwith the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can\nconduct VSR on longer input videos. DAM-VSR achieves state-of-the-art\nperformance on real-world data and AIGC data, demonstrating its powerful detail\ngeneration capabilities.", "AI": {"tldr": "DAM-VSR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5916\u89c2\u548c\u8fd0\u52a8\u89e3\u8026\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Stable Video Diffusion\u548cControlNet\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u751f\u6210\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u9762\u4e34\u590d\u6742\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u9000\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u751f\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDAM-VSR\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5206\u89e3\u4e3a\u5916\u89c2\u589e\u5f3a\uff08\u901a\u8fc7\u53c2\u8003\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff09\u548c\u8fd0\u52a8\u63a7\u5236\uff08\u901a\u8fc7\u89c6\u9891ControlNet\uff09\uff0c\u5e76\u91c7\u7528\u8fd0\u52a8\u5bf9\u9f50\u7684\u53cc\u5411\u91c7\u6837\u7b56\u7565\u3002", "result": "DAM-VSR\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548cAIGC\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u7ec6\u8282\u751f\u6210\u80fd\u529b\u3002", "conclusion": "DAM-VSR\u901a\u8fc7\u89e3\u8026\u5916\u89c2\u548c\u8fd0\u52a8\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u751f\u6210\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
