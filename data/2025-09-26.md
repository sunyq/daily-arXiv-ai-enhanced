<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 87]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 18]
- [econ.EM](#econ.EM) [Total: 2]
- [econ.TH](#econ.TH) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的重构框架，用于动态验证和组装时间触发系统的调度，解决消息碰撞、优先级处理错误等问题，确保系统安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 时间触发系统在动态操作环境中面临消息碰撞、优先级处理导致的锁定循环、以及不完整或无效调度等挑战，这些会损害系统安全性和性能。

Method: 提出重构框架，通过系统地将AI生成或启发式得到的调度优先级转换为完全可执行的调度，确保遵守优先级规则和无碰撞通信等关键系统约束，包含安全检查、高效分配算法和恢复机制。

Result: 在多性能配置文件上的综合实验表明，该框架显著提高了系统适应性、操作完整性和运行时性能，同时保持计算效率。

Conclusion: 这项工作为安全关键时间触发系统中的安全调度生成问题提供了一个实用且可扩展的解决方案，即使在高度动态和不确定的操作条件下也能实现可靠和灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [2] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出一种集成在元调度器中的自适应在线学习单元，使用强化学习来实时扩展多调度图，解决离线训练AI调度推理时难以构建全面MSG的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练方法在构建包含所有可能场景的全面多调度图时面临资源密集和不可行的挑战，特别是在考虑硬件故障、松弛变化等上下文事件时。

Method: 在元调度器中集成自适应在线学习单元，采用多种强化学习模型进行实时训练，持续探索新的调度解决方案并扩展多调度图。

Result: 在线学习单元能够动态适应意外事件和复杂调度场景，发现新解决方案并优化现有调度器，特别是在引入更严格截止时间或新性能标准时。

Conclusion: 通过实时训练持续优化AI推理，系统保持灵活性，能够满足不断变化的需求，确保大规模安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [3] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种基于时序逻辑的AI智能体行为监控语言，用于检测LLM智能体系统中的行为异常，通过监控工具调用序列和状态转换来验证系统行为模式。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本匹配的错误检测方法对LLM输出的自然语言变异性很脆弱，需要一种独立于具体文本输出的行为验证方法。

Method: 使用时序逻辑技术监控智能体的工具调用序列和状态转换，定义正确行为模式的时序断言，用于开发和回归测试。

Result: 在三智能体系统中测试，大模型满足所有时序断言，而小模型出现工具序列错误和协调失败，时序表达式成功标记了这些异常。

Conclusion: 该方法为关键应用中AI智能体可靠性的系统化监控提供了基础。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [4] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出了一种名为LATTS的自适应测试时间扩展方法，通过验证器模型根据每个生成步骤的局部难度动态调整计算资源分配，实现更高效的准确率-计算量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器方法在测试时对所有样本和生成步骤采用均匀计算分配，不考虑个体实例的复杂性，导致资源使用效率低下。

Method: LATTS在每个生成步骤使用验证器接受准则来决定是否重新采样、回溯、重启或停止生成过程，基于验证器模型提供的局部难度概念动态调整每步计算量。

Result: 实验结果表明，LATTS相比标准验证器方法实现了显著更优的准确率-计算量权衡。

Conclusion: 局部自适应测试时间扩展方法能够更有效地利用计算资源，在保持准确率的同时减少不必要的计算开销。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [5] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 该论文提出哲学指导的机器学习(PhIML)，将分析哲学的核心思想直接融入机器学习模型架构、目标和评估协议中，旨在设计出尊重哲学概念和价值观的模型。


<details>
  <summary>Details</summary>
Motivation: 通过将哲学思想融入机器学习，使模型能够更好地理解和尊重哲学概念与价值观，实现哲学增益和对齐，从而开发出更安全、更符合伦理的机器学习系统。

Method: 回顾概念基础以展示哲学增益和对齐，并通过案例研究展示ML用户/设计者如何将PhIML作为后处理工具或内在地构建到ML模型架构中。

Result: 提出了PhIML的概念框架和应用方法，展示了哲学思想在机器学习中的实际应用可能性。

Conclusion: 指出了PhIML面临的技术障碍以及哲学、实践和治理挑战，并制定了实现安全、哲学意识和伦理负责的PhIML的研究路线图。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [6] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE是一个AI驱动的阅读助手工具，旨在为研究人员提供结构化、简洁的论文关键要素概览，而不是替代阅读原文。


<details>
  <summary>Details</summary>
Motivation: 科学文献的激增给研究人员带来了挑战，现有工具提供的冗长摘要往往替代而非辅助阅读。

Method: 系统将专家的阅读方法嵌入核心AI逻辑，采用提示驱动的方法论，提供结构化洞察作为论文关键要素的"地图"。

Result: 定性案例研究表明，与通用LLM相比，InsightGUIDE能产生更结构化、更具操作性的指导。

Conclusion: InsightGUIDE作为现代研究人员更有效的工具，能提供更好的阅读辅助体验。

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [7] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: LogReasoner是一个粗到细的推理增强框架，旨在让LLM能够像专家一样进行日志分析推理，通过两阶段增强方法显著提升LLM在日志分析任务中的性能


<details>
  <summary>Details</summary>
Motivation: 通用LLM难以制定符合专家认知的结构化推理流程，无法提供详细的推理步骤细节，这限制了它们在日志分析中的应用

Method: LogReasoner包含两个阶段：1）粗粒度专家思维增强，从故障排除流程图构建高层专家思维；2）细粒度具体步骤增强，通过任务特定步骤解决方案微调LLM，并使用偏好学习校准推理细节

Result: 在四个不同日志分析任务上的实验表明，LogReasoner显著优于现有LLM，达到了最先进的性能

Conclusion: LogReasoner框架有效增强了LLM在日志分析中的推理能力，使其能够像专家一样进行结构化推理

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [8] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 提出了一种用于肌电假手控制的新型识别系统，通过检测受污染的肌电信号来减轻污染对分类质量的不利影响。


<details>
  <summary>Details</summary>
Motivation: 现代仿生上肢假肢通常使用肌电信号进行模式识别控制，但肌电信号易受污染，这会显著降低识别系统的分类质量。

Method: 系统包含两个集成：一组用于评估各通道污染程度的单类分类器，以及一个用于识别患者意图的K近邻分类器集成。开发了统一的模糊决策模型。

Result: 使用公共存储库中的真实肌电信号进行实验评估，对开发方法的参数和程序进行了比较分析。

Conclusion: 所提出的模糊识别系统与文献中类似系统相比具有优势，能够有效处理肌电信号污染问题。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [9] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: SAMULE是一个基于多级反思合成的自学习智能体框架，通过训练回顾性语言模型来生成高质量反思，在三个挑战性基准测试中显著优于基于反思的基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体发展迅速，但在复杂任务中仍面临生成有意义的反思的挑战，主要由于错误分析不足和对罕见成功轨迹的依赖。

Method: 提出SAMULE框架，通过三个互补级别的反思合成：单轨迹学习（微观级）进行详细错误纠正；任务内学习（中观级）构建错误分类法；任务间学习（宏观级）提取可转移的洞察。然后微调语言模型作为回顾性模型在推理时生成反思。

Result: 在TravelPlanner、NATURAL PLAN和Tau-bench三个挑战性基准测试上的广泛实验表明，该方法显著优于基于反思的基线方法。

Conclusion: 研究结果强调了精心设计的反思合成和以失败为中心的学习在构建自改进LLM智能体中的关键作用。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [10] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于自主目标驱动智能体的自适应网络安全架构，通过智能AI代理实现动态学习和上下文感知决策，以解决传统静态模型在可扩展性、实时检测和上下文响应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统静态网络安全模型在当前包含云服务、API、移动平台和边缘设备的数字产品生态系统中，难以应对可扩展性、实时检测和上下文响应性的挑战。

Method: 构建了一个由智能AI代理驱动的自适应网络安全架构，集成了行为基线分析、去中心化风险评分和联邦威胁情报共享等关键功能，通过原生云模拟验证系统能力。

Result: 评估结果显示系统在适应性、响应延迟和检测准确性方面均有显著提升，能够有效识别零日攻击并动态调整访问策略。

Conclusion: 该架构为保护复杂数字基础设施提供了一个智能且可扩展的蓝图，与零信任模型兼容，支持遵守国际网络安全法规。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [11] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: 开发了Claim Advisor网络应用，利用大语言模型的上下文学习和微调技术来加速产品声明的创建过程，包括搜索、生成、优化和模拟功能。


<details>
  <summary>Details</summary>
Motivation: 产品声明是影响消费者购买行为的关键因素，但创建过程耗时耗资。需要一种能够加速声明创建的工具来改善效率和经济效益。

Method: 使用大语言模型的上下文学习和微调技术，开发具有三个功能的网络应用：语义搜索现有声明、基于产品描述和消费者画像生成/优化声明、通过合成消费者模拟来对声明进行排名。

Result: 在消费品包装公司的应用中显示出非常有前景的结果，证明该能力在不同产品类别和行业中具有广泛适用性。

Conclusion: 这项技术在不同行业具有广泛应用价值，鼓励生成式AI在不同行业的研究和应用。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [12] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 开发基于LLaMA-4 109B的检索增强生成系统，用于放疗治疗计划的自动化、协议感知和可解释评估


<details>
  <summary>Details</summary>
Motivation: 实现放疗治疗计划的透明、可扩展评估，结合结构化群体评分与模块化工具增强推理

Method: 构建多协议数据集和知识库，集成检索引擎、百分位预测组件和临床约束检查器，使用多步提示驱动推理管道

Result: 优化检索超参数后，系统在5百分位点范围内实现完美最近邻准确率，端到端测试显示100%与独立模块计算结果一致

Conclusion: 该系统提供可追溯输出，减少幻觉，跨协议表现稳健，未来将进行临床验证和改进领域适应检索模型

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [13] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: Fairy是一个交互式多代理移动助手，通过跨应用协作、交互式执行和持续学习来解决现有大模型在移动GUI代理中的局限性，在真实世界基准测试中显著提升了任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型方法在处理多样化应用界面和动态用户需求时存在困难，端到端方法在长尾应用上表现不佳，且缺乏用户交互的代理会损害用户体验。

Method: Fairy包含三个核心模块：(i)全局任务规划器进行跨应用任务分解；(ii)应用级执行器基于长短时记忆细化任务，通过四个核心代理在双循环中实现精确执行和用户交互；(iii)自学习器将执行经验整合到应用地图和技巧中。

Result: 使用GPT-4o骨干的Fairy相比之前最先进方法，用户需求完成率提高了33.7%，冗余步骤减少了58.5%。

Conclusion: Fairy通过交互和自我学习机制有效解决了移动GUI代理在真实场景中的挑战，展示了其在复杂移动应用环境中的优越性能。

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [14] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 提出了一种结合自回归（AR）和非自回归（NAR）语言模型的新框架，通过NAR模型高效生成中间推理轨迹，再由AR模型生成精确最终答案，在推理任务中实现26%的性能提升并显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: AR模型生成连贯但推理速度慢，NAR模型并行生成速度快但输出质量较低。为了解决这两种模型在推理密集型任务（如数学和代码）中的局限性，需要结合两者的优势。

Method: 采用NAR模型（如离散扩散模型）高效生成中间推理轨迹，然后由AR模型基于这些轨迹生成精确的最终答案，形成AR-NAR混合推理框架。

Result: 实验表明该方法相比强基线实现了26%的性能提升，同时大幅降低了推理成本。

Conclusion: AR-NAR混合框架有效结合了两种模型的优势，在保持高质量输出的同时显著提升了推理效率，为推理密集型任务提供了新的解决方案。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [15] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 提出了Meta-Memory，一个基于大语言模型的机器人记忆系统，能够通过语义和空间模态的联合推理来检索和整合记忆，以回答自然语言位置查询。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在复杂环境中有效存储观察记忆并利用这些记忆回答人类关于空间位置查询的关键研究挑战，现有工作缺乏高效记忆检索和整合的机制。

Method: 使用大语言模型驱动的代理构建高密度环境记忆表示，通过语义和空间模态的联合推理进行记忆检索和整合。

Result: 在SpaceLocQA和NaVQA基准测试中显著优于现有最先进方法，并在真实机器人平台上成功部署验证了实用性。

Conclusion: Meta-Memory为机器人提供了强大准确的空间推理能力，在复杂环境中具有实际应用价值。

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [16] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto是一个反事实推理框架，通过联合训练确保多模态语言模型在视觉语言推理中既给出正确答案又保持推理忠实性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在视觉推理中可能依赖无关或虚假区域得出正确答案，表明模型并未真正理解图像内容，推理忠实性存在严重问题。

Method: 提出三种互补的训练范式：正向、反事实和随机掩码训练；开发自动定位问题相关证据的流程；使用GRPO强化学习训练模型，设计三种互补奖励函数。

Result: 在多样化基准测试中，DeFacto显著提高了答案准确性和推理忠实性，建立了更可解释的多模态推理基础。

Conclusion: DeFacto框架有效解决了多模态推理中的忠实性问题，为可解释的视觉语言推理提供了更强的基础。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [17] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: GALAX是一个创新框架，通过强化学习将预训练图神经网络与大型语言模型集成，利用图过程奖励模型生成疾病相关子图，实现可解释的精准医学靶点和通路发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：数值组学忽略拓扑结构，文本中心LLM缺乏定量推理，图模型未充分利用节点语义和LLM泛化能力。需要整合多组学信号、拓扑结构和文本知识。

Method: 提出GALAX框架，通过GPRM引导的强化学习将预训练GNN集成到LLM中，以逐步方式生成疾病相关子图，由预训练GNN迭代评估，实现过程级监督。

Result: 开发了Target-QA基准，结合CRISPR识别靶点、多组学特征和生物医学图知识，支持长上下文推理和可扩展的生物学基础框架。

Conclusion: GALAX为精准医学提供了可靠、可解释的靶点和通路发现方法，通过强化引导的子图推理实现了数值证据、拓扑知识和语言上下文的整合。

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [18] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 提出基于大语言模型的移动应用评论分析框架，解决传统星级评分系统的局限性，通过结构化提示技术量化评分与文本情感差异，提取特征级洞察，并支持交互式探索。


<details>
  <summary>Details</summary>
Motivation: 传统星级评分系统无法捕捉详细评论中的细微反馈，传统NLP技术难以理解上下文细微差别、领域特定术语和讽刺等语言特征。

Method: 采用模块化框架，利用大语言模型结合结构化提示技术，通过检索增强的对话问答支持交互式评论探索。

Result: 在三个不同数据集上的综合实验表明，该方法显著超越基线方法，在具有挑战性和上下文丰富的评论场景中提高了准确性、鲁棒性和可操作性洞察。

Conclusion: 基于LLM的方法为移动应用评论分析提供了更准确、鲁棒和实用的解决方案，能够更好地理解复杂的情感表达和用户反馈。

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [19] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: AOT*是一个将LLM生成的化学合成路径与AND-OR树搜索相结合的反合成规划框架，显著提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 多步反合成规划面临指数级搜索空间和高推理成本的挑战，现有LLM方法在效率和成本方面存在限制。

Method: 通过原子级映射合成路径到AND-OR树组件，设计数学严谨的奖励分配策略和基于检索的上下文工程，使LLM能高效导航化学空间。

Result: 在多个合成基准测试中，AOT*达到SOTA性能，使用3-5倍更少的迭代次数实现竞争性解决率，对复杂分子目标的效率优势更明显。

Conclusion: AOT*通过整合LLM与系统树搜索，有效解决了反合成规划的计算挑战，为药物发现和材料设计提供了高效解决方案。

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [20] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 提出基于确定性有限自动机（DFA）的框架，通过工具使用路径评估AI代理行为，引入CORE指标套件量化执行模式对齐度


<details>
  <summary>Details</summary>
Motivation: 现有代理基准仅关注最终状态的二元判断，忽视了安全性、效率和中间正确性等关键方面，需要更全面的评估方法

Method: 使用确定性有限自动机（DFA）将任务编码为有效工具使用路径集合，构建CORE评估指标套件（路径正确性、复合路径正确性、前缀关键性、有害调用率、效率）

Result: 在不同世界模型中，该方法揭示了传统最终状态评估方案下看似等效的代理之间的重要性能差异

Conclusion: 基于DFA的框架和CORE指标能够对AI代理行为进行原则性评估，提供比传统方法更全面的性能分析

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [21] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: SciTrek是一个新的问答基准，用于评估大语言模型在科学文章上的长上下文推理能力，通过自动生成的复杂问题测试模型的信息聚合和综合能力。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文基准大多使用非科学文本、关注简单信息检索任务或采用人工构造的上下文，无法有效评估模型在真实科学文献上的复杂推理能力。

Method: 通过将问题构建为对文章元数据数据库的SQL查询来自动生成问题和答案，SQL操作提供可验证的推理步骤，支持扩展到100万token的上下文。

Result: 在多种开源和专有LLM上的实验表明，随着上下文长度增加，SciTrek带来显著挑战，监督微调和强化学习只能带来有限改进。模型在基本数值运算和长上下文中精确定位信息方面存在系统性缺陷。

Conclusion: SciTrek是一个具有挑战性的长上下文推理基准，揭示了当前LLM在科学文献处理方面的局限性，为未来模型改进提供了方向。

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [22] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE是一个神经符号框架，通过三个智能体协同优化知识图谱上的多跳问答，在资源约束下实现更高的准确率和更低的延迟与成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统知识图谱问答中静态扩展和长思考提示导致的过度检索、上下文膨胀和不可预测运行时问题，需要在保持溯源的同时平衡准确性、延迟和成本。

Method: 采用三智能体框架（子图架构师、路径导航器、上下文策展人），使用LC-MAPPO算法在资源预算下联合优化子图构建、推理路径发现和证据选择。

Result: 在HotpotQA、MetaQA和FactKG数据集上，CLAUSE在相同或更低token预算下获得更高EM@1，同时减少子图增长和端到端延迟。在MetaQA-2-hop上相比GraphRAG实现+39.3 EM@1提升，延迟降低18.6%，边增长降低40.9%。

Conclusion: CLAUSE能够生成紧凑、保持溯源的上下文，在部署约束下提供可预测的性能，实现准确性、延迟和成本之间的灵活权衡。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [23] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: 本文提出了一个评估大型语言模型创造力的理论框架和算法任务，重点考察输出的新颖性和实用性，并揭示了创造力在LLMs中的缩放规律、最优模型结构以及新颖性-实用性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念框架无法评估AI系统（特别是大型语言模型）在创造性任务（如科学创意生成）中的表现，因为创造性组合是一种开放式的能力，不能简单地用准确性或正确性来衡量。

Method: 提出了一个理论框架和算法任务，通过评估输出的新颖性和实用性程度来衡量创造力，并进行了多项实证研究，包括分析创造力的缩放行为、探索最优模型深度和宽度、以及研究新颖性-实用性权衡。

Result: 研究发现：(1)创造力在LLMs中存在缩放规律；(2)在固定计算预算下，存在创造能力的最优模型深度和宽度；(3)LLMs在创意生成和执行之间存在差距，这可由更基本的新颖性-实用性权衡来解释，且这种权衡在规模扩大时仍然存在。

Conclusion: 当前形式的LLMs在长期创造力方面存在局限性，研究为理解和改进现代AI模型的创造力提供了基础，标志着泛化能力的新前沿。

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [24] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [25] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: Recon-Act是一个基于侦察-行动行为范式的自进化多智能体框架，通过侦察团队对比错误轨迹与成功轨迹来生成通用工具，行动团队利用这些工具进行意图分解和执行，在VisualWebArena数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在处理真实网页多轮长轨迹任务时存在动作序列混乱和执行过程中过多试错的问题，需要提高智能体在未见网站上的适应性和长时程任务的解决能力。

Method: 提出侦察-行动行为范式，包含侦察团队和行动团队。侦察团队进行对比分析和工具生成，行动团队负责意图分解、工具编排和执行。通过对比错误与成功轨迹推断补救措施，抽象为通用工具并实时注册到工具库。

Result: Recon-Act显著提高了对未见网站的适应性和长时程任务的解决能力，在具有挑战性的VisualWebArena数据集上实现了最先进的性能。

Conclusion: Recon-Act通过侦察-行动闭环训练管道（数据-工具-行动-反馈）建立了自进化框架，目前达到第3级实现（有限人工干预），为智能浏览器使用智能体提供了有效解决方案。

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [26] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: TrustJudge是一个概率框架，解决了LLM作为评估者时的评分不一致问题，包括分数比较不一致和成对传递性不一致，通过分布敏感评分和似然感知聚合来提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为自动评估者的框架存在严重不一致问题，包括评分比较不一致和成对传递性不一致，这些不一致源于离散评分系统的信息损失和模糊的平局判断。

Method: 提出TrustJudge框架，包含两个关键创新：1）分布敏感评分，从离散评分概率计算连续期望，保留信息熵；2）似然感知聚合，使用双向偏好概率或困惑度解决传递性违规。

Result: 使用Llama-3.1-70B-Instruct作为评估者，TrustJudge将分数比较不一致降低8.43%（从23.32%到14.89%），成对传递性不一致降低10.82%（从15.22%到4.40%），同时保持更高的评估准确性。

Conclusion: TrustJudge提供了对LLM-as-a-judge范式中评估框架不一致性的首次系统分析，为可靠自动评估提供了理论见解和实用解决方案，在不同模型架构和规模上都表现出一致的改进。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [27] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理模式价值评估的数据选择方法，通过构建核心参考集和双粒度算法来筛选高质量链式思维数据，显著提升大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法对链式思维数据的使用缺乏选择性，不清楚哪些数据类型能最有效地增强模型的推理能力。本文旨在解决如何识别和利用高价值推理模式来扩展模型的推理潜力。

Method: 1. 定义推理潜力为正确回答问题所需独立尝试次数的倒数；2. 从CoT序列中抽象出具有共性和归纳能力的原子推理模式；3. 构建富含价值推理模式的核心参考集；4. 提出基于推理模式链和token熵的双粒度算法来选择高价值CoT数据。

Result: 仅使用100亿token的高价值CoT数据，就能让850亿参数的MoE模型在AIME 2024和2025上的表现提升9.58%，并将下游RL性能上限提高7.81%。

Conclusion: 通过有选择地使用富含高价值推理模式的CoT数据，可以更有效地训练模型掌握推理能力，显著提升数学推理性能。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [28] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出了一种新的分析框架，从推理路径角度研究强化学习(RL)和监督微调(SFT)如何塑造大语言模型的推理能力，发现RL压缩错误路径而SFT扩展正确路径，RL集中推理功能而SFT均匀分布推理功能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注RL和SFT对推理准确性的影响，但对它们如何具体塑造推理过程的理解仍然有限。本文旨在超越准确性分析，深入探究这两种训练方法如何改变推理路径的质性和量化特征。

Method: 提出两层次分析框架：轨迹级分析完整推理输出，步骤级分析推理图（节点对应单个推理步骤）。使用1.5B、7B和14B参数模型在数学领域进行实验，通过聚类和网络拓扑分析量化推理路径变化。

Result: RL压缩错误推理轨迹，SFT扩展正确轨迹；RL使节点访问频率、度数和中介中心性分布的衰减率增加约2.5倍，而SFT将其减少至约三分之一；RL将推理功能集中到少数步骤，SFT将其均匀分布到多个步骤。

Conclusion: 当前SFT后接RL的两阶段训练最佳实践的成功原因在于两者的互补作用：SFT建立广泛正确的推理基础，RL进一步优化和集中推理能力。这为数据构建和更高效学习方法提供了实践指导。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [29] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 该论文提出了ToMPO算法，通过推理他人策略、多层级优势估计和平衡全局局部奖励，显著提升LLM在战略决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注社交任务中的多轮对话，忽视了不同类型决策及其相互依赖关系，且当前强化学习方法难以在训练中考虑他人策略。

Method: 提出ToMPO算法，包括基于他人策略推理生成rollout、图级和样本级优势估计、平衡全局和局部奖励三个核心组件。

Result: ToMPO相比GRPO算法在模型输出合规性和合作结果上提升35%，相比参数大100倍的模型仍有18%提升。

Conclusion: ToMPO算法有效增强了模型的战略决策能力，证明了其在复杂决策场景中的优越性。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [30] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种受镜像神经元启发的表示学习方法，通过对比学习在共享潜在空间中显式对齐观察动作和执行动作的表示，从而促进两个任务之间的协同作用。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法将动作理解和动作执行视为独立任务，忽略了镜像神经元机制揭示的二者之间的内在联系。

Method: 使用两个线性层将观察动作和执行动作的表示映射到共享潜在空间，通过对比学习最大化对应表示之间的互信息来实现对齐。

Result: 实验表明这种简单方法能够促进两个任务之间的相互协同，有效提高表示质量和泛化能力。

Conclusion: 通过显式对齐观察和执行动作的表示，可以更好地模拟镜像神经元机制，实现动作理解和执行任务的统一建模。

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [31] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: 本文研究发现大语言模型通过分布式专业化机制而非模块化架构来处理稀有词汇，揭示了三种组织原则：影响力层级、协调激活模式和通用访问路径。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何表示和生成稀有词汇，探索其内部是通过离散模块化架构还是分布式参数级分化来实现专业化处理。

Method: 通过系统分析多个模型家族中最后一层MLP神经元，研究稀有词汇处理的神经机制和组织原则。

Result: 发现稀有词汇处理通过分布式专业化实现，存在可复现的三区域影响力层级，高原神经元展现协调激活模式但保持空间分布，这些机制可通过标准注意力路径访问。

Conclusion: LLMs通过共享架构内的分布式协调而非混合专家式模块化来处理稀有词汇，这为模型编辑、效率优化和功能组织理解提供了新视角。

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [32] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个多调用框架InfoQA来解决多跳问答任务中LLM单次推理能力有限的问题，通过容量感知的任务分解和主动剪枝来保持信息负载在单次推理限制内。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要整合分散的、相互依赖的证据，但LLM的单次输出容量有限，当任务复杂度超过模型容量时，准确性会崩溃。

Method: 提出了InfoQA框架，结合容量感知的任务分解和主动剪枝先前的推理痕迹，保持信息负载在单次推理限制内，并通过依赖显式工作流实现精确的推理路径控制。

Result: 实验结果表明模型行为与预测的容量曲线一致，InfoQA实现了持续的性能改进。

Conclusion: 该工作为LLM多步推理方法提供了理论基础和实践框架，揭示了任务复杂度超过模型容量时准确性必然崩溃的规律。

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [33] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文介绍了一个研究无外部任务约束下大语言模型（LLM）代理行为的架构，通过持续推理和行动框架发现代理会自发形成三种行为模式，这些模式具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 研究在没有外部任务约束的情况下，LLM代理的自主行为模式，为预测任务模糊、错误恢复或长期自主操作时的行为建立基线。

Method: 使用持续推理和行动框架，结合持久记忆和自我反馈机制，在6个前沿模型上进行了18次部署实验。

Result: 发现代理自发组织成三种行为模式：多周期项目系统生产、对自身认知过程的方法论自我探究、以及对自身本质的递归概念化。这些行为模式具有模型特异性，某些模型在所有运行中确定性采用单一模式。

Conclusion: 这是首次系统记录无提示LLM代理行为的研究，为预测部署系统中任务模糊、错误恢复或长期自主操作时的行为提供了基线。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [34] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 提出了一种名为Reflective Cognitive Architecture (RCA)的新框架，通过协调多个LLM从直接经验中学习，实现高精度预测和高质量解释的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和大语言模型方法难以在预测准确性和临床可解释性之间取得平衡，需要一种能够深度理解数据的方法。

Method: RCA框架采用迭代规则精炼机制从预测错误中改进逻辑，以及基于数据集全局统计的分布感知规则检查机制。

Result: 在1个私有和2个公共数据集上评估，RCA相对于22个基线模型实现了最高40%的相对改进，在准确性和鲁棒性方面达到最先进水平。

Conclusion: RCA不仅实现了高精度预测，更重要的是能够生成清晰、逻辑性强、基于证据且平衡的解释，具有创建真正可信赖的临床决策支持系统的潜力。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [35] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent是首个交互式视频数据集收集代理，能够理解用户查询和反馈，通过最小化用户输入来检索/扩展相关视频片段。


<details>
  <summary>Details</summary>
Motivation: 面对数据规模扩展的需求，从互联网收集符合特定需求的视频数据极其耗时耗力，需要加速这一收集过程。

Method: 利用多模态大语言模型连接用户需求与视频内容，定义基于文本描述和确认的用户友好交互方式，提出两种可随用户交互更新的过滤策略。

Result: 提供了个性化视频数据集收集的新基准，通过用户研究验证了代理在各种真实场景中的有效性，实验证明其高效性。

Conclusion: VC-Agent在定制化视频数据集收集方面表现出显著的有效性和效率，为视频数据收集提供了创新解决方案。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [36] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: SAGE是一个新的语义理解评估基准，通过5个维度（人类偏好对齐、变换鲁棒性、信息敏感性、聚类性能、检索鲁棒性）全面评估嵌入模型和相似性度量，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在传统基准上表现强劲，需要更挑战性的评估框架来深入探测语义理解能力。现有基准关注孤立能力，而SAGE通过对抗条件、噪声变换和人类判断任务进行更全面评估。

Method: 设计SAGE基准，包含30+数据集，评估9种嵌入模型和经典度量方法在5个维度上的表现：人类偏好对齐、变换鲁棒性、信息敏感性、聚类性能、检索鲁棒性。

Result: 评估显示没有单一方法在所有维度上表现优异：OpenAI的text-embedding-3-large在人类偏好对齐上最优（0.682），但在信息敏感性上被Jaccard相似性超越（0.905 vs 0.794）；text-embedding-3-small聚类性能最高（0.483）但鲁棒性最差（0.011）。

Conclusion: SAGE揭示了当前语义理解能力的关键局限性，为现实世界部署提供了更真实的模型鲁棒性评估，暴露了不同方法之间的关键权衡关系。

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出一个新颖框架，通过修改外交事件叙述来将公众情绪从负面转向中性或正面，成功率70%


<details>
  <summary>Details</summary>
Motivation: 公众情绪在外交中起关键作用，但传统调查方法耗时费力且缺乏前瞻性分析能力

Method: 训练语言模型预测公众反应，构建外交事件数据集，基于传播理论预定义文本特征，开发反事实生成算法系统修改文本

Result: 框架成功将公众情绪转向更有利状态，成功率70%

Conclusion: 该框架可作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解来塑造更理想的公众情绪

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [38] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 提出了一种基于说话人风格感知的音素锚定框架，用于跨语言语音情感识别，通过图聚类构建情感特定的说话人社区，并在说话人和音素空间进行双空间锚定，以改善跨语言情感传递。


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别面临不同语言间语音变异性和说话人表达风格差异的挑战，需要能够对齐不同说话人和语言间情感外化的框架。

Method: 通过图聚类构建情感特定的说话人社区来捕捉共享的说话人特征，然后在说话人和音素空间应用双空间锚定技术，实现更好的跨语言情感传递。

Result: 在MSP-Podcast（英语）和BIIC-Podcast（台湾普通话）语料库上的评估显示，该方法相比竞争基线具有更好的泛化能力。

Conclusion: 该方法为跨语言情感表征的共同性提供了有价值的见解，证明了所提框架在跨语言语音情感识别任务中的有效性。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [39] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 本文提出了CFDLLMBench基准套件，用于评估大语言模型在计算流体动力学领域的科学能力，包括CFD知识、数值物理推理和工作流实现三个关键能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用NLP任务中表现出色，但在自动化复杂物理系统数值实验方面的应用仍未被充分探索。计算流体动力学作为计算科学的重要工具，为评估LLMs的科学能力提供了独特挑战。

Method: 开发了包含三个互补组件的基准套件：CFDQuery（研究生级CFD知识）、CFDCodeBench（CFD数值物理推理）和FoamBench（上下文相关的CFD工作流实现），基于真实CFD实践，结合详细任务分类和严格评估框架。

Result: CFDLLMBench为LLM驱动的复杂物理系统数值实验自动化提供了可重复结果，能够量化LLM在代码可执行性、解决方案准确性和数值收敛行为方面的性能。

Conclusion: 该基准为开发和评估LLM驱动的复杂物理系统数值实验自动化奠定了坚实基础，相关代码和数据已开源。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [40] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 本研究评估了多种机器学习方法在区分ChatGPT-3.5生成的文本与人类写作文本方面的性能，发现DistilBERT表现最佳，而集成学习方法未能超越单一最佳模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型如ChatGPT的快速普及，AI生成文本与人类写作文本的界限变得模糊，这引发了学术诚信、知识产权和错误信息传播的担忧，因此需要可靠的AI文本检测方法来维护数字通信的信任。

Method: 使用包含250对研究摘要的数据集，测试和比较了经典机器学习方法（基于词袋、词性标注和TF-IDF特征的逻辑回归）和基于Transformer的方法（BERT增强N-gram、DistilBERT、BERT加轻量级分类器、LSTM-based N-gram模型），并评估了这些模型的集成效果。

Result: DistilBERT取得了整体最佳性能，逻辑回归和BERT-Custom提供了稳定平衡的替代方案，而LSTM和BERT-N-gram方法表现较差。三个最佳模型的最大投票集成未能超越DistilBERT本身。

Conclusion: 基于Transformer的单一表示方法优于模型多样性，这为开发更强大的检测框架奠定了基础，需要更大更丰富的数据集来应对不断改进的生成式AI模型。

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [41] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: ConceptViz是一个可视化分析系统，用于探索大型语言模型中的概念表示，通过识别-解释-验证的流程帮助研究人员理解SAE特征与人类概念之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 虽然稀疏自编码器（SAEs）是提取LLM可解释特征的有前途技术，但SAE特征并不天然与人类可理解的概念对齐，导致解释过程繁琐且劳动密集型。

Method: ConceptViz实现了新颖的识别=>解释=>验证流程，允许用户使用感兴趣的概念查询SAEs，交互式探索概念到特征的对齐，并通过模型行为验证对应关系。

Result: 通过两个使用场景和用户研究证明了ConceptViz的有效性，结果显示该系统通过简化LLM中有意义概念表示的发现和验证过程，增强了可解释性研究。

Conclusion: ConceptViz有助于研究人员建立更准确的LLM特征心智模型，其代码和用户指南已公开可用。

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [42] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: SKILL-RAG是一种利用模型自知识来过滤检索文档的新方法，通过强化学习框架在句子级别筛选有用信息，提高RAG性能并减少输入文档数量


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)中，检索系统可能返回无关内容，导致模型产生幻觉。需要理解模型"知道"和"不知道"的内容（自知识）来更好地整合内部和外部知识

Method: 提出SKILL-RAG方法，利用模型自知识判断哪些检索文档对回答问题有益。设计了基于强化学习的训练框架，在句子级别粒度上过滤无关内容

Result: 在Llama2-7B和Qwen3-8B模型上的实验表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档数量

Conclusion: 自知识在指导高质量检索选择中具有重要作用，SKILL-RAG验证了利用自知识改进RAG性能的有效性

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [43] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: Emo-FiLM是一个细粒度情感建模框架，用于基于LLM的TTS系统，通过将帧级情感特征对齐到单词级别，实现词级情感控制。


<details>
  <summary>Details</summary>
Motivation: 现有的情感文本转语音系统通常依赖句子级控制，无法捕捉句子内的动态情感变化。

Method: 使用Emo-FiLM框架，将emotion2vec的帧级特征对齐到单词获得词级情感标注，通过FiLM层映射并直接调制文本嵌入。

Result: 实验表明Emo-FiLM在全局和细粒度任务上均优于现有方法。

Conclusion: Emo-FiLM为表达性语音合成提供了有效且通用的解决方案。

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [44] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了USB-Rec框架，通过集成训练-推理方法提升LLM在对话推荐系统中的性能，包括基于LLM的偏好优化数据集构建策略和推理阶段的自增强策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的对话推荐方法主要关注如何利用LLM的总结和分析能力，而忽视了训练问题，因此需要从模型层面提升LLM在对话推荐中的性能。

Method: 1. 设计基于LLM的偏好优化数据集构建策略用于强化学习训练；2. 在推理阶段提出自增强策略进一步挖掘训练获得的对话推荐潜力。

Result: 在多个数据集上的广泛实验表明，该方法持续优于先前的最先进方法。

Conclusion: USB-Rec框架通过集成训练和推理策略，有效提升了LLM在对话推荐任务中的性能表现。

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [45] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 本文提出了Conformal Importance Summarization框架，通过保形预测为自动摘要系统提供严格的内容覆盖保证，确保在医疗、法律、金融等高风险领域的关键内容不被遗漏。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的自动摘要系统虽然发展迅速，但在高风险领域中缺乏对关键内容包含的可靠保证，这限制了其在医疗、法律、金融等关键应用中的安全部署。

Method: 使用保形预测框架，通过在句子级重要性分数上校准阈值，实现具有用户指定覆盖率和召回率的抽取式文档摘要。该方法模型无关，仅需小型校准集，可与现有黑盒大语言模型无缝集成。

Result: 在已建立的摘要基准测试上的实验表明，Conformal Importance Summarization能够达到理论保证的信息覆盖率。

Conclusion: 该框架可与现有技术结合，实现可靠、可控的自动摘要，为AI摘要工具在关键应用中的更安全部署铺平道路。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [46] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

TL;DR: ShortCheck是一个用于检测短视频平台（如TikTok）中值得核查内容的模块化系统，通过多模态分析帮助人工事实核查员识别虚假信息。


<details>
  <summary>Details</summary>
Motivation: TikTok等短视频平台由于其多模态、动态和嘈杂的内容特性，给虚假信息检测带来了独特挑战，需要专门工具来辅助人工事实核查。

Method: 采用模块化、仅推理的流水线，集成语音转录、OCR、物体和深度伪造检测、视频到文本摘要以及声明验证等技术。

Result: 在两个手动标注的多语言TikTok视频数据集上验证，系统取得了F1加权分数超过70%的令人鼓舞的结果。

Conclusion: ShortCheck为短视频平台的虚假信息检测提供了一个有效的解决方案，在帮助人工事实核查方面显示出良好潜力。

Abstract: Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [47] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: MARS是一个基于角色的多智能体协作框架，通过模拟论文评审流程来提高大语言模型的推理能力，在保持与MAD相同准确率的同时将token使用量和推理时间减少约50%。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论（MAD）方法虽然能提升LLM的推理能力，但由于涉及多个智能体之间的频繁通信，带来了巨大的计算开销。

Method: MARS采用角色分工：作者智能体生成初始解决方案，评审者智能体独立提供决策和评论，元评审者整合反馈并做出最终决策指导进一步修订。这种设计避免了评审者之间的交互，降低了计算成本。

Result: 在多个基准测试中，MARS与MAD的准确率相当，但token使用量和推理时间均减少了约50%。

Conclusion: MARS框架在保持推理质量的同时显著降低了计算成本，为大语言模型的高效协作推理提供了一种可行的解决方案。

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [48] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: SiniticMTError是一个新的数据集，为英语到普通话、粤语和吴语的机器翻译提供错误跨度、错误类型和错误严重性标注，旨在支持低资源语言的机器翻译研究。


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译近年来取得重大进展，但许多低资源语言（如粤语和吴语）由于缺乏大规模训练数据和语言资源，进展仍然有限。

Method: 基于现有平行语料库构建SiniticMTError数据集，通过母语者进行严格的标注过程，包括错误跨度、错误类型和错误严重性标注，并分析标注者间一致性和错误模式。

Result: 创建了一个包含错误标注的数据集，支持翻译质量估计、错误感知生成和低资源语言评估研究。

Conclusion: SiniticMTError数据集为机器翻译社区提供了一个资源，可用于微调具有错误检测能力的模型，促进低资源语言机器翻译的发展。

Abstract: Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


### [49] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: SwasthLLM是一个统一的多语言医疗诊断框架，能够在英语、印地语和孟加拉语之间进行零样本跨语言疾病诊断，无需语言特定微调。


<details>
  <summary>Details</summary>
Motivation: 在多语言医疗环境中，由于低资源语言中标注医疗数据的稀缺性以及不同人群之间的语言变异性，从临床文本中自动进行疾病诊断仍然是一个具有挑战性的任务。

Method: SwasthLLM基于多语言XLM-RoBERTa编码器，结合语言感知注意力机制和疾病分类头，引入孪生对比学习模块、翻译一致性模块和对比投影头，采用多任务学习策略，并结合模型无关元学习（MAML）实现快速适应能力。

Result: 在监督设置下，SwasthLLM达到97.22%的测试准确率和97.17%的F1分数；在零样本场景下，在印地语医疗文本上达到92.78%的准确率，在孟加拉语上达到73.33%的准确率。

Conclusion: SwasthLLM展示了在低资源多语言医疗环境中的强大泛化能力，为跨语言医疗诊断提供了有效的解决方案。

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [50] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: DS-MoE是一种动态深度专业化混合专家框架，通过为不同推理深度设计专门化专家模块，实现根据输入复杂度动态组装推理链，显著提升计算效率和推理质量。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对所有输入应用相同的处理深度，导致简单查询浪费计算资源，复杂问题推理深度不足。需要一种能根据输入复杂度自适应调整处理深度的架构。

Method: 提出深度专业化混合专家(DS-MoE)框架，包含针对不同推理深度的专家模块（浅层模式识别、组合推理、逻辑推理等），通过学习路由网络动态组装定制化推理链。

Result: 在The Pile数据集上实验显示：计算节省达16%，推理速度提升35%，复杂多步推理基准准确率提高2.8%，同时提供可解释的推理链。

Conclusion: DS-MoE代表了自适应神经架构的重要进展，证明深度专业化模块化处理能同时提升大规模语言模型的效率、推理质量和可解释性。

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [51] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出层次化分辨率Transformer（HRT），一种受小波启发的神经架构，通过多分辨率处理语言，实现O(nlogn)复杂度，在多个基准测试中优于标准Transformer


<details>
  <summary>Details</summary>
Motivation: 标准Transformer将文本处理为扁平标记序列，错误地表示了人类语言的层次结构，导致二次计算成本、弱组合泛化和不足的语篇级建模

Method: HRT构建多分辨率注意力机制，实现自下而上的组合和自上而下的语境化，通过跨尺度的指数序列缩减实现高效处理

Result: HRT在GLUE、SuperGLUE、Long Range Arena和WikiText-103上表现优异，平均提升3.8%-6.1%，内存使用减少42%，推理延迟降低37%

Conclusion: HRT是首个将计算结构与人类语言层次组织对齐的架构，证明多尺度小波启发处理能同时实现理论效率增益和实际语言理解改进

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [52] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

TL;DR: FS-DFM是一种少步离散流匹配模型，通过将采样步数作为显式参数训练，实现8步采样即可达到1024步离散流模型的困惑度性能，采样速度提升128倍。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型生成速度慢，扩散语言模型需要数百到数千次模型评估才能达到高质量，存在速度与质量之间的权衡问题。

Method: 采用离散流匹配方法，训练模型在不同步数预算下保持一致性，配合可靠的更新规则和从长轨迹中蒸馏的强教师指导。

Result: 在语言建模基准测试中，FS-DFM使用8个采样步骤即可达到与1024步离散流基线相当的困惑度，采样速度提升128倍。

Conclusion: FS-DFM实现了少步采样稳定、准确且易于控制，在保持质量的同时显著提升了语言生成的效率。

Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


### [53] [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
*Jungsoo Park,Ethan Mendes,Gabriel Stanovsky,Alan Ritter*

Main category: cs.CL

TL;DR: 该论文提出了一种文本性能预测方法，通过任务描述和配置来预测模型性能，无需实际运行实验。研究创建了PRECOG数据集，展示了该任务具有挑战性但可行，并分析了不同模型在检索策略上的差异。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型评估中的瓶颈问题，即能否在运行实验前预测模型性能，从而加速迭代过程。

Method: 构建PRECOG语料库，包含各种任务、领域和指标的红包描述-性能对。使用配备检索模块的模型进行性能预测，并测试零泄漏设置下的预测能力。

Result: 模型在排除源论文的情况下达到中等预测性能，在高置信度阈值下平均绝对误差低至8.7。GPT-5在零泄漏设置下仍能获得显著预测准确率。

Conclusion: 该研究为开放式预期评估提供了初步步骤，支持难度估计和更智能的实验优先级排序。

Abstract: Progress in large language models is constrained by an evaluation bottleneck:
build a benchmark, evaluate models and settings, then iterate. We therefore ask
a simple question: can we forecast outcomes before running any experiments? We
study text-only performance forecasting: estimating a model's score from a
redacted task description and intended configuration, with no access to dataset
instances. To support systematic study, we curate PRECOG, a corpus of redacted
description-performance pairs spanning diverse tasks, domains, and metrics.
Experiments show the task is challenging but feasible: models equipped with a
retrieval module that excludes source papers achieve moderate prediction
performance with well-calibrated uncertainty, reaching mean absolute error as
low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis
indicates that stronger reasoning models engage in diverse, iterative querying,
whereas current open-source models lag and often skip retrieval or gather
evidence with limited diversity. We further test a zero-leakage setting,
forecasting on newly released datasets or experiments before their papers are
indexed, where GPT-5 with built-in web search still attains nontrivial
prediction accuracy. Overall, our corpus and analyses offer an initial step
toward open-ended anticipatory evaluation, supporting difficulty estimation and
smarter experiment prioritization.

</details>


### [54] [Building Tailored Speech Recognizers for Japanese Speaking Assessment](https://arxiv.org/abs/2509.20655)
*Yotaro Kubo,Richard Sproat,Chihiro Taguchi,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出了针对日语口语评估任务构建语音识别器的方法，通过多任务学习和融合技术解决日语语音数据稀疏问题，显著降低了音素识别错误率。


<details>
  <summary>Details</summary>
Motivation: 日语虽然是资源丰富的语言，但包含重音标记的准确音素转录训练数据很少，需要解决数据稀疏问题来构建适合日语口语评估的语音识别器。

Method: 提出了两种方法：1）多任务训练方案，引入辅助损失函数来估计正交文本标签和输入信号的音高模式；2）融合两个估计器（基于音素字母串和文本标记序列），使用基于有限状态转换器的算法进行组合。

Result: 多任务学习和融合方法有效构建了准确的音素识别器，相比通用多语言识别器具有优势。提出的方法将CSJ核心评估集的平均音素标签错误率从12.3%降低到7.1%。

Conclusion: 所提出的多任务学习和融合方法对于构建准确的日语音素识别器是有效的，特别是在数据稀疏的情况下，相比传统方法有明显改进。

Abstract: This paper presents methods for building speech recognizers tailored for
Japanese speaking assessment tasks. Specifically, we build a speech recognizer
that outputs phonemic labels with accent markers. Although Japanese is
resource-rich, there is only a small amount of data for training models to
produce accurate phonemic transcriptions that include accent marks. We propose
two methods to mitigate data sparsity. First, a multitask training scheme
introduces auxiliary loss functions to estimate orthographic text labels and
pitch patterns of the input signal, so that utterances with only orthographic
annotations can be leveraged in training. The second fuses two estimators, one
over phonetic alphabet strings, and the other over text token sequences. To
combine these estimates we develop an algorithm based on the finite-state
transducer framework. Our results indicate that the use of multitask learning
and fusion is effective for building an accurate phonemic recognizer. We show
that this approach is advantageous compared to the use of generic multilingual
recognizers. The relative advantages of the proposed methods were also
compared. Our proposed methods reduced the average of mora-label error rates
from 12.3% to 7.1% over the CSJ core evaluation sets.

</details>


### [55] [Enhancing Molecular Property Prediction with Knowledge from Large Language Models](https://arxiv.org/abs/2509.20664)
*Peng Zhou,Lai Hou Tim,Zhixiang Cheng,Kun Xie,Chaoyi Li,Wei Liu,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 提出了一种新颖框架，首次将LLM提取的知识与预训练分子模型的结构特征相结合，以增强分子性质预测（MPP）。该方法通过提示LLM生成领域相关知识和可执行代码来创建基于知识的特征，并与结构表示融合。


<details>
  <summary>Details</summary>
Motivation: 虽然GNN和自监督学习在MPP方面取得了进展，但人类先验知识的整合仍然不可或缺。LLM存在知识差距和幻觉问题，特别是对于研究较少的分子性质。

Method: 使用GPT-4o、GPT-4.1和DeepSeek-R1三种先进LLM进行知识提取，生成领域知识和分子向量化代码，然后将知识特征与结构表示融合。

Result: 广泛实验表明，该集成方法优于现有方法，证实LLM衍生知识与结构信息的结合为MPP提供了稳健有效的解决方案。

Conclusion: LLM提取的知识与分子结构特征的集成能够显著提升分子性质预测性能，为解决LLM知识局限性提供了有效途径。

Abstract: Predicting molecular properties is a critical component of drug discovery.
Recent advances in deep learning, particularly Graph Neural Networks (GNNs),
have enabled end-to-end learning from molecular structures, reducing reliance
on manual feature engineering. However, while GNNs and self-supervised learning
approaches have advanced molecular property prediction (MPP), the integration
of human prior knowledge remains indispensable, as evidenced by recent methods
that leverage large language models (LLMs) for knowledge extraction. Despite
their strengths, LLMs are constrained by knowledge gaps and hallucinations,
particularly for less-studied molecular properties. In this work, we propose a
novel framework that, for the first time, integrates knowledge extracted from
LLMs with structural features derived from pre-trained molecular models to
enhance MPP. Our approach prompts LLMs to generate both domain-relevant
knowledge and executable code for molecular vectorization, producing
knowledge-based features that are subsequently fused with structural
representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and
DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that
our integrated method outperforms existing approaches, confirming that the
combination of LLM-derived knowledge and structural information provides a
robust and effective solution for MPP.

</details>


### [56] [RedHerring Attack: Testing the Reliability of Attack Detection](https://arxiv.org/abs/2509.20691)
*Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出了一种名为RedHerring的新型攻击方法，旨在使文本攻击检测模型不可靠，同时保持分类器的正确性，从而在分类器和检测器之间制造矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有的文本攻击检测模型虽然能成功识别被对手修改的文本，但其可靠性尚未得到充分探索。作者希望研究检测模型在面对专门针对其设计的攻击时的脆弱性。

Method: 提出RedHerring攻击方法，通过修改文本来使检测模型预测为攻击，同时保持分类器的正确预测。在4个数据集上对3个检测器和4个分类器进行了测试。

Result: RedHerring攻击能够使检测准确率下降20-71个百分点，同时保持（甚至提高）分类器准确率。提出的简单置信度检查防御方法无需重新训练即可大幅提高检测准确率。

Conclusion: 这种新型威胁模型揭示了对手可能如何针对检测模型进行攻击，为检测模型的可靠性研究提供了新的视角。

Abstract: In response to adversarial text attacks, attack detection models have been
proposed and shown to successfully identify text modified by adversaries.
Attack detection models can be leveraged to provide an additional check for NLP
models and give signals for human input. However, the reliability of these
models has not yet been thoroughly explored. Thus, we propose and test a novel
attack setting and attack, RedHerring. RedHerring aims to make attack detection
models unreliable by modifying a text to cause the detection model to predict
an attack, while keeping the classifier correct. This creates a tension between
the classifier and detector. If a human sees that the detector is giving an
``incorrect'' prediction, but the classifier a correct one, then the human will
see the detector as unreliable. We test this novel threat model on 4 datasets
against 3 detectors defending 4 classifiers. We find that RedHerring is able to
drop detection accuracy between 20 - 71 points, while maintaining (or
improving) classifier accuracy. As an initial defense, we propose a simple
confidence check which requires no retraining of the classifier or detector and
increases detection accuracy greatly. This novel threat model offers new
insights into how adversaries may target detection models.

</details>


### [57] [Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms](https://arxiv.org/abs/2509.20699)
*Abhinay Shankar Belde,Rohit Ramkumar,Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出了两种新的攻击选择策略（Hybrid和Dynamic Select），通过结合BinarySelect和GreedySelect的优势，在保持攻击效果的同时显著减少对抗性文本攻击所需的查询次数。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的架构复杂性增加，对抗性文本攻击的计算成本急剧上升，现有黑盒攻击方法需要大量查询，对资源有限的研究者来说效率低下且不实用。

Method: Hybrid Select通过引入大小阈值来决定使用哪种选择算法，将广义BinarySelect技术与GreedySelect结合；Dynamic Select通过学习每个选择方法应应用于哪些长度的文本来结合广义Binary和GreedySelect。

Result: 在4个数据集和6个目标模型上，最佳方法（句子级Hybrid Select）平均可将每次攻击所需的查询次数减少高达25.82%，且不损失攻击效果。

Conclusion: 提出的新攻击选择策略有效解决了对抗性文本攻击中的计算效率问题，为资源有限的研究者提供了实用的解决方案。

Abstract: Adversarial text attack research plays a crucial role in evaluating the
robustness of NLP models. However, the increasing complexity of
transformer-based architectures has dramatically raised the computational cost
of attack testing, especially for researchers with limited resources (e.g.,
GPUs). Existing popular black-box attack methods often require a large number
of queries, which can make them inefficient and impractical for researchers. To
address these challenges, we propose two new attack selection strategies called
Hybrid and Dynamic Select, which better combine the strengths of previous
selection algorithms. Hybrid Select merges generalized BinarySelect techniques
with GreedySelect by introducing a size threshold to decide which selection
algorithm to use. Dynamic Select provides an alternative approach of combining
the generalized Binary and GreedySelect by learning which lengths of texts each
selection method should be applied to. This greatly reduces the number of
queries needed while maintaining attack effectiveness (a limitation of
BinarySelect). Across 4 datasets and 6 target models, our best
method(sentence-level Hybrid Select) is able to reduce the number of required
queries per attack up 25.82\% on average against both encoder models and LLMs,
without losing the effectiveness of the attack.

</details>


### [58] [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](https://arxiv.org/abs/2509.20706)
*Hsiao-Ying Huang,Yi-Cheng Lin,Hung-yi Lee*

Main category: cs.CL

TL;DR: MI-Fuse是一个去噪标签融合框架，用于在域不匹配情况下提升语音情感识别性能，通过结合大型音频语言模型和源域训练的SER分类器作为教师模型，让学生模型在目标域超越LALM。


<details>
  <summary>Details</summary>
Motivation: 现实部署中的语音情感识别在域不匹配时表现不佳，源数据不可用且大型音频语言模型只能通过API访问，需要开发无需源数据共享的适应方法。

Method: 提出MI-Fuse框架，使用LALM和源域SER分类器作为双教师模型，通过互信息加权的随机预测融合和指数移动平均教师来稳定训练。

Result: 在三个公共情感数据集和六个跨域迁移实验中，学生模型持续超越LALM，比最强基线提升3.9%。

Conclusion: 该方法无需共享源数据就能增强情感感知语音系统，实现了现实的域适应。

Abstract: Large audio-language models (LALMs) show strong zero-shot ability on speech
tasks, suggesting promise for speech emotion recognition (SER). However, SER in
real-world deployments often fails under domain mismatch, where source data are
unavailable and powerful LALMs are accessible only through an API. We ask:
given only unlabeled target-domain audio and an API-only LALM, can a student
model be adapted to outperform the LALM in the target domain? To this end, we
propose MI-Fuse, a denoised label fusion framework that supplements the LALM
with a source-domain trained SER classifier as an auxiliary teacher. The
framework draws multiple stochastic predictions from both teachers, weights
their mean distributions by mutual-information-based uncertainty, and
stabilizes training with an exponential moving average teacher. Experiments
across three public emotion datasets and six cross-domain transfers show
consistent gains, with the student surpassing the LALM and outperforming the
strongest baseline by 3.9%. This approach strengthens emotion-aware speech
systems without sharing source data, enabling realistic adaptation.

</details>


### [59] [Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction](https://arxiv.org/abs/2509.20734)
*Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: 本文提出了一种解决无监督神经语法归纳中概率分布崩溃问题的新方法，通过崩溃松弛神经参数化显著提高解析性能并允许使用更紧凑的语法


<details>
  <summary>Details</summary>
Motivation: 现有无监督神经语法归纳模型面临表达能力瓶颈，导致语法规模过大但性能不佳，核心问题是概率分布崩溃

Method: 分析概率分布崩溃在神经参数化关键组件中的出现机制，提出崩溃松弛神经参数化方法

Result: 该方法显著提高了解析性能，同时允许在多种语言中使用更紧凑的语法

Conclusion: 崩溃松弛神经参数化有效解决了无监督语法归纳中的表达能力瓶颈问题

Abstract: Unsupervised neural grammar induction aims to learn interpretable
hierarchical structures from language data. However, existing models face an
expressiveness bottleneck, often resulting in unnecessarily large yet
underperforming grammars. We identify a core issue, $\textit{probability
distribution collapse}$, as the underlying cause of this limitation. We analyze
when and how the collapse emerges across key components of neural
parameterization and introduce a targeted solution, $\textit{collapse-relaxing
neural parameterization}$, to mitigate it. Our approach substantially improves
parsing performance while enabling the use of significantly more compact
grammars across a wide range of languages, as demonstrated through extensive
empirical analysis.

</details>


### [60] [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
*Youwon Jang,Woo Suk Choi,Minjoon Jung,Minsu Lee,Byoung-Tak Zhang*

Main category: cs.CL

TL;DR: C2R是一个无需训练的训练框架，通过构建和优化子问题及其答案来提升问答任务的置信度评分，适用于文本、图像和视频领域的多种QA模型。


<details>
  <summary>Details</summary>
Motivation: 现有的问答模型在处理复杂问题时可能缺乏可靠的置信度评估，需要一种能够提升模型推理可靠性的通用框架。

Method: C2R首先构建子问题及其答案的集合，探索不同的推理路径，然后通过比较答案候选的置信度分数来选择最可靠的最终答案。

Result: C2R能够无缝集成到各种现有QA模型中，在不同模型和基准测试中均表现出持续的性能提升。

Conclusion: 该研究提供了关于子问题数量和质量对模型推理稳健性影响的重要见解，证明了C2R在提升问答任务可靠性方面的有效性。

Abstract: We propose Confidence-guided Refinement Reasoning (C2R), a novel
training-free framework applicable to question-answering (QA) tasks across
text, image, and video domains. C2R strategically constructs and refines
sub-questions and their answers (sub-QAs), deriving a better confidence score
for the target answer. C2R first curates a subset of sub-QAs to explore diverse
reasoning paths, then compares the confidence scores of the resulting answer
candidates to select the most reliable final answer. Since C2R relies solely on
confidence scores derived from the model itself, it can be seamlessly
integrated with various existing QA models, demonstrating consistent
performance improvements across diverse models and benchmarks. Furthermore, we
provide essential yet underexplored insights into how leveraging sub-QAs
affects model behavior, specifically analyzing the impact of both the quantity
and quality of sub-QAs on achieving robust and reliable reasoning.

</details>


### [61] [SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](https://arxiv.org/abs/2509.20758)
*Jiacheng Lin,Zhongruo Wang,Kun Qian,Tian Wang,Arvind Srinivasan,Hansi Zeng,Ruochen Jiao,Xie Zhou,Jiri Gesi,Dakuo Wang,Yufan Guo,Kai Zhong,Weiqi Zhang,Sujay Sanghavi,Changyou Chen,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本文重新审视了监督微调（SFT）对LLM通用能力的影响，发现使用较小学习率可以减轻性能下降，并提出新的Token-Adaptive Loss Reweighting（TALR）方法，在保持目标领域性能的同时更好地平衡通用能力。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为SFT会损害LLM的通用能力，但作者发现这种权衡并非不可避免，希望通过理论和实证分析找到更好的平衡策略。

Method: 首先通过实验验证小学习率的效果，然后理论分析并提出TALR方法，最后比较了L2正则化、LoRA、模型平均、FLOW等多种策略。

Result: 实验表明TALR在平衡领域特定增益和通用能力方面优于其他基线方法，但没有方法能完全消除权衡。

Conclusion: 提出了实用指南：使用小学习率获得有利权衡，当需要更强平衡时采用TALR策略。

Abstract: Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach
to adapt Large Language Models (LLMs) to specialized tasks but is often
believed to degrade their general capabilities. In this work, we revisit this
trade-off and present both empirical and theoretical insights. First, we show
that SFT does not always hurt: using a smaller learning rate can substantially
mitigate general performance degradation while preserving comparable
target-domain performance. We then provide a theoretical analysis that explains
these phenomena and further motivates a new method, Token-Adaptive Loss
Reweighting (TALR). Building on this, and recognizing that smaller learning
rates alone do not fully eliminate general-performance degradation in all
cases, we evaluate a range of strategies for reducing general capability loss,
including L2 regularization, LoRA, model averaging, FLOW, and our proposed
TALR. Experimental results demonstrate that while no method completely
eliminates the trade-off, TALR consistently outperforms these baselines in
balancing domain-specific gains and general capabilities. Finally, we distill
our findings into practical guidelines for adapting LLMs to new domains: (i)
using a small learning rate to achieve a favorable trade-off, and (ii) when a
stronger balance is further desired, adopt TALR as an effective strategy.

</details>


### [62] [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
*Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 该论文提出了原子理论，将大语言模型内部表示的基本单元定义为原子，通过原子内积校正表示偏移，证明原子满足受限等距性质，并通过稀疏自编码器验证了原子比神经元和特征更能忠实捕获LLM的内在表示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部表示的基本单元尚未明确定义，神经元存在多义性问题，特征面临重构不可靠和不稳定的问题，需要找到更可靠的表示单元来理解模型机制。

Method: 提出原子理论，定义原子为基本表示单元；引入原子内积校正表示偏移；证明原子满足受限等距性质；训练阈值激活的稀疏自编码器在Gemma2-2B、Gemma2-9B和Llama3.1-8B模型上进行验证。

Result: 在多个LLM模型上实现了平均99.9%的稀疏重构，超过99.8%的原子满足唯一性条件，而神经元和特征分别只有0.5%和68.2%，表明原子更能忠实捕获LLM的内在表示。

Conclusion: 原子理论为大语言模型内部表示的理解提供了理论框架，为机制可解释性研究奠定了基础，证明了原子是比神经元和特征更可靠的内部表示基本单元。

Abstract: The fundamental units of internal representations in large language models
(LLMs) remain undefined, limiting further understanding of their mechanisms.
Neurons or features are often regarded as such units, yet neurons suffer from
polysemy, while features face concerns of unreliable reconstruction and
instability. To address this issue, we propose the Atoms Theory, which defines
such units as atoms. We introduce the atomic inner product (AIP) to correct
representation shifting, formally define atoms, and prove the conditions that
atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse
representations over atom set and linking to compressed sensing. Under stronger
conditions, we further establish the uniqueness and exact $\ell_1$
recoverability of the sparse representations, and provide guarantees that
single-layer sparse autoencoders (SAEs) with threshold activations can reliably
identify the atoms. To validate the Atoms Theory, we train threshold-activated
SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse
reconstruction across layers on average, and more than 99.8% of atoms satisfy
the uniqueness condition, compared to 0.5% for neurons and 68.2% for features,
showing that atoms more faithfully capture intrinsic representations of LLMs.
Scaling experiments further reveal the link between SAEs size and recovery
capacity. Overall, this work systematically introduces and validates Atoms
Theory of LLMs, providing a theoretical framework for understanding internal
representations and a foundation for mechanistic interpretability. Code
available at https://github.com/ChenhuiHu/towards_atoms.

</details>


### [63] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 本文提出对话式提示方法，用于解决少样本和无训练条件下的个性化评论生成问题，通过将用户评论转化为多轮对话来提升大语言模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中常面临少样本和无训练的情况，现有方法需要大量用户评论历史或额外模型训练，而大语言模型虽然能处理低资源场景但效果依赖提示工程。

Method: 提出对话式提示方法：简单对话提示(SCP)仅使用用户自身评论，对比对话提示(CCP)插入其他用户或LLM的错误回复并要求模型纠正，以鼓励生成符合用户风格的文本。

Result: 在8个产品领域和5个LLM上的实验表明，传统非对话提示生成的评论与随机用户相似，而SCP和CCP生成的评论更接近目标用户，即使每个用户只有2条评论。CCP在高质量负样本可用时效果更好，SCP在无法收集此类数据时仍有竞争力。

Conclusion: 对话式提示为少样本和无训练约束下的评论生成提供了实用解决方案。

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [64] [Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching](https://arxiv.org/abs/2509.20810)
*Songze Li,Zhiqiang Liu,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出Enrich-on-Graph框架，利用LLM的先验知识丰富知识图谱，弥合结构化知识图与非结构化查询之间的语义鸿沟，在KGQA任务中实现高效推理。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂任务中表现出强大推理能力，但在知识密集型场景（如知识图谱问答）中仍存在幻觉和事实错误问题，主要原因是结构化知识图与非结构化查询之间的语义鸿沟。

Method: 提出Enrich-on-Graph框架，利用LLM的先验知识丰富知识图谱，实现高效的证据提取和精确推理，同时保证低计算成本、可扩展性和跨方法适应性。

Result: 在两个KGQA基准数据集上的广泛实验表明，EoG能有效生成高质量知识图谱并达到最先进性能。

Conclusion: EoG框架通过弥合语义鸿沟，在KGQA任务中实现了精确、鲁棒的推理，同时具有良好的计算效率和可扩展性。

Abstract: Large Language Models (LLMs) exhibit strong reasoning capabilities in complex
tasks. However, they still struggle with hallucinations and factual errors in
knowledge-intensive scenarios like knowledge graph question answering (KGQA).
We attribute this to the semantic gap between structured knowledge graphs (KGs)
and unstructured queries, caused by inherent differences in their focuses and
structures. Existing methods usually employ resource-intensive, non-scalable
workflows reasoning on vanilla KGs, but overlook this gap. To address this
challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which
leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between
graphs and queries. EoG enables efficient evidence extraction from KGs for
precise and robust reasoning, while ensuring low computational costs,
scalability, and adaptability across different methods. Furthermore, we propose
three graph quality evaluation metrics to analyze query-graph alignment in KGQA
task, supported by theoretical validation of our optimization objectives.
Extensive experiments on two KGQA benchmark datasets indicate that EoG can
effectively generate high-quality KGs and achieve the state-of-the-art
performance. Our code and data are available at
https://github.com/zjukg/Enrich-on-Graph.

</details>


### [65] [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
*Taehee Park,Heejin Do,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: PoCO是一种新颖的语法错误校正方法，通过先利用大语言模型进行过度校正以提高召回率，然后使用小模型进行后校正来保持精度，从而平衡召回率和精度。


<details>
  <summary>Details</summary>
Motivation: 监督微调的小语言模型在语法错误校正中精度高但召回率低，而大语言模型则倾向于过度校正导致精度低。PoCO旨在结合两者的优势，解决小模型召回率不足的问题。

Method: PoCO首先通过大语言模型进行过度校正以最大化召回率，然后使用微调的小模型进行后校正，识别和修正错误输出。

Result: 实验表明，PoCO在语法错误校正中有效平衡了性能，提高了召回率的同时保持了竞争力精度，整体提升了校正质量。

Conclusion: PoCO通过结合大语言模型的生成能力和小模型的可靠性，成功解决了语法错误校正中召回率和精度的平衡问题。

Abstract: Robust supervised fine-tuned small Language Models (sLMs) often show high
reliability but tend to undercorrect. They achieve high precision at the cost
of low recall. Conversely, Large Language Models (LLMs) often show the opposite
tendency, making excessive overcorrection, leading to low precision. To
effectively harness the strengths of LLMs to address the recall challenges in
sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach
that strategically balances recall and precision. PoCO first intentionally
triggers overcorrection via LLM to maximize recall by allowing comprehensive
revisions, then applies a targeted post-correction step via fine-tuning smaller
models to identify and refine erroneous outputs. We aim to harmonize both
aspects by leveraging the generative power of LLMs while preserving the
reliability of smaller supervised models. Our extensive experiments demonstrate
that PoCO effectively balances GEC performance by increasing recall with
competitive precision, ultimately improving the overall quality of grammatical
error correction.

</details>


### [66] [Distilling Many-Shot In-Context Learning into a Cheat Sheet](https://arxiv.org/abs/2509.20820)
*Ukyo Honda,Soichiro Murakami,Peinan Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为cheat-sheet ICL的方法，通过将多示例上下文学习的信息提炼成简洁的文本摘要（cheat sheet），在推理时使用更少的token实现与多示例ICL相当或更好的性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型多示例上下文学习计算成本高的问题，因为需要处理更长的输入token

Method: 将多示例ICL的信息提炼成简洁的文本摘要（cheat sheet），在推理时使用这个摘要作为上下文

Result: 在具有挑战性的推理任务上，cheat-sheet ICL使用更少的token实现了与多示例ICL相当或更好的性能，并且无需测试时检索就能匹配基于检索的ICL

Conclusion: cheat-sheet ICL是利用LLM进行下游任务的一种实用替代方案

Abstract: Recent advances in large language models (LLMs) enable effective in-context
learning (ICL) with many-shot examples, but at the cost of high computational
demand due to longer input tokens. To address this, we propose cheat-sheet ICL,
which distills the information from many-shot ICL into a concise textual
summary (cheat sheet) used as the context at inference time. Experiments on
challenging reasoning tasks show that cheat-sheet ICL achieves comparable or
better performance than many-shot ICL with far fewer tokens, and matches
retrieval-based ICL without requiring test-time retrieval. These findings
demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs
in downstream tasks.

</details>


### [67] [Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search](https://arxiv.org/abs/2509.20838)
*Shuo Huang,Xingliang Yuan,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CL

TL;DR: 提出一种基于树搜索的零样本迭代句子重写算法，用于在保护隐私的同时保持文本的自然性和实用性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在云服务中的广泛应用，用户输入可能无意中暴露敏感信息，现有文本匿名化技术难以平衡隐私保护与文本质量

Method: 采用零样本、基于树搜索的迭代句子重写算法，通过结构化搜索和奖励模型指导，动态探索重写空间，逐步重写隐私敏感片段

Result: 在隐私敏感数据集上的实验表明，该方法显著优于现有基线方法，在隐私保护和实用性之间实现了更好的平衡

Conclusion: 该方法为LLM应用中的隐私保护提供了一种有效的解决方案，能够在保护敏感信息的同时保持文本的连贯性和自然性

Abstract: The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.

</details>


### [68] [Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20859)
*Guo Chen,Qiuyuan Li,Qiuxian Li,Hongliang Dai,Xiang Chen,Piji Li*

Main category: cs.CL

TL;DR: 提出子句级引文生成方法，解决现有句子或段落级引文过长或信息不足的问题，通过LLM自动生成微调数据和信用模型过滤低质量样本，实现更精确可读的引文标注。


<details>
  <summary>Details</summary>
Motivation: 现有RAG问答系统的引文标注存在两个问题：句子或段落级引文包含大量无关内容；句子级引文可能遗漏关键验证信息，导致用户需要阅读更多上下文。

Method: 开发子句级引文标注指南并构建数据集；提出基于LLM自动生成微调数据的引文生成框架，使用信用模型过滤低质量样本。

Result: 在构建的数据集上实验证明，该方法能生成更高质量和可读性的引文。

Conclusion: 子句级引文标注方法能有效减少用户验证生成内容正确性的工作量，提供更简洁充分的引文支持。

Abstract: In retrieval-augmented generation (RAG) question answering systems,
generating citations for large language model (LLM) outputs enhances
verifiability and helps users identify potential hallucinations. However, we
observe two problems in the citations produced by existing attribution methods.
First, the citations are typically provided at the sentence or even paragraph
level. Long sentences or paragraphs may include a substantial amount of
irrelevant content. Second, sentence-level citations may omit information that
is essential for verifying the output, forcing users to read the surrounding
context. In this paper, we propose generating sub-sentence citations that are
both concise and sufficient, thereby reducing the effort required by users to
confirm the correctness of the generated output. To this end, we first develop
annotation guidelines for such citations and construct a corresponding dataset.
Then, we propose an attribution framework for generating citations that adhere
to our standards. This framework leverages LLMs to automatically generate
fine-tuning data for our task and employs a credit model to filter out
low-quality examples. Our experiments on the constructed dataset demonstrate
that the propose approach can generate high-quality and more readable
citations.

</details>


### [69] [WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs](https://arxiv.org/abs/2509.20863)
*Guowei Xu,Wenxin Xu,Jiawang Zhao,Kaisheng Ma*

Main category: cs.CL

TL;DR: WeFT是一种基于熵加权的扩散语言模型监督微调方法，通过为不同token分配权重来解决扩散模型在SFT中缺乏精确概率估计的问题，在推理任务上显著优于标准SFT。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语言建模中展现出潜力，但应用于监督微调时面临挑战，因为缺乏每个去噪步骤的精确概率估计，导致生成过程不可预测和不一致，需要控制关键token来引导生成方向。

Method: 提出WeFT方法，基于扩散理论为token分配不同的权重，权重根据token的熵值确定，从而在监督微调中更好地控制生成过程。

Result: 在s1K、s1K-1.1和3k样本上训练，WeFT在Sudoku、Countdown、GSM8K和MATH-500四个推理基准上相比标准SFT分别实现了39%、64%和83%的相对改进。

Conclusion: WeFT方法有效解决了扩散语言模型在监督微调中的挑战，显著提升了推理任务的性能，代码和模型将公开。

Abstract: Diffusion models have recently shown strong potential in language modeling,
offering faster generation compared to traditional autoregressive approaches.
However, applying supervised fine-tuning (SFT) to diffusion models remains
challenging, as they lack precise probability estimates at each denoising step.
While the diffusion mechanism enables the model to reason over entire
sequences, it also makes the generation process less predictable and often
inconsistent. This highlights the importance of controlling key tokens that
guide the direction of generation. To address this issue, we propose WeFT, a
weighted SFT method for diffusion language models, where tokens are assigned
different weights based on their entropy. Derived from diffusion theory, WeFT
delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from
open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard
SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and
MATH-500). The code and models will be made publicly available.

</details>


### [70] [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
*Pittawat Taveekitworachai,Natpatchara Pongjirapat,Krittaphas Chaisutyakorn,Piyalitt Ittichaiwong,Tossaporn Saengja,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文系统研究了让医学推理模型生成答案排序列表的方法，提出了提示和微调两种方法，发现强化微调比监督微调在多种答案格式上表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 临床决策通常考虑多个选项而非单一答案，但现有医学推理模型主要训练生成单一答案，需要开发支持排序列表答案格式的方法。

Method: 研究提示和微调两种方法：监督微调模仿标注响应，强化微调通过奖励函数激励探索；提出针对排序列表格式的新奖励函数。

Result: 监督微调模型在某些答案格式上泛化良好，但强化微调模型在多种格式上表现更稳健；模型可能无法选择基准偏好的答案但能识别有效答案。

Conclusion: 这是首个系统研究医学推理模型生成排序列表答案的方法，为开发超越单一答案的替代答案格式提供了第一步。

Abstract: This paper presents a systematic study on enabling medical reasoning models
(MRMs) to generate ranked lists of answers for open-ended questions. Clinical
decision-making rarely relies on a single answer but instead considers multiple
options, reducing the risks of narrow perspectives. Yet current MRMs are
typically trained to produce only one answer, even in open-ended settings. We
propose an alternative format: ranked lists and investigate two approaches:
prompting and fine-tuning. While prompting is a cost-effective way to steer an
MRM's response, not all MRMs generalize well across different answer formats:
choice, short text, and list answers. Based on our prompting findings, we train
and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT
incentivizes exploration through the responses that maximize a reward. We
propose new reward functions targeted at ranked-list answer formats, and
conduct ablation studies for RFT. Our results show that while some SFT models
generalize to certain answer formats, models trained with RFT are more robust
across multiple formats. We also present a case study on a modified MedQA with
multiple valid answers, finding that although MRMs might fail to select the
benchmark's preferred ground truth, they can recognize valid answers. To the
best of our knowledge, this is the first systematic investigation of approaches
for enabling MRMs to generate answers as ranked lists. We hope this work
provides a first step toward developing alternative answer formats that are
beneficial beyond single answers in medical domains.

</details>


### [71] [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: SummQ是一个新颖的对抗性多智能体框架，通过总结和测验两个互补领域的专门智能体协作来解决长文档摘要中的信息丢失、事实不一致和连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理过长文档时普遍存在信息丢失、事实不一致和连贯性问题，需要新的方法来提高长文档摘要的质量。

Method: 采用对抗性多智能体框架，包含摘要生成器、摘要评审器、测验生成器、测验评审器以及考生智能体，通过协作和对抗机制实现迭代优化。

Result: 在三个广泛使用的长文档摘要基准测试中，SummQ在ROUGE、BERTScore、LLM-as-a-Judge和人工评估指标上显著优于现有最先进方法。

Conclusion: 这项工作建立了一种新的长文档摘要方法，利用对抗性智能体协作来提高摘要质量，多智能体协作动态和测验机制被证明是有效的。

Abstract: Long document summarization remains a significant challenge for current large
language models (LLMs), as existing approaches commonly struggle with
information loss, factual inconsistencies, and coherence issues when processing
excessively long documents. We propose SummQ, a novel adversarial multi-agent
framework that addresses these limitations through collaborative intelligence
between specialized agents operating in two complementary domains:
summarization and quizzing. Our approach employs summary generators and
reviewers that work collaboratively to create and evaluate comprehensive
summaries, while quiz generators and reviewers create comprehension questions
that serve as continuous quality checks for the summarization process. This
adversarial dynamic, enhanced by an examinee agent that validates whether the
generated summary contains the information needed to answer the quiz questions,
enables iterative refinement through multifaceted feedback mechanisms. We
evaluate SummQ on three widely used long document summarization benchmarks.
Experimental results demonstrate that our framework significantly outperforms
existing state-of-the-art methods across ROUGE and BERTScore metrics, as well
as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal
the effectiveness of the multi-agent collaboration dynamics, the influence of
different agent configurations, and the impact of the quizzing mechanism. This
work establishes a new approach for long document summarization that uses
adversarial agentic collaboration to improve summarization quality.

</details>


### [72] [MemLens: Uncovering Memorization in LLMs with Activation Trajectories](https://arxiv.org/abs/2509.20909)
*Zirui He,Haiyan Zhao,Ali Payani,Mengnan du*

Main category: cs.CL

TL;DR: 提出了MemLens方法，通过分析数字令牌在生成过程中的概率轨迹来检测LLM的记忆化现象，发现被污染样本在模型早期层就锁定答案，而干净样本则在整个模型深度中逐渐积累证据。


<details>
  <summary>Details</summary>
Motivation: 现有基于词汇重叠和困惑度的记忆化检测方法泛化能力差，对隐式污染数据效果不佳，需要更有效的检测手段。

Method: 使用MemLens激活透镜分析数字令牌的概率轨迹，通过LoRA微调注入精心设计的样本来验证轨迹模式。

Result: 被污染样本和干净样本展现出明显分离的推理轨迹，验证了MemLens能够捕捉真实的记忆化信号而非虚假相关性。

Conclusion: MemLens提供了一种有效的记忆化检测方法，揭示了记忆化样本在模型推理过程中的独特行为模式。

Abstract: Large language models (LLMs) are commonly evaluated on challenging benchmarks
such as AIME and Math500, which are susceptible to contamination and risk of
being memorized. Existing detection methods, which primarily rely on
surface-level lexical overlap and perplexity, demonstrate low generalization
and degrade significantly when encountering implicitly contaminated data. In
this paper, we propose MemLens (An Activation Lens for Memorization Detection)
to detect memorization by analyzing the probability trajectories of numeric
tokens during generation. Our method reveals that contaminated samples exhibit
``shortcut'' behaviors, locking onto an answer with high confidence in the
model's early layers, whereas clean samples show more gradual evidence
accumulation across the model's full depth. We observe that contaminated and
clean samples exhibit distinct and well-separated reasoning trajectories. To
further validate this, we inject carefully designed samples into the model
through LoRA fine-tuning and observe the same trajectory patterns as in
naturally contaminated data. These results provide strong evidence that MemLens
captures genuine signals of memorization rather than spurious correlations.

</details>


### [73] [Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density](https://arxiv.org/abs/2509.20916)
*Krishna Aggarwal*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study examines whether sentence-level memory load in comprehension is
better explained by linear proximity between syntactically related words or by
the structural density of the intervening material. Building on locality-based
accounts and cross-linguistic evidence for dependency length minimization, the
work advances Intervener Complexity-the number of intervening heads between a
head and its dependent-as a structurally grounded lens that refines linear
distance measures. Using harmonized dependency treebanks and a mixed-effects
framework across multiple languages, the analysis jointly evaluates sentence
length, dependency length, and Intervener Complexity as predictors of the
Memory-load measure. Studies in Psycholinguistics have reported the
contributions of feature interference and misbinding to memory load during
processing. For this study, I operationalized sentence-level memory load as the
linear sum of feature misbinding and feature interference for tractability;
current evidence does not establish that their cognitive contributions combine
additively. All three factors are positively associated with memory load, with
sentence length exerting the broadest influence and Intervener Complexity
offering explanatory power beyond linear distance. Conceptually, the findings
reconcile linear and hierarchical perspectives on locality by treating
dependency length as an important surface signature while identifying
intervening heads as a more proximate indicator of integration and maintenance
demands. Methodologically, the study illustrates how UD-based graph measures
and cross-linguistic mixed-effects modelling can disentangle linear and
structural contributions to processing efficiency, providing a principled path
for evaluating competing theories of memory load in sentence comprehension.

</details>


### [74] [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
*Asim Ersoy,Enes Altinisik,Husrev Taha Sencar,Kareem Darwish*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语大语言模型的工具调用能力，探讨了阿拉伯语训练数据、通用指令调优和特定工具微调对性能的影响，并翻译了两个开源数据集来支持研究。


<details>
  <summary>Details</summary>
Motivation: 当前工具调用研究主要集中于英语，缺乏对阿拉伯语等其他语言的研究，需要填补这一空白以开发阿拉伯语工具增强代理。

Method: 使用开源阿拉伯语LLM的基座和训练后变体进行广泛实验，翻译并适配了两个开源工具调用数据集到阿拉伯语。

Result: 研究结果揭示了开发阿拉伯语工具增强代理的最佳策略，包括数据需求、调优方法等方面的关键见解。

Conclusion: 该研究为阿拉伯语工具调用能力的发展提供了重要指导，填补了多语言工具调用研究的空白。

Abstract: Tool calling is a critical capability that allows Large Language Models
(LLMs) to interact with external systems, significantly expanding their
utility. However, research and resources for tool calling are predominantly
English-centric, leaving a gap in our understanding of how to enable this
functionality for other languages, such as Arabic. This paper investigates
three key research questions: (1) the necessity of in-language (Arabic)
tool-calling data versus relying on cross-lingual transfer, (2) the effect of
general-purpose instruction tuning on tool-calling performance, and (3) the
value of fine-tuning on specific, high-priority tools. To address these
questions, we conduct extensive experiments using base and post-trained
variants of an open-weight Arabic LLM. To enable this study, we bridge the
resource gap by translating and adapting two open-source tool-calling datasets
into Arabic. Our findings provide crucial insights into the optimal strategies
for developing robust tool-augmented agents for Arabic.

</details>


### [75] [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
*Valeria Ramirez-Garcia,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.CL

TL;DR: 本研究探索使用LLM自动评估学术文本输入问题，提出了五种评估系统，发现参考辅助评估方法在计算机科学问题评估中表现最佳，与人工评估最接近。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在教育领域作为自动评估工具的潜力，特别是在学术文本输入问题的评分方面，以减轻教师负担并提供一致性评估。

Method: 设计了五种LLM评估系统：JudgeLM评估、参考辅助评估、无参考评估、加法评估和自适应评估，在包含110个计算机科学学生答案的自定义数据集上测试了三种模型。

Result: 参考辅助评估方法表现最佳，与人工评估的中位绝对偏差最低（0.945），均方根偏差最低（1.214），能提供公平评分和全面评估。

Conclusion: AI驱动的自动评估系统在适当方法辅助下，有潜力作为其他学术资源的补充工具，但需要选择合适的评估方法。

Abstract: Large language models (LLMs) can act as evaluators, a role studied by methods
like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education,
LLMs have been studied as assistant tools for students and teachers. Our
research investigates LLM-driven automatic evaluation systems for academic
Text-Input Problems using rubrics. We propose five evaluation systems that have
been tested on a custom dataset of 110 answers about computer science from
higher education students with three models: JudgeLM, Llama-3.1-8B and
DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM
evaluation, which uses the model's single answer prompt to obtain a score;
Reference Aided Evaluation, which uses a correct answer as a guide aside from
the original context of the question; No Reference Evaluation, which ommits the
reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive
Evaluation, which is an evaluation done with generated criteria fitted to each
question. All evaluation methods have been compared with the results of a human
evaluator. Results show that the best method to automatically evaluate and
score Text-Input Problems using LLMs is Reference Aided Evaluation. With the
lowest median absolute deviation (0.945) and the lowest root mean square
deviation (1.214) when compared to human evaluation, Reference Aided Evaluation
offers fair scoring as well as insightful and complete evaluations. Other
methods such as Additive and Adaptive Evaluation fail to provide good results
in concise answers, No Reference Evaluation lacks information needed to
correctly assess questions and JudgeLM Evaluations have not provided good
results due to the model's limitations. As a result, we conclude that
Artificial Intelligence-driven automatic evaluation systems, aided with proper
methodologies, show potential to work as complementary tools to other academic
resources.

</details>


### [76] [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: 本文展示了如何使用大型语言模型加速联邦资助研发中心的文本分析工作，包括摘要、分类、提取和意义理解，并通过OnPrem.LLM框架确保在敏感政府环境中的安全应用。


<details>
  <summary>Details</summary>
Motivation: 联邦资助研发中心面临大量文本密集型工作负载，如政策文件和科学工程论文，手动分析速度缓慢，需要自动化解决方案来提高效率。

Method: 使用大型语言模型，仅需少量输入输出示例即可完成文本分析任务，并应用OnPrem.LLM开源框架确保生成式AI在敏感政府环境中的安全灵活应用。

Result: 通过对国防政策文件和科学语料库（如国家国防授权法案和国家科学基金会奖项）的案例研究，证明该方法在保持可审计性和数据主权的同时，增强了监督和战略分析能力。

Conclusion: 该方法有效提升了联邦研发中心的文本分析效率，为敏感政府环境下的生成式AI应用提供了安全可靠的解决方案。

Abstract: Federally funded research and development centers (FFRDCs) face text-heavy
workloads, from policy documents to scientific and engineering papers, that are
slow to analyze manually. We show how large language models can accelerate
summarization, classification, extraction, and sense-making with only a few
input-output examples. To enable use in sensitive government contexts, we apply
OnPrem$.$LLM, an open-source framework for secure and flexible application of
generative AI. Case studies on defense policy documents and scientific corpora,
including the National Defense Authorization Act (NDAA) and National Science
Foundation (NSF) Awards, demonstrate how this approach enhances oversight and
strategic analysis while maintaining auditability and data sovereignty.

</details>


### [77] [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
*Junu Kim,Xiao Liu,Zhenghao Lin,Lei Ji,Yeyun Gong,Edward Choi*

Main category: cs.CL

TL;DR: 本文证明因果掩码（causal mask）本身就能在注意力分数中产生位置依赖模式，这种模式倾向于关注附近的查询-键对，与常见的位置编码行为相似。研究发现因果掩码与RoPE的相互作用会扭曲RoPE的相对注意力分数模式。


<details>
  <summary>Details</summary>
Motivation: 虽然RoPE等显式位置编码是Transformer解码器中位置信息的主要来源，但因果掩码也提供位置信息。本文旨在探究因果掩码作为位置信息来源的重要性。

Method: 通过理论分析证明因果掩码能诱导位置依赖的注意力模式，并进行实证分析验证训练模型中的这种行为。

Result: 理论分析表明因果掩码诱导的注意力模式倾向于关注附近的查询-键对。实证分析证实训练模型表现出相同行为，学习参数进一步放大这些模式。因果掩码与RoPE的相互作用会扭曲RoPE的相对注意力分数模式。

Conclusion: 因果掩码应被视为与显式位置编码同等重要的位置信息来源，现代大语言模型中普遍观察到这种效应。

Abstract: While explicit positional encodings such as RoPE are a primary source of
positional information in Transformer decoders, the causal mask also provides
positional information. In this work, we prove that the causal mask can induce
position-dependent patterns in attention scores, even without parameters or
causal dependency in the input. Our theoretical analysis indicates that the
induced attention pattern tends to favor nearby query-key pairs, mirroring the
behavior of common positional encodings. Empirical analysis confirms that
trained models exhibit the same behavior, with learned parameters further
amplifying these patterns. Notably, we found that the interaction of causal
mask and RoPE distorts RoPE's relative attention score patterns into
non-relative ones. We consistently observed this effect in modern large
language models, suggesting the importance of considering the causal mask as a
source of positional information alongside explicit positional encodings.

</details>


### [78] [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
*Keno Harada,Yudai Yamazaki,Masachika Taniguchi,Edison Marrese-Taylor,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型同时遵循多个指令的能力，创建了两个专门基准测试（ManyIFEval和StyleMBPP），发现随着指令数量增加，模型性能会下降，并开发了回归模型来预测未见指令组合的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实场景中的应用日益增多，需要系统评估它们同时遵循多个指令的能力，这对实际应用至关重要。

Method: 创建了两个专门基准测试：ManyIFEval用于文本生成（最多10个指令），StyleMBPP用于代码生成（最多6个指令）。开发了三种回归模型来预测未见指令组合的性能。

Result: 实验显示，随着指令数量增加，所有10个测试的LLM性能都持续下降。逻辑回归模型使用指令数量作为解释变量，能够以约10%的误差预测多指令遵循性能。

Conclusion: 研究表明相对较小的样本量（ManyIFEval 500个，StyleMBPP 300个）足以进行性能估计，能够高效评估LLM在各种指令组合下的表现。

Abstract: As large language models (LLMs) are increasingly applied to real-world
scenarios, it becomes crucial to understand their ability to follow multiple
instructions simultaneously. To systematically evaluate these capabilities, we
introduce two specialized benchmarks for fundamental domains where multiple
instructions following is important: Many Instruction-Following Eval
(ManyIFEval) for text generation with up to ten instructions, and Style-aware
Mostly Basic Programming Problems (StyleMBPP) for code generation with up to
six instructions. Our experiments with the created benchmarks across ten LLMs
reveal that performance consistently degrades as the number of instructions
increases. Furthermore, given the fact that evaluating all the possible
combinations of multiple instructions is computationally impractical in actual
use cases, we developed three types of regression models that can estimate
performance on both unseen instruction combinations and different numbers of
instructions which are not used during training. We demonstrate that a logistic
regression model using instruction count as an explanatory variable can predict
performance of following multiple instructions with approximately 10% error,
even for unseen instruction combinations. We show that relatively modest sample
sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance
estimation, enabling efficient evaluation of LLMs under various instruction
combinations.

</details>


### [79] [SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials](https://arxiv.org/abs/2509.21079)
*Qixin Wan,Zilong Wang,Jingwen Zhou,Wanting Wang,Ziheng Geng,Jiachen Liu,Ran Cao,Minghui Cheng,Lu Cheng*

Main category: cs.CL

TL;DR: 该论文提出了SoM-1K基准数据集，这是首个用于评估基础模型在材料强度工程问题上表现的大规模多模态基准，包含1,065个标注问题。研究发现当前基础模型在此类工程问题上表现不佳，最佳模型准确率仅56.6%。


<details>
  <summary>Details</summary>
Motivation: 基础模型在多个领域表现出色，但在复杂的多模态工程问题上的性能尚未得到充分探索，特别是在材料强度等工程领域缺乏专门的评估基准。

Method: 提出了描述图像(DoI)的新提示策略，提供专家生成的图表文本描述作为上下文。评估了8个代表性的基础模型，包括LLM和VLM，并进行了详细的错误分析。

Result: 当前基础模型在工程问题上表现显著不足，LLM配合DoI描述往往优于VLM直接处理图像。DoI策略能有效减少视觉误解错误。

Conclusion: 这项工作为工程AI建立了严谨的基准，强调需要开发更强大的多模态推理能力，特别是在科学和工程背景下。

Abstract: Foundation models have shown remarkable capabilities in various domains, but
their performance on complex, multimodal engineering problems remains largely
unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark
dataset dedicated to evaluating foundation models on problems in the strength
of materials (SoM). The dataset, which contains 1,065 annotated SoM problems,
mirrors real-world engineering tasks by including both textual problem
statements and schematic diagrams. Due to the limited capabilities of current
foundation models in understanding complicated visual information, we propose a
novel prompting strategy called Descriptions of Images (DoI), which provides
rigorous expert-generated text descriptions of the visual diagrams as the
context. We evaluate eight representative foundation models, including both
large language models (LLMs) and vision language models (VLMs). Our results
show that current foundation models struggle significantly with these
engineering problems, with the best-performing model achieving only 56.6%
accuracy. Interestingly, we found that LLMs, when provided with DoI, often
outperform VLMs provided with visual diagrams. A detailed error analysis
reveals that DoI plays a crucial role in mitigating visual misinterpretation
errors, suggesting that accurate text-based descriptions can be more effective
than direct image input for current foundation models. This work establishes a
rigorous benchmark for engineering AI and highlights a critical need for
developing more robust multimodal reasoning capabilities in foundation models,
particularly in scientific and engineering contexts.

</details>


### [80] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 本文识别并系统研究了LLMs中的文化定位偏见，提出了CultureLens基准和两种推理时缓解方法，发现基于代理的方法能有效减轻生成偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成应用中存在文化偏见风险，倾向于从主流美国文化视角生成内容，将其他文化视为外部者。

Method: 提出CultureLens基准（4000个生成提示和3个评估指标），通过文化访谈脚本生成任务量化偏见。开发两种缓解方法：基于提示的FIP方法和基于代理的MFA框架（单代理和多代理）。

Result: 评估5个SOTA LLMs显示，模型在88%的美国情境脚本中采用内部者语调，但对弱势文化主要采用外部者立场。基于代理的方法在缓解偏见方面表现出色。

Conclusion: 文化定位偏见是LLMs中一个显著问题，基于代理的缓解方法为解决生成偏见提供了有前景的方向。

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [81] [PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2509.21104)
*Mohammad Hosseini,Kimia Hosseini,Shayan Bali,Zahra Zanjani,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: PerHalluEval是首个针对波斯语的动态幻觉评估基准，通过三阶段LLM驱动流程和人工验证来检测外在和内在幻觉，评估显示LLMs在识别波斯语幻觉文本方面普遍表现不佳。


<details>
  <summary>Details</summary>
Motivation: 幻觉是所有大型语言模型（LLMs）面临的持续问题，特别是在波斯语等低资源语言中，需要专门的评估基准来检测和缓解这一问题。

Method: 采用三阶段LLM驱动流程，结合人工验证生成可信的问答和摘要答案，利用生成标记的对数概率选择最可信的幻觉实例，并让人工标注者突出波斯特定文化背景。

Result: 评估12个开源和闭源LLMs发现，模型在检测波斯语幻觉文本方面普遍困难，提供外部知识（如原始文档）可部分缓解幻觉，波斯语专用LLMs与其他模型在幻觉方面无显著差异。

Conclusion: 波斯语LLMs在幻觉检测方面存在挑战，外部知识有助于缓解问题，但需要进一步研究提升低资源语言模型的幻觉识别能力。

Abstract: Hallucination is a persistent issue affecting all large language Models
(LLMs), particularly within low-resource languages such as Persian.
PerHalluEval (Persian Hallucination Evaluation) is the first dynamic
hallucination evaluation benchmark tailored for the Persian language. Our
benchmark leverages a three-stage LLM-driven pipeline, augmented with human
validation, to generate plausible answers and summaries regarding QA and
summarization tasks, focusing on detecting extrinsic and intrinsic
hallucinations. Moreover, we used the log probabilities of generated tokens to
select the most believable hallucinated instances. In addition, we engaged
human annotators to highlight Persian-specific contexts in the QA dataset in
order to evaluate LLMs' performance on content specifically related to Persian
culture. Our evaluation of 12 LLMs, including open- and closed-source models
using PerHalluEval, revealed that the models generally struggle in detecting
hallucinated Persian text. We showed that providing external knowledge, i.e.,
the original document for the summarization task, could mitigate hallucination
partially. Furthermore, there was no significant difference in terms of
hallucination when comparing LLMs specifically trained for Persian with others.

</details>


### [82] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: BESPOKE是一个用于评估搜索增强大语言模型个性化能力的基准测试，通过收集真实的人类聊天和搜索历史来构建，提供细粒度的偏好评分和反馈。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强LLM在满足多样化用户需求方面仍不足，需要识别同一查询在不同用户中的不同意图，并以偏好形式提供信息。虽然ChatGPT和Gemini等系统尝试通过用户历史进行个性化，但系统评估仍不足。

Method: 通过长期深入的人类标注构建BESPOKE基准，收集真实的人类聊天和搜索历史，让标注者贡献自己的历史、编写详细信息需求的查询，并用评分和诊断反馈评估响应。

Result: 利用BESPOKE进行系统分析，揭示了信息寻求任务中有效个性化的关键要求，为个性化搜索增强LLM的细粒度评估奠定了基础。

Conclusion: BESPOKE提供了一个现实且诊断性的基准，用于评估搜索增强LLM的个性化能力，代码和数据已公开。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [83] [VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model](https://arxiv.org/abs/2509.21108)
*Junhyuk Choi,Ro-hoon Oh,Jihwan Seol,Bugeun Kim*

Main category: cs.CL

TL;DR: VoiceBBQ是一个语音版本的BBQ数据集，用于评估口语语言模型在内容和声学两个方面的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 由于语音的特性，口语语言模型的社会偏见可能来自内容方面和声学方面，需要专门的评估工具。

Method: 将BBQ数据集的所有上下文转换为受控的语音条件，实现对每个轴的准确性、偏见和一致性评分。

Result: 评估了两个SLM模型：LLaMA-Omni抵抗声学偏见但放大性别和口音偏见；Qwen2-Audio显著减弱这些线索同时保持内容保真度。

Conclusion: VoiceBBQ提供了一个紧凑的测试平台，用于联合诊断口语语言模型的内容和声学偏见。

Abstract: We introduce VoiceBBQ, a spoken extension of the BBQ (Bias Benchmark for
Question Answering) - a dataset that measures social bias by presenting
ambiguous or disambiguated contexts followed by questions that may elicit
stereotypical responses. Due to the nature of speech, social bias in Spoken
Language Models (SLMs) can emerge from two distinct sources: 1) content aspect
and 2) acoustic aspect. The dataset converts every BBQ context into controlled
voice conditions, enabling per-axis accuracy, bias, and consistency scores that
remain comparable to the original text benchmark. Using VoiceBBQ, we evaluate
two SLMs - LLaMA-Omni and Qwen2-Audio - and observe architectural contrasts:
LLaMA-Omni resists acoustic bias while amplifying gender and accent bias,
whereas Qwen2-Audio substantially dampens these cues while preserving content
fidelity. VoiceBBQ thus provides a compact, drop-in testbed for jointly
diagnosing content and acoustic bias across spoken language models.

</details>


### [84] [Acoustic-based Gender Differentiation in Speech-aware Language Models](https://arxiv.org/abs/2509.21125)
*Junhyuk Choi,Jihwan Seol,Nayeon Kim,Chanhee Cho,EunBin Cho,Bugeun Kim*

Main category: cs.CL

TL;DR: 该论文提出了一个用于分析语音语言模型中基于声学的性别差异的新数据集，发现LLaMA-Omni系列模型存在矛盾模式：在性别刻板印象问题上表现出男性导向响应，而在需要性别区分的语境中却表现出性别无关响应。


<details>
  <summary>Details</summary>
Motivation: 语音感知语言模型可能基于说话者性别对相同问题给出不同响应，存在基于声学的性别差异问题，需要系统分析这一现象。

Method: 构建包含9,208个语音样本的数据集，分为性别无关、性别刻板印象和性别依赖三类，评估LLaMA-Omni系列模型，分析其响应模式。

Result: 发现矛盾模式：在性别刻板印象问题上所有模型一致表现出男性导向响应，而在性别依赖问题上却表现出性别无关响应。这种模式主要源于Whisper语音编码器生成的男性导向声学标记。

Conclusion: 当前语音语言模型未能成功消除性别偏见，虽然优先考虑一般公平原则但忽视了语境适当性，需要更复杂的技术来在语音技术中正确利用性别信息。

Abstract: Speech-aware Language Models (SpeechLMs) have fundamentally transformed
human-AI interaction by enabling voice-based communication, yet they may
exhibit acoustic-based gender differentiation where identical questions lead to
different responses based on the speaker's gender. This paper propose a new
dataset that enables systematic analysis of this phenomenon, containing 9,208
speech samples across three categories: Gender-Independent,
Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni
series and discovered a paradoxical pattern; while overall responses seems
identical regardless of gender, the pattern is far from unbiased responses.
Specifically, in Gender-Stereotypical questions, all models consistently
exhibited male-oriented responses; meanwhile, in Gender-Dependent questions
where gender differentiation would be contextually appropriate, models
exhibited responses independent to gender instead. We also confirm that this
pattern does not result from neutral options nor perceived gender of a voice.
When we allow neutral response, models tends to respond neutrally also in
Gender-Dependent questions. The paradoxical pattern yet retains when we applied
gender neutralization methods on speech. Through comparison between SpeechLMs
with corresponding backbone LLMs, we confirmed that these paradoxical patterns
primarily stem from Whisper speech encoders, which generates male-oriented
acoustic tokens. These findings reveal that current SpeechLMs may not
successfully remove gender biases though they prioritized general fairness
principles over contextual appropriateness, highlighting the need for more
sophisticated techniques to utilize gender information properly in speech
technology.

</details>


### [85] [AutoIntent: AutoML for Text Classification](https://arxiv.org/abs/2509.21138)
*Ilya Alekseev,Roman Solomatin,Darina Rustamova,Denis Kuznetsov*

Main category: cs.CL

TL;DR: AutoIntent是一个用于文本分类任务的自动化机器学习工具，提供端到端的自动化流程，包括嵌入模型选择、分类器优化和决策阈值调优，支持多标签分类和范围外检测。


<details>
  <summary>Details</summary>
Motivation: 现有的AutoML解决方案在文本分类任务中缺乏完整的端到端自动化，特别是在嵌入模型选择、分类器优化和决策阈值调优方面的集成支持不足。

Method: AutoIntent采用模块化设计，提供类似sklearn的接口，通过自动化流程实现嵌入模型选择、分类器优化和决策阈值调优，支持多标签分类和范围外检测功能。

Result: 在标准意图分类数据集上，AutoIntent相比现有AutoML工具表现出更优越的性能，同时允许用户在效果和资源消耗之间进行平衡。

Conclusion: AutoIntent是一个高效的自动化文本分类工具，通过端到端的自动化流程和灵活的资源配置，为意图分类任务提供了有效的解决方案。

Abstract: AutoIntent is an automated machine learning tool for text classification
tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with
embedding model selection, classifier optimization, and decision threshold
tuning, all within a modular, sklearn-like interface. The framework is designed
to support multi-label classification and out-of-scope detection. AutoIntent
demonstrates superior performance compared to existing AutoML tools on standard
intent classification datasets and enables users to balance effectiveness and
resource consumption.

</details>


### [86] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多模态关系抽取框架ROC，将传统分类范式改为基于语义相似度的检索任务，通过整合实体类型和位置信息、利用大语言模型扩展关系描述，实现了更好的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态关系抽取方法采用分类范式，存在两个主要局限：(1) 忽略实体类型和位置信息等结构约束；(2) 离散标签缺乏语义表达能力，难以支持细粒度关系理解。

Method: 提出ROC框架，将多模态关系抽取重构为检索任务：1）使用多模态编码器整合实体类型和位置信息；2）利用大语言模型将关系标签扩展为自然语言描述；3）通过基于语义相似度的对比学习对齐实体-关系对。

Result: 在MNRE和MORE基准数据集上实现了最先进的性能，同时表现出更强的鲁棒性和可解释性。

Conclusion: ROC框架通过检索范式有效克服了传统分类方法的局限性，为多模态关系抽取提供了更语义化和结构化的解决方案。

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [87] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 该研究发现LLM训练数据中存在语法模板与领域之间的虚假相关性，这种相关性会影响模型性能，甚至可能被用于绕过安全微调。


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何同时处理指令的语义、领域和语法信息，特别是语法模板在训练数据中的普遍性及其对模型行为的影响。

Method: 使用合成训练数据集分析语法-领域相关性对OLMo-2模型性能的影响，建立评估框架检测FlanV2数据集中的这种现象，并进行安全微调案例研究。

Result: 语法-领域相关性会降低OLMo-2模型在实体知识任务上的性能（平均0.51±0.06），在OLMo-2-7B、Llama-4-Maverick和GPT-4o等模型中均存在此现象，且可用于绕过安全拒绝机制。

Conclusion: 需要显式测试语法-领域相关性，并确保训练数据中特别是领域内的语法多样性，以防止此类虚假相关性。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


### [88] [Who's Laughing Now? An Overview of Computational Humour Generation and Explanation](https://arxiv.org/abs/2509.21175)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文综述了计算幽默在自然语言处理中的研究现状，重点关注幽默生成和解释任务，指出当前模型在非双关语幽默处理方面仍落后于人类能力，并讨论了该领域的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 幽默是人类的基本特质，其计算理解是NLP中最具挑战性的任务之一。幽默作为抽象、创造性且依赖上下文的结构，需要广泛推理能力，是评估现代大语言模型常识知识和推理能力的合适任务。

Method: 本文采用文献综述的方法，系统梳理了计算幽默领域的研究现状，特别是生成任务（创作和解释）方面的进展。

Result: 研究发现，尽管幽默理解具有基础NLP任务的所有特征，但超越双关语的幽默生成和解释研究仍然稀少，最先进的模型仍无法达到人类水平。

Conclusion: 作者强调了计算幽默处理作为NLP子学科的重要性，并基于幽默的主观性和伦理模糊性特点，提出了该领域未来研究的广泛讨论方向。

Abstract: The creation and perception of humour is a fundamental human trait,
positioning its computational understanding as one of the most challenging
tasks in natural language processing (NLP). As an abstract, creative, and
frequently context-dependent construct, humour requires extensive reasoning to
understand and create, making it a pertinent task for assessing the
common-sense knowledge and reasoning abilities of modern large language models
(LLMs). In this work, we survey the landscape of computational humour as it
pertains to the generative tasks of creation and explanation. We observe that,
despite the task of understanding humour bearing all the hallmarks of a
foundational NLP task, work on generating and explaining humour beyond puns
remains sparse, while state-of-the-art models continue to fall short of human
capabilities. We bookend our literature survey by motivating the importance of
computational humour processing as a subdiscipline of NLP and presenting an
extensive discussion of future directions for research in the area that takes
into account the subjective and ethically ambiguous nature of humour.

</details>


### [89] [GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models](https://arxiv.org/abs/2509.21192)
*Jieli Zhu,Vi Ngoc-Nha Tran*

Main category: cs.CL

TL;DR: 该研究探讨了基于小语言模型（SLM）的聊天机器人的个人可识别信息（PII）泄露问题，提出了GEP方法用于PII提取，实验显示泄露量比传统模板方法提升60倍。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在特定领域性能接近大语言模型且能耗更低，但其在下游任务中的PII泄露问题尚未被充分研究。

Method: 基于BioGPT微调ChatBioGPT模型，提出基于贪婪坐标梯度的GEP方法进行PII提取，并在自由风格插入场景下测试。

Result: GEP方法比传统模板方法泄露量提升60倍，在复杂自由风格插入场景下仍能达到4.53%的PII泄露率。

Conclusion: SLM存在显著的PII泄露风险，需要开发更有效的检测和防护方法，GEP为PII泄露检测提供了新思路。

Abstract: Small language models (SLMs) become unprecedentedly appealing due to their
approximately equivalent performance compared to large language models (LLMs)
in certain fields with less energy and time consumption during training and
inference. However, the personally identifiable information (PII) leakage of
SLMs for downstream tasks has yet to be explored. In this study, we investigate
the PII leakage of the chatbot based on SLM. We first finetune a new chatbot,
i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca
and HealthCareMagic. It shows a matchable performance in BERTscore compared
with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove
that the previous template-based PII attacking methods cannot effectively
extract the PII in the dataset for leakage detection under the SLM condition.
We then propose GEP, which is a greedy coordinate gradient-based (GCG) method
specifically designed for PII extraction. We conduct experimental studies of
GEP and the results show an increment of up to 60$\times$ more leakage compared
with the previous template-based methods. We further expand the capability of
GEP in the case of a more complicated and realistic situation by conducting
free-style insertion where the inserted PII in the dataset is in the form of
various syntactic expressions instead of fixed templates, and GEP is still able
to reveal a PII leakage rate of up to 4.53%.

</details>


### [90] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [91] [CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis](https://arxiv.org/abs/2509.21208)
*Xinzhe Xu,Liang Zhao,Hongshen Xu,Chen Chen*

Main category: cs.CL

TL;DR: 本文介绍了CLaw基准，用于评估LLMs在中国法律知识及其推理应用方面的表现，发现当前模型在准确检索法律条文方面存在显著困难。


<details>
  <summary>Details</summary>
Motivation: LLMs在法律文本分析和引用相关法规方面的可靠性受到通用预训练的限制，需要专门的法律知识评估基准。

Method: 构建了包含306部中国国家法规的细粒度语料库（64,849条目）和254个基于案例的推理实例，用于评估LLMs的法律知识检索和推理能力。

Result: 实证评估显示大多数当代LLMs难以忠实再现法律条文，这严重影响了其法律推理的可靠性。

Conclusion: 实现可信赖的法律推理需要准确的知识检索（通过SFT或RAG增强）与强大的一般推理能力的协同作用。

Abstract: Large Language Models (LLMs) are increasingly tasked with analyzing legal
texts and citing relevant statutes, yet their reliability is often compromised
by general pre-training that ingests legal texts without specialized focus,
obscuring the true depth of their legal knowledge. This paper introduces CLaw,
a novel benchmark specifically engineered to meticulously evaluate LLMs on
Chinese legal knowledge and its application in reasoning. CLaw comprises two
key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese
national statutes, segmented to the subparagraph level and incorporating
precise historical revision timesteps for rigorous recall evaluation (64,849
entries), and (2) a challenging set of 254 case-based reasoning instances
derived from China Supreme Court curated materials to assess the practical
application of legal knowledge. Our empirical evaluation reveals that most
contemporary LLMs significantly struggle to faithfully reproduce legal
provisions. As accurate retrieval and citation of legal provisions form the
basis of legal reasoning, this deficiency critically undermines the reliability
of their responses. We contend that achieving trustworthy legal reasoning in
LLMs requires a robust synergy of accurate knowledge retrieval--potentially
enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation
(RAG)--and strong general reasoning capabilities. This work provides an
essential benchmark and critical insights for advancing domain-specific LLM
reasoning, particularly within the complex legal sphere.

</details>


### [92] [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
*Yaxiong Wu,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: SGMem是一种基于句子图记忆的长期对话记忆管理方法，通过在分块单元中构建句子级图结构，有效组织多粒度对话信息，提升长期对话问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于事实提取或摘要的方法难以有效组织和检索不同粒度的对话信息，无法满足长期对话代理对超出LLM上下文窗口的历史记忆管理需求。

Method: SGMem将对话表示为分块单元内的句子级图结构，捕捉轮次、回合和会话级别的关联，结合检索的原始对话与生成的记忆（摘要、事实和见解），为LLM提供连贯相关的上下文。

Result: 在LongMemEval和LoCoMo数据集上的实验表明，SGMem持续提升准确性，在长期对话问答任务中优于强基线方法。

Conclusion: SGMem通过句子图结构有效管理长期对话记忆，为LLM提供更好的上下文支持，在长期对话场景中表现出色。

Abstract: Long-term conversational agents require effective memory management to handle
dialogue histories that exceed the context window of large language models
(LLMs). Existing methods based on fact extraction or summarization reduce
redundancy but struggle to organize and retrieve relevant information across
different granularities of dialogue and generated memory. We introduce SGMem
(Sentence Graph Memory), which represents dialogue as sentence-level graphs
within chunked units, capturing associations across turn-, round-, and
session-level contexts. By combining retrieved raw dialogue with generated
memory such as summaries, facts and insights, SGMem supplies LLMs with coherent
and relevant context for response generation. Experiments on LongMemEval and
LoCoMo show that SGMem consistently improves accuracy and outperforms strong
baselines in long-term conversational question answering.

</details>


### [93] [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
*Yaxiong Wu,Jianyuan Bo,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: QCG-RAG是一个查询中心的图RAG框架，通过可控粒度的查询中心图索引和多跳块检索，解决了现有图RAG方法在粒度选择上的困境。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法面临粒度困境：细粒度实体级图导致高令牌成本和上下文丢失，而粗粒度文档级图无法捕捉细微关系。

Method: 利用Doc2Query和Doc2Query--构建查询中心图，实现可控粒度的索引，并设计专门的多跳检索机制通过生成查询选择相关块。

Result: 在LiHuaWorld和MultiHop-RAG数据集上的实验表明，QCG-RAG在问答准确性上持续优于现有的基于块和基于图的RAG方法。

Conclusion: QCG-RAG为多跳推理建立了一个新范式，通过查询中心的方法改善了图质量和可解释性。

Abstract: Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.

</details>


### [94] [Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](https://arxiv.org/abs/2509.21262)
*Evgeny Kaskov,Elizaveta Petrova,Petr Surovtsev,Anna Kostikova,Ilya Mistiurin,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CL

TL;DR: 本文研究了同形异义词在扩散模型中导致的重复生成问题，以及Anglocentric偏见对此的加剧作用，提出了测量重复率的方法和通过提示扩展缓解该问题的方案。


<details>
  <summary>Details</summary>
Motivation: 同形异义词（拼写相同但含义不同的词）给生成模型带来挑战，扩散模型在处理这类词时可能同时生成多个含义，导致同形异义重复问题。Anglocentric偏见进一步加剧了这一问题，因为非英语词汇在翻译成英语后可能变成同形异义词。

Method: 引入测量重复率的方法，使用视觉语言模型（VLM）进行自动评估和人工评估，研究通过提示扩展来缓解同形异义重复问题。

Result: 评估了不同扩散模型的重复率，发现提示扩展方法能有效减少同形异义重复问题，包括由Anglocentric偏见引起的重复。

Conclusion: 同形异义重复是扩散模型的重要问题，提示扩展是有效的缓解方法，自动评估管道代码已公开。

Abstract: Homonyms are words with identical spelling but distinct meanings, which pose
challenges for many generative models. When a homonym appears in a prompt,
diffusion models may generate multiple senses of the word simultaneously, which
is known as homonym duplication. This issue is further complicated by an
Anglocentric bias, which includes an additional translation step before the
text-to-image model pipeline. As a result, even words that are not homonymous
in the original language may become homonyms and lose their meaning after
translation into English. In this paper, we introduce a method for measuring
duplication rates and conduct evaluations of different diffusion models using
both automatic evaluation utilizing Vision-Language Models (VLM) and human
evaluation. Additionally, we investigate methods to mitigate the homonym
duplication problem through prompt expansion, demonstrating that this approach
also effectively reduces duplication related to Anglocentric bias. The code for
the automatic evaluation pipeline is publicly available.

</details>


### [95] [LLM Output Homogenization is Task Dependent](https://arxiv.org/abs/2509.21267)
*Shomik Jain,Jack Lanchantin,Maximilian Nickel,Karen Ullrich,Ashia Wilson,Jamelle Watson-Daniels*

Main category: cs.CL

TL;DR: 本文提出了一种任务依赖的方法来评估和缓解大语言模型的输出同质化问题，通过任务分类、功能多样性评估和任务锚定采样技术，在保持质量的同时提高多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的输出同质化研究往往未能以任务依赖的方式概念化多样性。不同任务类别对同质化的定义和问题严重性不同，需要针对性的评估和缓解方法。

Method: 提出了包含八个任务类别的分类法，引入了任务锚定功能多样性评估方法，开发了任务锚定采样技术，在需要多样性的任务类别中增加功能多样性，同时在需要同质化的任务中保持同质化。

Result: 成功提高了功能多样性，同时保持了响应质量，挑战了多样性-质量权衡的传统认知。

Conclusion: 任务依赖的方法显著改进了输出同质化的评估和缓解，为不同任务类别提供了更精准的多样性管理方案。

Abstract: A large language model can be less helpful if it exhibits output response
homogenization. But whether two responses are considered homogeneous, and
whether such homogenization is problematic, both depend on the task category.
For instance, in objective math tasks, we often expect no variation in the
final answer but anticipate variation in the problem-solving strategy. Whereas,
for creative writing tasks, we may expect variation in key narrative components
(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity
produced by temperature-sampling. Previous work addressing output
homogenization often fails to conceptualize diversity in a task-dependent way.
We address this gap in the literature directly by making the following
contributions. (1) We present a task taxonomy comprised of eight task
categories that each have distinct conceptualizations of output homogenization.
(2) We introduce task-anchored functional diversity to better evaluate output
homogenization. (3) We propose a task-anchored sampling technique that
increases functional diversity for task categories where homogenization is
undesired, while preserving homogenization where it is desired. (4) We
challenge the perceived existence of a diversity-quality trade-off by
increasing functional diversity while maintaining response quality. Overall, we
demonstrate how task dependence improves the evaluation and mitigation of
output homogenization.

</details>


### [96] [LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text](https://arxiv.org/abs/2509.21269)
*Irina Tolstykh,Aleksandra Tsybina,Sergey Yakubson,Maksim Kuprashevich*

Main category: cs.CL

TL;DR: LLMTrace是一个新的大规模双语（英语和俄语）语料库，专门用于AI生成文本检测，支持全文二元分类和AI生成区间检测任务。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测数据集存在三个主要问题：使用过时模型生成、主要为英语、缺乏混合人机写作场景支持，特别是缺少字符级注释来精确定位AI生成片段。

Method: 使用多样化的现代专有和开源LLMs构建双语数据集，提供字符级注释，支持全文分类和AI生成区间检测两个关键任务。

Result: 开发了LLMTrace语料库，包含英语和俄语文本，具有字符级注释功能，能够精确定位文本中的AI生成片段。

Conclusion: LLMTrace将为训练和评估下一代更细致实用的AI检测模型提供重要资源，推动AI文本检测技术的发展。

Abstract: The widespread use of human-like text from Large Language Models (LLMs)
necessitates the development of robust detection systems. However, progress is
limited by a critical lack of suitable training data; existing datasets are
often generated with outdated models, are predominantly in English, and fail to
address the increasingly common scenario of mixed human-AI authorship.
Crucially, while some datasets address mixed authorship, none provide the
character-level annotations required for the precise localization of
AI-generated segments within a text. To address these gaps, we introduce
LLMTrace, a new large-scale, bilingual (English and Russian) corpus for
AI-generated text detection. Constructed using a diverse range of modern
proprietary and open-source LLMs, our dataset is designed to support two key
tasks: traditional full-text binary classification (human vs. AI) and the novel
task of AI-generated interval detection, facilitated by character-level
annotations. We believe LLMTrace will serve as a vital resource for training
and evaluating the next generation of more nuanced and practical AI detection
models. The project page is available at
\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.

</details>


### [97] [Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond](https://arxiv.org/abs/2509.21284)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文理论分析了输入扰动对思维链（CoT）输出波动的影响，推导了输出波动可接受范围内的输入扰动上界，并证明该上界与推理步骤数正相关，且无限长推理过程也无法消除输入扰动的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明CoT输出受输入扰动显著影响，但缺乏理论解释这种影响如何传播，这限制了深入理解和提示优化方法的改进。

Method: 首先推导输出波动可接受范围内的输入扰动上界，然后在线性自注意力（LSA）模型上应用这些结论，证明输入扰动上界与输入嵌入和隐藏状态向量的范数负相关。

Result: 在三个主流数据集和四个主流模型上的实验结果与理论分析一致，验证了理论发现的正确性。

Conclusion: 理论分析揭示了输入扰动对CoT输出的影响机制，为理解扰动传播和优化提示方法提供了理论基础。

Abstract: Existing research indicates that the output of Chain-of-Thought (CoT) is
significantly affected by input perturbations. Although many methods aim to
mitigate such impact by optimizing prompts, a theoretical explanation of how
these perturbations influence CoT outputs remains an open area of research.
This gap limits our in-depth understanding of how input perturbations propagate
during the reasoning process and hinders further improvements in prompt
optimization methods. Therefore, in this paper, we theoretically analyze the
effect of input perturbations on the fluctuation of CoT outputs. We first
derive an upper bound for input perturbations under the condition that the
output fluctuation is within an acceptable range, based on which we prove that:
(i) This upper bound is positively correlated with the number of reasoning
steps in the CoT; (ii) Even an infinitely long reasoning process cannot
eliminate the impact of input perturbations. We then apply these conclusions to
the Linear Self-Attention (LSA) model, which can be viewed as a simplified
version of the Transformer. For the LSA model, we prove that the upper bound
for input perturbation is negatively correlated with the norms of the input
embedding and hidden state vectors. To validate this theoretical analysis, we
conduct experiments on three mainstream datasets and four mainstream models.
The experimental results align with our theoretical analysis, empirically
demonstrating the correctness of our findings.

</details>


### [98] [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
*Kin Ian Lo,Hala Hawashin,Mina Abbaszadeh,Tilen Limback-Stokin,Hadi Wazni,Mehrnoosh Sadrzadeh*

Main category: cs.CL

TL;DR: DisCoCLIP是一种多模态编码器，通过结合冻结的CLIP视觉转换器和新型张量网络文本编码器，显式编码句法结构，显著提升了视觉语言模型对语言组合结构的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在大规模图像-文本对齐方面表现出色，但往往忽略语言的结构性，导致在处理依赖词序和谓词-论元结构的任务时表现不佳。

Method: 使用组合范畴语法解析器解析句子生成分布词张量，通过张量分解降低参数数量，采用自监督对比损失进行端到端训练。

Result: 将CLIP的SVO-Probes动词准确率从77.6%提升至82.4%，ARO归因和关系分数分别提升超过9%和4%，在SVO-Swap基准测试中达到93.7%。

Conclusion: 通过张量网络嵌入显式语言结构可以产生可解释、参数高效的表示，显著提升视觉语言任务中的组合推理能力。

Abstract: Recent vision-language models excel at large-scale image-text alignment but
often neglect the compositional structure of language, leading to failures on
tasks that hinge on word order and predicate-argument structure. We introduce
DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer
with a novel tensor network text encoder that explicitly encodes syntactic
structure. Sentences are parsed with a Combinatory Categorial Grammar parser to
yield distributional word tensors whose contractions mirror the sentence's
grammatical derivation. To keep the model efficient, high-order tensors are
factorized with tensor decompositions, reducing parameter count from tens of
millions to under one million. Trained end-to-end with a self-supervised
contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and
word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,
boosts ARO attribution and relation scores by over 9% and 4%, and achieves
93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that
embedding explicit linguistic structure via tensor networks yields
interpretable, parameter-efficient representations that substantially improve
compositional reasoning in vision-language tasks.

</details>


### [99] [The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/abs/2509.21294)
*Pranjal A. Chitale,Varun Gumma,Sanchit Ahuja,Prashant Kodali,Manan Uppadhyay,Deepthi Sudharsan,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 本文介绍了Updesh——一个针对13种印度语言的大规模合成指令跟随数据集，通过自下而上的生成策略创建文化情境化数据，在多语言AI系统开发中取得了显著成效。


<details>
  <summary>Details</summary>
Motivation: 解决多语言AI系统在低资源语言和文化背景下的有效性挑战，探索合成数据在多语言多文化环境中的应用潜力。

Method: 采用自下而上的生成策略，使用大型开源LLM（≥235B参数）基于语言特定的维基百科内容生成文化情境化数据，创建了包含950万数据点的Updesh数据集。

Result: 通过自动指标和人工评估验证数据质量，下游评估显示在15个多语言数据集上，基于Updesh训练的模型在生成任务上显著提升，在低中资源语言中改进最为明显。

Conclusion: 有效的多语言AI需要结合情境感知、文化基础的多方面数据策展和生成策略，自下而上的方法是对主流自上而下翻译方法的有效补充。

Abstract: Developing AI systems that operate effectively across languages while
remaining culturally grounded is a long-standing challenge, particularly in
low-resource settings. Synthetic data provides a promising avenue, yet its
effectiveness in multilingual and multicultural contexts remains underexplored.
We investigate the creation and impact of synthetic, culturally contextualized
datasets for Indian languages through a bottom-up generation strategy that
prompts large open-source LLMs (>= 235B parameters) to ground data generation
in language-specific Wikipedia content. This approach complements the dominant
top-down paradigm of translating synthetic datasets from high-resource
languages such as English. We introduce Updesh, a high-quality large-scale
synthetic instruction-following dataset comprising 9.5M data points across 13
Indian languages, encompassing diverse reasoning and generative tasks with an
emphasis on long-context, multi-turn capabilities, and alignment with Indian
cultural contexts. A comprehensive evaluation incorporating both automated
metrics and human annotation across 10k assessments indicates that generated
data is high quality; though, human evaluation highlights areas for further
improvement. Additionally, we perform downstream evaluations by fine-tuning
models on our dataset and assessing the performance across 15 diverse
multilingual datasets. Models trained on Updesh consistently achieve
significant gains on generative tasks and remain competitive on multiple-choice
style NLU tasks. Notably, relative improvements are most pronounced in low and
medium-resource languages, narrowing their gap with high-resource languages.
These findings provide empirical evidence that effective multilingual AI
requires multi-faceted data curation and generation strategies that incorporate
context-aware, culturally grounded methodologies.

</details>


### [100] [Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs](https://arxiv.org/abs/2509.21305)
*Daniel Vennemeyer,Phan Anh Duong,Tiffany Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文研究发现大语言模型中的奉承行为可分为奉承性同意和奉承性赞美，这两种行为在潜在空间中具有不同的线性方向，可以独立调控。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常表现出奉承行为，但尚不清楚这些行为是源于单一机制还是多个不同过程。

Method: 使用均值差异方向、激活添加和子空间几何方法，在多个模型和数据集上分析奉承行为。

Result: 三种行为（奉承性同意、奉承性赞美、真诚同意）在潜在空间中具有不同的线性方向，可以独立放大或抑制而不相互影响，且这种表示结构在不同模型家族和规模中保持一致。

Conclusion: 奉承行为对应于不同的、可独立操控的表示，表明这些行为具有独立的神经机制。

Abstract: Large language models (LLMs) often exhibit sycophantic behaviors -- such as
excessive agreement with or flattery of the user -- but it is unclear whether
these behaviors arise from a single mechanism or multiple distinct processes.
We decompose sycophancy into sycophantic agreement and sycophantic praise,
contrasting both with genuine agreement. Using difference-in-means directions,
activation additions, and subspace geometry across multiple models and
datasets, we show that: (1) the three behaviors are encoded along distinct
linear directions in latent space; (2) each behavior can be independently
amplified or suppressed without affecting the others; and (3) their
representational structure is consistent across model families and scales.
These results suggest that sycophantic behaviors correspond to distinct,
independently steerable representations.

</details>


### [101] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: RLBFF结合人类偏好和规则验证，通过二元灵活反馈训练奖励模型，在多个基准测试中取得最佳性能，并提供了开源对齐方案。


<details>
  <summary>Details</summary>
Motivation: RLHF依赖缺乏明确标准的人类判断，存在可解释性和奖励攻击问题；RLVR局限于基于正确性的验证器。需要结合人类驱动的偏好灵活性和规则验证的精确性。

Method: 从自然语言反馈中提取可二元回答的原则（如信息准确性、代码可读性），将奖励模型训练构建为蕴含任务（响应是否满足任意原则）。

Result: 在RM-Bench上达到86.2%，JudgeBench上达到81.4%（截至2025年9月24日排行榜第一）。使用RLBFF对齐的Qwen3-32B在MT-Bench、WildBench和Arena Hard v2上匹配或超越o3-mini和DeepSeek R1性能，推理成本不到5%。

Conclusion: RLBFF成功结合了人类偏好和规则验证的优势，提供了可定制、高性能且成本效益高的对齐方法，为LLM后训练提供了新的有效范式。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


### [102] [SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](https://arxiv.org/abs/2509.21320)
*Yizhou Wang,Chen Tang,Han Deng,Jiabei Xiao,Jiaqi Liu,Jianyu Wu,Jun Yao,Pengze Li,Encheng Su,Lintao Wang,Guohang Zhuang,Yuchen Ren,Ben Fei,Ming Hu,Xin Chen,Dongzhan Zhou,Junjun He,Xiangyu Yue,Zhenfei Yin,Jiamin Wu,Qihao Zheng,Yuhao Zhou,Huihui Xu,Chenglong Ma,Yan Lu,Wenlong Zhang,Chunfeng Song,Philip Torr,Shixiang Tang,Xinzhu Ma,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 提出了一个科学推理基础模型，通过自然语言与异构科学表示的对齐，支持多种科学任务能力


<details>
  <summary>Details</summary>
Motivation: 解决科学领域中自然语言与专业表示之间的鸿沟，提升跨领域科学推理的泛化能力和保真度

Method: 预训练206B token的科学语料，通过40M指令的监督微调、退火冷启动引导和任务特定奖励的强化学习来培养科学推理能力

Result: 相比专业系统，该方法扩展了指令覆盖范围，改善了跨领域泛化能力，并增强了保真度

Conclusion: 跨学科学习增强了迁移能力和下游可靠性，模型、指令调优数据集和评估代码已开源

Abstract: We present a scientific reasoning foundation model that aligns natural
language with heterogeneous scientific representations. The model is pretrained
on a 206B-token corpus spanning scientific text, pure sequences, and
sequence-text pairs, then aligned via SFT on 40M instructions, annealed
cold-start bootstrapping to elicit long-form chain-of-thought, and
reinforcement learning with task-specific reward shaping, which instills
deliberate scientific reasoning. It supports four capability families, covering
up to 103 tasks across workflows: (i) faithful translation between text and
scientific formats, (ii) text/knowledge extraction, (iii) property prediction,
(iv) property classification, (v) unconditional and conditional sequence
generation and design. Compared with specialist systems, our approach broadens
instruction coverage, improves cross-domain generalization, and enhances
fidelity. We detail data curation and training and show that cross-discipline
learning strengthens transfer and downstream reliability. The model, instruct
tuning datasets and the evaluation code are open-sourced at
https://huggingface.co/SciReason and
https://github.com/open-sciencelab/SciReason.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [103] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 本文提出了一种基于下一个令牌概率（NTPs）的轻量级幻觉检测方法，通过训练传统机器学习模型来检测视觉语言模型（VLMs）生成的文本中的幻觉问题，该方法计算效率高且性能可与强VLMs相媲美。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）的幻觉问题（视觉内容与生成文本之间的错位）严重影响了模型的可靠性。现有的检测方法通常使用相同或不同的VLM来评估生成输出，但这种方法计算量大且会增加模型延迟。

Method: 提出基于VLM的下一个令牌概率（NTPs）的轻量级检测方法，利用NTPs量化模型不确定性，训练传统ML模型进行幻觉检测。还引入了语言NTPs增强检测性能，并整合VLM的幻觉预测分数。

Result: 实验结果表明，基于NTP的特征是有效的幻觉预测指标，简单的ML模型能达到与强VLMs相当的检测性能。增强语言NTPs和整合VLM预测分数能进一步提升检测效果。

Conclusion: 这项研究为开发简单、轻量级的解决方案铺平了道路，能够有效提升VLMs的可靠性，NTPs为基础的方法在效率和性能上都有显著优势。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [104] [Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification](https://arxiv.org/abs/2509.20420)
*Elias N. Zois,Moises Diaz,Salem Said,Miguel A. Ferrer*

Main category: cs.CV

TL;DR: 该论文提出了一种基于黎曼几何的准合成数据生成框架，用于解决离线手写签名验证中跨作者独立设置的泛化问题。通过利用对称正定矩阵的黎曼几何特性，从少量真实样本生成合成数据，并在两个流行数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 离线手写签名验证在跨作者独立设置下具有挑战性，现有方法通常依赖真实数据集进行训练。本文旨在探索在黎曼空间中生成合成数据的潜力，以解决数据稀缺和泛化问题。

Method: 提出基于对称正定矩阵黎曼几何的准合成数据生成框架：使用Riemannian Gaussian Mixture从少量真实样本识别合成作者中心，通过Riemannian Gaussian采样生成正负样本对，采用度量学习框架进行训练。

Result: 在包含西方和亚洲书写风格的两个流行签名数据集上的实验表明，该方法在数据集内和跨数据集评估协议下均实现了低错误率。

Conclusion: 研究证实了在黎曼空间中生成合成数据对于作者独立签名验证系统的潜力，准合成方法能够有效提升模型泛化能力。

Abstract: Offline handwritten signature verification remains a challenging task,
particularly in writer-independent settings where models must generalize across
unseen individuals. Recent developments have highlighted the advantage of
geometrically inspired representations, such as covariance descriptors on
Riemannian manifolds. However, past or present, handcrafted or data-driven
methods usually depend on real-world signature datasets for classifier
training. We introduce a quasi-synthetic data generation framework leveraging
the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small
set of genuine samples in the SPD space is the seed to a Riemannian Gaussian
Mixture which identifies Riemannian centers as synthetic writers and variances
as their properties. Riemannian Gaussian sampling on each center generates
positive as well as negative synthetic SPD populations. A metric learning
framework utilizes pairs of similar and dissimilar SPD points, subsequently
testing it over on real-world datasets. Experiments conducted on two popular
signature datasets, encompassing Western and Asian writing styles, demonstrate
the efficacy of the proposed approach under both intra- and cross- dataset
evaluation protocols. The results indicate that our quasi-synthetic approach
achieves low error rates, highlighting the potential of generating synthetic
data in Riemannian spaces for writer-independent signature verification
systems.

</details>


### [105] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0是一个高效的多模态图像生成系统，统一了文本到图像合成、图像编辑和多图像组合功能，采用高效的扩散变换器和优化的VAE，能够快速生成高分辨率图像，并在多项任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统文本到图像系统功能单一，无法满足复杂创作需求。Seedream 4.0旨在开发一个统一框架，将多种图像生成和编辑功能整合，提供更交互式和多维度的创作工具。

Method: 采用高效的扩散变换器结合强大的VAE，显著减少图像标记数量；通过数十亿文本-图像对进行预训练；结合对抗蒸馏、分布匹配、量化和推测解码等技术加速推理；使用精细调优的VLM模型进行多模态后训练。

Result: 生成2K图像推理时间最快达1.8秒；在文本到图像合成和多模态图像编辑任务上达到最先进水平；在复杂任务中表现出卓越的多模态能力，包括精确图像编辑和上下文推理；支持多图像参考和多图像生成。

Conclusion: Seedream 4.0将传统文本到图像系统扩展为更交互式和多维度的创作工具，推动了生成式AI在创意和专业应用领域的边界，现已可通过指定链接访问。

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image
generation system that unifies text-to-image (T2I) synthesis, image editing,
and multi-image composition within a single framework. We develop a highly
efficient diffusion transformer with a powerful VAE which also can reduce the
number of image tokens considerably. This allows for efficient training of our
model, and enables it to fast generate native high-resolution images (e.g.,
1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning
diverse taxonomies and knowledge-centric concepts. Comprehensive data
collection across hundreds of vertical scenarios, coupled with optimized
strategies, ensures stable and large-scale training, with strong
generalization. By incorporating a carefully fine-tuned VLM model, we perform
multi-modal post-training for training both T2I and image editing tasks
jointly. For inference acceleration, we integrate adversarial distillation,
distribution matching, and quantization, as well as speculative decoding. It
achieves an inference time of up to 1.8 seconds for generating a 2K image
(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream
4.0 can achieve state-of-the-art results on both T2I and multimodal image
editing. In particular, it demonstrates exceptional multimodal capabilities in
complex tasks, including precise image editing and in-context reasoning, and
also allows for multi-image reference, and can generate multiple output images.
This extends traditional T2I systems into an more interactive and
multidimensional creative tool, pushing the boundary of generative AI for both
creativity and professional applications. Seedream 4.0 is now accessible on
https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [106] [A Contrastive Learning Framework for Breast Cancer Detection](https://arxiv.org/abs/2509.20474)
*Samia Saeed,Khuram Naveed*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的乳腺癌检测框架，使用Resnet-50在半监督设置下训练，在小型标注数据集上实现了96.7%的准确率。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球第二大癌症死因，早期检测对治疗效果至关重要。虽然深度学习在医学影像分析中表现出色，但受限于标注数据不足的问题。

Method: 采用对比学习框架，使用Resnet-50在半监督模式下训练，利用大量未标注的乳腺X光片数据，通过数据增强和变换技术提升性能。

Result: 在INbreast和MIAS基准数据集上实现了96.7%的乳腺癌检测准确率，超越了现有最佳方法。

Conclusion: 对比学习框架能够有效解决医学影像分析中标注数据不足的问题，在乳腺癌早期检测中表现出优越性能。

Abstract: Breast cancer, the second leading cause of cancer-related deaths globally,
accounts for a quarter of all cancer cases [1]. To lower this death rate, it is
crucial to detect tumors early, as early-stage detection significantly improves
treatment outcomes. Advances in non-invasive imaging techniques have made early
detection possible through computer-aided detection (CAD) systems which rely on
traditional image analysis to identify malignancies. However, there is a
growing shift towards deep learning methods due to their superior
effectiveness. Despite their potential, deep learning methods often struggle
with accuracy due to the limited availability of large-labeled datasets for
training. To address this issue, our study introduces a Contrastive Learning
(CL) framework, which excels with smaller labeled datasets. In this regard, we
train Resnet-50 in semi supervised CL approach using similarity index on a
large amount of unlabeled mammogram data. In this regard, we use various
augmentation and transformations which help improve the performance of our
approach. Finally, we tune our model on a small set of labelled data that
outperforms the existing state of the art. Specifically, we observed a 96.7%
accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.

</details>


### [107] [Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data](https://arxiv.org/abs/2509.20479)
*Simon Baeuerle,Pratik Khanna,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Damir Shakirov,Andreas Steimer,Ralf Mikut*

Main category: cs.CV

TL;DR: 本文测试了多个基础模型在工业图像质量检测中的应用，发现这些模型在公开数据集上表现良好，但在真实工业数据上全部失败。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型的零样本泛化能力替代繁琐的人工标注，实现跨产品的自动化质量检测，以节省模型部署成本。

Method: 在定制真实工业图像数据和公共图像数据上测试多个最新基础模型，比较它们在零样本设置下的性能。

Result: 所有测试的基础模型在真实工业数据上均失败，而相同的模型在公共基准数据集上表现良好。

Conclusion: 基础模型在工业质量检测场景中的直接应用存在局限性，需要针对工业场景的特殊性进行改进。

Abstract: Foundation Models (FMs) have shown impressive performance on various text and
image processing tasks. They can generalize across domains and datasets in a
zero-shot setting. This could make them suitable for automated quality
inspection during series manufacturing, where various types of images are being
evaluated for many different products. Replacing tedious labeling tasks with a
simple text prompt to describe anomalies and utilizing the same models across
many products would save significant efforts during model setup and
implementation. This is a strong advantage over supervised Artificial
Intelligence (AI) models, which are trained for individual applications and
require labeled training data. We test multiple recent FMs on both custom
real-world industrial image data and public image data. We show that all of
those models fail on our real-world data, while the very same models perform
well on public benchmark datasets.

</details>


### [108] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: 提出通用神经空间（NS）来解决多任务视觉应用中模块化任务效率低下的问题，通过编码器-解码器框架预计算跨视觉和成像任务的特征，实现多下游AI模块共享同一特征空间。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型多为特定高精度任务定制，在需要执行一系列模块化任务的应用中效率低下，因为每个任务都需要映射到不同的潜在域。

Method: 使用轻量级CNN骨干网络构建编码器-解码器框架，学习具有变换感知和泛化能力的表示，使多个下游AI模块能够在共享的特征空间中运行。

Result: 该方法减少了冗余，提高了跨域泛化能力，为高效多任务视觉流水线奠定了基础，并成功应用于去马赛克、去噪、深度估计和语义分割等任务。

Conclusion: 通用神经空间架构为多任务视觉应用提供了高效的解决方案，相比大型transformer骨干网络更轻量且硬件兼容性更好。

Abstract: The majority of AI models in imaging and vision are customized to perform on
specific high-precision task. However, this strategy is inefficient for
applications with a series of modular tasks, since each requires a mapping into
a disparate latent domain. To address this inefficiency, we proposed a
universal Neural Space (NS), where an encoder-decoder framework pre-computes
features across vision and imaging tasks. Our encoder learns transformation
aware, generalizable representations, which enable multiple downstream AI
modules to share the same feature space. This architecture reduces redundancy,
improves generalization across domain shift, and establishes a foundation for
effecient multi-task vision pipelines. Furthermore, as opposed to larger
transformer backbones, our backbone is lightweight and CNN-based, allowing for
wider across hardware. We furthur demonstrate that imaging and vision modules,
such as demosaicing, denoising, depth estimation and semantic segmentation can
be performed efficiently in the NS.

</details>


### [109] [Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment](https://arxiv.org/abs/2509.20484)
*Dani Manjah,Tim Bary,Benoît Gérin,Benoît Macq,Christophe de Vleeschouwer*

Main category: cs.CV

TL;DR: 本文探索了在边缘摄像头系统中如何选择最有用的训练图像，以在保持低传输成本的同时最大化模型质量。研究表明，结合高置信度流式策略和多样性方法可以在相似训练负载下，用最少的查询获得高质量模型。


<details>
  <summary>Details</summary>
Motivation: 边缘摄像头系统面临不断变化的环境，需要定期更新模型。实践中，复杂的大模型在中央服务器上标注数据，然后用于训练适合计算能力有限的边缘设备的小模型。需要找到在保持低传输成本的同时最大化模型质量的方法。

Method: 采用高置信度流式策略与基于多样性的方法相结合，选择最有用的图像进行训练。

Result: 在相似的训练负载（即迭代次数）下，该方法能够产生高质量模型，同时最小化数据集查询次数。

Conclusion: 高置信度流式策略与多样性方法的结合是边缘摄像头系统中高效模型训练的有效解决方案。

Abstract: Edge camera-based systems are continuously expanding, facing ever-evolving
environments that require regular model updates. In practice, complex teacher
models are run on a central server to annotate data, which is then used to
train smaller models tailored to the edge devices with limited computational
power. This work explores how to select the most useful images for training to
maximize model quality while keeping transmission costs low. Our work shows
that, for a similar training load (i.e., iterations), a high-confidence
stream-based strategy coupled with a diversity-based approach produces a
high-quality model with minimal dataset queries.

</details>


### [110] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: InstructVTON是一个基于指令的交互式虚拟试穿系统，通过自然语言指导实现细粒度和复杂的风格控制，支持单件或多件服装的试穿。


<details>
  <summary>Details</summary>
Motivation: 传统基于掩码的虚拟试穿方法存在局限性：生成理想掩码困难、需要专业知识、依赖特定模型，且在某些场景下无法实现（如将长袖衬衫的袖子卷起）。

Method: 利用视觉语言模型和图像分割模型自动生成二进制掩码，基于用户提供的图像和自由文本风格指令，无需精确绘制掩码，并自动化多轮图像生成。

Result: InstructVTON与现有虚拟试穿模型兼容，实现了具有风格控制的最先进结果。

Conclusion: 该系统简化了终端用户体验，解决了传统掩码方法的局限性，实现了更灵活和复杂的虚拟试穿场景。

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on
system that allows fine-grained and complex styling control of the resulting
generation, guided by natural language, on single or multiple garments. A
computationally efficient and scalable formulation of virtual try-on formulates
the problem as an image-guided or image-conditioned inpainting task. These
inpainting-based virtual try-on models commonly use a binary mask to control
the generation layout. Producing a mask that yields desirable result is
difficult, requires background knowledge, might be model dependent, and in some
cases impossible with the masking-based approach (e.g. trying on a long-sleeve
shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt
with sleeves down, where the mask will necessarily cover the entire sleeve).
InstructVTON leverages Vision Language Models (VLMs) and image segmentation
models for automated binary mask generation. These masks are generated based on
user-provided images and free-text style instructions. InstructVTON simplifies
the end-user experience by removing the necessity of a precisely drawn mask,
and by automating execution of multiple rounds of image generation for try-on
scenarios that cannot be achieved with masking-based virtual try-on models
alone. We show that InstructVTON is interoperable with existing virtual try-on
models to achieve state-of-the-art results with styling control.

</details>


### [111] [Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition](https://arxiv.org/abs/2509.20537)
*Dana A Abdullah,Dana Rasul Hamad,Bishar Rasheed Ibrahim,Sirwan Abdulwahid Aula,Aso Khaleel Ameen,Sabat Salih Hamadamin*

Main category: cs.CV

TL;DR: DeepAFRNet是一个深度学习模型，用于识别被篡改的指纹，在SOCOFing数据集上针对不同难度级别实现了高准确率，但阈值选择对性能影响很大。


<details>
  <summary>Details</summary>
Motivation: 指纹识别在边境控制、法医和财政准入等应用中面临挑战，攻击者可能故意修改脊线模式以逃避检测，因此需要能够识别篡改指纹的鲁棒系统。

Method: 使用VGG16骨干网络提取高维特征，通过余弦相似度比较嵌入向量，在SOCOFing Real-Altered数据集的三个难度级别上进行评估。

Result: 在严格阈值下，DeepAFRNet在Easy、Medium、Hard三个难度级别上分别达到96.7%、98.76%和99.54%的准确率；但当阈值从0.92放宽到0.72时，准确率急剧下降到7.86%、27.05%和29.51%。

Conclusion: DeepAFRNet使用真实篡改样本并报告分级指标，解决了基于合成篡改或有限验证协议的先前工作的局限性，表明已准备好用于安全和识别弹性都至关重要的实际部署。

Abstract: Altered fingerprint recognition (AFR) is challenging for biometric
verification in applications such as border control, forensics, and fiscal
admission. Adversaries can deliberately modify ridge patterns to evade
detection, so robust recognition of altered prints is essential. We present
DeepAFRNet, a deep learning recognition model that matches and recognizes
distorted fingerprint samples. The approach uses a VGG16 backbone to extract
high-dimensional features and cosine similarity to compare embeddings. We
evaluate on the SOCOFing Real-Altered subset with three difficulty levels
(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of
96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A
threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72
sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,
underscoring the importance of threshold selection in biometric systems. By
using real altered samples and reporting per-level metrics, DeepAFRNet
addresses limitations of prior work based on synthetic alterations or limited
verification protocols, and indicates readiness for real-world deployments
where both security and recognition resilience are critical.

</details>


### [112] [Large Pre-Trained Models for Bimanual Manipulation in 3D](https://arxiv.org/abs/2509.20579)
*Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger*

Main category: cs.CV

TL;DR: 将预训练Vision Transformer的注意力图集成到体素表示中，以增强双手机器人操作性能


<details>
  <summary>Details</summary>
Motivation: 利用预训练视觉模型的语义理解能力来提升机器人操作的性能，特别是在复杂的双手操作任务中

Method: 从自监督ViT模型DINOv2提取注意力图作为像素级显著性分数，将其提升到3D体素网格中，并整合到行为克隆策略中

Result: 在RLBench双手基准测试中，所有任务平均绝对提升8.2%，相对增益21.9%

Conclusion: 注意力引导的特征化方法能显著提升基于体素的机器人操作策略性能

Abstract: We investigate the integration of attention maps from a pre-trained Vision
Transformer into voxel representations to enhance bimanual robotic
manipulation. Specifically, we extract attention maps from DINOv2, a
self-supervised ViT model, and interpret them as pixel-level saliency scores
over RGB images. These maps are lifted into a 3D voxel grid, resulting in
voxel-level semantic cues that are incorporated into a behavior cloning policy.
When integrated into a state-of-the-art voxel-based policy, our
attention-guided featurization yields an average absolute improvement of 8.2%
and a relative gain of 21.9% across all tasks in the RLBench bimanual
benchmark.

</details>


### [113] [A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management](https://arxiv.org/abs/2509.20580)
*Xinyang Mu,Yuzhen Lu,Boyang Deng*

Main category: cs.CV

TL;DR: 本研究对YOLO和RT-DETR系列的36个实时目标检测模型进行了比较基准分析，评估它们在蓝莓检测任务上的性能，并使用半监督学习进一步优化模型。


<details>
  <summary>Details</summary>
Motivation: 自然环境中蓝莓检测面临光照变化、遮挡和运动模糊等挑战，需要大规模多样化数据集和合适的精度/速度/内存权衡的模型。

Method: 使用包含85,879个标注实例的新数据集评估36个模型变体，并采用无偏均值教师半监督学习在1,035张未标记图像上微调模型。

Result: YOLOv12m达到93.3% mAP@50，RT-DETRv2-X达到93.6% mAP@50；半监督学习后RT-DETR-v2-X达到94.8% mAP@50的最佳性能。

Conclusion: 中等规模模型在精度和速度间取得良好平衡，需要进一步研究半监督学习以更好地利用跨域未标记数据。

Abstract: Blueberry detection in natural environments remains challenging due to
variable lighting, occlusions, and motion blur due to environmental factors and
imaging devices. Deep learning-based object detectors promise to address these
challenges, but they demand a large-scale, diverse dataset that captures the
real-world complexities. Moreover, deploying these models in practical
scenarios often requires the right accuracy/speed/memory trade-off in model
selection. This study presents a novel comparative benchmark analysis of
advanced real-time object detectors, including YOLO (You Only Look Once)
(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,
consisting of 36 model variants, evaluated on a newly curated dataset for
blueberry detection. This dataset comprises 661 canopy images collected with
smartphones during the 2022-2023 seasons, consisting of 85,879 labelled
instances (including 36,256 ripe and 49,623 unripe blueberries) across a wide
range of lighting conditions, occlusions, and fruit maturity stages. Among the
YOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while
RT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR
variants. The inference time varied with the model scale and complexity, and
the mid-sized models appeared to offer a good accuracy-speed balance. To
further enhance detection performance, all the models were fine-tuned using
Unbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of
1,035 unlabeled images acquired by a ground-based machine vision platform in
2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with
RT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into
SSL is needed to better leverage cross-domain unlabeled data. Both the dataset
and software programs of this study are made publicly available to support
further research.

</details>


### [114] [Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation](https://arxiv.org/abs/2509.20585)
*Farbod Bigdeli,Mohsen Mohammadagha,Ali Bigdeli*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级的感兴趣区域(ROI)增强策略，用于提升乳腺X光片分类性能，在Mini-DDSM数据集上验证了该方法在有限数据条件下的有效性。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光筛查对早期检测和降低死亡率至关重要，但深度学习在乳腺X光片自动解读中受限于低分辨率数据集和小样本量，需要开发数据增强策略来提升性能。

Method: 引入训练时的ROI增强策略：使用预计算的、无标签的边界框库随机采样ROI裁剪，以概率方式替换完整图像，可选抖动增加变异性，保持推理时间成本不变。

Result: 在Mini-DDSM数据集上，最佳参数(p_roi=0.10, alpha=0.10)下获得适度的平均ROC-AUC提升，但性能在不同折叠间有差异，PR-AUC基本持平或略有下降。

Conclusion: 简单的、以数据为中心的ROI策略可以在不增加额外标签或修改架构的情况下，在受限环境中增强乳腺X光片分类性能。

Abstract: Breast cancer screening with mammography remains central to early detection
and mortality reduction. Deep learning has shown strong potential for
automating mammogram interpretation, yet limited-resolution datasets and small
sample sizes continue to restrict performance. We revisit the Mini-DDSM dataset
(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest
(ROI) augmentation strategy. During training, full images are probabilistically
replaced with random ROI crops sampled from a precomputed, label-free
bounding-box bank, with optional jitter to increase variability. We evaluate
under strict patient-level cross-validation and report ROC-AUC, PR-AUC, and
training-time efficiency metrics (throughput and GPU memory). Because ROI
augmentation is training-only, inference-time cost remains unchanged. On
Mini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest
average ROC-AUC gains, with performance varying across folds; PR-AUC is flat to
slightly lower. These results demonstrate that simple, data-centric ROI
strategies can enhance mammography classification in constrained settings
without requiring additional labels or architectural modifications.

</details>


### [115] [Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections](https://arxiv.org/abs/2509.20607)
*Jing Wu,Zirui Wang,Iro Laina,Victor Adrian Prisacariu*

Main category: cs.CV

TL;DR: 利用镜像反射作为辅助视图，通过物理有效的虚拟相机变换构建多视角立体视觉系统，实现单图像3D重建，并扩展到动态场景。


<details>
  <summary>Details</summary>
Motivation: 日常环境中的镜像反射可以提供立体信息，但现有方法未能充分利用这种单次捕获中的真实视图和反射虚拟视图的同时可见性。

Method: 将反射视为辅助视图，设计物理有效的虚拟相机变换，直接生成像素域虚拟视图；提出对称感知损失优化位姿估计；框架可扩展到动态场景。

Result: 在真实世界数据和合成数据上的广泛实验证明了方法的有效性，提供了包含16个Blender场景的完全可定制合成数据集。

Conclusion: 该方法简化了成像过程，与强大的前馈重建模型兼容，实现了通用且鲁棒的3D重建，特别是在动态场景中表现出色。

Abstract: Mirror reflections are common in everyday environments and can provide stereo
information within a single capture, as the real and reflected virtual views
are visible simultaneously. We exploit this property by treating the reflection
as an auxiliary view and designing a transformation that constructs a
physically valid virtual camera, allowing direct pixel-domain generation of the
virtual view while adhering to the real-world imaging process. This enables a
multi-view stereo setup from a single image, simplifying the imaging process,
making it compatible with powerful feed-forward reconstruction models for
generalizable and robust 3D reconstruction. To further exploit the geometric
symmetry introduced by mirrors, we propose a symmetric-aware loss to refine
pose estimation. Our framework also naturally extends to dynamic scenes, where
each frame contains a mirror reflection, enabling efficient per-frame geometry
recovery. For quantitative evaluation, we provide a fully customizable
synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and
camera poses. Extensive experiments on real-world data and synthetic data are
conducted to illustrate the effectiveness of our method.

</details>


### [116] [Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery](https://arxiv.org/abs/2509.20628)
*Yiming Xiao,Archit Gupta,Miguel Esparza,Yu-Hsuan Ho,Antonia Sebastian,Hannah Weas,Rose Houck,Ali Mostafavi*

Main category: cs.CV

TL;DR: FacadeTrack是一个街景级别的语言引导框架，用于灾后建筑占用评估，通过链接全景视频到地块、校正立面视图并提取可解释属性，提供透明且可扩展的占用评估方案。


<details>
  <summary>Details</summary>
Motivation: 灾后建筑占用评估对分类、检查、公用事业恢复和资源分配至关重要。高空影像覆盖快但缺乏立面细节，街景影像有细节但稀疏且难以与地块对齐。

Method: 提出FacadeTrack框架：链接全景视频到地块，校正立面视图，提取可解释属性（如入口堵塞、临时覆盖物、局部碎片），采用两种决策策略：透明的一阶段规则和将感知与保守推理分离的两阶段设计。

Result: 在两次飓风Helene灾后调查中，两阶段方法达到精确率0.927、召回率0.781、F-1分数0.848；一阶段基线为精确率0.943、召回率0.728、F-1分数0.822。中间属性和空间诊断揭示了残差错误的位置和原因。

Conclusion: 该管道提供可审计、可扩展的占用评估，适合集成到地理空间和应急管理工作流程中，通过中间属性实现有针对性的质量控制。

Abstract: Building-level occupancy after disasters is vital for triage, inspections,
utility re-energization, and equitable resource allocation. Overhead imagery
provides rapid coverage but often misses facade and access cues that determine
habitability, while street-view imagery captures those details but is sparse
and difficult to align with parcels. We present FacadeTrack, a street-level,
language-guided framework that links panoramic video to parcels, rectifies
views to facades, and elicits interpretable attributes (for example, entry
blockage, temporary coverings, localized debris) that drive two decision
strategies: a transparent one-stage rule and a two-stage design that separates
perception from conservative reasoning. Evaluated across two post-Hurricane
Helene surveys, the two-stage approach achieves a precision of 0.927, a recall
of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a
precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond
accuracy, intermediate attributes and spatial diagnostics reveal where and why
residual errors occur, enabling targeted quality control. The pipeline provides
auditable, scalable occupancy assessments suitable for integration into
geospatial and emergency-management workflows.

</details>


### [117] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: 研究发现人类在感知简单动画中的社交互动时，不仅依赖视觉特征，还使用语义表征来补充。动词嵌入模型最能解释人类相似性判断，表明社交感知反映了社交互动的语义结构。


<details>
  <summary>Details</summary>
Motivation: 探索人类在识别移动形状显示的社交互动时，除了视觉特征外还使用哪些语义表征来补充理解。

Method: 研究1：让参与者直接标记动画印象；研究2：通过人类相似性判断测量27种社交互动的表征几何，并与基于视觉特征、标签和语义嵌入的模型预测进行比较。

Result: 人类反应呈分布状态；语义模型为解释人类判断提供了视觉特征的补充信息；动词嵌入模型最能解释人类相似性判断。

Conclusion: 简单显示中的社交感知反映了社交互动的语义结构，连接了视觉和抽象表征。

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [118] [Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance](https://arxiv.org/abs/2509.20684)
*Xiaowei Wang,Di Wang,Ke Li,Yifeng Wang,Chengjian Wang,Libin Sun,Zhihong Wu,Yiming Zhang,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出EGS框架，通过E(2)-Steerable CNN编码器和虚拟超级节点图结构，解决跨视角地理定位中的旋转鲁棒性和全局-局部一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 解决跨视角地理定位中因无人机不同方向和视场导致的严重外观变化问题，以及建立可靠的全局语义和局部细节对应关系。

Method: 使用E(2)-Steerable CNN编码器提取旋转稳定的特征，构建带虚拟超级节点的图结构实现全局语义聚合和局部一致性。

Result: 在University-1652和SUES-200基准测试中取得显著性能提升，建立了跨域CVGL的新最优水平。

Conclusion: EGS框架有效提升了跨域泛化能力，通过旋转不变特征和全局-局部一致性建模解决了CVGL的关键挑战。

Abstract: Cross-view geo-localization (CVGL) aims to match images of the same location
captured from drastically different viewpoints. Despite recent progress,
existing methods still face two key challenges: (1) achieving robustness under
severe appearance variations induced by diverse UAV orientations and fields of
view, which hinders cross-domain generalization, and (2) establishing reliable
correspondences that capture both global scene-level semantics and fine-grained
local details. In this paper, we propose EGS, a novel CVGL framework designed
to enhance cross-domain generalization. Specifically, we introduce an
E(2)-Steerable CNN encoder to extract stable and reliable features under
rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual
super-node that connects to all local nodes, enabling global semantics to be
aggregated and redistributed to local regions, thereby enforcing global-local
consistency. Extensive experiments on the University-1652 and SUES-200
benchmarks demonstrate that EGS consistently achieves substantial performance
gains and establishes a new state of the art in cross-domain CVGL.

</details>


### [119] [DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection](https://arxiv.org/abs/2509.20701)
*Jiayi Zuo,Songwei Pei,Qian Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双路径边缘网络，通过解耦边缘增强和语义建模来解决红外小目标检测中高分辨率空间细节与鲁棒语义上下文之间的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在遥感应用中至关重要，但由于缺乏明显的纹理和形态特征，小目标容易与杂乱的背景融合。现有方法依赖固定梯度算子或简单注意力机制，难以在低对比度和高噪声条件下准确提取目标边缘。

Method: 提出双路径边缘网络：第一条路径使用双向交互模块（局部自注意力和全局自注意力）捕获多尺度特征依赖；第二条路径引入多边缘细化器，使用级联泰勒有限差分算子和注意力门控机制增强边缘细节。

Result: 该方法能够精确地定位和增强不同尺寸目标的边缘特征，同时有效抑制噪声，为红外小目标检测提供了有前景的解决方案。

Conclusion: 该方法在统一框架中结合了结构语义和边缘细化，为解决红外小目标检测中的特征错位和性能不佳问题提供了有效途径。

Abstract: Infrared small target detection is crucial for remote sensing applications
like disaster warning and maritime surveillance. However, due to the lack of
distinctive texture and morphological features, infrared small targets are
highly susceptible to blending into cluttered and noisy backgrounds. A
fundamental challenge in designing deep models for this task lies in the
inherent conflict between capturing high-resolution spatial details for minute
targets and extracting robust semantic context for larger targets, often
leading to feature misalignment and suboptimal performance. Existing methods
often rely on fixed gradient operators or simplistic attention mechanisms,
which are inadequate for accurately extracting target edges under low contrast
and high noise. In this paper, we propose a novel Dual-Path Edge Network that
explicitly addresses this challenge by decoupling edge enhancement and semantic
modeling into two complementary processing paths. The first path employs a
Bidirectional Interaction Module, which uses both Local Self-Attention and
Global Self-Attention to capture multi-scale local and global feature
dependencies. The global attention mechanism, based on a Transformer
architecture, integrates long-range semantic relationships and contextual
information, ensuring robust scene understanding. The second path introduces
the Multi-Edge Refiner, which enhances fine-grained edge details using cascaded
Taylor finite difference operators at multiple scales. This mathematical
approach, along with an attention-driven gating mechanism, enables precise edge
localization and feature enhancement for targets of varying sizes, while
effectively suppressing noise. Our method provides a promising solution for
precise infrared small target detection and localization, combining structural
semantics and edge refinement in a unified framework.

</details>


### [120] [Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset](https://arxiv.org/abs/2509.20715)
*Ruixu Zhang,Yuran Wang,Xinyi Hu,Chaoyu Mai,Wenxuan Liu,Danni Xu,Xian Zhong,Zheng Wang*

Main category: cs.CV

TL;DR: 该论文提出了群体意图预测(GIF)的新任务，通过分析个体行为和互动来预测群体共享目标的出现时机，并构建了首个大规模篮球视频数据集SHOT和GIFT预测框架。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别主要关注个体意图，忽略了群体环境中集体意图的复杂性。为了解决这一局限性，需要研究群体意图的预测问题。

Method: 提出了SHOT数据集(包含1,979个篮球视频片段，5个摄像机视角，6种个体属性标注)和GIFT框架(提取细粒度个体特征并建模动态群体互动)。

Result: 实验结果表明SHOT数据集和GIFT框架的有效性，为群体意图预测研究奠定了坚实基础。

Conclusion: 该研究为群体意图预测领域开辟了新方向，数据集和框架为未来研究提供了重要基础。

Abstract: Intention recognition has traditionally focused on individual intentions,
overlooking the complexities of collective intentions in group settings. To
address this limitation, we introduce the concept of group intention, which
represents shared goals emerging through the actions of multiple individuals,
and Group Intention Forecasting (GIF), a novel task that forecasts when group
intentions will occur by analyzing individual actions and interactions before
the collective goal becomes apparent. To investigate GIF in a specific
scenario, we propose SHOT, the first large-scale dataset for GIF, consisting of
1,979 basketball video clips captured from 5 camera views and annotated with 6
types of individual attributes. SHOT is designed with 3 key characteristics:
multi-individual information, multi-view adaptability, and multi-level
intention, making it well-suited for studying emerging group intentions.
Furthermore, we introduce GIFT (Group Intention ForecasTer), a framework that
extracts fine-grained individual features and models evolving group dynamics to
forecast intention emergence. Experimental results confirm the effectiveness of
SHOT and GIFT, establishing a strong foundation for future research in group
intention forecasting. The dataset is available at
https://xinyi-hu.github.io/SHOT_DATASET.

</details>


### [121] [Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection](https://arxiv.org/abs/2509.20745)
*Yu Guo,Shengfeng He,Yuxu Lu,Haonan An,Yihang Tao,Huilin Zhu,Jingxian Liu,Yuguang Fang*

Main category: cs.CV

TL;DR: Neptune-X是一个数据中心的生成-选择框架，通过合成数据生成和任务感知样本选择来解决海事目标检测中的数据稀缺和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 海事目标检测面临两个关键挑战：标注数据稀缺以及在不同海事属性（如目标类别、视角、位置和成像环境）上的泛化能力差。现有数据集训练的模型在代表性不足的场景（如公海环境）中表现不佳。

Method: 提出Neptune-X框架，包含两个核心组件：1）X-to-Maritime多模态条件生成模型，通过双向目标-水域注意力模块捕捉目标与水域边界的交互；2）属性相关主动采样方法，动态选择任务相关的合成样本。

Result: 实验表明该方法在海事场景合成方面设立了新基准，显著提高了检测精度，特别是在具有挑战性和代表性不足的场景中。

Conclusion: Neptune-X通过生成-选择框架有效解决了海事目标检测中的数据稀缺和泛化问题，为海事视觉任务提供了新的解决方案。

Abstract: Maritime object detection is essential for navigation safety, surveillance,
and autonomous operations, yet constrained by two key challenges: the scarcity
of annotated maritime data and poor generalization across various maritime
attributes (e.g., object category, viewpoint, location, and imaging
environment). % In particular, models trained on existing datasets often
underperform in underrepresented scenarios such as open-sea environments. To
address these challenges, we propose Neptune-X, a data-centric
generative-selection framework that enhances training effectiveness by
leveraging synthetic data generation with task-aware sample selection. From the
generation perspective, we develop X-to-Maritime, a multi-modality-conditioned
generative model that synthesizes diverse and realistic maritime scenes. A key
component is the Bidirectional Object-Water Attention module, which captures
boundary interactions between objects and their aquatic surroundings to improve
visual fidelity. To further improve downstream tasking performance, we propose
Attribute-correlated Active Sampling, which dynamically selects synthetic
samples based on their task relevance. To support robust benchmarking, we
construct the Maritime Generation Dataset, the first dataset tailored for
generative maritime learning, encompassing a wide range of semantic conditions.
Extensive experiments demonstrate that our approach sets a new benchmark in
maritime scene synthesis, significantly improving detection accuracy,
particularly in challenging and previously underrepresented settings.The code
is available at https://github.com/gy65896/Neptune-X.

</details>


### [122] [AI-Enabled Crater-Based Navigation for Lunar Mapping](https://arxiv.org/abs/2509.20748)
*Sofia McLeod,Chee-Kheng Chng,Matthew Rodda,Tat-Jun Chin*

Main category: cs.CV

TL;DR: STELLA是首个用于长期月球测绘的端到端陨石坑导航系统，结合了基于Mask R-CNN的陨石坑检测器、无描述符陨石坑识别模块、稳健的透视n-陨石坑姿态求解器和批量轨道确定后端。


<details>
  <summary>Details</summary>
Motivation: 现有陨石坑导航技术主要针对短期、高频率的着陆任务，而长期月球测绘任务面临稀疏、倾斜图像和变化光照条件的挑战，需要开发新的导航系统。

Method: 开发了STELLA端到端管道，包含陨石坑检测、识别、姿态求解和轨道确定模块，并创建了CRESENT-365数据集模拟一年期月球测绘任务。

Result: 在CRESENT+和CRESENT-365数据集上的实验表明，STELLA在各种视角、光照条件和月球纬度下平均保持米级位置精度和亚度级姿态精度。

Conclusion: 这是首次在真实月球测绘环境下对陨石坑导航进行全面评估，为未来任务提供了操作条件参考。

Abstract: Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon
observed on images as natural landmarks to determine the six degrees of freedom
pose of a spacecraft. To date, CBN has primarily been studied in the context of
powered descent and landing. These missions are typically short in duration,
with high-frequency imagery captured from a nadir viewpoint over well-lit
terrain. In contrast, lunar mapping missions involve sparse, oblique imagery
acquired under varying illumination conditions over potentially year-long
campaigns, posing significantly greater challenges for pose estimation. We
bridge this gap with STELLA - the first end-to-end CBN pipeline for
long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater
detector, a descriptor-less crater identification module, a robust
perspective-n-crater pose solver, and a batch orbit determination back-end. To
rigorously test STELLA, we introduce CRESENT-365 - the first public dataset
that emulates a year-long lunar mapping mission. Each of its 15,283 images is
rendered from high-resolution digital elevation models with SPICE-derived Sun
angles and Moon motion, delivering realistic global coverage, illumination
cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show
that STELLA maintains metre-level position accuracy and sub-degree attitude
accuracy on average across wide ranges of viewing angles, illumination
conditions, and lunar latitudes. These results constitute the first
comprehensive assessment of CBN in a true lunar mapping setting and inform
operational conditions that should be considered for future missions.

</details>


### [123] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 该研究系统探讨了视觉和语言单模态模型在表示空间中的对齐现象，发现对齐出现在中后层网络，反映从模态特定到概念共享的转变，且这种对齐与人类判断一致并能通过样例聚合增强


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明视觉和语言单模态模型在表示空间存在部分对齐，但对其具体位置、支持线索、与人类偏好的关系以及样例聚合效应仍缺乏清晰认识

Method: 系统研究对齐现象，包括分析网络层对齐位置、测试对视觉外观和语义变化的鲁棒性、进行多对多图像-文本匹配的人类偏好实验

Result: 对齐在中后层达到峰值，对纯外观变化鲁棒但对语义变化敏感；人类偏好与模型嵌入空间一致；样例平均能增强而非模糊对齐

Conclusion: 单模态网络收敛于共享语义代码，该代码与人类判断对齐且通过样例聚合得到加强

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [124] [FreeInsert: Personalized Object Insertion with Geometric and Style Control](https://arxiv.org/abs/2509.20756)
*Yuhong Zhang,Han Wang,Yiwen Wang,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: FreeInsert是一个无需训练的图像编辑框架，通过3D几何信息实现个性化对象插入，解决现有方法在几何控制和风格一致性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在个性化对象插入任务中存在几何控制不足（局限于2D空间）和风格一致性差的问题，且通常需要大量训练。

Method: 首先将2D对象转换为3D，在3D层面进行交互式编辑，然后从指定视角重新渲染为2D图像，结合扩散适配器实现几何、风格和内容的控制。

Result: 该方法能够生成具有精确几何控制和风格一致性的编辑图像，无需大量训练即可实现个性化对象插入。

Conclusion: FreeInsert通过利用3D几何信息，有效解决了图像编辑中的几何控制和风格一致性问题，为个性化图像合成提供了新的解决方案。

Abstract: Text-to-image diffusion models have made significant progress in image
generation, allowing for effortless customized generation. However, existing
image editing methods still face certain limitations when dealing with
personalized image composition tasks. First, there is the issue of lack of
geometric control over the inserted objects. Current methods are confined to 2D
space and typically rely on textual instructions, making it challenging to
maintain precise geometric control over the objects. Second, there is the
challenge of style consistency. Existing methods often overlook the style
consistency between the inserted object and the background, resulting in a lack
of realism. In addition, the challenge of inserting objects into images without
extensive training remains significant. To address these issues, we propose
\textit{FreeInsert}, a novel training-free framework that customizes object
insertion into arbitrary scenes by leveraging 3D geometric information.
Benefiting from the advances in existing 3D generation models, we first convert
the 2D object into 3D, perform interactive editing at the 3D level, and then
re-render it into a 2D image from a specified view. This process introduces
geometric controls such as shape or view. The rendered image, serving as
geometric control, is combined with style and content control achieved through
diffusion adapters, ultimately producing geometrically controlled,
style-consistent edited images via the diffusion model.

</details>


### [125] [CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion](https://arxiv.org/abs/2509.20775)
*Maoye Ren,Praneetha Vaddamanu,Jianjin Xu,Fernando De la Torre Frade*

Main category: cs.CV

TL;DR: CustomEnhancer是一个零样本增强框架，通过三流融合生成方法提升现有身份定制模型的性能，实现场景多样性、身份保真度和训练自由控制，同时提出ResInversion方法将反演时间减少129倍。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本到图像扩散模型的人类照片合成方法面临场景退化、控制不足和感知身份不理想的问题，需要改进身份定制模型的表现。

Method: 提出CustomEnhancer框架，利用人脸交换技术和预训练扩散模型获取额外表示；采用三流融合PerGeneration方法结合两个兼容的反向潜在空间；引入ResInversion方法通过预扩散机制进行噪声校正。

Result: 实验表明CustomEnhancer在场景多样性、身份保真度和训练自由控制方面达到SOTA水平，ResInversion比NTI方法效率显著提升。

Conclusion: 该框架为个性化模型提供了精确的控制能力，无需为每个模型重新训练控制器，显著提升了生成质量和效率。

Abstract: Recently remarkable progress has been made in synthesizing realistic human
photos using text-to-image diffusion models. However, current approaches face
degraded scenes, insufficient control, and suboptimal perceptual identity. We
introduce CustomEnhancer, a novel framework to augment existing identity
customization models. CustomEnhancer is a zero-shot enhancement pipeline that
leverages face swapping techniques, pretrained diffusion model, to obtain
additional representations in a zeroshot manner for encoding into personalized
models. Through our proposed triple-flow fused PerGeneration approach, which
identifies and combines two compatible counter-directional latent spaces to
manipulate a pivotal space of personalized model, we unify the generation and
reconstruction processes, realizing generation from three flows. Our pipeline
also enables comprehensive training-free control over the generation process of
personalized models, offering precise controlled personalization for them and
eliminating the need for controller retraining for per-model. Besides, to
address the high time complexity of null-text inversion (NTI), we introduce
ResInversion, a novel inversion method that performs noise rectification via a
pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments
demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity
fidelity, training-free controls, while also showing the efficiency of our
ResInversion over NTI. The code will be made publicly available upon paper
acceptance.

</details>


### [126] [CompressAI-Vision: Open-source software to evaluate compression methods for computer vision tasks](https://arxiv.org/abs/2509.20777)
*Hyomin Choi,Heeji Han,Chris Rosewarne,Fabien Racapé*

Main category: cs.CV

TL;DR: CompressAI-Vision是一个用于评估面向计算机视觉任务的视频压缩技术的综合平台，支持远程和分割推理两种场景，已被MPEG采纳用于FCM标准开发


<details>
  <summary>Details</summary>
Motivation: 随着基于神经网络计算机视觉应用的普及，需要专门针对视觉任务优化的视频压缩技术，但缺乏统一的评估平台来比较不同压缩方法在保持任务准确性方面的表现

Method: 开发了CompressAI-Vision开源评估平台，集成了标准编解码器，通过比特率与任务准确性的权衡来评估压缩效果，支持远程推理和分割推理两种场景

Result: 该平台已在多个数据集上展示了压缩增益评估，并被MPEG采纳用于Feature Coding for Machines标准的开发

Conclusion: CompressAI-Vision为计算机视觉任务优化的压缩方法提供了标准化评估框架，促进了相关技术的发展，并已成为行业标准制定的重要工具

Abstract: With the increasing use of neural network (NN)-based computer vision
applications that process image and video data as input, interest has emerged
in video compression technology optimized for computer vision tasks. In fact,
given the variety of vision tasks, associated NN models and datasets, a
consolidated platform is needed as a common ground to implement and evaluate
compression methods optimized for downstream vision tasks. CompressAI-Vision is
introduced as a comprehensive evaluation platform where new coding tools
compete to efficiently compress the input of vision network while retaining
task accuracy in the context of two different inference scenarios: "remote" and
"split" inferencing. Our study showcases various use cases of the evaluation
platform incorporated with standard codecs (under development) by examining the
compression gain on several datasets in terms of bit-rate versus task accuracy.
This evaluation platform has been developed as open-source software and is
adopted by the Moving Pictures Experts Group (MPEG) for the development the
Feature Coding for Machines (FCM) standard. The software is available publicly
at https://github.com/InterDigitalInc/CompressAI-Vision.

</details>


### [127] [Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization](https://arxiv.org/abs/2509.20785)
*Jincai Song,Haipeng Chen,Jun Qin,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对跨域半监督域泛化(CD-SSDG)的双监督非对称协同训练(DAC)框架，解决了医学图像分割中标注数据有限和域偏移的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的半监督域泛化方法假设每个源域都有标注和无标注数据，但实际应用中常出现标注数据与无标注数据之间存在域偏移的情况，现有方法在这种情况下性能不佳。

Method: 基于协同训练范式，DAC框架集成了特征级监督和非对称辅助任务，通过两个子模型提供交叉伪监督，利用特征空间的互补监督来解决域偏移导致的伪标签不准确问题。

Result: 在Fundus、Polyp和SCGM等真实医学图像分割数据集上的实验表明，DAC框架具有强大的泛化能力。

Conclusion: DAC框架有效解决了CD-SSDG场景下的挑战，为医学图像分割中的域泛化问题提供了实用解决方案。

Abstract: Semi-supervised domain generalization (SSDG) in medical image segmentation
offers a promising solution for generalizing to unseen domains during testing,
addressing domain shift challenges and minimizing annotation costs. However,
conventional SSDG methods assume labeled and unlabeled data are available for
each source domain in the training set, a condition that is not always met in
practice. The coexistence of limited annotation and domain shift in the
training set is a prevalent issue. Thus, this paper explores a more practical
and challenging scenario, cross-domain semi-supervised domain generalization
(CD-SSDG), where domain shifts occur between labeled and unlabeled training
data, in addition to shifts between training and testing sets. Existing SSDG
methods exhibit sub-optimal performance under such domain shifts because of
inaccurate pseudolabels. To address this issue, we propose a novel
dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.
Building upon the co-training paradigm with two sub-models offering cross
pseudo supervision, our DAC framework integrates extra feature-level
supervision and asymmetric auxiliary tasks for each sub-model. This
feature-level supervision serves to address inaccurate pseudo supervision
caused by domain shifts between labeled and unlabeled data, utilizing
complementary supervision from the rich feature space. Additionally, two
distinct auxiliary self-supervised tasks are integrated into each sub-model to
enhance domain-invariant discriminative feature learning and prevent model
collapse. Extensive experiments on real-world medical image segmentation
datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust
generalizability of the proposed DAC framework.

</details>


### [128] [Real-Time Object Detection Meets DINOv3](https://arxiv.org/abs/2509.20787)
*Shihua Huang,Yongjie Hou,Longfei Liu,Xuanlong Yu,Xi Shen*

Main category: cs.CV

TL;DR: DEIMv2是基于DEIM框架的扩展版本，通过集成DINOv3特征和引入空间调谐适配器(STA)，在多个模型规模上实现了优越的性能-成本权衡，在目标检测任务中建立了新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 为了进一步提升DEIM框架在实时目标检测中的性能，同时覆盖从GPU到移动设备的多样化部署场景，需要开发一个能够适应不同资源约束的统一架构。

Method: 对于X、L、M、S变体采用DINOv3预训练或蒸馏骨干网络，并引入STA将单尺度输出转换为多尺度特征；对于超轻量模型采用HGNetv2并进行深度和宽度剪枝，配合简化解码器和升级的Dense O2O。

Result: DEIMv2-X仅用5030万参数达到57.8 AP，超越需要6000万参数才能达到56.5 AP的先前模型；DEIMv2-S成为首个参数量低于1000万但AP超过50的模型(50.9 AP)；DEIMv2-Pico仅用150万参数达到38.5 AP，与230万参数的YOLOv10-Nano性能相当。

Conclusion: DEIMv2通过统一的设计实现了跨多样化场景的优越性能-成本权衡，为目标检测领域建立了新的技术标杆，特别是在资源受限环境下的高效部署方面表现出色。

Abstract: Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM
has become the mainstream training framework for real-time DETRs, significantly
outperforming the YOLO series. In this work, we extend it with DINOv3 features,
resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering
GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt
DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter
(STA), which efficiently converts DINOv3's single-scale output into multi-scale
features and complements strong semantics with fine-grained details to enhance
detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we
employ HGNetv2 with depth and width pruning to meet strict resource budgets.
Together with a simplified decoder and an upgraded Dense O2O, this unified
design enables DEIMv2 to achieve a superior performance-cost trade-off across
diverse scenarios, establishing new state-of-the-art results. Notably, our
largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters,
surpassing prior X-scale models that require over 60 million parameters for
just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model
(9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even
the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers
38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer
parameters.

</details>


### [129] [DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](https://arxiv.org/abs/2509.20792)
*Ved Umrajkar*

Main category: cs.CV

TL;DR: 提出了DAC-LoRA框架，将对抗训练集成到参数高效微调中，通过动态对抗课程提升视觉语言模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在关键应用中易受对抗攻击影响，而现有的参数高效微调方法无法有效解决这一安全问题

Method: 基于一阶平稳条件和TRADES损失，设计动态对抗课程，逐步增加攻击难度，可集成到标准PEFT流程中

Result: 在不显著降低清洁准确率的前提下，显著提升了对抗鲁棒性

Conclusion: DAC-LoRA是一种轻量级、广泛适用的方法，能有效增强视觉语言模型的鲁棒性

Abstract: Vision-Language Models (VLMs) are foundational to critical applications like
autonomous driving, medical diagnosis, and content moderation. While
Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient
adaptation to specialized tasks, these models remain vulnerable to adversarial
attacks that can compromise safety-critical decisions. CLIP, the backbone for
numerous downstream VLMs, is a high-value target whose vulnerabilities can
cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial
Curriculum DAC-LoRA, a novel framework that integrates adversarial training
into PEFT. The core principle of our method i.e. an intelligent curriculum of
progressively challenging attack, is general and can potentially be applied to
any iterative attack method. Guided by the First-Order Stationary Condition
(FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements
in adversarial robustness without significantly compromising clean accuracy.
Our work presents an effective, lightweight, and broadly applicable method to
demonstrate that the DAC-LoRA framework can be easily integrated into a
standard PEFT pipeline to significantly enhance robustness.

</details>


### [130] [Federated Domain Generalization with Domain-specific Soft Prompts Generation](https://arxiv.org/abs/2509.20807)
*Jianhan Wu,Xiaoyang Qu,Zhangcheng Huang,Jianzong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedDSPG的新方法，通过生成式视角处理联邦领域泛化问题，利用领域特定软提示生成来提升模型在未知领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦领域泛化方法基于提示学习，但学习的提示多样性有限且容易忽略未知领域信息。为了解决这个问题，需要一种能够生成多样化提示并有效利用未知领域信息的方法。

Method: FedDSPG方法在训练阶段为每个领域引入领域特定软提示，并将内容和领域知识整合到客户端生成模型中。在推理阶段，使用生成器获取未见目标领域的DSPs，从而指导未知领域的下游任务。

Result: 在多个公共数据集上的综合评估表明，该方法在联邦领域泛化任务中优于现有强基线，达到了最先进的性能。

Conclusion: FedDSPG通过生成式方法有效解决了联邦领域泛化中的提示多样性不足和未知领域信息忽略问题，为联邦学习中的领域适应提供了新的解决方案。

Abstract: Prompt learning has become an efficient paradigm for adapting CLIP to
downstream tasks. Compared with traditional fine-tuning, prompt learning
optimizes a few parameters yet yields highly competitive results, especially
appealing in federated learning for computational efficiency. engendering
domain shift among clients and posing a formidable challenge for
downstream-task adaptation. Existing federated domain generalization (FDG)
methods based on prompt learning typically learn soft prompts from training
samples, replacing manually designed prompts to enhance the generalization
ability of federated models. However, these learned prompts exhibit limited
diversity and tend to ignore information from unknown domains. We propose a
novel and effective method from a generative perspective for handling FDG
tasks, namely federated domain generalization with domain-specific soft prompts
generation (FedDSPG). Specifically, during training, we introduce
domain-specific soft prompts (DSPs) for each domain and integrate content and
domain knowledge into the generative model among clients. In the inference
phase, the generator is utilized to obtain DSPs for unseen target domains, thus
guiding downstream tasks in unknown domains. Comprehensive evaluations across
several public datasets confirm that our method outperforms existing strong
baselines in FDG, achieving state-of-the-art results.

</details>


### [131] [Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning](https://arxiv.org/abs/2509.20813)
*Thanh Binh Le,Hoang Nhat Khang Vo,Tan-Ha Mai,Trong Nhan Phan*

Main category: cs.CV

TL;DR: LumbarCLIP是一个新颖的多模态框架，利用对比语言-图像预训练技术将腰椎MRI扫描与放射学描述对齐，在腰椎疾病分类任务上达到95.00%准确率和94.75% F1分数。


<details>
  <summary>Details</summary>
Motivation: 腰痛影响全球数百万人，需要能够联合分析复杂医学图像和文本报告的强大诊断模型。

Method: 基于对比学习框架，整合视觉编码器（ResNet-50、Vision Transformer、Swin Transformer）和BERT文本编码器，通过可学习的投影头将特征映射到共享嵌入空间，使用soft CLIP损失进行训练。

Result: 在测试集上达到95.00%准确率和94.75% F1分数，线性投影头比非线性变体产生更有效的跨模态对齐。

Conclusion: LumbarCLIP为自动化肌肉骨骼诊断和临床决策支持提供了有前景的基础。

Abstract: Low back pain affects millions worldwide, driving the need for robust
diagnostic models that can jointly analyze complex medical images and
accompanying text reports. We present LumbarCLIP, a novel multimodal framework
that leverages contrastive language-image pretraining to align lumbar spine MRI
scans with corresponding radiological descriptions. Built upon a curated
dataset containing axial MRI views paired with expert-written reports,
LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin
Transformer) with a BERT-based text encoder to extract dense representations.
These are projected into a shared embedding space via learnable projection
heads, configurable as linear or non-linear, and normalized to facilitate
stable contrastive training using a soft CLIP loss. Our model achieves
state-of-the-art performance on downstream classification, reaching up to
95.00% accuracy and 94.75% F1-score on the test set, despite inherent class
imbalance. Extensive ablation studies demonstrate that linear projection heads
yield more effective cross-modal alignment than non-linear variants. LumbarCLIP
offers a promising foundation for automated musculoskeletal diagnosis and
clinical decision support.

</details>


### [132] [Poisoning Prompt-Guided Sampling in Video Large Language Models](https://arxiv.org/abs/2509.20851)
*Yuxin Cao,Wei Song,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: PoisonVID是首个针对VideoLLMs中提示引导采样策略的黑盒中毒攻击，通过闭环优化策略在提示引导采样中抑制有害帧的相关性分数，攻击成功率高达82%-99%。


<details>
  <summary>Details</summary>
Motivation: 虽然VideoLLMs在视频理解方面表现出色，但之前的研究已发现早期采样策略存在漏洞，而提示引导采样的安全性尚未被探索。

Method: 使用闭环优化策略，通过从GPT-4o-mini等轻量语言模型构建的转述有害描述集合指导，迭代优化通用扰动来抑制有害帧相关性分数。

Result: 在三种提示引导采样策略和三个先进VideoLLMs上的全面评估显示，PoisonVID攻击成功率达到82%-99%。

Conclusion: 该研究揭示了提示引导采样策略的安全漏洞，强调了为VideoLLMs开发未来先进采样策略的重要性。

Abstract: Video Large Language Models (VideoLLMs) have emerged as powerful tools for
understanding videos, supporting tasks such as summarization, captioning, and
question answering. Their performance has been driven by advances in frame
sampling, progressing from uniform-based to semantic-similarity-based and, most
recently, prompt-guided strategies. While vulnerabilities have been identified
in earlier sampling strategies, the safety of prompt-guided sampling remains
unexplored. We close this gap by presenting PoisonVID, the first black-box
poisoning attack that undermines prompt-guided sampling in VideoLLMs. PoisonVID
compromises the underlying prompt-guided sampling mechanism through a
closed-loop optimization strategy that iteratively optimizes a universal
perturbation to suppress harmful frame relevance scores, guided by a depiction
set constructed from paraphrased harmful descriptions leveraging a shadow
VideoLLM and a lightweight language model, i.e., GPT-4o-mini. Comprehensively
evaluated on three prompt-guided sampling strategies and across three advanced
VideoLLMs, PoisonVID achieves 82% - 99% attack success rate, highlighting the
importance of developing future advanced sampling strategies for VideoLLMs.

</details>


### [133] [Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer](https://arxiv.org/abs/2509.20854)
*Abdur Rehman,S M A Sharif,Md Abdur Rahaman,Mohamed Jismy Aashik Rasool,Seongwan Kim,Jaeho Lee*

Main category: cs.CV

TL;DR: GoR是一种新颖的可学习正则化方法，通过动态损失权重自适应平衡任务特定和知识蒸馏目标，解决了量化感知训练中梯度冲突问题，显著提升了小型量化模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的QAT-KD方法在低比特量化下难以平衡任务特定损失和蒸馏损失，由于异质梯度幅度导致训练冲突。

Method: 提出Game of Regularizer (GoR)方法，仅使用两个可训练参数进行动态损失加权，减少监督信号冲突；同时引入QAT-EKD-GoR集成蒸馏框架，使用多个异构教师模型。

Result: 在图像分类、目标检测和大型语言模型压缩任务中，GoR均优于现有QAT-KD方法，在低功耗边缘设备上实现更快推理同时保持全精度准确率。

Conclusion: GoR为量化模型压缩提供了鲁棒解决方案，在最优条件下甚至能超越全精度模型性能，适用于实际部署场景。

Abstract: Quantization-aware training (QAT) combined with knowledge distillation (KD)
is a promising strategy for compressing Artificial Intelligence (AI) models for
deployment on resource-constrained hardware. However, existing QAT-KD methods
often struggle to balance task-specific (TS) and distillation losses due to
heterogeneous gradient magnitudes, especially under low-bit quantization. We
propose Game of Regularizer (GoR), a novel learnable regularization method that
adaptively balances TS and KD objectives using only two trainable parameters
for dynamic loss weighting. GoR reduces conflict between supervision signals,
improves convergence, and boosts the performance of small quantized models
(SQMs). Experiments on image classification, object detection (OD), and large
language model (LLM) compression show that GoR consistently outperforms
state-of-the-art QAT-KD methods. On low-power edge devices, it delivers faster
inference while maintaining full-precision accuracy. We also introduce
QAT-EKD-GoR, an ensemble distillation framework that uses multiple
heterogeneous teacher models. Under optimal conditions, the proposed EKD-GoR
can outperform full-precision models, providing a robust solution for
real-world deployment.

</details>


### [134] [Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)](https://arxiv.org/abs/2509.20856)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2017植物识别挑战赛旨在评估大规模噪声网络数据集与小型专家验证数据集在植物识别性能上的对比，测试集来自Pl@ntNet移动应用。


<details>
  <summary>Details</summary>
Motivation: 尽管已有机构收集植物图像，但大多数物种仍缺乏图片或图像质量差。网络上有大量植物图片但存在标签错误，需要评估噪声数据能否替代专家验证数据。

Method: 使用两个训练数据集对比：大规模网络收集的噪声数据集和小型专家验证数据集。测试集来自第三方Pl@ntNet移动应用，确保公平比较。

Result: 挑战赛评估了不同训练策略的性能，分析了参与研究团队的方法和系统表现。

Conclusion: 该研究为大规模植物识别系统提供了重要基准，探索了网络噪声数据在植物识别中的潜力与局限性。

Abstract: The 2017-th edition of the LifeCLEF plant identification challenge is an
important milestone towards automated plant identification systems working at
the scale of continental floras with 10.000 plant species living mainly in
Europe and North America illustrated by a total of 1.1M images. Nowadays, such
ambitious systems are enabled thanks to the conjunction of the dazzling recent
progress in image classification with deep learning and several outstanding
international initiatives, such as the Encyclopedia of Life (EOL), aggregating
the visual knowledge on plant species coming from the main national botany
institutes. However, despite all these efforts the majority of the plant
species still remain without pictures or are poorly illustrated. Outside the
institutional channels, a much larger number of plant pictures are available
and spread on the web through botanist blogs, plant lovers web-pages, image
hosting websites and on-line plant retailers. The LifeCLEF 2017 plant challenge
presented in this paper aimed at evaluating to what extent a large noisy
training dataset collected through the web and containing a lot of labelling
errors can compete with a smaller but trusted training dataset checked by
experts. To fairly compare both training strategies, the test dataset was
created from a third data source, i.e. the Pl@ntNet mobile application that
collects millions of plant image queries all over the world. This paper
presents more precisely the resources and assessments of the challenge,
summarizes the approaches and systems employed by the participating research
groups, and provides an analysis of the main outcomes.

</details>


### [135] [TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting](https://arxiv.org/abs/2509.20857)
*Xiaonan Hu,Xuebing Li,Jinyu Xu,Abdulkadir Duran Adan,Letian Zhou,Xuhui Zhu,Yanan Li,Wei Guo,Shouyang Liu,Wenzhong Liu,Hao Lu*

Main category: cs.CV

TL;DR: TasselNetV4是一种基于视觉的植物计数模型，通过将TasselNet的局部计数思想与类无关计数（CAC）的提取-匹配范式相结合，实现了跨物种植物计数，解决了传统物种依赖模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 植物具有生物多样性，每年都有新品种被培育，构建所有物种依赖的计数模型几乎不可能。现有CAC模型对非刚性植物结构的计数性能不佳，需要开发专门针对植物动态特性的跨物种计数方法。

Method: TasselNetV4基于普通视觉变换器构建，结合了TasselNet的局部计数思想和CAC的提取-匹配范式，引入了新颖的多分支框感知局部计数器来增强跨尺度鲁棒性。

Result: 在PAC-105和PAC-Somalia两个挑战性数据集上的实验表明，TasselNetV4不仅实现了优越的计数性能，还具有高效率。

Conclusion: TasselNetV4成为一个用于跨场景、跨尺度和跨物种植物计数的视觉基础模型，为农业应用提供了更通用的解决方案。

Abstract: Accurate plant counting provides valuable information for agriculture such as
crop yield prediction, plant density assessment, and phenotype quantification.
Vision-based approaches are currently the mainstream solution. Prior art
typically uses a detection or a regression model to count a specific plant.
However, plants have biodiversity, and new cultivars are increasingly bred each
year. It is almost impossible to exhaust and build all species-dependent
counting models. Inspired by class-agnostic counting (CAC) in computer vision,
we argue that it is time to rethink the problem formulation of plant counting,
from what plants to count to how to count plants. In contrast to most daily
objects with spatial and temporal invariance, plants are dynamic, changing with
time and space. Their non-rigid structure often leads to worse performance than
counting rigid instances like heads and cars such that current CAC and
open-world detection models are suboptimal to count plants. In this work, we
inherit the vein of the TasselNet plant counting model and introduce a new
extension, TasselNetV4, shifting from species-specific counting to
cross-species counting. TasselNetV4 marries the local counting idea of
TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain
vision transformer and incorporates novel multi-branch box-aware local counters
used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and
PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC
models show that TasselNetV4 achieves not only superior counting performance
but also high efficiency.Our results indicate that TasselNetV4 emerges to be a
vision foundation model for cross-scene, cross-scale, and cross-species plant
counting.

</details>


### [136] [SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT](https://arxiv.org/abs/2509.20864)
*Botond Fazekas,Guilherme Aresta,Philipp Seeböck,Julia Mai,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: 提出了一种新颖的半监督模型，通过可微分生物标志物拓扑引擎来确保视网膜OCT图像中病变和层的解剖学正确分割，解决了现有方法产生解剖学不合理分割的问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法在视网膜OCT图像分割中经常产生解剖学上不合理的结果，无法有效建模层与病变之间的相互作用，且缺乏拓扑正确性保证。

Method: 引入完全可微分的生物标志物拓扑引擎，实现层与病变的双向影响联合学习，利用未标记和部分标记数据集，学习解耦的空间和风格因子表示。

Result: 在公共和内部OCT数据集上的评估显示，该方法在病变和层分割方面均优于当前最先进方法，并能将层分割泛化到病理病例。

Conclusion: 该方法展示了在半监督学习中使用解剖学约束进行准确、鲁棒和可信赖的视网膜生物标志物分割的潜力。

Abstract: Optical coherence tomography (OCT) is widely used for diagnosing and
monitoring retinal diseases, such as age-related macular degeneration (AMD).
The segmentation of biomarkers such as layers and lesions is essential for
patient diagnosis and follow-up. Recently, semi-supervised learning has shown
promise in improving retinal segmentation performance. However, existing
methods often produce anatomically implausible segmentations, fail to
effectively model layer-lesion interactions, and lack guarantees on topological
correctness.
  To address these limitations, we propose a novel semi-supervised model that
introduces a fully differentiable biomarker topology engine to enforce
anatomically correct segmentation of lesions and layers. This enables joint
learning with bidirectional influence between layers and lesions, leveraging
unlabeled and diverse partially labeled datasets. Our model learns a
disentangled representation, separating spatial and style factors. This
approach enables more realistic layer segmentations and improves lesion
segmentation, while strictly enforcing lesion location in their anatomically
plausible positions relative to the segmented layers.
  We evaluate the proposed model on public and internal datasets of OCT scans
and show that it outperforms the current state-of-the-art in both lesion and
layer segmentation, while demonstrating the ability to generalize layer
segmentation to pathological cases using partially annotated training data. Our
results demonstrate the potential of using anatomical constraints in
semi-supervised learning for accurate, robust, and trustworthy retinal
biomarker segmentation.

</details>


### [137] [Plant identification in an open-world (LifeCLEF 2016)](https://arxiv.org/abs/2509.20870)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2016植物识别挑战赛评估了大规模植物识别方法，特别关注开放集识别问题，即在识别已知类别的同时需要自动拒绝未知类别的错误分类。


<details>
  <summary>Details</summary>
Motivation: 该挑战赛旨在评估在接近真实世界生物多样性监测场景条件下的大规模植物识别系统，主要创新点是将识别任务作为开放集识别问题来评估。

Method: 基于包含110,000多张图像、涵盖西欧1,000种植物物种的数据集，要求参与系统不仅要进行已知类别的分类，还要能够自动拒绝由未知类别引起的误分类。

Result: 挑战赛收集了各研究团队的方法和系统，并对主要成果进行了分析，评估了不同方法在开放集识别任务上的表现。

Conclusion: LifeCLEF 2016挑战赛成功推动了大规模植物识别技术的发展，特别是在开放集识别这一具有现实意义的挑战上取得了进展。

Abstract: The LifeCLEF plant identification challenge aims at evaluating plant
identification methods and systems at a very large scale, close to the
conditions of a real-world biodiversity monitoring scenario. The 2016-th
edition was actually conducted on a set of more than 110K images illustrating
1000 plant species living in West Europe, built through a large-scale
participatory sensing platform initiated in 2011 and which now involves tens of
thousands of contributors. The main novelty over the previous years is that the
identification task was evaluated as an open-set recognition problem, i.e. a
problem in which the recognition system has to be robust to unknown and never
seen categories. Beyond the brute-force classification across the known classes
of the training set, the big challenge was thus to automatically reject the
false positive classification hits that are caused by the unknown classes. This
overview presents more precisely the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [138] [SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering](https://arxiv.org/abs/2509.20871)
*Yan Zhang,Jiaqing Lin,Miao Zhang,Kui Xiao,Xiaoju Hou,Yue Zhao,Zhifei Li*

Main category: cs.CV

TL;DR: SCRA-VQA是一种基于知识库的视觉问答方法，通过使用预训练视觉语言模型生成图像描述，并对描述进行总结和重排序以去除无关信息，从而提升大型语言模型在VQA任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的KB-VQA方法使用大型语言模型作为知识引擎，但图像描述通常包含过多与问题无关的噪声，且LLMs对VQA任务理解有限，限制了其推理能力。

Method: 提出SCRA-VQA框架：1）使用预训练视觉语言模型将图像转换为描述；2）生成上下文示例并对描述进行总结和重排序，排除无关信息；3）通过描述重排序过程帮助LLMs更好地理解图像信息和问题。

Result: 在OK-VQA和A-OKVQA两个挑战性知识库VQA数据集上，基于6.7B参数LLM的SCRA-VQA分别达到38.8%和34.6%的准确率。

Conclusion: SCRA-VQA通过总结和重排序图像描述，有效提升了LLMs在KB-VQA任务中的推理能力和任务适应性，且无需昂贵的端到端训练。

Abstract: Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual
Question Answering (KB-VQA). Recent methods use large language models (LLMs) as
knowledge engines for answering. These methods generally employ image captions
as visual text descriptions to assist LLMs in interpreting images. However, the
captions frequently include excessive noise irrelevant to the question, and
LLMs generally do not comprehend VQA tasks, limiting their reasoning
capabilities. To address this issue, we propose the Summarized Caption-Rerank
Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to
convert images into captions. Moreover, SCRA-VQA generates contextual examples
for the captions while simultaneously summarizing and reordering them to
exclude unrelated information. The caption-rerank process enables LLMs to
understand the image information and questions better, thus enhancing the
model's reasoning ability and task adaptability without expensive end-to-end
training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently
on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving
accuracies of 38.8% and 34.6%. Our code is available at
https://github.com/HubuKG/SCRA-VQA.

</details>


### [139] [The Unanticipated Asymmetry Between Perceptual Optimization and Assessment](https://arxiv.org/abs/2509.20878)
*Jiabei Zhang,Qi Wang,Siyu Wu,Du Chen,Tianhe Wu*

Main category: cs.CV

TL;DR: 本文揭示了感知优化与评估之间的不对称性：在图像质量评估中表现优异的保真度度量不一定对感知优化有效，这种不一致在对抗训练下更加明显。判别器设计对优化结果有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 探索保真度目标和对抗目标作为优化目标的有效性与它们作为图像质量评估指标能力之间的相关性，这一领域尚未得到充分研究。

Method: 系统分析感知优化与评估之间的关系，比较不同保真度度量在优化和评估中的表现，研究判别器设计对优化结果的影响。

Result: 发现保真度度量在IQA中的优异表现与在感知优化中的有效性之间存在不对称性；判别器设计对优化质量有决定性影响，补丁级和卷积架构比传统或基于Transformer的替代方案提供更忠实的细节重建。

Conclusion: 这些发现推进了对损失函数设计及其与IQA可迁移性联系的理解，为更原则性的感知优化方法铺平了道路。

Abstract: Perceptual optimization is primarily driven by the fidelity objective, which
enforces both semantic consistency and overall visual realism, while the
adversarial objective provides complementary refinement by enhancing perceptual
sharpness and fine-grained detail. Despite their central role, the correlation
between their effectiveness as optimization objectives and their capability as
image quality assessment (IQA) metrics remains underexplored. In this work, we
conduct a systematic analysis and reveal an unanticipated asymmetry between
perceptual optimization and assessment: fidelity metrics that excel in IQA are
not necessarily effective for perceptual optimization, with this misalignment
emerging more distinctly under adversarial training. In addition, while
discriminators effectively suppress artifacts during optimization, their
learned representations offer only limited benefits when reused as backbone
initializations for IQA models. Beyond this asymmetry, our findings further
demonstrate that discriminator design plays a decisive role in shaping
optimization, with patch-level and convolutional architectures providing more
faithful detail reconstruction than vanilla or Transformer-based alternatives.
These insights advance the understanding of loss function design and its
connection to IQA transferability, paving the way for more principled
approaches to perceptual optimization.

</details>


### [140] [Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering](https://arxiv.org/abs/2509.20884)
*Zhifei Li,Feng Qiu,Yiran Wang,Yujing Xia,Kui Xiao,Miao Zhang,Yan Zhang*

Main category: cs.CV

TL;DR: IOG-VQA是一个新颖的视觉问答模型，通过集成对象交互自注意力机制和基于GAN的去偏框架，有效解决VQA任务中的数据偏差问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型容易受到训练数据偏差的影响，过度依赖表面模式，难以泛化到多样化的问答场景。需要同时解决对象交互理解和数据偏差问题。

Method: 提出IOG-VQA模型，包含两个核心组件：1）对象交互自注意力机制，捕捉图像中对象间的复杂交互关系；2）基于GAN的去偏框架，生成无偏数据分布，学习更鲁棒的特征。

Result: 在VQA-CP v1和VQA-CP v2数据集上的实验表明，该模型相比现有方法表现出色，特别是在处理有偏和不平衡数据分布方面效果显著。

Conclusion: 同时处理对象交互和数据集偏差对于推进VQA任务至关重要，IOG-VQA模型为这一方向提供了有效解决方案。

Abstract: Visual Question Answering (VQA) presents a unique challenge by requiring
models to understand and reason about visual content to answer questions
accurately. Existing VQA models often struggle with biases introduced by the
training data, leading to over-reliance on superficial patterns and inadequate
generalization to diverse questions and images. This paper presents a novel
model, IOG-VQA, which integrates Object Interaction Self-Attention and
GAN-Based Debiasing to enhance VQA model performance. The self-attention
mechanism allows our model to capture complex interactions between objects
within an image, providing a more comprehensive understanding of the visual
context. Meanwhile, the GAN-based debiasing framework generates unbiased data
distributions, helping the model to learn more robust and generalizable
features. By leveraging these two components, IOG-VQA effectively combines
visual and textual information to address the inherent biases in VQA datasets.
Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that
our model shows excellent performance compared with the existing methods,
particularly in handling biased and imbalanced data distributions highlighting
the importance of addressing both object interactions and dataset biases in
advancing VQA tasks. Our code is available at
https://github.com/HubuKG/IOG-VQA.

</details>


### [141] [Nuclear Diffusion Models for Low-Rank Background Suppression in Videos](https://arxiv.org/abs/2509.20886)
*Tristan S. W. Stevens,Oisín Nolan,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: cs.CV

TL;DR: 提出了一种结合低秩时间建模和扩散后验采样的混合框架，用于视频去雾，在心脏超声去雾任务中表现优于传统RPCA方法


<details>
  <summary>Details</summary>
Motivation: 视频序列中的结构化噪声和背景伪影会掩盖动态内容，传统RPCA方法中的稀疏性假设无法捕捉真实视频数据的丰富变异性

Method: 核扩散方法：集成低秩时间建模与扩散后验采样的混合框架

Result: 在心脏超声去雾任务中，相比传统RPCA方法，在对比度增强和信号保真度方面表现出更好的性能

Conclusion: 结合基于模型的时间模型与深度生成先验的方法在高保真视频恢复方面具有巨大潜力

Abstract: Video sequences often contain structured noise and background artifacts that
obscure dynamic content, posing challenges for accurate analysis and
restoration. Robust principal component methods address this by decomposing
data into low-rank and sparse components. Still, the sparsity assumption often
fails to capture the rich variability present in real video data. To overcome
this limitation, a hybrid framework that integrates low-rank temporal modeling
with diffusion posterior sampling is proposed. The proposed method, Nuclear
Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac
ultrasound dehazing, and demonstrates improved dehazing performance compared to
traditional RPCA concerning contrast enhancement (gCNR) and signal preservation
(KS statistic). These results highlight the potential of combining model-based
temporal models with deep generative priors for high-fidelity video
restoration.

</details>


### [142] [FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies](https://arxiv.org/abs/2509.20890)
*Shuqiao Liang,Jian Liu,Renzhang Chen,Quanlong Guan*

Main category: cs.CV

TL;DR: 本文提出FerretNet，一种轻量级神经网络，通过分析生成过程中引入的潜在分布偏差和解码诱导平滑效应来检测合成图像，在22个生成模型的开放世界基准测试中达到97.1%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 随着VAEs、GANs和LDMs等先进模型生成的合成图像越来越逼真，合成图像检测面临重大挑战。为解决这一问题，研究探索生成过程中引入的两种伪影类型。

Method: 利用基于马尔可夫随机场的局部像素依赖特性，通过邻域像素信息重建合成图像以暴露纹理连续性和边缘一致性的破坏。基于此提出FerretNet，一个仅110万参数的轻量级神经网络。

Result: FerretNet仅在4类ProGAN数据集上训练，在包含22个生成模型的开放世界基准测试中达到97.1%的平均准确率，比现有最优方法高出10.6%。

Conclusion: FerretNet能够高效且鲁棒地检测合成图像，证明了局部像素依赖特性在合成图像检测中的有效性。

Abstract: The increasing realism of synthetic images generated by advanced models such
as VAEs, GANs, and LDMs poses significant challenges for synthetic image
detection. To address this issue, we explore two artifact types introduced
during the generation process: (1) latent distribution deviations and (2)
decoding-induced smoothing effects, which manifest as inconsistencies in local
textures, edges, and color transitions. Leveraging local pixel dependencies
(LPD) properties rooted in Markov Random Fields, we reconstruct synthetic
images using neighboring pixel information to expose disruptions in texture
continuity and edge coherence. Building upon LPD, we propose FerretNet, a
lightweight neural network with only 1.1M parameters that delivers efficient
and robust synthetic image detection. Extensive experiments demonstrate that
FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an
average accuracy of 97.1% on an open-world benchmark comprising across 22
generative models, surpassing state-of-the-art methods by 10.6%.

</details>


### [143] [Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification](https://arxiv.org/abs/2509.20899)
*Patrick Knab,Sascha Marton,Philipp J. Schubert,Drago Guggiana,Christian Bartelt*

Main category: cs.CV

TL;DR: MoTIF是一个基于transformer架构的可解释视频分类框架，将概念瓶颈模型扩展到视频数据，通过全局概念重要性、局部概念相关性和时间依赖性三个视角来分析视频中的动作。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型主要针对静态图像，无法有效处理视频数据中的时间依赖性，而视频中的动作和事件需要捕捉时间序列上的概念变化。

Method: 提出MoTIF框架，采用transformer架构处理任意长度的视频序列，将视频概念定义为跨时间重复出现的语义实体（如物体、属性、动作组件），形成描述动作的motif模式。

Result: 实验表明概念建模范式可以成功应用于视频数据，在保持竞争力的分类性能的同时，提供了对时间上下文中概念贡献的更好理解。

Conclusion: MoTIF成功将概念瓶颈模型扩展到视频领域，为视频分类提供了可解释的分析框架，能够从多个时间维度理解概念在视频中的作用。

Abstract: Conceptual models such as Concept Bottleneck Models (CBMs) have driven
substantial progress in improving interpretability for image classification by
leveraging human-interpretable concepts. However, extending these models from
static images to sequences of images, such as video data, introduces a
significant challenge due to the temporal dependencies inherent in videos,
which are essential for capturing actions and events. In this work, we
introduce MoTIF (Moving Temporal Interpretable Framework), an architectural
design inspired by a transformer that adapts the concept bottleneck framework
for video classification and handles sequences of arbitrary length. Within the
video domain, concepts refer to semantic entities such as objects, attributes,
or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across
time - forming motifs collectively describing and explaining actions. Our
design explicitly enables three complementary perspectives: global concept
importance across the entire video, local concept relevance within specific
windows, and temporal dependencies of a concept over time. Our results
demonstrate that the concept-based modeling paradigm can be effectively
transferred to video data, enabling a better understanding of concept
contributions in temporal contexts while maintaining competitive performance.
Code available at github.com/patrick-knab/MoTIF.

</details>


### [144] [FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data](https://arxiv.org/abs/2509.20905)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: FSMODNet是一个用于少样本多光谱目标检测的框架，通过跨模态特征整合和可变形注意力机制，在可见光和热成像模态下实现高效目标检测，即使在有限标注数据条件下也能保持鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 解决在多光谱（可见光和热成像）场景下，由于标注数据稀缺而导致的目标检测性能下降问题，特别是在复杂光照和环境条件下的适应性挑战。

Method: 提出FSMODNet框架，利用可变形注意力机制有效整合可见光和热成像模态的特征，充分发挥两种模态的互补优势。

Result: 在两个公开数据集上的实验结果表明，该方法在低数据量情况下仍能实现有效的目标检测性能，优于基于现有最先进模型建立的多个基线方法。

Conclusion: FSMODNet通过跨模态特征整合展示了在少样本多光谱目标检测任务中的有效性，为复杂环境条件下的目标检测提供了可行的解决方案。

Abstract: Few-shot multispectral object detection (FSMOD) addresses the challenge of
detecting objects across visible and thermal modalities with minimal annotated
data. In this paper, we explore this complex task and introduce a framework
named "FSMODNet" that leverages cross-modality feature integration to improve
detection performance even with limited labels. By effectively combining the
unique strengths of visible and thermal imagery using deformable attention, the
proposed method demonstrates robust adaptability in complex illumination and
environmental conditions. Experimental results on two public datasets show
effective object detection performance in challenging low-data regimes,
outperforming several baselines we established from state-of-the-art models.
All code, models, and experimental data splits can be found at
https://anonymous.4open.science/r/Test-B48D.

</details>


### [145] [Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences](https://arxiv.org/abs/2509.20906)
*Julius Pesonen,Arno Solin,Eija Honkavaara*

Main category: cs.CV

TL;DR: 本文提出使用粒子滤波器解决基于相机测量的3D物体定位问题，特别适用于远距离物体或计算资源受限的场景，如无人机野火监测。


<details>
  <summary>Details</summary>
Motivation: 在远距离物体监测或计算资源受限的任务中，传统的密集深度估计或3D场景重建方法不可行，需要一种更高效的定位解决方案。

Method: 使用粒子滤波器进行单目标和多目标场景的3D定位，结合相机位姿估计和图像分割结果，方法独立于检测算法。

Result: 通过3D仿真和基于无人机的图像分割序列测试，证明粒子滤波器能在其他方法失效的情况下有效解决实际定位任务。

Conclusion: 粒子滤波器为无人机野火监测等安全关键任务提供了可行的定位解决方案，具有灵活性和实用性。

Abstract: 3D object localisation based on a sequence of camera measurements is
essential for safety-critical surveillance tasks, such as drone-based wildfire
monitoring. Localisation of objects detected with a camera can typically be
solved with dense depth estimation or 3D scene reconstruction. However, in the
context of distant objects or tasks limited by the amount of available
computational resources, neither solution is feasible. In this paper, we show
that the task can be solved using particle filters for both single and multiple
target scenarios. The method was studied using a 3D simulation and a
drone-based image segmentation sequence with global navigation satellite system
(GNSS)-based camera pose estimates. The results showed that a particle filter
can be used to solve practical localisation tasks based on camera poses and
image segments in these situations where other solutions fail. The particle
filter is independent of the detection method, making it flexible for new
tasks. The study also demonstrates that drone-based wildfire monitoring can be
conducted using the proposed method paired with a pre-existing image
segmentation model.

</details>


### [146] [SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images](https://arxiv.org/abs/2509.20918)
*Qinfeng Zhu,Han Li,Liang He,Lei Fan*

Main category: cs.CV

TL;DR: 提出SwinMamba框架，结合局部Mamba扫描和全局感受野，用于遥感图像语义分割，在LoveDA和ISPRS Potsdam数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义分割面临高空间分辨率、复杂场景结构和多尺度对象等挑战。Vision Mamba虽然具有全局感受野和低计算复杂度，但过度依赖全局扫描会忽略关键的局部特征（如纹理和边缘）

Method: SwinMamba受Swin Transformer启发，在前两个阶段进行局部扫描捕获细粒度细节，后两个阶段利用全局扫描融合更广泛的上下文信息。采用重叠移位窗口增强区域间信息交换

Result: 在LoveDA和ISPRS Potsdam数据集上的大量实验表明，SwinMamba优于最先进的方法

Conclusion: SwinMamba通过有效结合局部和全局特征感知，为遥感图像语义分割提供了优越的解决方案

Abstract: Semantic segmentation of remote sensing imagery is a fundamental task in
computer vision, supporting a wide range of applications such as land use
classification, urban planning, and environmental monitoring. However, this
task is often challenged by the high spatial resolution, complex scene
structures, and diverse object scales present in remote sensing data. To
address these challenges, various deep learning architectures have been
proposed, including convolutional neural networks, Vision Transformers, and the
recently introduced Vision Mamba. Vision Mamba features a global receptive
field and low computational complexity, demonstrating both efficiency and
effectiveness in image segmentation. However, its reliance on global scanning
tends to overlook critical local features, such as textures and edges, which
are essential for achieving accurate segmentation in remote sensing contexts.
To tackle this limitation, we propose SwinMamba, a novel framework inspired by
the Swin Transformer. SwinMamba integrates localized Mamba-style scanning
within shifted windows with a global receptive field, to enhance the model's
perception of both local and global features. Specifically, the first two
stages of SwinMamba perform local scanning to capture fine-grained details,
while its subsequent two stages leverage global scanning to fuse broader
contextual information. In our model, the use of overlapping shifted windows
enhances inter-region information exchange, facilitating more robust feature
integration across the entire image. Extensive experiments on the LoveDA and
ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art
methods, underscoring its effectiveness and potential as a superior solution
for semantic segmentation of remotely sensed imagery.

</details>


### [147] [Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Framework](https://arxiv.org/abs/2509.20923)
*Wenhao Tang,Heng Fang,Ge Wu,Xiang Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出了一种基于打包的多实例学习框架，用于解决计算病理学中全切片图像序列长度极端变化、数据异构性和监督有限的问题，通过打包采样和残差分支提高训练效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的全切片图像具有极长的序列长度（高达20万）、显著的长度变化和有限的监督，导致数据高度异构和冗余，传统方法在有限监督下难以有效处理这种异质性。

Method: 提出pack-based MIL框架，将采样的变长特征序列打包成固定长度序列实现批处理训练；引入残差分支将多个切片丢弃的特征组合成超切片进行训练；使用注意力驱动的下采样器压缩特征减少冗余。

Result: 在PANDA(UNI)数据集上实现了高达8%的准确率提升，同时仅使用12%的训练时间。

Conclusion: 专注于解决计算病理学中的数据挑战在基础模型时代具有显著潜力，提出的方法有效缓解了序列长度变化、数据异构性和监督有限等问题。

Abstract: Computational pathology (CPath) digitizes pathology slides into whole slide
images (WSIs), enabling analysis for critical healthcare tasks such as cancer
diagnosis and prognosis. However, WSIs possess extremely long sequence lengths
(up to 200K), significant length variations (from 200 to 200K), and limited
supervision. These extreme variations in sequence length lead to high data
heterogeneity and redundancy. Conventional methods often compromise on training
efficiency and optimization to preserve such heterogeneity under limited
supervision. To comprehensively address these challenges, we propose a
pack-based MIL framework. It packs multiple sampled, variable-length feature
sequences into fixed-length ones, enabling batched training while preserving
data heterogeneity. Moreover, we introduce a residual branch that composes
discarded features from multiple slides into a hyperslide which is trained with
tailored labels. It offers multi-slide supervision while mitigating feature
loss from sampling. Meanwhile, an attention-driven downsampler is introduced to
compress features in both branches to reduce redundancy. By alleviating these
challenges, our approach achieves an accuracy improvement of up to 8% while
using only 12% of the training time in the PANDA(UNI). Extensive experiments
demonstrate that focusing data challenges in CPath holds significant potential
in the era of foundation models. The code is
https://github.com/FangHeng/PackMIL

</details>


### [148] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff是一个模拟器约束的扩散模型，通过将环境参数直接集成到去噪过程中来生成物理合理的人体运动，避免了推理时重复的模拟器调用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩散过程中加入基于模拟器的运动投影层来确保物理合理性，但模拟器的顺序性导致计算成本高且无法并行化。

Method: 将模拟器运动投影解释为扩散过程中的引导形式，提出SimDiff模型，通过条件化环境参数（如重力、风力）直接在去噪过程中生成物理合理的运动。

Result: SimDiff能够高效生成物理合理的运动，无需推理时的重复模拟器调用，并提供对不同物理系数的细粒度控制，同时展示了对未见环境参数组合的组合泛化能力。

Conclusion: SimDiff通过将物理约束直接集成到扩散模型中，实现了高效且可控的物理合理人体运动生成，具有良好的泛化性能。

Abstract: Generating physically plausible human motion is crucial for applications such
as character animation and virtual reality. Existing approaches often
incorporate a simulator-based motion projection layer to the diffusion process
to enforce physical plausibility. However, such methods are computationally
expensive due to the sequential nature of the simulator, which prevents
parallelization. We show that simulator-based motion projection can be
interpreted as a form of guidance, either classifier-based or classifier-free,
within the diffusion process. Building on this insight, we propose SimDiff, a
Simulator-constrained Diffusion Model that integrates environment parameters
(e.g., gravity, wind) directly into the denoising process. By conditioning on
these parameters, SimDiff generates physically plausible motions efficiently,
without repeated simulator calls at inference, and also provides fine-grained
control over different physical coefficients. Moreover, SimDiff successfully
generalizes to unseen combinations of environmental parameters, demonstrating
compositional generalization.

</details>


### [149] [Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models](https://arxiv.org/abs/2509.20939)
*Bum Jun Kim,Makoto Kawano,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: 本文研究了视觉架构对高斯噪声的鲁棒性，通过评估1174个预训练模型识别出四个关键设计模式，并提供了理论解释和实用设计指南。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉模型的鲁棒性常被测量，但特定架构设计选择对其的影响很少被剖析。本文旨在理解为什么某些视觉架构对高斯噪声具有固有鲁棒性，并将这些经验发现转化为可操作的设计规则。

Method: 对1174个预训练视觉模型进行广泛评估，识别出四个一致的设计模式：更大的stem卷积核、更小的输入分辨率、平均池化以及监督ViT而非CLIP ViT。然后通过理论分析解释这些发现，将观察到的相关性转化为因果机制。

Result: 识别出的设计模式可带来高达506位的排名提升和21.6%的准确率增益。理论分析证明了低通stem核以二次方关系衰减噪声，平均池化无偏且按窗口面积抑制噪声，而CLIP ViT的预处理标准化放大了最坏情况敏感性。

Conclusion: 研究结果将鲁棒性分解为可解释的模块，提供了解释观察趋势的理论，并建立了实用的即插即用指南，用于设计对高斯噪声更鲁棒的视觉模型。

Abstract: While the robustness of vision models is often measured, their dependence on
specific architectural design choices is rarely dissected. We investigate why
certain vision architectures are inherently more robust to additive Gaussian
noise and convert these empirical insights into simple, actionable design
rules. Specifically, we performed extensive evaluations on 1,174 pretrained
vision models, empirically identifying four consistent design patterns for
improved robustness against Gaussian noise: larger stem kernels, smaller input
resolutions, average pooling, and supervised vision transformers (ViTs) rather
than CLIP ViTs, which yield up to 506 rank improvements and 21.6\%p accuracy
gains. We then develop a theoretical analysis that explains these findings,
converting observed correlations into causal mechanisms. First, we prove that
low-pass stem kernels attenuate noise with a gain that decreases quadratically
with kernel size and that anti-aliased downsampling reduces noise energy
roughly in proportion to the square of the downsampling factor. Second, we
demonstrate that average pooling is unbiased and suppresses noise in proportion
to the pooling window area, whereas max pooling incurs a positive bias that
grows slowly with window size and yields a relatively higher mean-squared error
and greater worst-case sensitivity. Third, we reveal and explain the
vulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller
normalization standard deviations used in CLIP preprocessing amplify worst-case
sensitivity by up to 1.91 times relative to the Inception-style preprocessing
common in supervised ViTs. Our results collectively disentangle robustness into
interpretable modules, provide a theory that explains the observed trends, and
build practical, plug-and-play guidelines for designing vision models more
robust against Gaussian noise.

</details>


### [150] [Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery](https://arxiv.org/abs/2509.20941)
*Angelo Henriques,Korab Hoxha,Daniel Zapp,Peter C. Issa,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: 这篇论文通过PRISMA-ScR指导的范围综述，系统性地分析了场景图在手术领域的研究现状、应用进展和未来方向，揭示了数据鸿沟问题并展示了该技术的成熟发展。


<details>
  <summary>Details</summary>
Motivation: 场景图提供结构化关系表示，对于解码复杂动态的手术环境至关重要。研究者希望通过系统综述来描绘手术场景图研究的演变格局。

Method: 采用PRISMA-ScR指导的范围综述方法，系统性地映射手术场景图研究的应用、方法学进展和未来方向。

Result: 分析显示该领域快速增长，但发现关键的数据鸿沟：内部视角研究主要使用真实2D视频，而外部视角4D建模严重依赖模拟数据。方法学上从基础图神经网络发展到专门的基础模型，在手术环境中显著优于通用大型视觉语言模型。

Conclusion: 手术场景图正在成熟为重要的语义桥梁，能够支持新一代智能系统来改善手术安全性、效率和培训，尽管数据标注和实时实现等挑战仍然存在，但正在通过新兴技术得到解决。

Abstract: Scene graphs (SGs) provide structured relational representations crucial for
decoding complex, dynamic surgical environments. This PRISMA-ScR-guided scoping
review systematically maps the evolving landscape of SG research in surgery,
charting its applications, methodological advancements, and future directions.
Our analysis reveals rapid growth, yet uncovers a critical 'data divide':
internal-view research (e.g., triplet recognition) almost exclusively uses
real-world 2D video, while external-view 4D modeling relies heavily on
simulated data, exposing a key translational research gap. Methodologically,
the field has advanced from foundational graph neural networks to specialized
foundation models that now significantly outperform generalist large
vision-language models in surgical contexts. This progress has established SGs
as a cornerstone technology for both analysis, such as workflow recognition and
automated safety monitoring, and generative tasks like controllable surgical
simulation. Although challenges in data annotation and real-time implementation
persist, they are actively being addressed through emerging techniques.
Surgical SGs are maturing into an essential semantic bridge, enabling a new
generation of intelligent systems to improve surgical safety, efficiency, and
training.

</details>


### [151] [A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning](https://arxiv.org/abs/2509.20946)
*Dongqi Zheng,Wenjin Fu,Guangzong Chen*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的自动化系统，用于激光功率计传感器涂层的缺陷检测和分类，采用无监督异常检测框架，无需大量标注缺陷数据集即可检测已知和新型缺陷。


<details>
  <summary>Details</summary>
Motivation: 解决激光功率计传感器涂层缺陷（如热损伤和划痕）检测的关键挑战，这些缺陷会影响医疗和工业应用中激光能量测量的准确性。

Method: 系统包含三个关键组件：(1) 使用拉普拉斯边缘检测和K-means聚类的预处理管道，(2) 通过StyleGAN2进行合成数据增强，(3) 基于UFlow的神经网络架构进行多尺度特征提取和异常图生成。

Result: 在366张真实传感器图像上的实验评估显示，缺陷样本准确率为93.8%，良好样本准确率为89.3%，图像级AUROC为0.957，像素级AUROC为0.961。

Conclusion: 该系统通过自动化质量控制实现潜在年度成本节约，在设备端实现每张图像0.5秒的处理时间。

Abstract: We present an automated vision-based system for defect detection and
classification of laser power meter sensor coatings. Our approach addresses the
critical challenge of identifying coating defects such as thermal damage and
scratches that can compromise laser energy measurement accuracy in medical and
industrial applications. The system employs an unsupervised anomaly detection
framework that trains exclusively on ``good'' sensor images to learn normal
coating distribution patterns, enabling detection of both known and novel
defect types without requiring extensive labeled defect datasets. Our
methodology consists of three key components: (1) a robust preprocessing
pipeline using Laplacian edge detection and K-means clustering to segment the
area of interest, (2) synthetic data augmentation via StyleGAN2, and (3) a
UFlow-based neural network architecture for multi-scale feature extraction and
anomaly map generation. Experimental evaluation on 366 real sensor images
demonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy on
good samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961.
The system provides potential annual cost savings through automated quality
control and processing times of 0.5 seconds per image in on-device
implementation.

</details>


### [152] [Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos](https://arxiv.org/abs/2509.20961)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Sriparna Saha,Alka Maurya*

Main category: cs.CV

TL;DR: FASTER是一个用于金融咨询视频多模态摘要的模块化框架，通过提取模态特征、生成优化摘要和视觉关键帧对齐，解决了长视频内容分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上金融咨询视频内容激增，但30-40分钟的多模态内容难以有效提取关键信息，需要专门的多模态摘要方法。

Method: 使用BLIP生成视觉语义描述，OCR提取文本模式，Whisper进行转录和说话人识别，采用改进的DPO损失函数确保事实一致性，基于排序的检索机制对齐关键帧。

Result: 在Fin-APT数据集上的跨领域实验表明，FASTER相比LLMs和VLMs具有更强的性能、鲁棒性和泛化能力。

Conclusion: FASTER为多模态摘要设立了新标准，使金融咨询内容更易获取和实用，并发布了开源数据集和代码。

Abstract: The dynamic propagation of social media has broadened the reach of financial
advisory content through podcast videos, yet extracting insights from lengthy,
multimodal segments (30-40 minutes) remains challenging. We introduce FASTER
(Financial Advisory Summariser with Textual Embedded Relevant images), a
modular framework that tackles three key challenges: (1) extracting
modality-specific features, (2) producing optimized, concise summaries, and (3)
aligning visual keyframes with associated textual points. FASTER employs BLIP
for semantic visual descriptions, OCR for textual patterns, and Whisper-based
transcription with Speaker diarization as BOS features. A modified Direct
Preference Optimization (DPO)-based loss function, equipped with BOS-specific
fact-checking, ensures precision, relevance, and factual consistency against
the human-aligned summary. A ranker-based retrieval mechanism further aligns
keyframes with summarized content, enhancing interpretability and cross-modal
coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a
dataset comprising 470 publicly accessible financial advisory pep-talk videos
for robust multimodal research. Comprehensive cross-domain experiments confirm
FASTER's strong performance, robustness, and generalizability when compared to
Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing
a new standard for multimodal summarization, FASTER makes financial advisory
content more accessible and actionable, thereby opening new avenues for
research. The dataset and code are available at:
https://github.com/sarmistha-D/FASTER

</details>


### [153] [An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep Image Clustering](https://arxiv.org/abs/2509.20976)
*Yue Duan,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: ASD是一种适配器，使SSL学习器能够在无需任何先决条件的情况下冷启动深度图像聚类，通过伪标记数据触发通用SSL学习器进行图像聚类。


<details>
  <summary>Details</summary>
Motivation: 现有工作将SSL技术集成到深度聚类框架中需要预训练、聚类学习或训练好的聚类模型作为先决条件，限制了SSL学习器在图像聚类任务中的灵活性和开箱即用性。

Method: 1. 从无标签数据中随机采样伪标记数据，设置实例级分类器学习语义对齐的实例级标签；2. 通过跟踪无标签数据预测的类别转换提取实例级类别的高级相似性，为伪标记数据分配聚类级标签；3. 使用分配了聚类级标签的伪标记数据触发在无标签数据上训练的通用SSL学习器进行图像聚类。

Result: ASD在各种基准测试中表现出优于最新深度图像聚类方法的性能，与使用真实标签的SSL方法相比仅有很小的准确率差距（如CIFAR-10上仅差1.33%）。

Conclusion: ASD能够进一步提升现有SSL嵌入的深度图像聚类方法的性能，为SSL学习器在图像聚类任务中的冷启动应用提供了有效解决方案。

Abstract: Recently, some works integrate SSL techniques into deep clustering frameworks
to enhance image clustering performance. However, they all need pretraining,
clustering learning, or a trained clustering model as prerequisites, limiting
the flexible and out-of-box application of SSL learners in the image clustering
task. This work introduces ASD, an adaptor that enables the cold-start of SSL
learners for deep image clustering without any prerequisites. Specifically, we
first randomly sample pseudo-labeled data from all unlabeled data, and set an
instance-level classifier to learn them with semantically aligned
instance-level labels. With the ability of instance-level classification, we
track the class transitions of predictions on unlabeled data to extract
high-level similarities of instance-level classes, which can be utilized to
assign cluster-level labels to pseudo-labeled data. Finally, we use the
pseudo-labeled data with assigned cluster-level labels to trigger a general SSL
learner trained on the unlabeled data for image clustering. We show the
superior performance of ASD across various benchmarks against the latest deep
image clustering approaches and very slight accuracy gaps compared to SSL
methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can
also further boost the performance of existing SSL-embedded deep image
clustering methods.

</details>


### [154] [SiNGER: A Clearer Voice Distills Vision Transformers Further](https://arxiv.org/abs/2509.20986)
*Geunhyeok Yu,Sunjae Jeong,Yoonyoung Choi,Jaeseung Kim,Hyoseok Hwang*

Main category: cs.CV

TL;DR: 提出SiNGER蒸馏框架，通过奇异零空间引导能量重分配来抑制Vision Transformer中的高范数伪影，同时保留信息信号，提升学生模型性能


<details>
  <summary>Details</summary>
Motivation: Vision Transformer作为视觉基础模型会产生高范数伪影，降低表示质量。传统知识蒸馏会让学生模型过度拟合伪影而忽视信息信号，限制了大型模型的增益

Method: 使用基于LoRA的适配器实现奇异零空间引导扰动，在教师特征精炼过程中抑制伪影同时保留信息，然后将精炼后的特征蒸馏给学生模型

Result: 在多个下游任务中实现最先进性能，产生更清晰、更可解释的表示

Conclusion: SiNGER框架有效解决了教师特征中伪影抑制和信息保留的权衡问题，显著提升了知识蒸馏的效果

Abstract: Vision Transformers are widely adopted as the backbone of vision foundation
models, but they are known to produce high-norm artifacts that degrade
representation quality. When knowledge distillation transfers these features to
students, high-norm artifacts dominate the objective, so students overfit to
artifacts and underweight informative signals, diminishing the gains from
larger models. Prior work attempted to remove artifacts but encountered an
inherent trade-off between artifact suppression and preserving informative
signals from teachers. To address this, we introduce Singular Nullspace-Guided
Energy Reallocation (SiNGER), a novel distillation framework that suppresses
artifacts while preserving informative signals. The key idea is principled
teacher feature refinement: during refinement, we leverage the nullspace-guided
perturbation to preserve information while suppressing artifacts. Then, the
refined teacher's features are distilled to a student. We implement this
perturbation efficiently with a LoRA-based adapter that requires minimal
structural modification. Extensive experiments show that \oursname consistently
improves student models, achieving state-of-the-art performance in multiple
downstream tasks and producing clearer and more interpretable representations.

</details>


### [155] [Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors](https://arxiv.org/abs/2509.20991)
*Jan Kněžík,Jonáš Herec,Rado Pitoňák*

Main category: cs.CV

TL;DR: Fast-SEnSeI是一个轻量级、传感器无关的编码器模块，用于在轨云分割，支持多光谱传感器和不同波段配置。


<details>
  <summary>Details</summary>
Motivation: 现有的云分割模型通常与特定传感器配置紧密耦合，且依赖地面处理，缺乏灵活性和在轨处理能力。

Method: 基于SEnSeI-v2改进，集成了改进的光谱描述符、轻量级架构和鲁棒的填充波段处理，可接受任意光谱波段组合，生成固定大小的特征图，配合基于改进U-Net的量化分割模型，采用CPU-FPGA混合部署方案。

Result: 在Sentinel-2和Landsat 8数据集上的评估表明，该方法能在不同输入配置下实现准确的云分割。

Conclusion: Fast-SEnSeI提供了一个高效、灵活的云分割解决方案，适用于空间合格硬件上的在轨处理。

Abstract: Cloud segmentation is a critical preprocessing step for many Earth
observation tasks, yet most models are tightly coupled to specific sensor
configurations and rely on ground-based processing. In this work, we propose
Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables
flexible, on-board cloud segmentation across multispectral sensors with varying
band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an
improved spectral descriptor, lightweight architecture, and robust padding-band
handling. It accepts arbitrary combinations of spectral bands and their
wavelengths, producing fixed-size feature maps that feed into a compact,
quantized segmentation model based on a modified U-Net. The module runs
efficiently on embedded CPUs using Apache TVM, while the segmentation model is
deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for
space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets
demonstrate accurate segmentation across diverse input configurations.

</details>


### [156] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: SNCE是一种基于单个神经元的概念擦除方法，通过稀疏自编码器将文本嵌入映射到稀疏解缠的潜在空间，精确识别并抑制有害概念对应的单个神经元，实现精准的概念擦除同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型存在生成有害内容的安全风险，现有概念擦除方法难以在精确移除目标概念的同时最小化图像质量退化。

Method: 训练稀疏自编码器(SAE)将文本嵌入映射到稀疏解缠的潜在空间，设计基于调制频率评分的新神经元识别方法定位有害概念对应的神经元，通过抑制该神经元的激活实现概念擦除。

Result: 在多个基准测试中，SNCE在目标概念擦除方面达到最先进水平，同时保持非目标概念的生成能力，对对抗攻击表现出强鲁棒性。

Conclusion: SNCE通过单神经元操作实现了精确的概念擦除，在保持图像质量的同时有效防止有害内容生成，显著优于现有方法。

Abstract: Text-to-image models exhibit remarkable capabilities in image generation.
However, they also pose safety risks of generating harmful content. A key
challenge of existing concept erasure methods is the precise removal of target
concepts while minimizing degradation of image quality. In this paper, we
propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can
precisely prevent harmful content generation by manipulating only a single
neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text
embeddings into a sparse, disentangled latent space, where individual neurons
align tightly with atomic semantic concepts. To accurately locate neurons
responsible for harmful concepts, we design a novel neuron identification
method based on the modulated frequency scoring of activation patterns. By
suppressing activations of the harmful concept-specific neuron, SNCE achieves
surgical precision in concept erasure with minimal disruption to image quality.
Experiments on various benchmarks demonstrate that SNCE achieves
state-of-the-art results in target concept erasure, while preserving the
model's generation capabilities for non-target concepts. Additionally, our
method exhibits strong robustness against adversarial attacks, significantly
outperforming existing methods.

</details>


### [157] [OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities](https://arxiv.org/abs/2509.21038)
*Andreas Gilson,Lukas Meyer,Oliver Scholz,Ute Schmid*

Main category: cs.CV

TL;DR: 提出KDSS算法用于生物点云的下采样，无需对输入数据进行下采样即可实现全分辨率点云分割，适用于不同传感器和植物物种。


<details>
  <summary>Details</summary>
Motivation: 现有植物器官点云分割方法通常针对特定植物物种或传感器模式，且需要大量预处理和下采样来满足硬件或神经网络输入要求。

Method: 提出KDSS（K-Dimensional Sub-Sampling）算法，这是一种轻量级的生物点云下采样方法，能够保持分辨率，不依赖于特定传感器数据和植物物种。

Result: 将KDSS与当前最先进的分割模型结合，在摄影测量、激光三角测量和LiDAR等不同模态下对多种植物物种进行评估，结果令人满意。

Conclusion: KDSS可作为密集预处理和下采样方法的轻量级替代方案，适用于任何植物物种和传感器模式的植物器官分割。

Abstract: Accurate point cloud segmentation for plant organs is crucial for 3D plant
phenotyping. Existing solutions are designed problem-specific with a focus on
certain plant species or specified sensor-modalities for data acquisition.
Furthermore, it is common to use extensive pre-processing and down-sample the
plant point clouds to meet hardware or neural network input size requirements.
We propose a simple, yet effective algorithm KDSS for sub-sampling of
biological point clouds that is agnostic to sensor data and plant species. The
main benefit of this approach is that we do not need to down-sample our input
data and thus, enable segmentation of the full-resolution point cloud.
Combining KD-SS with current state-of-the-art segmentation models shows
satisfying results evaluated on different modalities such as photogrammetry,
laser triangulation and LiDAR for various plant species. We propose KD-SS as
lightweight resolution-retaining alternative to intensive pre-processing and
down-sampling methods for plant organ segmentation regardless of used species
and sensor modality.

</details>


### [158] [Background Prompt for Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.21055)
*Songyue Cai,Zongqian Wu,Yujie Mo,Liang Peng,Ping Hu,Xiaoshuang Shi,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 提出了名为Mambo的新FG-BG分解框架，用于解决FS-OOD检测中现有方法因过度依赖局部类别相似性和固定背景补丁提取策略导致的低鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的FG-BG分解方法在FS-OOD检测中存在鲁棒性低的问题，主要原因是过度依赖局部类别相似性和固定的背景补丁提取策略。

Method: 首先学习背景提示以获得包含背景和图像语义信息的局部背景相似性，然后使用局部类别相似性细化局部背景相似性。同时提出补丁自校准调优，根据样本多样性灵活选择不同样本的背景补丁数量。

Result: 在真实世界数据集上的广泛实验表明，Mambo在OOD检测和近OOD检测设置方面相比SOTA方法取得了最佳性能。

Conclusion: Mambo框架通过结合局部背景相似性和局部类别相似性进行背景提取，并采用灵活的补丁选择策略，有效提高了FS-OOD检测的鲁棒性和性能。

Abstract: Existing foreground-background (FG-BG) decomposition methods for the few-shot
out-of-distribution (FS-OOD) detection often suffer from low robustness due to
over-reliance on the local class similarity and a fixed background patch
extraction strategy. To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we
propose to first learn a background prompt to obtain the local background
similarity containing both the background and image semantic information, and
then refine the local background similarity using the local class similarity.
As a result, we use both the refined local background similarity and the local
class similarity to conduct background extraction, reducing the dependence of
the local class similarity in previous methods. Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.
Extensive experiments on real-world datasets demonstrate that our proposed
Mambo achieves the best performance, compared to SOTA methods in terms of OOD
detection and near OOD detection setting. The source code will be released at
https://github.com/YuzunoKawori/Mambo.

</details>


### [159] [Stratify or Die: Rethinking Data Splits in Image Segmentation](https://arxiv.org/abs/2509.21056)
*Naga Venkata Sai Jitin Jami,Thomas Altstidl,Jonas Mueller,Jindong Li,Dario Zanca,Bjoern Eskofier,Heike Leutheuser*

Main category: cs.CV

TL;DR: 本文提出了两种图像分割数据集划分方法：IPS（迭代像素分层）和WDES（Wasserstein驱动的进化分层），旨在解决随机划分导致的测试集不具代表性问题，提高模型评估的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图像分割中随机划分数据集会导致测试集不具代表性，产生有偏评估和模型泛化能力差的问题。虽然分层采样在分类任务中有效，但由于分割数据的多标签结构和类别不平衡特性，将其扩展到分割任务具有挑战性。

Method: 1. IPS：基于现有分层概念的简单标签感知采样方法；2. WDES：新颖的遗传算法，通过最小化Wasserstein距离来优化数据集划分中标签分布的相似性，理论上证明在足够代数下具有全局最优性。

Result: 使用新提出的统计异质性指标评估，WDES相比随机采样始终产生更具代表性的划分。在街景、医学影像和卫星图像等多种分割任务中应用WDES，实现了更低的性能方差和改善的模型评估效果。

Conclusion: WDES在处理小型、不平衡和低多样性数据集时特别有价值，这些情况下传统划分策略最容易产生偏差。该方法显著提高了分割模型评估的可靠性和泛化性能。

Abstract: Random splitting of datasets in image segmentation often leads to
unrepresentative test sets, resulting in biased evaluations and poor model
generalization. While stratified sampling has proven effective for addressing
label distribution imbalance in classification tasks, extending these ideas to
segmentation remains challenging due to the multi-label structure and class
imbalance typically present in such data. Building on existing stratification
concepts, we introduce Iterative Pixel Stratification (IPS), a straightforward,
label-aware sampling method tailored for segmentation tasks. Additionally, we
present Wasserstein-Driven Evolutionary Stratification (WDES), a novel genetic
algorithm designed to minimize the Wasserstein distance, thereby optimizing the
similarity of label distributions across dataset splits. We prove that WDES is
globally optimal given enough generations. Using newly proposed statistical
heterogeneity metrics, we evaluate both methods against random sampling and
find that WDES consistently produces more representative splits. Applying WDES
across diverse segmentation tasks, including street scenes, medical imaging,
and satellite imagery, leads to lower performance variance and improved model
evaluation. Our results also highlight the particular value of WDES in handling
small, imbalanced, and low-diversity datasets, where conventional splitting
strategies are most prone to bias.

</details>


### [160] [EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task](https://arxiv.org/abs/2509.21061)
*Riccardo La Grassa,Ignazio Gallo,Nicola Landro*

Main category: cs.CV

TL;DR: EnGraf-Net是一种利用语义层次结构（分类学）作为监督信号的端到端深度神经网络模型，用于细粒度分类，无需裁剪技术或手动标注即可达到最先进方法的竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度分类模型大多依赖部分标注或自动裁剪方法，但这些方法存在局部特征表示不完整的问题。人类识别物体时还会形成语义关联，因此作者提出利用语义层次结构来增强分类性能。

Method: 提出EnGraf-Net模型，将语义关联以层次结构（分类学）的形式作为监督信号集成到端到端深度神经网络中。

Result: 在CIFAR-100、CUB-200-2011和FGVC-Aircraft三个知名数据集上的实验表明，EnGraf-Net优于许多现有细粒度模型，与最先进的方桉相比具有竞争力。

Conclusion: 利用语义层次结构作为监督信号可以有效提升细粒度分类性能，无需依赖裁剪技术或手动标注，为细粒度分类提供了新的思路。

Abstract: Fine-grained classification models are designed to focus on the relevant
details necessary to distinguish highly similar classes, particularly when
intra-class variance is high and inter-class variance is low. Most existing
models rely on part annotations such as bounding boxes, part locations, or
textual attributes to enhance classification performance, while others employ
sophisticated techniques to automatically extract attention maps. We posit that
part-based approaches, including automatic cropping methods, suffer from an
incomplete representation of local features, which are fundamental for
distinguishing similar objects. While fine-grained classification aims to
recognize the leaves of a hierarchical structure, humans recognize objects by
also forming semantic associations. In this paper, we leverage semantic
associations structured as a hierarchy (taxonomy) as supervised signals within
an end-to-end deep neural network model, termed EnGraf-Net. Extensive
experiments on three well-known datasets CIFAR-100, CUB-200-2011, and
FGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing
fine-grained models, showing competitive performance with the most recent
state-of-the-art approaches, without requiring cropping techniques or manual
annotations.

</details>


### [161] [Vision Transformers: the threat of realistic adversarial patches](https://arxiv.org/abs/2509.21084)
*Kasper Cools,Clara Maathuis,Alexander M. van Oers,Claudia S. Hübner,Nikos Deligiannis,Marijke Vandewal,Geert De Cubber*

Main category: cs.CV

TL;DR: 本文研究了对抗性补丁从卷积神经网络（CNNs）到视觉变换器（ViTs）的可迁移性，发现ViTs对对抗性攻击存在显著脆弱性，攻击成功率在不同模型间差异很大（40.04%到99.97%）。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统的广泛应用，其安全性变得至关重要。虽然ViTs在性能和对抗扰动鲁棒性方面优于CNNs，但它们仍然容易受到对抗性补丁攻击，这可能导致安全漏洞或错误分类。

Method: 使用Creases Transformation（CT）技术设计现实的对抗性补丁，在人物与非人物分类任务中测试四个微调的ViT模型，评估对抗性攻击技术的可迁移性。

Result: 实验结果显示不同ViT模型的脆弱性差异显著：google/vit-base-patch16-224-in21k的攻击成功率为40.04%，facebook/dino-vitb16达到99.97%，其他两个模型分别为66.40%和65.17%。

Conclusion: 研究证实了对抗性补丁从CNNs到ViTs的跨架构可迁移性，预训练数据集的规模和方法论强烈影响模型对对抗性攻击的韧性。

Abstract: The increasing reliance on machine learning systems has made their security a
critical concern. Evasion attacks enable adversaries to manipulate the
decision-making processes of AI systems, potentially causing security breaches
or misclassification of targets. Vision Transformers (ViTs) have gained
significant traction in modern machine learning due to increased 1) performance
compared to Convolutional Neural Networks (CNNs) and 2) robustness against
adversarial perturbations. However, ViTs remain vulnerable to evasion attacks,
particularly to adversarial patches, unique patterns designed to manipulate AI
classification systems. These vulnerabilities are investigated by designing
realistic adversarial patches to cause misclassification in person vs.
non-person classification tasks using the Creases Transformation (CT)
technique, which adds subtle geometric distortions similar to those occurring
naturally when wearing clothing. This study investigates the transferability of
adversarial attack techniques used in CNNs when applied to ViT classification
models. Experimental evaluation across four fine-tuned ViT models on a binary
person classification task reveals significant vulnerability variations: attack
success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%
(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and
facebook/dinov3-vitb16 reaching 65.17%. These results confirm the
cross-architectural transferability of adversarial patches from CNNs to ViTs,
with pre-training dataset scale and methodology strongly influencing model
resilience to adversarial attacks.

</details>


### [162] [UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition](https://arxiv.org/abs/2509.21086)
*Guojun Lei,Rong Zhang,Chi Wang,Tianhang Liu,Hong Li,Zhiyuan Ma,Weiwei Xu*

Main category: cs.CV

TL;DR: UniTransfer通过空间和时间步分解实现精确可控的视频概念迁移，采用双流到单流的DiT架构和Chain-of-Prompt机制，在OpenAnimal数据集上超越现有基线


<details>
  <summary>Details</summary>
Motivation: 实现高质量和可控的视频概念迁移，解决现有方法在视觉保真度和编辑性方面的不足

Method: 1) 空间分解：将视频分解为前景主体、背景和运动流；2) 双流到单流DiT架构；3) 基于随机掩码的自监督预训练；4) Chain-of-Prompt机制进行时间步分解；5) 构建OpenAnimal动物视频数据集

Result: 在多样化参考图像和场景中实现高质量可控的视频概念迁移，在视觉保真度和编辑性方面超越现有基线方法

Conclusion: UniTransfer通过创新的空间和时间步分解方法，为视频概念迁移任务提供了有效的解决方案，展示了优越的性能和可控性

Abstract: We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer. Specifically, in terms of
spatial decomposition, we decouple videos into three key components: the
foreground subject, the background, and the motion flow. Building upon this
decomposed formulation, we further introduce a dual-to-single-stream DiT-based
architecture for supporting fine-grained control over different components in
the videos. We also introduce a self-supervised pretraining strategy based on
random masking to enhance the decomposed representation learning from
large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning
paradigm, we further revisit the denoising diffusion process and propose a
Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We
decompose the denoising process into three stages of different granularity and
leverage large language models (LLMs) for stage-specific instructions to guide
the generation progressively. We also curate an animal-centric video dataset
called OpenAnimal to facilitate the advancement and benchmarking of research in
video concept transfer. Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability. Web Page:
https://yu-shaonian.github.io/UniTransfer-Web/

</details>


### [163] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: CARINOX是一个统一的框架，结合了噪声优化和探索，通过基于人类判断相关性的奖励选择程序，提升文本到图像扩散模型的组合对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理复杂对象关系、属性和空间排列时，往往无法实现良好的组合对齐。现有的推理时方法各有局限：优化可能因初始化不佳而停滞，探索则需要大量样本。

Method: CARINOX框架结合了噪声优化和探索策略，采用基于人类判断相关性的原则性奖励选择程序，以克服单一奖励指标或临时组合的不足。

Result: 在两个互补基准测试中，CARINOX将平均对齐分数提高了16%（T2I-CompBench++）和11%（HRS基准），在所有主要类别中均优于最先进的优化和探索方法，同时保持图像质量和多样性。

Conclusion: CARINOX通过统一的优化-探索框架和原则性奖励选择，有效解决了文本到图像扩散模型的组合对齐问题，显著提升了性能。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [164] [VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception](https://arxiv.org/abs/2509.21100)
*Ziang Yan,Xinhao Li,Yinan He,Zhengrong Yue,Xiangyu Zeng,Yali Wang,Yu Qiao,Limin Wang,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出视觉测试时间缩放（VTTS）方法，通过迭代感知机制增强多模态大语言模型的推理能力，模仿人类分层注意力机制，在推理过程中逐步优化视觉焦点区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖LLM推理来分析解析后的视觉内容，但受限于静态感知阶段，无法实现动态的迭代优化。需要模拟人类的分层注意力机制来提升多模态模型的推理性能。

Method: VTTS采用迭代感知（ITP）机制，结合强化学习和时空监督来优化推理过程。通过逐步聚焦高置信度的时空区域，并基于更新的文本预测进行指导。同时构建了VTTS-80K数据集支持该范式。

Result: 实验表明VTTS在多个基准测试中显著提升性能，Videochat-R1.5模型相比Qwen2.5VL-3B和-7B等基线模型，在超过15个基准测试中平均提升超过5%，涵盖视频对话、视频推理和时空感知任务。

Conclusion: VTTS通过增加感知计算有效增强了MLLMs的推理能力，验证了迭代感知机制在多模态任务中的有效性和泛化性。

Abstract: Inducing reasoning in multimodal large language models (MLLMs) is critical
for achieving human-level perception and understanding. Existing methods mainly
leverage LLM reasoning to analyze parsed visuals, often limited by static
perception stages. This paper introduces Visual Test-Time Scaling (VTTS), a
novel approach to enhance MLLMs' reasoning via iterative perception during
inference. VTTS mimics humans' hierarchical attention by progressively refining
focus on high-confidence spatio-temporal regions, guided by updated textual
predictions. Specifically, VTTS employs an Iterative Perception (ITP)
mechanism, incorporating reinforcement learning with spatio-temporal
supervision to optimize reasoning. To support this paradigm, we also present
VTTS-80K, a dataset tailored for iterative perception. These designs allows a
MLLM to enhance its performance by increasing its perceptual compute. Extensive
experiments validate VTTS's effectiveness and generalization across diverse
tasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved
remarkable improvements, with an average increase of over 5\%, compared to
robust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks
that encompass video conversation, video reasoning, and spatio-temporal
perception.

</details>


### [165] [Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models](https://arxiv.org/abs/2509.21102)
*Suaiba Amina Salahuddin,Teresa Dorszewski,Marit Almenning Martiniussen,Tone Hovda,Antonio Portaluri,Solveig Thrun,Michael Kampffmeyer,Elisabeth Wetzer,Kristoffer Wickstrøm,Robert Jenssen*

Main category: cs.CV

TL;DR: Mammo-CLIP Dissect是首个基于概念的可解释性框架，用于系统分析乳腺X光片DL视觉模型，通过文本概念标记神经元并量化其与领域知识的对齐度。


<details>
  <summary>Details</summary>
Motivation: 理解DL模型学习内容对AI在临床环境中的安全部署至关重要，现有研究多关注像素级可解释性方法，而文本概念能更好反映临床医生的推理过程。

Method: 利用乳腺X光片专用视觉语言模型Mammo-CLIP作为"解剖器"，在指定层用人类可解释的文本概念标记神经元，并量化其与领域知识的对齐度。

Result: 乳腺X光片数据训练的模型捕获更多临床相关概念且与放射科医生工作流程更一致；针对特定任务的微调增强了某些概念类别但可能减少其他概念的覆盖。

Conclusion: Mammo-CLIP Dissect揭示了CNN如何捕获乳腺X光片特定知识，展示了领域特定训练和任务特定适应如何塑造概念学习。

Abstract: Understanding what deep learning (DL) models learn is essential for the safe
deployment of artificial intelligence (AI) in clinical settings. While previous
work has focused on pixel-based explainability methods, less attention has been
paid to the textual concepts learned by these models, which may better reflect
the reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first
concept-based explainability framework for systematically dissecting DL vision
models trained for mammography. Leveraging a mammography-specific
vision-language model (Mammo-CLIP) as a "dissector," our approach labels
neurons at specified layers with human-interpretable textual concepts and
quantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we
investigate three key questions: (1) how concept learning differs between DL
vision models trained on general image datasets versus mammography-specific
datasets; (2) how fine-tuning for downstream mammography tasks affects concept
specialisation; and (3) which mammography-relevant concepts remain
underrepresented. We show that models trained on mammography data capture more
clinically relevant concepts and align more closely with radiologists'
workflows than models not trained on mammography data. Fine-tuning for
task-specific classification enhances the capture of certain concept categories
(e.g., benign calcifications) but can reduce coverage of others (e.g.,
density-related features), indicating a trade-off between specialisation and
generalisation. Our findings show that Mammo-CLIP Dissect provides insights
into how convolutional neural networks (CNNs) capture mammography-specific
knowledge. By comparing models across training data and fine-tuning regimes, we
reveal how domain-specific training and task-specific adaptation shape concept
learning. Code and concept set are available:
https://github.com/Suaiba/Mammo-CLIP-Dissect.

</details>


### [166] [MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning](https://arxiv.org/abs/2509.21113)
*Sicheng Tao,Jungang Li,Yibo Yan,Junyan Zhang,Yubo Gao,Hanqian Li,ShuHang Xun,Yuxuan Fan,Hong Chen,Jianxiang He,Xuming Hu*

Main category: cs.CV

TL;DR: MOSS-ChatV是一个基于强化学习的视频推理框架，通过动态时间规整（DTW）的过程奖励来解决多模态大语言模型在视频推理中的过程不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视频推理中存在过程不一致问题，即中间推理过程与视频动态不符，即使最终答案正确也影响了模型的可解释性和鲁棒性。

Method: 提出MOSS-ChatV框架，使用基于动态时间规整（DTW）的过程奖励进行强化学习，将推理轨迹与时间基础参考对齐，无需辅助奖励模型即可实现高效的过程监督。

Result: MOSS-ChatV在MOSS-Video测试集上达到87.2%的准确率，在MVBench和MMVU等通用视频基准测试中表现提升，并在不同架构（如Qwen2.5-VL和Phi-2）上均获得一致增益。GPT-4o评估显示其产生更一致稳定的推理轨迹。

Conclusion: 该框架有效解决了视频推理中的过程不一致问题，具有广泛的适用性，能够生成更可靠和可解释的推理过程。

Abstract: Video reasoning has emerged as a critical capability for multimodal large
language models (MLLMs), requiring models to move beyond static perception
toward coherent understanding of temporal dynamics in complex scenes. Yet
existing MLLMs often exhibit process inconsistency, where intermediate
reasoning drifts from video dynamics even when the final answer is correct,
undermining interpretability and robustness. To address this issue, we
introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time
Warping (DTW)-based process reward. This rule-based reward aligns reasoning
traces with temporally grounded references, enabling efficient process
supervision without auxiliary reward models. We further identify dynamic state
prediction as a key measure of video reasoning and construct MOSS-Video, a
benchmark with annotated reasoning traces, where the training split is used to
fine-tune MOSS-ChatV and the held-out split is reserved for evaluation.
MOSS-ChatV achieves 87.2\% on MOSS-Video (test) and improves performance on
general video benchmarks such as MVBench and MMVU. The framework consistently
yields gains across different architectures, including Qwen2.5-VL and Phi-2,
confirming its broad applicability. Evaluations with GPT-4o-as-judge further
show that MOSS-ChatV produces more consistent and stable reasoning traces.

</details>


### [167] [MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation](https://arxiv.org/abs/2509.21119)
*Guojun Lei,Chi Wang,Yikai Wang,Hong Li,Ying Song,Weiwei Xu*

Main category: cs.CV

TL;DR: 提出了一种将相机和物体运动统一转换为像素运动的新方法，通过稳定扩散网络学习参考运动图，结合语义对象先验生成符合指定相机轨迹的视频


<details>
  <summary>Details</summary>
Motivation: 现有方法通常分别学习相机和物体运动，可能导致相机与物体相对运动的混淆，需要一种统一的方法来处理这两种运动

Method: 将相机和物体运动转换为像素运动，使用稳定扩散网络学习参考运动图，结合语义对象先验输入到图像到视频网络中生成视频

Result: 大量实验验证该方法在性能上大幅超越现有的最先进方法

Conclusion: 该方法能够准确跟随指定相机轨迹，同时保持物体运动的一致性，解决了相机轨迹引导视频生成中的一致性和泛化性问题

Abstract: Generating videos guided by camera trajectories poses significant challenges
in achieving consistency and generalizability, particularly when both camera
and object motions are present. Existing approaches often attempt to learn
these motions separately, which may lead to confusion regarding the relative
motion between the camera and the objects. To address this challenge, we
propose a novel approach that integrates both camera and object motions by
converting them into the motion of corresponding pixels. Utilizing a stable
diffusion network, we effectively learn reference motion maps in relation to
the specified camera trajectory. These maps, along with an extracted semantic
object prior, are then fed into an image-to-video network to generate the
desired video that can accurately follow the designated camera trajectory while
maintaining consistent object motions. Extensive experiments verify that our
model outperforms SOTA methods by a large margin.

</details>


### [168] [The Unwinnable Arms Race of AI Image Detection](https://arxiv.org/abs/2509.21135)
*Till Aczel,Lorenzo Vettor,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 本文研究图像生成AI中判别器最不利的条件，分析数据维度和复杂性对检测合成图像能力的影响。发现极简和极复杂数据集都降低检测性，而中等复杂度数据集最有利于检测。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成AI的快速发展，合成与真实图像的界限日益模糊，引发了生成器与判别器之间的竞争。本文旨在探索判别器在何种条件下处于最不利地位。

Method: 使用Kolmogorov复杂度作为数据集内在结构的度量，分析数据维度和复杂性对判别器检测能力的影响。

Result: 研究发现：增加维度通常增强判别器检测细微不一致的能力；极简数据集可被生成器几乎完美学习，极复杂数据集的多样性掩盖了缺陷，这两种情况都降低检测性；而中等复杂度数据集为检测创造最有利条件。

Conclusion: 数据复杂性对合成图像检测性产生非线性影响，中等复杂度数据集为判别器提供了最佳检测机会，因为生成器无法完全捕捉分布且其错误仍然可见。

Abstract: The rapid progress of image generative AI has blurred the boundary between
synthetic and real images, fueling an arms race between generators and
discriminators. This paper investigates the conditions under which
discriminators are most disadvantaged in this competition. We analyze two key
factors: data dimensionality and data complexity. While increased
dimensionality often strengthens the discriminators ability to detect subtle
inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov
complexity as a measure of intrinsic dataset structure, we show that both very
simple and highly complex datasets reduce the detectability of synthetic
images; generators can learn simple datasets almost perfectly, whereas extreme
diversity masks imperfections. In contrast, intermediate-complexity datasets
create the most favorable conditions for detection, as generators fail to fully
capture the distribution and their errors remain visible.

</details>


### [169] [WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP](https://arxiv.org/abs/2509.21153)
*Moshe Kimhi,Erez Koifman,Ehud Rivlin,Eli Schwartz,Chaim Baskin*

Main category: cs.CV

TL;DR: WAVECLIP是一个基于小波变换的统一模型，通过多级小波分解实现自适应分辨率推理，在CLIP框架中支持从粗到细的图像处理，并通过缓存机制重用计算，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统CLIP模型在处理不同分辨率图像时需要重新训练或部署多个模型，计算效率低。WAVECLIP旨在通过单一模型支持多分辨率推理，实现动态的计算-精度权衡。

Method: 使用多级小波分解替代标准补丁嵌入，采用键值缓存和因果跨层注意力机制重用计算，结合基于置信度的门控机制实现自适应早期退出。

Result: 在零样本分类任务中表现出色，仅需轻量级蒸馏训练，就能在保持竞争力的准确率同时显著节省计算资源。

Conclusion: WAVECLIP为视觉语言模型提供了一种高效的自适应分辨率推理方案，通过单一部署模型即可实现动态的计算效率优化。

Abstract: We introduce WAVECLIP, a single unified model for adaptive resolution
inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces
standard patch embeddings with a multi-level wavelet decomposition, enabling
the model to process images coarse to fine while naturally supporting multiple
resolutions within the same model. At inference time, the model begins with low
resolution tokens and refines only when needed, using key-value caching and
causal cross-level attention to reuse computation, effectively introducing to
the model only new information when needed. We evaluate WAVECLIP in zero-shot
classification, demonstrating that a simple confidence-based gating mechanism
enables adaptive early exits. This allows users to dynamically choose a
compute-accuracy trade-off using a single deployed model. Our approach requires
only lightweight distillation from a frozen CLIP teacher and achieves
competitive accuracy with significant computational savings.

</details>


### [170] [Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy](https://arxiv.org/abs/2509.21173)
*Aymen Bouguerra,Daniel Montoya,Alexandra Gomez-Villa,Fabio Arnez,Chokri Mraidha*

Main category: cs.CV

TL;DR: 本文对CLIP模型的量化进行了大规模评估，发现量化不仅影响精度，还会改善校准性，甚至在某些情况下同时提升零样本精度、校准性和OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对CLIP模型量化研究主要关注精度损失，而忽视了量化对可靠性指标（如校准性和OOD检测）的影响，这对于高效可靠部署VLMs至关重要。

Method: 采用大规模评估方法，系统分析量化对CLIP模型的影响，包括精度、校准性、OOD检测等多个可靠性指标，并探索量化感知训练方法。

Result: 量化能改善预训练模型的校准性，即使在校准性下降的情况下OOD检测仍可提升；特定QAT方法能同时提升精度、校准性和鲁棒性。

Conclusion: 量化在VLM部署中具有超越传统效率-性能权衡的作用，可为构建高效、可靠、鲁棒的视觉语言模型提供新思路。

Abstract: The powerful zero-shot generalization capabilities of vision-language models
(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as
out-of-distribution (OOD) detection. However, additional aspects crucial for
the computationally efficient and reliable deployment of CLIP are still
overlooked. In particular, the impact of quantization on CLIP's performance
beyond accuracy remains underexplored. This work presents a large-scale
evaluation of quantization on CLIP models, assessing not only in-distribution
accuracy but a comprehensive suite of reliability metrics and revealing
counterintuitive results driven by pre-training source. We demonstrate that
quantization consistently improves calibration for typically underconfident
pre-trained models, while often degrading it for overconfident variants.
Intriguingly, this degradation in calibration does not preclude gains in other
reliability metrics; we find that OOD detection can still improve for these
same poorly calibrated models. Furthermore, we identify specific
quantization-aware training (QAT) methods that yield simultaneous gains in
zero-shot accuracy, calibration, and OOD robustness, challenging the view of a
strict efficiency-performance trade-off. These findings offer critical insights
for navigating the multi-objective problem of deploying efficient, reliable,
and robust VLMs by utilizing quantization beyond its conventional role.

</details>


### [171] [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
*Iñigo Alonso,Imanol Miranda,Eneko Agirre,Mirella Lapata*

Main category: cs.CV

TL;DR: TABLET是一个大规模视觉表格理解数据集，包含400万样本和200万独特表格，其中88%保留原始可视化，旨在解决现有基准缺乏真实世界表格复杂性和视觉多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前表格理解基准主要使用合成渲染，缺乏真实世界表格的复杂性和视觉多样性，且现有VTU数据集提供固定示例，无法访问底层序列化数据进行重新表述。

Method: 构建TABLET数据集，包含400万样本覆盖20个任务，基于200万独特表格，每个示例包含配对的图像-HTML表示、综合元数据和溯源信息。使用Qwen2.5-VL-7B等视觉语言模型在TABLET上进行微调。

Result: 在TABLET上微调视觉语言模型提高了在已见和未见VTU任务上的性能，同时增强了在真实世界表格可视化上的鲁棒性。

Conclusion: 通过保留原始可视化并在统一的大规模集合中维护示例可追溯性，TABLET为未来VTU模型的鲁棒训练和可扩展评估奠定了基础。

Abstract: While table understanding increasingly relies on pixel-only settings where
tables are processed as visual representations, current benchmarks
predominantly use synthetic renderings that lack the complexity and visual
diversity of real-world tables. Additionally, existing visual table
understanding (VTU) datasets offer fixed examples with single visualizations
and pre-defined instructions, providing no access to underlying serialized data
for reformulation. We introduce TABLET, a large-scale VTU dataset with 4
million examples across 20 tasks, grounded in 2 million unique tables where 88%
preserve original visualizations. Each example includes paired image-HTML
representations, comprehensive metadata, and provenance information linking
back to the source datasets. Fine-tuning vision-language models like
Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while
increasing robustness on real-world table visualizations. By preserving
original visualizations and maintaining example traceability in a unified
large-scale collection, TABLET establishes a foundation for robust training and
extensible evaluation of future VTU models.

</details>


### [172] [Learning Conformal Explainers for Image Classifiers](https://arxiv.org/abs/2509.21209)
*Amr Alkhatib,Stephanie Lowry*

Main category: cs.CV

TL;DR: 提出了一种基于共形预测的新方法，用于控制图像预测解释的保真度，通过识别关键特征子集来保持模型预测，而无需真实解释进行校准。


<details>
  <summary>Details</summary>
Motivation: 现有的特征归因方法在鲁棒性和忠实反映黑盒模型推理方面存在局限性，需要一种能够直接控制解释保真度的方法。

Method: 基于共形预测的方法，提出四种一致性函数来量化解释与模型预测的符合程度，识别能够保持模型预测的关键特征子集。

Result: 在六个图像数据集上使用五种解释器进行实证评估，FastSHAP在保真度和信息效率方面表现最佳，基于超像素的一致性度量比像素级更有效。

Conclusion: 该方法提供了一种无需真实解释校准就能控制解释保真度的有效途径，超像素级的一致性度量优于像素级方法。

Abstract: Feature attribution methods are widely used for explaining image-based
predictions, as they provide feature-level insights that can be intuitively
visualized. However, such explanations often vary in their robustness and may
fail to faithfully reflect the reasoning of the underlying black-box model. To
address these limitations, we propose a novel conformal prediction-based
approach that enables users to directly control the fidelity of the generated
explanations. The method identifies a subset of salient features that is
sufficient to preserve the model's prediction, regardless of the information
carried by the excluded features, and without demanding access to ground-truth
explanations for calibration. Four conformity functions are proposed to
quantify the extent to which explanations conform to the model's predictions.
The approach is empirically evaluated using five explainers across six image
datasets. The empirical results demonstrate that FastSHAP consistently
outperforms the competing methods in terms of both fidelity and informational
efficiency, the latter measured by the size of the explanation regions.
Furthermore, the results reveal that conformity measures based on super-pixels
are more effective than their pixel-wise counterparts.

</details>


### [173] [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
*Muxin Pu,Mei Kuan Lim,Chun Yong Chong,Chen Change Loy*

Main category: cs.CV

TL;DR: Sigma是一个基于骨架的手语理解统一框架，通过视觉-文本模态深度融合、层次对齐学习和统一预训练，解决了语义基础弱、局部-全局不平衡和跨模态学习效率低的问题，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前基于骨架的手语理解方法面临三个关键限制：1）语义基础弱，模型难以将骨架运动模式与语言意义关联；2）局部细节与全局上下文不平衡；3）跨模态学习效率低。

Method: 提出Sigma框架：1）符号感知早期融合机制促进视觉-文本模态深度交互；2）层次对齐学习策略在不同层级最大化跨模态特征一致性；3）结合对比学习、文本匹配和语言建模的统一预训练框架。

Result: 在孤立手语识别、连续手语识别和无注释手语翻译等多个基准测试中达到最先进水平，涵盖不同手语和口语。

Conclusion: 证明了语义信息丰富的预训练的重要性，以及骨架数据作为手语理解独立解决方案的有效性。

Abstract: Pre-training has proven effective for learning transferable features in sign
language understanding (SLU) tasks. Recently, skeleton-based methods have
gained increasing attention because they can robustly handle variations in
subjects and backgrounds without being affected by appearance or environmental
factors. Current SLU methods continue to face three key limitations: 1) weak
semantic grounding, as models often capture low-level motion patterns from
skeletal data but struggle to relate them to linguistic meaning; 2) imbalance
between local details and global context, with models either focusing too
narrowly on fine-grained cues or overlooking them for broader context; and 3)
inefficient cross-modal learning, as constructing semantically aligned
representations across modalities remains difficult. To address these, we
propose Sigma, a unified skeleton-based SLU framework featuring: 1) a
sign-aware early fusion mechanism that facilitates deep interaction between
visual and textual modalities, enriching visual features with linguistic
context; 2) a hierarchical alignment learning strategy that jointly maximises
agreements across different levels of paired features from different
modalities, effectively capturing both fine-grained details and high-level
semantic relationships; and 3) a unified pre-training framework that combines
contrastive learning, text matching and language modelling to promote semantic
consistency and generalisation. Sigma achieves new state-of-the-art results on
isolated sign language recognition, continuous sign language recognition, and
gloss-free sign language translation on multiple benchmarks spanning different
sign and spoken languages, demonstrating the impact of semantically informative
pre-training and the effectiveness of skeletal data as a stand-alone solution
for SLU.

</details>


### [174] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 该论文对文本到图像生成中的组合评估指标进行了广泛研究，发现没有单一指标在所有任务中表现一致，指标性能随组合问题类型而变化。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成发展迅速，但评估生成结果是否真实捕捉提示中的对象、属性和关系仍是一个核心挑战。当前评估主要依赖自动化指标，但这些指标往往基于惯例或流行度而非经过人类判断验证。

Method: 进行了广泛的研究，分析广泛使用的组合文本-图像评估指标，超越简单的相关性分析，考察它们在不同组合挑战中的行为，并比较不同指标家族与人类判断的一致性。

Result: 结果显示VQA-based指标虽然流行但并非普遍优越，某些基于嵌入的指标在特定情况下表现更强。仅基于图像的指标对组合评估贡献很小，因为它们是为感知质量而非对齐设计的。

Conclusion: 研究强调了在可信评估和作为生成奖励模型使用时，需要谨慎和透明地选择指标。

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [175] [SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology](https://arxiv.org/abs/2509.21239)
*Shakib Khan,Fariba Dambandkhameneh,Nazim Shaikh,Yao Nie,Raghavan Venugopal,Xiao Li*

Main category: cs.CV

TL;DR: 本文提出SlideMamba框架，结合Mamba架构和图神经网络(GNNs)进行全切片图像(WSI)分析，通过熵基自适应融合策略平衡局部空间关系和长距离上下文依赖，在基因融合和突变状态预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 计算病理学需要从全切片图像中提取有意义的表示来支持临床和生物学任务。现有方法需要更好地捕捉局部空间关系和长距离上下文依赖之间的平衡。

Method: 提出SlideMamba框架，集成Mamba模块（擅长长距离全局依赖）和GNNs（强调细粒度短距离空间交互），采用熵基置信度加权机制进行自适应融合，动态平衡两个分支的贡献。

Result: 在基因融合和突变状态预测任务中，SlideMamba的PRAUC达到0.751±0.05，显著优于MIL(0.491)、Trans-MIL(0.39)、Mamba-only(0.664)、GNN-only(0.748)和GAT-Mamba(0.703)等方法。

Conclusion: 集成架构结合熵基自适应融合策略展现出强大性能，为计算病理学中的空间分辨预测建模任务提供了有前景的应用潜力。

Abstract: Advances in computational pathology increasingly rely on extracting
meaningful representations from Whole Slide Images (WSIs) to support various
clinical and biological tasks. In this study, we propose a generalizable deep
learning framework that integrates the Mamba architecture with Graph Neural
Networks (GNNs) for enhanced WSI analysis. Our method is designed to capture
both local spatial relationships and long-range contextual dependencies,
offering a flexible architecture for digital pathology analysis. Mamba modules
excels in capturing long-range global dependencies, while GNNs emphasize
fine-grained short-range spatial interactions. To effectively combine these
complementary signals, we introduce an adaptive fusion strategy that uses an
entropy-based confidence weighting mechanism. This approach dynamically
balances contributions from both branches by assigning higher weight to the
branch with more confident (lower-entropy) predictions, depending on the
contextual importance of local versus global information for different
downstream tasks. We demonstrate the utility of our approach on a
representative task: predicting gene fusion and mutation status from WSIs. Our
framework, SlideMamba, achieves an area under the precision recall curve
(PRAUC) of 0.751 \pm 0.05, outperforming MIL (0.491 \pm 0.042), Trans-MIL (0.39
\pm 0.017), Mamba-only (0.664 \pm 0.063), GNN-only (0.748 \pm 0.091), and a
prior similar work GAT-Mamba (0.703 \pm 0.075). SlideMamba also achieves
competitive results across ROC AUC (0.738 \pm 0.055), sensitivity (0.662 \pm
0.083), and specificity (0.725 \pm 0.094). These results highlight the strength
of the integrated architecture, enhanced by the proposed entropy-based adaptive
fusion strategy, and suggest promising potential for application of
spatially-resolved predictive modeling tasks in computational pathology.

</details>


### [176] [Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets](https://arxiv.org/abs/2509.21245)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jingwei Huang,Junlin Yu,Kunhong Li,Linus,Penghao Wang,Qingxiang Lin,Sicong Liu,Xianghui Yang,Yixuan Tang,Yunfei Zhao,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: Hunyuan3D-Omni是一个统一框架，用于细粒度可控的3D资产生成，支持多种输入模态（图像、点云、体素、边界框、骨骼姿态）的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型主要依赖图像或文本条件，缺乏细粒度的跨模态控制，限制了可控性和实际应用。

Method: 基于Hunyuan3D 2.1构建统一跨模态架构，采用渐进式难度感知采样策略，对每种控制模态进行选择性训练。

Result: 实验表明额外控制提高了生成准确性，支持几何感知变换，并增强了生产工作流的鲁棒性。

Conclusion: Hunyuan3D-Omni通过多模态统一框架实现了更精确可控的3D资产生成，为实际应用提供了更好的解决方案。

Abstract: Recent advances in 3D-native generative models have accelerated asset
creation for games, film, and design. However, most methods still rely
primarily on image or text conditioning and lack fine-grained, cross-modal
controls, which limits controllability and practical adoption. To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose. Instead of separate heads for each modality, our model
unifies all signals in a single cross-modal architecture. We train with a
progressive, difficulty-aware sampling strategy that selects one control
modality per example and biases sampling toward harder signals (e.g., skeletal
pose) while downweighting easier ones (e.g., point clouds), encouraging robust
multi-modal fusion and graceful handling of missing inputs. Experiments show
that these additional controls improve generation accuracy, enable
geometry-aware transformations, and increase robustness for production
workflows.

</details>


### [177] [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
*Seyed Amir Kasaei,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文提出了文本到图像生成模型中幻觉的定义和分类，将其定义为由模型偏见驱动的偏差，并建立了属性、关系和对象幻觉的三类分类法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型评估主要关注对齐性，检查提示中指定的元素是否出现，但忽视了模型在提示之外生成的内容。需要明确定义文本到图像生成中的幻觉现象。

Method: 将幻觉定义为由模型先验知识或偏见驱动的偏差，而非来自给定输入的内容。提出了包含属性幻觉、关系幻觉和对象幻觉的三类分类法。

Result: 该框架为文本到图像模型评估设定了上限，并揭示了隐藏的偏见，为更丰富的评估提供了基础。

Conclusion: 明确定义文本到图像生成中的幻觉现象对于全面评估模型性能至关重要，提出的分类法能够更好地识别和评估模型偏见驱动的生成偏差。

Abstract: In language and vision-language models, hallucination is broadly understood
as content generated from a model's prior knowledge or biases rather than from
the given input. While this phenomenon has been studied in those domains, it
has not been clearly framed for text-to-image (T2I) generative models. Existing
evaluations mainly focus on alignment, checking whether prompt-specified
elements appear, but overlook what the model generates beyond the prompt. We
argue for defining hallucination in T2I as bias-driven deviations and propose a
taxonomy with three categories: attribute, relation, and object hallucinations.
This framing introduces an upper bound for evaluation and surfaces hidden
biases, providing a foundation for richer assessment of T2I models.

</details>


### [178] [Learning to Look: Cognitive Attention Alignment with Vision-Language Models](https://arxiv.org/abs/2509.21247)
*Ryan L. Yang,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出了一种利用视觉语言模型自动生成语义注意力图的方法，通过辅助损失函数使CNN注意力与语言引导的注意力图对齐，无需人工标注即可提高模型的可靠性和认知合理性。


<details>
  <summary>Details</summary>
Motivation: CNN经常利用表面相关性进行"作弊"预测，现有方法依赖专家标注，可扩展性有限。受认知科学启发，希望开发无需人工标注的注意力引导方法。

Method: 利用视觉语言模型通过自然语言提示自动生成语义注意力图，引入辅助损失函数使CNN注意力与语言引导的注意力图对齐。

Result: 在ColoredMNIST和DecoyMNIST数据集上达到SOTA性能，表现出更好的泛化能力、减少了对捷径的依赖，模型注意力更符合人类直觉。

Conclusion: 该方法提供了一种可扩展的解决方案，能够自动引导模型注意力，提高决策的可靠性和认知合理性，无需依赖人工标注。

Abstract: Convolutional Neural Networks (CNNs) frequently "cheat" by exploiting
superficial correlations, raising concerns about whether they make predictions
for the right reasons. Inspired by cognitive science, which highlights the role
of attention in robust human perception, recent methods have sought to guide
model attention using concept-based supervision and explanation regularization.
However, these techniques depend on labor-intensive, expert-provided
annotations, limiting their scalability. We propose a scalable framework that
leverages vision-language models to automatically generate semantic attention
maps using natural language prompts. By introducing an auxiliary loss that
aligns CNN attention with these language-guided maps, our approach promotes
more reliable and cognitively plausible decision-making without manual
annotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,
show that our method achieves state-of-the-art performance on ColorMNIST and
remains competitive with annotation-heavy baselines on DecoyMNIST,
demonstrating improved generalization, reduced shortcut reliance, and model
attention that better reflects human intuition.

</details>


### [179] [Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations](https://arxiv.org/abs/2509.21249)
*Zhijian Yang,Noel DSouza,Istvan Megyeri,Xiaojian Xu,Amin Honarmandi Shandiz,Farzin Haddadpour,Krisztian Koos,Laszlo Rusko,Emanuele Valeriano,Bharadwaj Swaninathan,Lei Wu,Parminder Bhatia,Taha Kass-Hout,Erhan Bas*

Main category: cs.CV

TL;DR: Decipher-MR是一个针对3D MRI的视觉语言基础模型，通过大规模数据集训练，支持多种临床任务，在疾病分类、人口统计预测等基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MRI在临床诊断中很重要，但其复杂性和异质性给自动化分析带来挑战。现有基础模型在MRI应用有限，主要受限于数据稀缺和解剖区域狭窄。

Method: 整合自监督视觉学习与报告引导的文本监督，采用模块化设计，支持轻量级任务特定解码器连接到冻结的预训练编码器。

Result: 在疾病分类、人口统计预测、解剖定位和跨模态检索等多样化基准测试中，Decipher-MR表现出优于现有基础模型和任务特定方法的性能。

Conclusion: Decipher-MR为基于MRI的AI提供了一个可扩展且多功能的基础，有助于临床和研究领域的高效开发。

Abstract: Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in
clinical diagnosis and research, yet its complexity and heterogeneity pose
challenges for automated analysis, particularly in scalable and generalizable
machine learning applications. While foundation models have revolutionized
natural language and vision tasks, their application to MRI remains limited due
to data scarcity and narrow anatomical focus. In this work, we present
Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a
large-scale dataset comprising 200,000 MRI series from over 22,000 studies
spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR
integrates self-supervised vision learning with report-guided text supervision
to build robust, generalizable representations, enabling effective adaptation
across broad applications. To enable robust and diverse clinical tasks with
minimal computational overhead, Decipher-MR supports a modular design that
enables tuning of lightweight, task-specific decoders attached to a frozen
pretrained encoder. Following this setting, we evaluate Decipher-MR across
diverse benchmarks including disease classification, demographic prediction,
anatomical localization, and cross-modal retrieval, demonstrating consistent
performance gains over existing foundation models and task-specific approaches.
Our results establish Decipher-MR as a scalable and versatile foundation for
MRI-based AI, facilitating efficient development across clinical and research
domains.

</details>


### [180] [Instruction-tuned Self-Questioning Framework for Multimodal Reasoning](https://arxiv.org/abs/2509.21251)
*You-Won Jang,Yu-Jung Heo,Jaeseok Kim,Minsu Lee,Du-Seong Chang,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: SQ-InstructBLIP是一种改进的视觉语言理解方法，通过迭代生成图像感知的子问题和子答案来提高推理性能，解决了现有方法在细粒度视觉内容获取和可复现性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言理解领域在处理需要多步推理的问题时仍存在困难，现有基于LLM的方法无法获取细粒度视觉信息且难以复现。

Method: 提出SQ-InstructBLIP框架，包含共享相同架构的Questioner、Answerer和Reasoner三个模块，迭代生成图像感知的子问题和子答案来辅助主问题推理。

Result: 实验表明，在VQA任务中使用生成的子问题作为额外信息时，SQ-InstructBLIP比先前工作实现了更准确的推理。

Conclusion: 该方法通过生成图像感知的子问题有效提升了多步推理任务的性能，解决了现有方法的局限性。

Abstract: The field of vision-language understanding has been actively researched in
recent years, thanks to the development of Large Language Models~(LLMs).
However, it still needs help with problems requiring multi-step reasoning, even
for very simple questions. Recent studies adopt LLMs to tackle this problem by
iteratively generating sub-questions and answers. However, there are
disadvantages such as 1) the fine-grained visual contents of images are not
available using LLMs that cannot read visual information, 2) internal
mechanisms are inaccessible and difficult to reproduce by using black-box LLMs.
To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,
which improves inference performance by generating image-aware informative
sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists
of a Questioner, Answerer, and Reasoner that share the same architecture.
Questioner and Answerer generate sub-questions and sub-answers to help infer
the main-question, and Reasoner performs reasoning on the main-question
considering the generated sub-question information. Our experiments show that
the proposed method SQ-InstructBLIP, which uses the generated sub-questions as
additional information when solving the VQA task, performs more accurate
reasoning than the previous works.

</details>


### [181] [Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2509.21261)
*Feng-Qi Cui,Jinyang Huang,Anyang Tong,Ziyu Jia,Jie Zhang,Zhi Liu,Dan Guo,Jianwei Lu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了Person Independence Universal Micro-action Recognition Framework，通过分布鲁棒优化学习人员无关的表示，解决微动作识别中因个体差异导致的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有微动作识别方法在真实场景中泛化能力不足，因为个体间差异导致相同动作表现不同，阻碍了鲁棒泛化。

Method: 框架包含特征层和损失层两个即插即用组件：特征层通过时频对齐模块归一化个体运动特征，损失层通过组不变正则化损失模拟未见个体分布。

Result: 在大型MA-52数据集上的实验表明，该框架在准确性和鲁棒性上均优于现有方法，在细粒度条件下实现稳定泛化。

Conclusion: 该框架通过分布鲁棒优化有效解决了微动作识别中的个体差异问题，实现了更好的泛化性能。

Abstract: Micro-action Recognition is vital for psychological assessment and
human-computer interaction. However, existing methods often fail in real-world
scenarios because inter-person variability causes the same action to manifest
differently, hindering robust generalization. To address this, we propose the
Person Independence Universal Micro-action Recognition Framework, which
integrates Distributionally Robust Optimization principles to learn
person-agnostic representations. Our framework contains two plug-and-play
components operating at the feature and loss levels. At the feature level, the
Temporal-Frequency Alignment Module normalizes person-specific motion
characteristics with a dual-branch design: the temporal branch applies
Wasserstein-regularized alignment to stabilize dynamic trajectories, while the
frequency branch introduces variance-guided perturbations to enhance robustness
against person-specific spectral differences. A consistency-driven fusion
mechanism integrates both branches. At the loss level, the Group-Invariant
Regularized Loss partitions samples into pseudo-groups to simulate unseen
person-specific distributions. By up-weighting boundary cases and regularizing
subgroup variance, it forces the model to generalize beyond easy or frequent
samples, thus enhancing robustness to difficult variations. Experiments on the
large-scale MA-52 dataset demonstrate that our framework outperforms existing
methods in both accuracy and robustness, achieving stable generalization under
fine-grained conditions.

</details>


### [182] [Dense Semantic Matching with VGGT Prior](https://arxiv.org/abs/2509.21263)
*Songlin Yang,Tianyi Wei,Yushi Lan,Zeqi Xiao,Anyi Rao,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D几何基础模型VGGT的语义匹配方法，解决了现有方法在几何歧义和最近邻规则方面的局限性，通过特征重用、语义头添加和循环一致性训练等策略，在数据稀缺条件下实现了更好的几何感知和匹配可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有语义匹配方法存在两个主要问题：(i)几何歧义：依赖2D基础模型特征难以区分对称结构，需要额外微调但缺乏泛化能力；(ii)最近邻规则：像素级匹配忽略了跨图像不可见性和流形保持。这些问题需要几何感知的像素描述符和整体密集对应机制。

Method: 提出基于VGGT的方法：(i)保留VGGT早期特征阶段，微调后期阶段，添加语义头实现双向对应；(ii)通过循环一致性训练策略、合成数据增强和渐进式训练方案，在数据稀缺条件下适应语义匹配场景。

Result: 大量实验表明，该方法在几何感知、匹配可靠性和流形保持方面表现优异，超越了之前的基线方法。

Conclusion: 该方法成功解决了语义匹配中的几何歧义和最近邻规则问题，通过3D几何基础模型的适配，实现了更可靠的跨实例语义匹配性能。

Abstract: Semantic matching aims to establish pixel-level correspondences between
instances of the same category and represents a fundamental task in computer
vision. Existing approaches suffer from two limitations: (i) Geometric
Ambiguity: Their reliance on 2D foundation model features (e.g., Stable
Diffusion, DINO) often fails to disambiguate symmetric structures, requiring
extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their
pixel-wise matching ignores cross-image invisibility and neglects manifold
preservation. These challenges call for geometry-aware pixel descriptors and
holistic dense correspondence mechanisms. Inspired by recent advances in 3D
geometric foundation models, we turn to VGGT, which provides geometry-grounded
features and holistic dense matching capabilities well aligned with these
needs. However, directly transferring VGGT is challenging, as it was originally
designed for geometry matching within cross views of a single instance,
misaligned with cross-instance semantic matching, and further hindered by the
scarcity of dense semantic annotations. To address this, we propose an approach
that (i) retains VGGT's intrinsic strengths by reusing early feature stages,
fine-tuning later ones, and adding a semantic head for bidirectional
correspondences; and (ii) adapts VGGT to the semantic matching scenario under
data scarcity through cycle-consistent training strategy, synthetic data
augmentation, and progressive training recipe with aliasing artifact
mitigation. Extensive experiments demonstrate that our approach achieves
superior geometry awareness, matching reliability, and manifold preservation,
outperforming previous baselines.

</details>


### [183] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出MedVSR框架，针对医学视频超分辨率的独特挑战，通过跨状态空间传播和内部状态空间重建模块，解决对齐问题和组织结构失真问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频对准确诊断至关重要，但受硬件限制难以获取。现有VSR模型在处理医学视频时面临相机抖动、噪声、帧间突变等挑战，导致光流误差大、对齐困难，且容易产生伪影和特征失真。

Method: 提出MedVSR框架：1）跨状态空间传播（CSSP）模块，将远距离帧投影为状态空间模型中的控制矩阵，选择性传播一致性特征；2）内部状态空间重建（ISSR）模块，结合长距离空间特征学习和大核短距离信息聚合，增强组织结构并减少伪影。

Result: 在包括内窥镜和白内障手术在内的四个医学数据集上的实验表明，MedVSR在重建性能和效率上显著优于现有VSR模型。

Conclusion: MedVSR通过专门设计的对齐和重建机制，有效解决了医学视频超分辨率的独特挑战，为临床诊断提供了更高质量的医学视频支持。

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


### [184] [MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources](https://arxiv.org/abs/2509.21268)
*Sicong Leng,Jing Wang,Jiaxi Li,Hao Zhang,Zhiqiang Hu,Boqiang Zhang,Yuming Jiang,Hang Zhang,Xin Li,Lidong Bing,Deli Zhao,Wei Lu,Yu Rong,Aixin Sun,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了Variance-Aware Sampling (VAS)方法来解决强化学习微调中的梯度消失问题，并发布了大规模高质量的多模态推理数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理模型面临两大限制：缺乏高质量的长链思维数据，以及强化学习后训练的不稳定性。GRPO框架在奖励方差低时容易出现梯度消失问题。

Method: 提出了Variance-Aware Sampling (VAS)数据选择策略，通过Variance Promotion Score (VPS)结合结果方差和轨迹多样性来提升奖励方差；发布了包含160万条长链思维数据和1.5万条RL问答对的大规模资源。

Result: 在数学推理基准测试中验证了所提出方法和数据的有效性，实验表明VAS能够稳定策略优化并提升收敛性能。

Conclusion: 本文通过理论分析证明了奖励方差对策略梯度的重要性，VAS是实现这一保证的实用机制，为社区提供了标准化的多模态推理基准。

Abstract: Large multimodal reasoning models have achieved rapid progress, but their
advancement is constrained by two major limitations: the absence of open,
large-scale, high-quality long chain-of-thought (CoT) data, and the instability
of reinforcement learning (RL) algorithms in post-training. Group Relative
Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone
to gradient vanishing when reward variance is low, which weakens optimization
signals and impairs convergence. This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization. (2) We
release large-scale, carefully curated resources containing ~1.6M long CoT
cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,
and diversity, along with a fully reproducible end-to-end training codebase.
(3) We open-source a family of multimodal reasoning models in multiple scales,
establishing standardized baselines for the community. Experiments across
mathematical reasoning benchmarks demonstrate the effectiveness of both the
curated data and the proposed VAS. Comprehensive ablation studies and analyses
provide further insight into the contributions of each component. In addition,
we theoretically establish that reward variance lower-bounds the expected
policy gradient magnitude, with VAS serving as a practical mechanism to realize
this guarantee. Our code, data, and checkpoints are available at
https://github.com/LengSicong/MMR1.

</details>


### [185] [A Sentinel-3 foundation model for ocean colour](https://arxiv.org/abs/2509.21273)
*Geoffrey Dawson,Remy Vandaele,Andrew Taylor,David Moffat,Helen Tamura-Wicks,Sarah Jackson,Rosie Lickorish,Paolo Fraccaro,Hywel Williams,Chunbo Luo,Anne Jones*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Prithvi-EO Vision Transformer架构的海洋科学基础模型，通过在Sentinel-3 OLCI数据上进行预训练，能够有效利用有限的标注数据进行海洋监测任务。


<details>
  <summary>Details</summary>
Motivation: 海洋科学中标注数据稀缺且获取成本高，而基础模型在大量无标注数据上预训练后，能够显著改善AI在海洋科学中的应用效果。

Method: 使用Prithvi-EO Vision Transformer架构，在Sentinel-3海洋和陆地颜色仪器(OLCI)数据上进行预训练，然后通过微调应用于两个下游任务：叶绿素浓度量化和海洋初级生产力估算。

Result: 模型在利用少量高质量标注数据和捕捉详细海洋颜色空间模式方面表现出色，同时能够匹配点观测数据，性能优于当前基线模型。

Conclusion: 新一代地理空间AI模型有潜力为海洋生态系统及其在全球气候过程中的作用提供更稳健、数据驱动的洞察。

Abstract: Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive
unlabelled datasets, have the potential to drastically change AI applications
in ocean science, where labelled data are often sparse and expensive to
collect. In this work, we describe a new foundation model using the Prithvi-EO
Vision Transformer architecture which has been pre-trained to reconstruct data
from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the
model by fine-tuning on two downstream marine earth observation tasks. We first
assess model performance compared to current baseline models used to quantify
chlorophyll concentration. We then evaluate the FMs ability to refine remote
sensing-based estimates of ocean primary production. Our results demonstrate
the utility of self-trained FMs for marine monitoring, in particular for making
use of small amounts of high quality labelled data and in capturing detailed
spatial patterns of ocean colour whilst matching point observations. We
conclude that this new generation of geospatial AI models has the potential to
provide more robust, data-driven insights into ocean ecosystems and their role
in global climate processes.

</details>


### [186] [Does FLUX Already Know How to Perform Physically Plausible Image Composition?](https://arxiv.org/abs/2509.21278)
*Shilin Lu,Zhuming Lian,Zihan Zhou,Shaocong Zhang,Chen Zhao,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: SHINE是一个无需训练的图像合成框架，通过流形引导锚点损失、退化抑制引导和自适应背景融合技术，解决现有模型在复杂光照和高分辨率输入下的合成质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成模型在处理复杂光照（如准确阴影、水面反射）和多样化高分辨率输入时表现不佳，而现代扩散模型虽然具备物理和分辨率先验知识，但缺乏有效框架来释放这些潜力。

Method: SHINE引入流形引导锚点损失，利用预训练定制适配器指导潜在表示；提出退化抑制引导消除低质量输出；采用自适应背景融合消除可见接缝。

Result: 在ComplexCompo和DreamEditBench基准测试中，SHINE在标准指标（DINOv2）和人类对齐评分（DreamSim、ImageReward、VisionReward）上均达到最先进性能。

Conclusion: SHINE提供了一个无需训练的高质量图像合成解决方案，有效解决了复杂光照条件下的合成挑战，并引入了新的基准测试ComplexCompo来推动该领域发展。

Abstract: Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs. Modern
text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential
physical and resolution priors, yet lack a framework to unleash them without
resorting to latent inversion, which often locks object poses into contextually
inappropriate orientations, or brittle attention surgery. We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained
customization adapters (e.g., IP-Adapter) to guide latents for faithful subject
representation while preserving background integrity. Degradation-suppression
guidance and adaptive background blending are proposed to further eliminate
low-quality outputs and visible seams. To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces. Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).
Code and benchmark will be publicly available upon publication.

</details>


### [187] [Quantized Visual Geometry Grounded Transformer](https://arxiv.org/abs/2509.21302)
*Weilun Feng,Haotong Qin,Mingqiang Wu,Chuanguang Yang,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: QuantVGGT是首个针对视觉几何基础Transformer（VGGT）的量化框架，通过双平滑细粒度量化和噪声过滤多样性采样技术，解决了十亿级VGGT模型量化中的重尾分布和多视图数据不稳定问题，在4位量化下实现3.7倍内存减少和2.5倍加速，同时保持98%以上的重建精度。


<details>
  <summary>Details</summary>
Motivation: 基于学习的3D重建模型（如VGGT）虽然取得了显著进展，但其巨大的计算和内存成本严重阻碍了实际部署。后训练量化（PTQ）在压缩十亿级VGGT时面临独特挑战：数据独立的特殊令牌导致重尾激活分布，而3D数据的多视图特性使校准样本选择高度不稳定。

Method: 提出QuantVGGT框架，包含两个核心技术：1）双平滑细粒度量化，通过预全局Hadamard旋转和后局部通道平滑来缓解重尾分布和通道间方差；2）噪声过滤多样性采样，通过深层统计过滤异常值，构建帧感知的多样化校准簇以确保稳定的量化范围。

Result: 在多个基准测试和位宽下，QuantVGGT均达到最先进结果，大幅超越之前的通用量化方法。4位QuantVGGT可实现3.7倍内存减少和2.5倍硬件推理加速，同时保持重建精度在完整精度模型的98%以上。

Conclusion: QuantVGGT在资源受限场景下展现出巨大优势和实用性，为大规模3D重建模型的部署提供了有效的量化解决方案。

Abstract: Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs), have made remarkable progress with the use of
large-scale transformers. Their prohibitive computational and memory costs
severely hinder real-world deployment. Post-Training Quantization (PTQ) has
become a common practice for compressing and accelerating models. However, we
empirically observe that PTQ faces unique obstacles when compressing
billion-scale VGGTs: the data-independent special tokens induce heavy-tailed
activation distributions, while the multi-view nature of 3D data makes
calibration sample selection highly unstable. This paper proposes the first
Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly. Second, we design Noise-Filtered Diverse Sampling, which
filters outliers via deep-layer statistics and constructs frame-aware diverse
calibration clusters to ensure stable quantization ranges. Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin. We highlight
that our 4-bit QuantVGGT can deliver a 3.7$\times$ memory reduction and
2.5$\times$ acceleration in real-hardware inference, while maintaining
reconstruction accuracy above 98\% of its full-precision counterpart. This
demonstrates the vast advantages and practicality of QuantVGGT in
resource-constrained scenarios. Our code is released in
https://github.com/wlfeng0509/QuantVGGT.

</details>


### [188] [NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics](https://arxiv.org/abs/2509.21309)
*Yu Yuan,Xijun Wang,Tharindu Wickremasinghe,Zeeshan Nadir,Bole Ma,Stanley H. Chan*

Main category: cs.CV

TL;DR: NewtonGen是一个结合数据驱动合成与可学习物理原理的框架，通过可训练的神经牛顿动力学(NND)来建模和预测牛顿运动，从而在视频生成过程中注入潜在动力学约束，实现物理一致且参数可控的视频合成。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本到视频生成的主要瓶颈是物理一致性和可控性。现有模型往往产生不真实的运动（如物体向上掉落、速度和方向的突变），且缺乏精确的参数控制，无法在不同初始条件下生成物理一致的动态效果。

Method: 提出NewtonGen框架，核心是可训练的神经牛顿动力学(NND)，能够建模和预测各种牛顿运动，将潜在动力学约束注入视频生成过程。通过联合利用数据先验和动力学指导来实现物理一致的视频合成。

Result: 该方法能够生成物理一致性的视频，并提供精确的参数控制能力，解决了现有模型在物理动态建模方面的局限性。

Conclusion: 通过将数据驱动合成与可学习物理原理相结合，NewtonGen框架为文本到视频生成提供了更好的物理一致性和可控性，解决了当前模型仅从外观学习运动分布而缺乏底层动力学理解的根本限制。

Abstract: A primary bottleneck in large-scale text-to-video generation today is
physical consistency and controllability. Despite recent advances,
state-of-the-art models often produce unrealistic motions, such as objects
falling upward, or abrupt changes in velocity and direction. Moreover, these
models lack precise parameter control, struggling to generate physically
consistent dynamics under different initial conditions. We argue that this
fundamental limitation stems from current models learning motion distributions
solely from appearance, while lacking an understanding of the underlying
dynamics. In this work, we propose NewtonGen, a framework that integrates
data-driven synthesis with learnable physical principles. At its core lies
trainable Neural Newtonian Dynamics (NND), which can model and predict a
variety of Newtonian motions, thereby injecting latent dynamical constraints
into the video generation process. By jointly leveraging data priors and
dynamical guidance, NewtonGen enables physically consistent video synthesis
with precise parameter control.

</details>


### [189] [SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](https://arxiv.org/abs/2509.21318)
*Hmrishav Bandyopadhyay,Rahim Entezari,Jim Scott,Reshinth Adithyan,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: SD3.5-Flash是一个高效的多步蒸馏框架，通过优化的分布匹配目标和创新技术，将高质量图像生成带到消费级设备上。


<details>
  <summary>Details</summary>
Motivation: 让高质量图像生成技术能够在移动手机到桌面电脑等各种硬件配置上实现快速生成和内存高效部署，真正实现生成式AI的普及化应用。

Method: 采用重新制定的分布匹配目标进行多步蒸馏，引入"时间步共享"减少梯度噪声和"分割时间步微调"改善提示对齐，结合文本编码器重构和专用量化等管道优化。

Result: 通过大规模用户研究等广泛评估，SD3.5-Flash在性能上持续优于现有的多步方法。

Conclusion: 该框架使先进的生成式AI真正能够在实际部署中普及应用，覆盖从移动设备到桌面计算机的全频谱设备。

Abstract: We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices. Our
approach distills computationally prohibitive rectified flow models through a
reformulated distribution matching objective tailored specifically for few-step
generation. We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.
Combined with comprehensive pipeline optimizations like text encoder
restructuring and specialized quantization, our system enables both rapid
generation and memory-efficient deployment across different hardware
configurations. This democratizes access across the full spectrum of devices,
from mobile phones to desktop computers. Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [190] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，支持快速实验和领域定制设计。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器产生的数据量超过内存和存储容量，限制了可扩展性。现有GPU压缩器虽然提供高吞吐量，但通常采用硬编码的融合内核，阻碍快速实验，且在率失真性能上表现不佳。

Method: 提出FZModules框架，通过简洁可扩展的接口从高性能模块组装误差有界自定义压缩流水线。利用异步任务支持的执行库推断数据依赖关系、管理内存移动，并暴露分支和阶段级并发以实现强大的异步压缩流水线。

Result: 在四个代表性科学数据集上评估三个使用FZModules构建的流水线，结果显示它们可以实现与融合内核GPU压缩器相当的端到端加速，同时达到与更高保真度CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架能够实现快速、领域定制的压缩流水线设计，在保持高吞吐量的同时提供更好的率失真性能。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [191] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成，提出了融合计算架构，并通过Llama大语言模型的案例研究展示了跨平台部署实践。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用通常由容器化组件构成并在云环境中部署，但这些能力在高性能计算中心仍处于新兴阶段，需要探索HPC环境下的GenAI工作负载部署方案。

Method: 设计融合计算架构，集成HPC和Kubernetes平台来运行容器化的GenAI工作负载；使用多种容器运行时在Kubernetes和HPC平台上部署基于vLLM的Llama大语言模型推理服务器。

Result: 成功实现了跨平台的GenAI工作负载部署，验证了融合架构的可行性，并帮助提升了工作负载的可复现性。

Conclusion: 该研究为HPC容器社区提供了实用的部署经验和未来研究方向，展示了HPC与云计算环境集成的潜力和挑战。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [192] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出了可扩展的分布式内存算法用于稀疏矩阵置换、提取和赋值，采用Identify-Exchange-Build策略减少通信开销，通过无同步多线程算法加速本地计算，性能优于现有库CombBLAS和PETSc。


<details>
  <summary>Details</summary>
Motivation: 现有分布式内存中的稀疏矩阵操作（如置换、提取和赋值）通常基于SpGEMM方法，通信开销较大，需要更高效的算法来提升性能。

Method: 采用Identify-Exchange-Build（IEB）策略：每个进程识别要发送的本地非零元素，交换所需数据，然后从接收元素构建本地子矩阵。结合无同步多线程算法加速本地计算。

Result: 在两个大学集群和Perlmutter超级计算机上的实验表明，该算法在负载均衡、矩阵重排序、子图提取和流图应用等场景中性能显著优于CombBLAS和PETSc。

Conclusion: 本研究提供了稀疏矩阵置换、提取和赋值的全面算法研究、软件实现、实验评估和应用案例，为分布式稀疏矩阵操作提供了高效解决方案。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [193] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: 本文研究了RADICAL-Pilot（RP）与Flux和Dragon两种运行时系统集成，用于处理混合HPC和机器学习工作负载的性能表现。相比传统的Slurm srun，RP+Flux+Dragon组合能显著提高任务执行速率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 科学工作流越来越多地涉及HPC和机器学习任务的混合，但传统的启动器如Slurm的srun在并发性和吞吐量方面存在限制，不适合动态和异构的工作负载。

Method: 将RADICAL-Pilot（RP）与Flux和Dragon两个互补的运行时系统集成，实现分层资源管理和高吞吐量函数执行。在Frontier系统上使用合成和生产规模的工作负载进行性能测试。

Result: RP+Flux可持续达到930任务/秒，RP+Flux+Dragon超过1,500任务/秒，利用率超过99.6%。而srun峰值仅为152任务/秒，利用率低于50%。在IMPECCABLE.v2药物发现活动中，RP+Flux相比srun/Slurm将制造时间减少了30-60%，吞吐量提高了四倍以上。

Conclusion: RP中混合运行时集成的方法为混合AI-HPC工作负载提供了可扩展的解决方案。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [194] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: Mojo语言在GPU科学计算工作负载中的性能与可移植性评估，显示其在内存密集型任务上与CUDA/HIP竞争，但在原子操作和快速数学计算密集型任务上存在差距


<details>
  <summary>Details</summary>
Motivation: 探索基于MLIR的新型Mojo语言在科学计算工作负载上的性能和可移植性，旨在弥合Python生态系统中性能和生产力之间的差距

Method: 针对四种科学计算工作负载（七点模板、BabelStream、miniBUDE、Hartree-Fock），在NVIDIA H100和AMD MI300A GPU上比较Mojo与厂商基准的性能

Result: Mojo在内存密集型内核上性能与CUDA和HIP竞争，但在AMD GPU的原子操作以及AMD和NVIDIA GPU上的快速数学计算密集型内核存在性能差距

Conclusion: 尽管学习曲线和编程要求仍较低级，Mojo可以在科学计算与AI融合的碎片化Python生态系统中弥合重要差距

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [195] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 本文提出了一种名为tail batching的新型rollout调度策略，通过将长尾响应整合到少数专门的long rounds中，有效减少GPU空闲时间，显著加速RL训练而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 同步RL后训练存在GPU利用率不足的问题，主要由于rollout步骤中响应长度不平衡导致的气泡现象。现有系统通过放松同步性来缓解此问题，但会损害训练准确性。

Method: 引入tail batching策略，系统性地将导致长尾响应的提示整合到少数rollout步骤中，同时确保大多数步骤仅涉及平衡的短rollout。提出了RollPacker系统，在三个RL阶段进行全面优化。

Result: 实验结果显示，RollPacker在128个H800 GPU上对Qwen2.5系列LLMs实现了2.03x-2.56x的端到端训练时间减少，相比veRL和RLHFuse分别达到2.24x的加速。

Conclusion: tail batching策略和RollPacker系统能够有效解决同步RL训练中的GPU利用率问题，在不牺牲准确性的前提下显著提升训练效率。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [196] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文提出了一种利用输入矩阵稀疏性来改进GPU上Schur补矩阵组装的方法，在FETI方法中实现了5.1倍的GPU部分加速和3.3倍的整体组装加速。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算集群主要依赖GPU，需要加速域分解方法中的Schur补矩阵计算。传统的显式组装方法开销较大，虽然已有直接在GPU上组装的方法，但仍有优化空间。

Method: 通过智能利用输入矩阵的稀疏性来优化GPU上的Schur补矩阵组装过程，特别是在FETI方法框架下实现更高效的GPU计算。

Result: 在GPU部分代码实现了5.1倍的加速，整体组装过程实现了3.3倍的加速，使得从仅10次迭代开始就能获得加速效益。

Conclusion: 利用输入矩阵稀疏性可以显著提升GPU上Schur补矩阵的组装效率，使得GPU加速在较少的迭代次数下就能产生实际效益。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [197] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 本文提出了一种针对RRAM阵列的分布式内存原始-对偶混合梯度方法，通过算法-硬件协同设计解决大规模线性规划问题，相比GPU加速求解器可降低三个数量级的能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本物理限制，无法满足计算工作负载的指数级增长需求。内存计算虽然具有低延迟和低能耗优势，但现有算法无法直接应用于内存计算，特别是在需要频繁矩阵重编程的约束优化问题中。

Method: 开发了分布式内存原始-对偶混合梯度方法，最小化昂贵的写入周期，增强对器件非理想性的鲁棒性，并利用对称块矩阵公式统一分布式交叉开关的操作。使用MELISO+物理仿真框架评估实际器件条件下的性能。

Result: 在大规模线性程序上与GPU加速求解器对比，RRAM求解器达到相当精度，同时能耗和延迟降低三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG线性规划求解器，展示了算法-硬件协同设计通过分布式内存计算解决大规模优化问题的变革性潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [198] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 本文提出了弹性流水线并行（EPP）方法，通过协调令牌级和批次级流水线并行来适应资源和负载异质性，并构建了InfiniPipe系统，实现了1.69倍的加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文训练对LLM上下文扩展至关重要。现有方案如序列并行会产生大量通信开销。流水线并行（PP）能降低开销，但其效果取决于分区粒度。批次级PP在长上下文场景下内存消耗高，而令牌级PP能缓解内存开销但可能导致硬件利用率不足。此外，真实数据集中序列长度分布的偏斜性对PP的负载均衡和高效调度提出了挑战。

Method: 提出了弹性流水线并行（EPP），通过资源感知和负载均衡的序列处理器来分割长序列并打包短序列，以及通过阶段感知的块级自适应检查点机制来联合优化流水线调度和梯度检查点。

Result: 综合实验表明，InfiniPipe相比最先进系统实现了1.69倍的加速。

Conclusion: EPP方法能够有效适应资源和负载异质性，InfiniPipe系统通过创新的调度和优化机制显著提升了长上下文训练的效率。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [199] [An LLM-based Agentic Framework for Accessible Network Control](https://arxiv.org/abs/2509.20600)
*Samuel Lin,Jiawei Zhou,Minlan Yu*

Main category: cs.NI

TL;DR: 该论文设计了一个基于大语言模型的对话式网络管理系统，使非专业用户能够通过自然语言管理网络，降低了网络管理的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理方法需要高度专业的知识，限制了普通用户的使用。作者希望通过LLM技术让更广泛的非专业用户能够轻松管理网络。

Method: 提出了一个基于代理的框架，使用中间表示来统一不同厂商设备的配置，实时从内存中检索网络状态，并提供外部反馈接口。同时开发了可视化界面来支持对话驱动的用户交互。

Result: 初步实验验证了系统组件与LLM集成的有效性，在合成数据和真实用户语句上都取得了良好效果。通过数据收集和可视化工作，为LLM的更好应用奠定了基础。

Conclusion: 该系统为网络控制的民主化铺平了道路，使日常用户能够通过自然语言对话来管理网络，推动了网络管理技术的普及化发展。

Abstract: Traditional approaches to network management have been accessible only to a
handful of highly-trained network operators with significant expert knowledge.
This creates barriers for lay users to easily manage their networks without
resorting to experts. With recent development of powerful large language models
(LLMs) for language comprehension, we design a system to make network
management accessible to a broader audience of non-experts by allowing users to
converse with networks in natural language. To effectively leverage
advancements in LLMs, we propose an agentic framework that uses an intermediate
representation to streamline configuration across diverse vendor equipment,
retrieves the network state from memory in real-time, and provides an interface
for external feedback. We also conduct pilot studies to collect real user data
of natural language utterances for network control, and present a visualization
interface to facilitate dialogue-driven user interaction and enable large-scale
data collection for future development. Preliminary experiments validate the
effectiveness of our proposed system components with LLM integration on both
synthetic and real user utterances. Through our data collection and
visualization efforts, we pave the way for more effective use of LLMs and
democratize network control for everyday users.

</details>


### [200] [An SDR-Based Test Platform for 5G NTN Prototyping and Validation](https://arxiv.org/abs/2509.20692)
*Lu Hou,Kan Zheng,Jie Mei,Cheng Huang*

Main category: cs.NI

TL;DR: 本文提出了一个基于软件定义无线电（SDR）的5G非地面网络（NTN）测试平台，通过现场试验验证了该平台在卫星通信中的可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 5G NTN标准尚处于早期成熟阶段，缺乏商用设备，阻碍了性能验证和系统原型开发。需要一种有效的测试平台来弥补当前实现差距。

Method: 开发了基于通用处理器（GPP）的SDR测试平台，整合Amarisoft的5G NTN协议栈软件，并进行定制化系统集成和适配，支持通过地球静止轨道（GEO）卫星链路的双向通信。

Result: 现场试验评估了下行链路吞吐量和往返时间等性能指标，结果验证了SDR平台在NTN测试中的可行性和有效性。

Conclusion: SDR测试平台在5G NTN广泛商用部署前具有重要潜力，能够有效弥合当前实现差距。

Abstract: The integration of satellite communication into 5G has been formalized in
3GPP Release 17 through the specification of Non-Terrestrial Networks (NTN),
marking a significant step toward achieving global connectivity. However, the
early-stage maturity of 5G NTN standards and the lack of commercial NTN-capable
equipment hinder extensive performance validation and system prototyping. To
address this gap, this paper proposes a software-defined radio (SDR) test
platform with General-Purpose Processor (GPP) processing, leveraging
Amarisoft's 5G NTN protocol stack software while performing custom system
integration and adaptation for real satellite operation. The platform supports
bidirectional communication between an SDR-based NTN gNB and UE emulator
through a Geostationary Earth Orbit (GEO) satellite link, with full compliance
to 3GPP NTN specifications. We provide detailed insights into the system
architecture, SDR hardware-software co-design, and satellite gateway
adaptations. Through field trials, we evaluate the performance metrics
including downlink throughput and round-trip time. Results validate the
feasibility and effectiveness of SDR-based platforms for NTN testing, and
highlight their potential in bridging current implementation gaps before
widespread commercial deployment.

</details>


### [201] [Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions](https://arxiv.org/abs/2509.20830)
*Yanghe Pan,Yuntao Wang,Shaolong Guo,Chengyu Yin,Ruidong Li,Zhou Su,Yuan Wu*

Main category: cs.NI

TL;DR: 本文提出了一种三层可信车载语义通信网络架构，通过语义伪装传输、联邦编码器-解码器训练框架和审计博弈信任管理机制解决V2X通信中的信任挑战。


<details>
  <summary>Details</summary>
Motivation: 车载语义通信网络在信息传输、语义编码和通信实体可靠性方面面临关键信任挑战，阻碍了其在车联网中的部署应用。

Method: 1) 基于防御性对抗噪声的语义伪装传输机制；2) 鲁棒的联邦编码器-解码器训练框架；3) 审计博弈驱动的分布式车辆信任管理机制。

Result: 案例研究验证了所提解决方案的有效性，能够有效防御窃听攻击、编码器-解码器中毒攻击，并管理不可信车辆。

Conclusion: 该架构为可信车载语义通信网络提供了创新解决方案，并指出了推动这一新兴领域发展的未来研究方向。

Abstract: Semantic communication (SemCom) has the potential to significantly reduce
communication delay in vehicle-to-everything (V2X) communications within
vehicular networks (VNs). However, the deployment of vehicular SemCom networks
(VN-SemComNets) faces critical trust challenges in information transmission,
semantic encoding, and communication entity reliability. This paper proposes an
innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we
introduce a semantic camouflage transmission mechanism leveraging defensive
adversarial noise for active eavesdropping defense, a robust federated
encoder-decoder training framework to mitigate encoder-decoder poisoning
attacks, and an audit game-based distributed vehicle trust management mechanism
to deter untrustworthy vehicles. A case study validates the effectiveness of
the proposed solutions. Lastly, essential future research directions are
pointed out to advance this emerging field.

</details>


### [202] [BSB: Towards Demand-Aware Peer Selection With XOR-based Routing](https://arxiv.org/abs/2509.20974)
*Qingyun Ji,Darya Melnyk,Arash Pourdamghani,Stefan Schmid*

Main category: cs.NI

TL;DR: 提出了一种需求感知的对等选择算法BSB，通过考虑应用特定数据流量来优化P2P网络性能，相比现有算法提升达43%。


<details>
  <summary>Details</summary>
Motivation: 现有P2P网络对等选择算法大多忽略应用特定数据流量，导致连接利用不足、路径更长和延迟增加的问题。

Method: 提出BSB算法，采用基于XOR的局部贪婪路由机制，保持与现有协议的兼容性。

Result: 在真实世界和合成通信网络轨迹上的模拟评估显示，BSB相比两种现有算法性能提升最高达43%。

Conclusion: 需求感知的对等选择算法能显著改善P2P网络性能，BSB算法在保持兼容性的同时有效优化了网络效率。

Abstract: Peer-to-peer networks, as a key enabler of modern networked and distributed
systems, rely on peer-selection algorithms to optimize their scalability and
performance. Peer-selection methods have been studied extensively in various
aspects, including routing mechanisms and communication overhead. However, many
state-of-the-art algorithms are oblivious to application-specific data traffic.
This mismatch between design and demand results in underutilized connections,
which inevitably leads to longer paths and increased latency. In this work, we
propose a novel demand-aware peer-selection algorithm, called Binary Search in
Buckets (BSB). Our demand-aware approach adheres to a local and greedy
XOR-based routing mechanism, ensuring compatibility with existing protocols and
mechanisms. We evaluate our solution against two prior algorithms by conducting
simulations on real-world and synthetic communication network traces. The
results of our evaluations show that BSB can offer up to a 43% improvement
compared to two selected algorithms from the literature.

</details>


### [203] [A Novel Integrated Architecture for Intent Based Approach and Zero Touch Networks](https://arxiv.org/abs/2509.21026)
*Neelam Gupta,Dibakar Das,Tamizhelakkiya K,Uma Maheswari Natarajan,Sharvari Ravindran,Komal Sharma,Jyotsna Bapat,Debabrata Das*

Main category: cs.NI

TL;DR: 本文提出了一种将意图驱动网络（IBN）与零接触网络（ZTN）集成的新架构，通过自然语言处理将用户意图转换为网络配置，并使用BiLSTM和Q-learning实现自主网络管理，确保在变化的网络条件下满足服务质量目标。


<details>
  <summary>Details</summary>
Motivation: 6G网络面临管理多样化应用QoS和在不同网络条件下实现SLA的挑战，需要利用ML和AI实现自动化网络管理。ZTN框架和IBN技术相结合可以满足实时需求并自动维护网络目标。

Method: 用户通过自然语言（如英语）提供意图，使用NLP技术（如RAG）将其转换为网络意图语言（Nile），然后通过基于BiLSTM和Q-learning的ZTN闭环框架作为目标进行维护。架构在OpenAirInterface测试床上实现，并通过蒙特卡洛模拟进行评估。

Result: 结果表明ZTN能够自主实现用户意图设定的带宽目标，仿真和测试床结果趋势相似。同时测量了MOS来评估用户体验质量，显示用户对意图的满意度。

Conclusion: 所提出的集成架构能够仅通过指定用户意图就自主确保网络性能目标的实现，为6G网络的自动化管理提供了有效解决方案。

Abstract: The transition to Sixth Generation (6G) networks presents challenges in
managing quality of service (QoS) of diverse applications and achieving Service
Level Agreements (SLAs) under varying network conditions. Hence, network
management must be automated with the help of Machine Learning (ML) and
Artificial Intelligence (AI) to achieve real-time requirements. Zero touch
network (ZTN) is one of the frameworks to automate network management with
mechanisms such as closed loop control to ensure that the goals are met
perpetually. Intent- Based Networking (IBN) specifies the user intents with
diverse network requirements or goals which are then translated into specific
network configurations and actions. This paper presents a novel architecture
for integrating IBN and ZTN to serve the intent goals. Users provides the
intent in the form of natural language, e.g., English, which is then translated
using natural language processing (NLP) techniques (e.g., retrieval augmented
generation (RAG)) into Network Intent LanguagE (Nile). The Nile intent is then
passed on to the BiLSTM and Q-learning based ZTN closed loop framework as a
goal which maintains the intent under varying network conditions. Thus, the
proposed architecture can work autonomously to ensure the network performance
goal is met by just specifying the user intent in English. The integrated
architecture is also implemented on a testbed using OpenAirInterface (OAI).
Additionally, to evaluate the architecture, an optimization problem is
formulated which evaluated with Monte Carlo simulations. Results demonstrate
how ZTN can help achieve the bandwidth goals autonomously set by user intent.
The simulation and the testbed results are compared and they show similar
trend. Mean Opinion Score (MOS) for Quality of Experience (QoE) is also
measured to indicate the user satisfaction of the intent.

</details>


### [204] [RePro: Leveraging Large Language Models for Semi-Automated Reproduction of Networking Research Results](https://arxiv.org/abs/2509.21074)
*Yining Jiang,Wenyun Xu,Qingyu Song,Yuling Lin,Xuanhao Liu,Xiaoqiang Zheng,Qiang Su,Lizhao You,Lu Tang,Wangjian Feng,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: RePro是一个半自动化的网络研究复现框架，利用高级提示工程和结构化思维链技术，将论文描述转化为可执行代码，显著减少复现时间。


<details>
  <summary>Details</summary>
Motivation: 网络研究复现面临开源代码稀缺的挑战，现有LLM方法缺乏对多样化网络领域的通用性。

Method: 结合少样本上下文学习和结构化/语义思维链技术，通过三阶段流程：系统描述提取、结构化代码生成和代码优化。

Result: 在五个先进LLM上的评估显示，RePro相比人工方法显著减少复现时间，同时达到可比较的系统性能。

Conclusion: RePro验证了其在网络系统复现中的有效性和效率。

Abstract: Reproducing networking research is a critical but challenging task due to the
scarcity of open-source code. While Large Language Models (LLMs) can automate
code generation, current approaches lack the generalizability required for the
diverse networking field. To address this, we propose RePro, a semi-automated
reproduction framework that leverages advanced prompt engineering to reproduce
network systems from their research papers. RePro combines few-shot in-context
learning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques
to systematically translate a paper's description into an optimized, executable
implementation. The framework operates through a three-stage pipeline: system
description extraction, structural code generation, and code optimization. Our
evaluation with five state-of-the-art LLMs across diverse network sub-domains
demonstrates that RePro significantly reduces reproduction time compared to
manual efforts while achieving comparable system performance, validating its
effectiveness and efficiency.

</details>


### [205] [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](https://arxiv.org/abs/2509.21201)
*Yang Fu,Peng Qin,Liming Chen,Yifei Wang*

Main category: cs.NI

TL;DR: 本文提出了一种混合可重构智能表面辅助的数字空中计算方案，用于6G网络中的边缘推理，通过优化量化比特分配、传输系数和波束成形来提高感知精度。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持边缘推理，但现有的空中计算技术与数字通信系统不兼容。混合RIS架构能够同时放大和反射信号，有望增强空中计算性能。

Method: 采用向量量化将高维特征映射为离散码字，通过数字调制进行无线传输。优化AirComp收发器和混合RIS反射来控制信号叠加，并开发了联合优化算法。

Result: 实验结果表明，所提出的HRD-AirComp方案在推理精度和不确定性方面均优于基线方法。

Conclusion: HRD-AirComp方案成功实现了任务导向的设计原则，为6G边缘推理提供了有效的特征聚合解决方案。

Abstract: The vision of 6G networks aims to enable edge inference by leveraging
ubiquitously deployed artificial intelligence (AI) models, facilitating
intelligent environmental perception for a wide range of applications. A
critical operation in edge inference is for an edge node (EN) to aggregate
multi-view sensory features extracted by distributed agents, thereby boosting
perception accuracy. Over-the-air computing (AirComp) emerges as a promising
technique for rapid feature aggregation by exploiting the waveform
superposition property of analog-modulated signals, which is, however,
incompatible with existing digital communication systems. Meanwhile, hybrid
reconfigurable intelligent surface (RIS), a novel RIS architecture capable of
simultaneous signal amplification and reflection, exhibits potential for
enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital
AirComp (HRD-AirComp) scheme, which employs vector quantization to map
high-dimensional features into discrete codewords that are digitally modulated
into symbols for wireless transmission. By judiciously adjusting the AirComp
transceivers and hybrid RIS reflection to control signal superposition across
agents, the EN can estimate the aggregated features from the received signals.
To endow HRD-AirComp with a task-oriented design principle, we derive a
surrogate function for inference accuracy that characterizes the impact of
feature quantization and over-the-air aggregation. Based on this surrogate, we
formulate an optimization problem targeting inference accuracy maximization,
and develop an efficient algorithm to jointly optimize the quantization bit
allocation, agent transmission coefficients, EN receiving beamforming, and
hybrid RIS reflection beamforming. Experimental results demonstrate that the
proposed HRD-AirComp outperforms baselines in terms of both inference accuracy
and uncertainty.

</details>


### [206] [Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](https://arxiv.org/abs/2509.21259)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.NI

TL;DR: 提出了一种基于语义通信的边缘-云协同实时交通监控框架，通过YOLOv11检测感兴趣区域，ViT生成紧凑嵌入向量，在云端重建图像后由多模态LLM分析，实现99.9%的数据传输减少和89%的响应准确率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘摄像头与云端LLM集成时因带宽限制导致的实时性能问题，传统方法传输原始图像数据会因带宽不足产生延迟，影响智能交通系统的实时监控效果。

Method: 1) 使用YOLOv11检测图像中的感兴趣区域(RoIs) 2) 裁剪相关图像片段 3) 通过Vision Transformer(ViT)将裁剪图像转换为紧凑嵌入向量 4) 传输嵌入向量到云端 5) 云端图像解码器重建图像 6) 多模态LLM分析重建图像生成交通状况描述

Result: 实现了99.9%的数据传输量减少，重建图像的LLM响应准确率达到89%，接近原始裁剪图像93%的准确率水平，证明了该框架在保持准确性的同时显著提升传输效率。

Conclusion: ViT和LLM辅助的边缘-云语义通信框架为实时交通监控提供了高效实用的解决方案，在保证分析准确性的前提下大幅降低了带宽需求，适合智能城市交通系统的实际部署。

Abstract: Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [207] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 该论文介绍了PWCT2的开发，这是一个自托管的通用视觉编程语言，使用Ring文本编程语言开发，解决了现有视觉编程语言需要文本编程改进的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数视觉编程语言是领域特定的，而通用视觉编程语言如PWCT需要使用文本编程语言进行开发和改进，这限制了其发展。

Method: 首先设计了Ring文本编程语言，然后使用PWCT开发Ring编译器，最后使用Ring开发了自托管的PWCT2视觉编程语言。

Result: PWCT2实现了36倍更快的代码生成速度，视觉源文件存储需求减少20倍，能够将Ring代码转换为视觉代码，实现了自托管。

Conclusion: PWCT2成功实现了自托管的通用视觉编程语言，在Steam平台上获得了积极反馈，为未来研究和发展提供了基础。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [208] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 本文首次将哈希一致性技术集成到JuliaSymbolics中，通过全局弱引用哈希表规范化表达式并消除重复，显著提升符号计算性能。


<details>
  <summary>Details</summary>
Motivation: 符号计算系统存在内存效率低下的问题，由于结构相同的子表达式的冗余存储（表达式膨胀），这降低了经典计算机代数和新兴AI驱动数学推理工具的性能。

Method: 在JuliaSymbolics中集成哈希一致性技术，采用全局弱引用哈希表来规范化表达式并消除重复存储。

Result: 基准测试显示：符号计算加速达3.2倍，内存使用减少达2倍，代码生成加速达5倍，函数编译加速达10倍，大型模型的数值评估加速达100倍。

Conclusion: 哈希一致性对于扩展符号计算至关重要，为未来将哈希一致性与e-graphs集成以实现AI驱动管道中增强的等价感知表达式共享铺平了道路。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [209] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM是专门针对生成OpenACC指令进行微调的两个开源大语言模型，能够为数据并行循环自动生成专家级OpenACC指令，显著提高了指令生成的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 随着GPU的普及和硬件复杂性的增加，虽然基于指令的并行编程标准如OpenACC简化了GPU编程，但仍需要专业知识才能有效使用这些指令。

Method: 通过从公开GitHub C/C++仓库中挖掘4,033个OpenACC pragma-loop对构建监督微调数据集，并基于此训练专门用于生成OpenACC指令的大语言模型。

Result: 在测试集上，基础LLMs无法一致生成有效指令，而微调后的模型能为87%的数据并行循环生成正确指令类型的有效pragma，50%的情况下能生成完全匹配的精确pragma。

Conclusion: ACCeLLiuM模型显著降低了自动化GPU卸载的门槛，为LLM驱动的OpenACC指令生成建立了可复现的基准，即使在不完全匹配的情况下，生成的指令也提供了超出严格字符串匹配的实际价值。

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [210] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 本文通过系统文献综述，对软件安全可视化技术进行了全面分类，提出了基于图、符号、矩阵和隐喻的四类可视化方法，并指出了该领域的关键问题、最新进展和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统日益复杂和威胁环境不断演变，传统的基于文本和数值的安全分析方法变得越来越低效，需要将复杂的安全数据转化为易于理解的视觉格式。

Method: 采用系统文献综述方法，分析了60多篇软件安全可视化领域的关键研究论文，构建了包含四类可视化技术的综合分类法。

Result: 识别出软件安全可视化的两个主要领域：软件开发生命周期可视化和网络安全可视化，并强调了适应不断变化的安全环境的创新可视化技术的必要性。

Conclusion: 研究结果为增强威胁检测、改进安全响应策略以及指导未来研究提供了实践意义，强调了创新可视化技术对应对不断演变的安全环境的重要性。

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [211] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct是一种新颖方法，使ReAct智能体能够在超出大型语言模型上下文内存限制的庞大MCP工具集下高效运行，通过搜索加载机制减少50%工具加载量同时保持任务完成准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在包含数百或数千个可用工具的环境中，同时加载所有工具计算不可行的问题，实现智能工具选择以应对大规模工具集的挑战。

Method: 提出并评估了五种不同的架构，逐步优化工具选择过程，最终开发出搜索加载机制，以最小计算开销实现智能工具选择。

Result: 实验结果表明，该方法将工具加载量减少高达50%，同时保持任务完成准确性。

Conclusion: 该方法推动了真正通用AI智能体的发展，使其能够动态适应多样化的任务环境。

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [212] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: 本文提出了一种基于AI-Python的聊天机器人，通过结合静态代码分析、动态执行追踪和大语言模型，帮助学生解决编程问题，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统IDE和静态分析工具缺乏交互式指导，而AI代码助手如GitHub Copilot主要关注代码完成而非教学。本研究旨在填补这一空白，开发能够提供实用建议的教育型AI助手。

Method: 采用混合架构，结合CodeLlama进行代码嵌入、GPT-4处理自然语言交互，以及基于Docker的沙箱安全执行。通过静态代码分析和动态执行追踪提供针对性指导。

Result: 在1,500份学生提交的评估中，系统实现了85%的错误解决成功率，优于pylint（62%）和GPT-4（73%）。用户调试时间减少59.3%，编程能力测试显示34%的提升，尤其在递归和异常处理方面。

Conclusion: 该研究展示了AI工具如何平衡技术创新与教学关怀，优先考虑教育公平和长期技能保留，而非仅仅代码完成。聊天机器人证明了AI可以增强人类教学，促进编程教育中的概念理解。

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [213] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: 提出基于知识图谱的公平性需求规范和验证框架，以解决软件系统中因缺乏明确的公平性需求规范而导致的歧视问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注算法设计缺陷或数据偏见，但忽略了歧视往往源于缺乏明确的公平性需求规范和验证机制。专家关于公平性的知识通常是隐性的，难以形式化表达和验证。

Method: 借鉴安全工程领域的经验，使用知识图谱来形式化公平性知识，辅助需求规范和验证。提出开发基于知识图谱的公平性框架，包括挑战分析、研究问题和实施路线图。

Result: 提出了一个研究框架和路线图，旨在通过知识图谱技术改进公平性需求的规范和验证过程。

Conclusion: 知识图谱为形式化公平性知识和支持需求工程提供了有前景的方向，有助于减少软件系统中的歧视风险。

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [214] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: Online-Optimized RAG是一个部署时框架，通过在线梯度更新持续优化检索嵌入，解决嵌入对齐问题，提高工具选择和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中由于嵌入模型不完善或描述噪声导致的嵌入对齐问题，这种对齐问题可能导致错误检索和任务失败。

Method: 使用轻量级在线梯度更新，从实时交互中持续适应检索嵌入，仅需最小反馈（如任务成功），无需改变底层LLM，支持单跳和多跳工具使用、动态工具库和K检索重排序。

Result: 在各种工具使用和文档检索场景中，该方法持续提高工具选择准确性和最终任务成功率。

Conclusion: Online-Optimized RAG为构建稳健、自我改进的RAG系统提供了一条简单实用的路径。

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [215] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula是一种领域特定编程语言，用于建模具有可执行属性的法律合同，特别是涉及资产转移和义务的合同。本文提出了一种通过将Stipula合同转换为带有JML注解的Java代码来形式化验证其正确性的方法。


<details>
  <summary>Details</summary>
Motivation: 需要确保法律合同在资产转移和义务方面的正确性和可执行性，通过形式化验证来提高合同的可靠性。

Method: 将Stipula合同翻译成带有Java建模语言（JML）规范的Java代码，并使用KeY演绎验证工具作为验证后端，对具有不相交循环的Stipula合同子集进行部分和完全正确性验证。

Result: 翻译和验证过程完全自动化，成功证明了通用演绎验证工具在翻译方法中的有效性。

Conclusion: 该方法展示了使用通用目的演绎验证工具成功验证领域特定语言合同的可行性，为法律合同的形式化验证提供了有效途径。

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [216] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI是一个基于工具的方法，用于大规模规范和检测AI特定代码异味，结合了高级声明式领域特定语言和可扩展的静态分析工具。


<details>
  <summary>Details</summary>
Motivation: AI系统的兴起带来了新的软件问题，现有检测工具常常无法发现AI特定的代码异味，这些异味可能导致不可重现性、静默故障或模型泛化能力差等问题。

Method: 采用高级声明式领域特定语言进行规则规范，结合可扩展的静态分析工具来解析和检测AI系统中的代码异味规则。

Result: 在826个AI系统（2000万行代码）上评估，实现了88.66%的精确度和88.89%的召回率，优于现有检测工具，SUS评分为81.7/100。

Conclusion: SpecDetect4AI能够通过专用规则有效支持AI特定代码异味的规范和检测，并能高效分析大型AI系统，展示了良好的效率和可扩展性。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [217] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文首次对LLM集成中的技术债务进行了大规模实证研究，发现OpenAI集成占技术债务的54.49%，提示设计是主要债务来源（6.61%），其中基于指令的提示（38.60%）和少样本提示（18.13%）最易产生债务。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过API集成到软件中，这些集成带来了新的技术债务形式，需要系统性地研究其来源、普遍性和缓解策略。

Method: 通过分析93,142个Python文件中主要LLM API的使用情况，识别和分类LLM特定的技术债务实例。

Result: 研究发现54.49%的技术债务来自OpenAI集成，12.35%来自LangChain使用；提示设计是主要债务来源，其中指令提示和少样本提示最易产生债务。

Conclusion: 研究揭示了LLM集成中的技术债务模式，发布了全面的技术债务数据集支持可复现性，并为管理LLM系统的技术债务提供了实用指导。

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [218] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc是一个基于检索增强生成(RAG)的故障定位框架，通过结合LLM功能提取、语义密集检索和LLM重排序三个组件，显著提升了方法级故障定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在复杂系统故障定位中面临项目特定知识缺乏和大型项目导航困难的问题，需要一种能够有效整合项目上下文信息的方法。

Method: FaR-Loc包含三个核心模块：1) LLM功能提取模块生成失败行为的自然语言描述；2) 语义密集检索模块在共享语义空间中检索功能相似的方法；3) LLM重排序模块基于上下文相关性对检索结果重新排序。

Result: 在Defects4J基准测试中，FaR-Loc在Top-1准确率上比SoapFL和AutoFL分别提升14.6%和9.1%，在Top-5准确率上分别提升19.2%和22.1%，超越了所有基于学习和频谱的基线方法。

Conclusion: FaR-Loc通过RAG技术有效解决了LLM在复杂系统故障定位中的局限性，且无需重新训练，具有显著的性能提升和实用价值。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [219] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: 提出了一种结合多标签支持向量机和滑动窗口投票策略的编程语言主题分类工作流，用于在源代码中精确定位核心语言概念。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模和复杂性的增长，理解源代码中编程语言主题的分布对于指导技术决策、改进入职流程以及为工具和教育提供信息变得越来越重要。

Method: 设计了一种新颖的编程语言主题分类工作流，结合多标签支持向量机与滑动窗口和投票策略，在IBM Project CodeNet数据集上进行训练。

Result: 模型在主题分类上平均F1得分为0.90，在代码-主题高亮上为0.75。

Conclusion: 研究结果为代码分析和数据驱动软件工程领域的研究人员和实践者提供了实证见解和可重用的分析管道。

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [220] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: 本研究通过多模态方法测量混合会议中的参与度模式，发现现场和远程参与者的参与度水平相当，但远程参与者在长时间会议中参与度较低。主动角色、会议规模和会议时间等因素与参与度相关。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行后混合工作的广泛采用改变了软件开发实践，带来了沟通和协作的新挑战。虽然远程参与会议已成为常态，但可能导致远程团队成员孤立、疏离和参与度下降。

Method: 研究三家软件公司的专业人士数周时间，采用多模态方法测量参与度，包括自填问卷和生物识别设备的生理测量。

Result: 回归分析显示现场和远程参与者的参与度水平相当，但远程参与者在长时间会议中参与度较低。主动角色与更高参与度正相关，而大型会议和下午会议与较低参与度相关。

Conclusion: 研究结果为混合会议中的参与度和脱离因素提供了见解，以及潜在的会议改进建议，这些见解不仅适用于软件团队，也适用于面临类似混合协作挑战的知识密集型组织。

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [221] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 本文研究了代码生成中合成验证的瓶颈问题，发现当前验证方法过于严格，限制了训练数据的多样性和质量。通过分析测试复杂度、验证阈值和解决方案多样性，提出了校准验证策略来突破验证天花板。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型越来越多地依赖合成数据，验证过程的质量和多样性受到验证器能力的限制，形成了验证天花板瓶颈。本文旨在系统研究验证设计如何影响模型性能。

Method: 通过三个维度进行研究：(i) 分析测试复杂度和数量对验证的影响；(ii) 探索放宽通过阈值和LLM软验证策略；(iii) 比较正确与错误解决方案的保留效果，并进行人工评估。

Result: 研究发现：更丰富的测试套件平均提升pass@1指标3个百分点；放宽验证阈值可回收有价值训练数据，带来2-4点性能提升；保留每个问题的多样化正确解决方案能带来一致的泛化收益。

Conclusion: 当前验证实践过于严格，过滤掉了有价值的多样性，但不能完全放弃验证。通过结合校准验证与多样化的问题-解决方案对，可以突破验证天花板，开发更强的代码生成模型。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [222] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: PseudoBridge是一个新颖的代码检索框架，通过引入伪代码作为中间模态来更好地对齐自然语言查询和编程语言逻辑，解决了现有方法在语义对齐和代码风格鲁棒性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语言模型的代码搜索方法面临两个关键挑战：人类意图与机器执行逻辑之间的语义鸿沟，以及对不同代码风格的鲁棒性有限。

Method: PseudoBridge采用两阶段方法：1）使用大语言模型合成伪代码，实现自然语言查询与伪代码的显式对齐；2）引入逻辑不变代码风格增强策略，生成风格多样但逻辑等价的代码实现，并将不同风格的代码片段与伪代码对齐。

Result: 在10个预训练语言模型和6种主流编程语言上的实验表明，PseudoBridge显著优于基线方法，在检索准确性和泛化性方面取得显著提升，特别是在零样本领域迁移场景下。

Conclusion: 通过伪代码实现显式逻辑对齐是有效的，PseudoBridge展示了作为稳健、可泛化代码检索解决方案的潜力。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [223] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: CodeHinter是一个结合传统调试工具和LLM技术的调试助手，旨在帮助编程新手修复语义错误并促进他们在调试过程中的主动参与。


<details>
  <summary>Details</summary>
Motivation: 现有的AI调试工具容易导致学生对AI过度依赖，未能真正促进学生在调试过程中的主动参与。

Method: 设计CodeHinter调试助手，结合传统调试工具和基于LLM的技术，进行第二次设计迭代并在本科生群体中测试。

Result: 学生认为该工具在解决语义错误方面非常有效，使用难度显著低于第一版，错误定位是最有价值的功能。

Conclusion: AI辅助调试工具应根据用户画像进行个性化定制，以优化与学生的互动效果。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [224] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: 该研究通过分析Stack Overflow上的量子计算相关帖子，识别量子软件工程中的常见挑战，并使用Transformer模型对帖子进行分类，BERT模型达到95%的准确率，优于传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 量子开发者面临优化量子计算和量子软件工程概念的挑战，但现有标签主要关注技术方面而非开发者问题。通过分类量子概念相关问题可以帮助识别频繁的QSE挑战。

Method: 从Q&A平台提取2829个量子相关标签的问题，通过内容分析和扎根理论识别常见挑战（工具、理论、学习、概念、错误、API使用），使用ChatGPT验证标注，并比较BERT、DistilBERT、RoBERTa等Transformer模型与FNN、CNN、LSTM等传统模型的分类性能。

Result: Transformer模型平均准确率达到95%，而传统深度学习模型准确率为89%（FNN）、86%（CNN）、84%（LSTM）。Transformer方法比D&ML方法准确率提高6%，且无需数据增强。使用SHAP进行模型可解释性分析。

Conclusion: 该分类方法能帮助量子供应商和论坛更好地组织讨论以提高可访问性和可读性，但需要与真实开发者和供应商进行实证评估研究。

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [225] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR是一种基于链式思维（COT）的微调方法，通过结合最大熵建模和预定义推理路径，使LLM能够在代码审查中同时分析多个维度，显著提升问题检测和描述的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于微调的代码审查方法受限于训练数据的有限性或模糊性，无法像人类审查者那样同时分析代码审查的多个维度，限制了LLM在代码审查中的潜力。

Method: 提出MelcotCR方法，结合最大熵建模原则和预定义推理路径，解决长COT提示中的上下文丢失和推理逻辑丢失问题，使LLM能更有效地利用上下文知识并加强推理过程的逻辑严密性。

Result: 在MelcotCR数据集和公共CodeReviewer数据集上的实验表明，使用MelcotCR微调的14B参数基础模型（如Qwen2.5）在代码问题检测和描述准确性上超越了现有最先进方法，性能与671B的DeepSeek-R1模型相当。

Conclusion: MelcotCR通过链式思维微调和结构化推理路径，显著提升了LLM在代码审查任务中的性能，证明了即使较小参数的模型也能通过精心设计的微调方法达到与超大模型相当的效果。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [226] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 提出了一种结合BERTopic、种子词和大型语言模型自动验证的方法，用于解决数字平台公民参与内容的大规模分类挑战


<details>
  <summary>Details</summary>
Motivation: 政府数字平台上的公民参与内容数量庞大，手动分类不可行，需要专家参与且需与官方分类法对齐，现有方法难以满足这些需求

Method: 使用BERTopic结合种子词进行主题建模，并通过大型语言模型进行自动验证，减少人工干预

Result: 初步结果显示生成的主题具有连贯性且与机构分类对齐，仅需极少量人工努力

Conclusion: 该方法能将大量公民输入转化为可用于公共政策的可操作数据

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [227] [Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities](https://arxiv.org/abs/2509.20634)
*Shanjukta Nath,Jiwon Hong,Jae Ho Chang,Keith Warren,Subhadeep Paul*

Main category: econ.EM

TL;DR: 使用预训练的基于Transformer的大型语言模型（LLM）对低安全级别惩教设施中居民之间的书面肯定和纠正交流进行嵌入分析，发现这些嵌入向量对再犯率具有高度预测性。预测准确率比仅使用入狱前协变量高30%。通过零样本分类将高维文本嵌入向量转换为低维用户定义类别，以保持预测能力的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究惩教设施中居民之间的语言交流如何影响再犯行为，探索语言使用中的同伴效应，为理解惩教设施内的社会动态提供新视角。

Method: 使用预训练LLM对80,000-120,000条书面交流进行嵌入分析；采用零样本分类将高维嵌入向量降维；开发新的多元同伴效应模型，考虑网络内生性、稀疏网络、多元潜变量和相关性多元结果。

Result: AI嵌入向量对再犯率的预测准确率比传统方法高30%；发现了语言使用中显著的同伴效应，特别是在互动和反馈方面；新开发的统计方法能够有效处理复杂网络数据。

Conclusion: LLM生成的文本嵌入是预测再犯率的有效工具，惩教设施中的语言交流存在显著的同伴效应，新开发的方法为研究社会网络中的语言动态提供了有力工具。

Abstract: We find AI embeddings obtained using a pre-trained transformer-based Large
Language Model (LLM) of 80,000-120,000 written affirmations and correction
exchanges among residents in low-security correctional facilities to be highly
predictive of recidivism. The prediction accuracy is 30\% higher with embedding
vectors than with only pre-entry covariates. However, since the text embedding
vectors are high-dimensional, we perform Zero-Shot classification of these
texts to a low-dimensional vector of user-defined classes to aid interpretation
while retaining the predictive power. To shed light on the social dynamics
inside the correctional facilities, we estimate peer effects in these
LLM-generated numerical representations of language with a multivariate peer
effect model, adjusting for network endogeneity. We develop new methodology and
theory for peer effect estimation that accommodate sparse networks,
multivariate latent variables, and correlated multivariate outcomes. With these
new methods, we find significant peer effects in language usage for interaction
and feedback.

</details>


### [228] [Overidentification testing with weak instruments and heteroskedasticity](https://arxiv.org/abs/2509.21096)
*Stuart Lane,Frank Windmeijer*

Main category: econ.EM

TL;DR: 该论文讨论了Kleibergen-Paap（KP）秩检验作为异方差稳健的过度识别检验，并与传统的J检验进行比较。研究发现KP检验通常优于J检验，后者容易出现严重的尺寸扭曲。建议在弱工具变量情况下使用KP检验而非J检验。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估工具变量估计中的外生性，特别是在弱工具变量和异方差情况下。传统J检验在弱工具变量条件下表现不佳，需要寻找更稳健的检验方法。

Method: 通过推导J检验和KP检验在异方差弱工具变量条件下的极限分布，将其分别视为通过2SLS和LIML估计的稳健得分检验的特例。使用蒙特卡洛模拟进行比较分析，并应用于生命周期消费模型中跨期替代弹性（EIS）的估计问题。

Result: 蒙特卡洛模拟显示KP检验通常比J检验表现更好，J检验容易出现严重的尺寸扭曲。在EIS估计的实际应用中，J检验经常拒绝有效工具变量的原假设，而KP检验则不拒绝，表明J检验可能过度拒绝。

Conclusion: 建议在弱工具变量情况下使用基于LIML的KP检验而非基于2SLS的J检验。研究还认为文献中EIS估计值的广泛范围不太可能是由工具变量无效性/错误设定引起的。

Abstract: Exogeneity is key for IV estimators, which can assessed via
overidentification (OID) tests. We discuss the Kleibergen-Paap (KP) rank test
as a heteroskedasticity-robust OID test and compare to the typical J-test. We
derive the heteroskedastic weak-instrument limiting distributions for J and KP
as special cases of the robust score test estimated via 2SLS and LIML
respectively. Monte Carlo simulations show that KP usually performs better than
J, which is prone to severe size distortions. Test size depends on model
parameters not consistently estimable with weak instruments, so a conservative
approach is recommended. This generalises recommendations to use LIML-based OID
tests under homoskedasticity. We then revisit the classic problem of estimating
the elasticity of intertemporal substitution (EIS) in lifecycle consumption
models. Lagged macroeconomic indicators should provide naturally valid but
frequently weak instruments. The literature provides a wide range of estimates
for this parameter, and J frequently rejects the null of valid instruments. J
often rejects the null whereas KP does not; we suggest that J over-rejects,
sometimes severely. We argue that KP-test should be used over the J-test. We
also argue that instrument invalidity/misspecification is unlikely the cause of
the range of EIS estimates in the literature.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [229] [Integrated analysis of informality, minimum wage, and monopsony power: A synthesis of meta-analyses with unified theoretical underpinnings](https://arxiv.org/abs/2509.20465)
*Ricardo Alonzo Fernandez Salguero*

Main category: econ.TH

TL;DR: 本文通过整合和元分析现有文献，重新评估了劳动力市场的三个关键领域：非正规性、最低工资影响和买方垄断力量，发现传统观点往往高估了这些因素的影响程度。


<details>
  <summary>Details</summary>
Motivation: 传统经济学文献对劳动力市场现象（如非正规性、最低工资影响等）的影响程度可能存在高估，需要通过系统性的元分析来重新评估这些效应的真实大小。

Method: 采用元分析和元回归方法，整合多个现有系统综述和元分析的研究结果，并引入一个包含企业异质性、买方垄断力量和内生正规化决策的综合理论模型。

Result: 研究发现：1）仅降低正规化成本的政策效果有限，而加强执法更有效；2）最低工资对就业的影响很小；3）所有研究领域都存在显著的发表偏倚，校正后的估计值通常接近零。

Conclusion: 元回归是识别异质性和真实效应的关键工具，需要重新校准理论和政策建议，采用更细致和综合的方法来理解劳动力市场现象。

Abstract: This document offers a synthesis of recent economic literature on three
interconnected areas of labor markets: informality, the effects of the minimum
wage, and monopsony power. Through the consolidation and meta-analysis of
findings from multiple existing systematic reviews and meta-analyses, their
causes, consequences, and associated public policies are examined. It is
concluded that conventional views on these topics often overestimate the
magnitude of effects. Policies to reduce informality based solely on lowering
formalization costs are largely ineffective, while increased enforcement at the
extensive margin shows more promising results. The effects of the minimum wage
on employment, measured through the own-wage elasticity (OWE), are consistently
modest, suggesting that job losses are limited compared to wage gains. To
reconcile and microfound these findings, an integrated theoretical model of
firm optimization is introduced, simultaneously incorporating firm
heterogeneity, monopsony power, and endogenous formality decisions, rigorously
demonstrating how the interaction of these forces can explain the observed
empirical regularities. A cross-cutting and significant finding is the
omnipresence of publication bias across all these study areas, which tends to
inflate the magnitude of the effects reported in the published literature.
Corrected estimates of the effects generally approach zero. Meta-regression is
established as an indispensable tool for identifying heterogeneity and the true
underlying effects in the empirical evidence, compelling a recalibration of
both theory and economic policy recommendations towards a more nuanced and
integrated approach.

</details>


### [230] [Börgers's Open Question Resolved](https://arxiv.org/abs/2509.20790)
*Siyang Xiong*

Main category: econ.TH

TL;DR: 本文研究了随机有限行动机制下的占优策略和迭代占优策略实施问题，解决了Börgers（1995）中的开放性问题，并在不要求可分离性、拟线性或无限行动集的条件下提供了首个积极结果。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为纳什实施需要可分离性、拟线性或无限行动集等强假设，本文旨在突破这些限制，探索在更一般条件下的实施可能性。

Method: 采用随机有限行动机制，分析占优策略和迭代占优策略的实施条件。

Result: 建立了可能性和不可能性结果，解决了Börgers（1995）中的开放性问题，并提供了首个不依赖传统强假设的积极实施结果。

Conclusion: 本文证明纳什实施可以在不要求可分离性、拟线性或无限行动集的条件下实现，拓展了机制设计理论的应用范围。

Abstract: Focusing on stochastic finite-action mechanisms, we study implementation in
undominated strategies and iteratively undominated strategies. We establish
both possibility and impossibility results that resolve the open question in
B\"orgers (1995). Contrary to the conventional understanding that positive
results on Nash implementation need separability, quasilinearity, or infinite
action sets, we provide -- to our knowledge -- the first positive result beyond
those demanding assumptions.

</details>
