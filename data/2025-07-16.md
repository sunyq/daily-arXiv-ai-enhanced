<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 20]
- [econ.GN](#econ.GN) [Total: 4]
- [econ.TH](#econ.TH) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP是一种新型框架，通过持久、安全且可语义搜索的内存共享解决AI代理的短暂内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构存在短暂内存限制，阻碍跨会话和代理边界的协作与知识共享。

Method: SAMEP采用分布式内存存储库，结合向量语义搜索、加密访问控制（AES-256-GCM）和标准化API。

Result: 实验显示，SAMEP减少了73%冗余计算，提升89%上下文相关性，并完全符合监管要求。

Conclusion: SAMEP为持久协作的AI代理生态系统提供了新范式，同时保障安全与隐私。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [2] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于VQ-VAE的"AI母语"（AIM）框架，证明无需外部归纳偏置，代理的内生符号系统可实现自然语义压缩和纳什均衡驱动的语义收敛，从而实现高效符号通信。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化多智能体强化学习（MARL）中"联合探索困境"导致的"通信真空均衡"问题，质疑传统方法中人工归纳偏置的必要性。

Method: 采用基于VQ-VAE的AIM框架，研究代理内生符号系统的自发语义压缩和语义收敛。

Result: AIM框架在无需外部偏置的情况下实现了高效通信，符号使用呈现幂律分布，并提出了三大理论见解。

Conclusion: AIM框架为符号主义与连接主义的结合提供了新途径，未来将探索HQ-VAE和RL低层预训练以增强表达能力。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [3] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 提出了一种新型模块化AI视觉分类框架，结合多模态代理与非视觉推理协调器，通过信任校准提升零样本场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态AI在零样本场景下的信任问题，特别是在无需微调的情况下如何确保代理的可靠性。

Method: 采用模块化框架，包括多模态代理、非视觉推理协调器和RAG模块，通过置信度校准和图像检索优化性能。

Result: 在零样本设置下，信任感知协调和RAG使准确率提升77.94%，总体达到85.63%。GPT-4o校准效果更好，而Qwen-2.5-VL表现出过度自信。

Conclusion: 该系统分离感知与元推理，可扩展至诊断、生物学等信任关键领域，并开源了所有模型和代码以支持复现和社区基准测试。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [4] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: 论文指出大语言模型（LLMs）在符号推理、算术准确性和逻辑一致性任务中表现不佳，揭示了其理解与能力之间的鸿沟，并提出了一种称为“计算分裂脑综合征”的现象。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在复杂任务中失败的根本原因，揭示其表面流畅性与实际能力之间的不一致性。

Method: 通过控制实验和架构分析，研究LLMs在任务中的表现，并分析其计算执行过程。

Result: 发现LLMs能够表达正确原则但无法可靠应用，原因是计算执行中的功能分离，而非知识获取问题。

Conclusion: LLMs缺乏结构化推理能力，未来模型需引入元认知控制和原则提升机制。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [5] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data结合知识图谱、LLM、ReAct代理和工具使用技术，提升气象领域数据查询和分析能力，性能优于RAG2data和chat2data。


<details>
  <summary>Details</summary>
Motivation: 探索LLM通过API调用在知识密集型领域（如气象学）中的工具使用能力，解决传统LLM系统在复杂查询中的局限性。

Method: 引入KG2data系统，整合知识图谱、LLM、ReAct代理和工具技术，通过虚拟API评估API调用准确性。

Result: KG2data在名称识别失败（1.43%）、幻觉失败（0%）和调用正确性（88.57%）上表现优于对比系统。

Conclusion: KG2data为高知识需求领域提供了智能问答和数据分析的新解决方案，减少微调成本并适应知识演进。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [6] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文综述了Web of Agents (WoA)的演变，提出了一种四轴分类法，揭示了从语义Web到现代Agentic AI的范式转变，并呼吁解决社会技术挑战。


<details>
  <summary>Details</summary>
Motivation: 研究WoA的演变，揭示现代协议与早期标准的联系，并提供一个统一的分析框架。

Method: 引入四轴分类法（语义基础、通信范式、智能中心、发现机制）来比较不同世代的代理架构。

Result: 发现智能中心从外部数据或平台转移到代理核心模型的范式转变，支持了现代Agentic AI的可扩展性和适应性。

Conclusion: 新协议虽重要，但不足以构建强大、开放、可信的生态系统，未来研究需解决去中心化身份、经济模型、安全与治理等挑战。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [7] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的音乐生成方法，通过变异现有曲调生成新音乐。利用Sequitur算法解析曲调为语法结构，随机应用19种变异类型，生成新曲调，并分析其变化。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过规则和变异方法从现有曲调生成新音乐，并分析变异对曲调的影响。

Method: 使用Sequitur算法解析曲调为语法结构（PA），随机应用19种变异类型（如添加、删除、交换等），生成新曲调。

Result: 通过编辑距离、结构复杂度和曲调长度分析变异效果，并评估每种变异类型的影响。

Conclusion: 该方法能有效生成与原始曲调相关的新音乐，但仅关注音高序列生成，未涉及其他音乐元素。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [8] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: AI的发展短期内可能增加数据中心的能源消耗和CO2排放，但长期来看，AI有望通过优化能源生产和物流等行业流程，显著减少碳足迹。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对数据中心能源消耗和温室气体排放的影响，评估其短期和长期对CO2排放的净效应。

Method: 分析数据中心的能源消耗情景，结合近远期（2030年及以后）的预测，评估AI对CO2排放的影响。

Result: 短期内AI需求增长可能导致CO2排放增加，但长期来看AI有望通过优化流程显著减少碳排放。

Conclusion: AI初期可能对环境造成压力，但长期将成为气候缓解的有力工具。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [9] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 该论文通过深度学习模型检测物联网恶意攻击，评估了多种模型（如GraphSAGE、BERT、TCN等）在恶意流量检测中的表现，其中BERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 物联网流量具有时序性和多样性，需要高效模型来检测恶意攻击。

Method: 使用GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM等模型进行实验。

Result: BERT表现最优，准确率达99.94%，其他模型如Multi-Head Attention和GraphSAGE各有优劣。

Conclusion: BERT在捕获时序依赖方面表现卓越，但需权衡模型性能和计算成本。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [10] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 论文提出将AI辅助检测视为分类任务，通过预处理数据并构建神经网络友好的图像和时间序列表示，证明了常见模型能有效分类抽象任务数据。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂任务中的普及，检测AI辅助变得重要，但人类难以处理抽象任务数据，因此需要利用神经网络的优势。

Method: 构建四种神经网络友好的图像表示和一种时间序列表示，通过三种经典深度学习架构和一种并行CNN-RNN架构进行测试。

Result: 实验表明，适当预处理的数据和结合时空信息的架构能有效提升AI辅助检测的性能。

Conclusion: 编码时空信息对检测抽象任务中的AI辅助至关重要，提出的方法具有通用性。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [11] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling动态调整决策点，提高移动健康干预的及时性。


<details>
  <summary>Details</summary>
Motivation: 固定间隔的决策点调度对习惯性行为干预效果不佳，尤其是对作息不规律的用户。

Method: 提出SigmaScheduling方法，根据行为时间预测的不确定性动态调整决策点。

Result: 在68名参与者的试验中，SigmaScheduling使70%以上的决策点成功在刷牙行为前触发。

Conclusion: SigmaScheduling提升了精准移动健康干预的效果，适用于时间敏感的习惯性行为。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [12] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: LLMs can automate thematic analysis with few-shot prompting, closely matching expert classifications for high-prevalence themes.


<details>
  <summary>Details</summary>
Motivation: To evaluate if LLMs can replicate expert-driven thematic analysis of social media data, addressing challenges in inductive thematic analysis.

Method: Used two Reddit datasets on xylazine, modeled as binary classifications with zero-, single-, and few-shot prompting, measuring performance via accuracy, precision, recall, and F1-score.

Result: GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71), closely mirroring expert classifications for high-prevalence themes.

Conclusion: Few-shot LLM-based approaches can automate thematic analyses, providing a scalable supplement for qualitative research.

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [13] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY是一个开源工具包，用于探索、分析和可视化法律推理中的抽象论证框架，帮助非专家识别歧义来源并解释论证接受性。


<details>
  <summary>Details</summary>
Motivation: 法律推理中的论证框架（AFs）存在歧义性和解释性挑战，非专家难以理解。AF-XRAY旨在解决这些问题。

Method: AF-XRAY提供分层可视化、攻击边分类、替代解决方案叠加可视化，并通过生成关键攻击集解决歧义。

Result: 工具能将歧义场景转化为明确的解决方案，帮助用户识别歧义原因并探索替代方案。

Conclusion: AF-XRAY通过实际法律案例验证，支持目的性法律推理，展示不同假设如何导致不同结论。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [14] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer是一个自动生成高质量导航指令的框架，通过分解和重组语义实体（如动作、场景和对象）来生成自然语言指令。NavInstrCritic是一个无需标注的评估系统，用于评估指令质量。


<details>
  <summary>Details</summary>
Motivation: 专家提供的导航指令数量有限，合成的指令质量不足，限制了大规模研究。

Method: NavComposer通过分解语义实体并重组为指令，NavInstrCritic评估指令的对比匹配、语义一致性和语言多样性。

Result: 实验证明该方法有效，支持大规模和通用化研究。

Conclusion: NavComposer和NavInstrCritic为语言导航提供了可扩展和高质量的解决方案。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [15] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 研究探讨了基于大型语言模型（LLM）的多代理系统（MAS）在慢性多病共存患者治疗推荐中的可行性和价值，模拟多学科团队（MDT）决策，结果显示单代理系统表现与MDT相当，但建议存在不完整和不必要的药物问题。


<details>
  <summary>Details</summary>
Motivation: 慢性多病共存患者的治疗推荐因治疗冲突风险而复杂，现有决策支持系统扩展性有限，研究旨在探索LLM-MAS模拟MDT决策的潜力。

Method: 设计单代理和多代理框架，模拟MDT讨论解决医疗冲突，通过基准案例评估系统性能，并与单代理方法和真实基准比较。

Result: 当前LLM下，单代理系统表现与MDT相当，最佳模型能提供正确建议但存在不完整和不必要的药物问题。

Conclusion: LLM-MAS在治疗推荐中具有潜力，但需改进建议完整性和减少不必要药物，未来可优化模型以提升临床实用性。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [16] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一种知识引导的偏好优化（KPO）框架，通过蛋白质安全知识图谱整合先验知识，以减少生成有害蛋白质序列的风险。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型在序列生成中具有优势，但也可能生成有害序列，如增强病毒传播性或逃避免疫反应的蛋白质，这带来了生物安全和伦理挑战。

Method: 采用知识图谱和强化学习，结合高效的图剪枝策略，识别偏好序列并最小化有害蛋白质生成风险。

Result: 实验表明，KPO能有效减少有害序列的生成，同时保持高功能性。

Conclusion: KPO为生物技术中生成模型的应用提供了可靠的安全保障框架。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [17] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 结合卷积神经网络（CNN）和表格数据，通过卫星图像和环境特征预测鸟类在不同气候中的分布，准确率达85%。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化导致栖息地范围变化，需要准确预测鸟类在特定栖息地的存在。

Method: 使用CNN分析卫星图像的空间特征（如森林、水体、城市化），并结合表格数据（如温度、降水、海拔）进行预测。

Result: 模型预测鸟类分布的准确率为85%。

Conclusion: 该方法提供了一种可扩展且可靠的方式，用于理解鸟类迁徙。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [18] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec是一个基于语义知识追踪的个性化练习推荐框架，通过结合问题语义和学生学习的结构化进展，优化了强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有练习推荐方法忽略问题语义和学生学习的结构化进展，ExRec旨在解决这一问题。

Method: ExRec采用端到端流程，包括问题知识组件标注、语义表示学习、知识追踪模型训练和强化学习优化，并改进了基于Q学习的连续强化学习方法。

Result: 在四个真实数学学习任务中验证了ExRec的有效性，展示了其对未见问题的鲁棒性和可解释的学习轨迹。

Conclusion: ExRec证明了知识追踪引导的强化学习在教育个性化中的潜力。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [19] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于视觉语言模型的指挥官系统，用于解决多无人地面车辆对抗中的智能感知到决策推理问题，结合了场景理解和战略推理，实现了高胜率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统手工规则方法在复杂战场环境中脆弱，现有强化学习方法缺乏可解释性且主要关注动作操作而非战略决策。

Method: 结合视觉语言模型进行场景理解和轻量级大语言模型进行战略推理，实现统一的感知与决策。

Result: 仿真和消融实验显示，该方法相比基线模型胜率超过80%。

Conclusion: 该方法通过模拟人类指挥官的认知过程，实现了高效、适应性强的智能决策系统。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [20] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans提出了一种分阶段的代码翻译方法，通过功能学习和风格学习提升LLM的翻译性能，并在新基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码翻译任务中难以同时保证正确性和可读性，限制了其在实际开发中的应用。

Method: F2STrans采用两阶段方法：功能学习（优化正确性）和风格学习（提升可读性），并结合新基准测试进行综合评估。

Result: 实验表明，F2STrans显著提升性能，使较小模型（Qwen-1.5B）在20种场景中优于更大模型（Qwen-32B和GPT-4）。

Conclusion: F2STrans通过分阶段优化，有效解决了代码翻译中的正确性和可读性问题，具有实际应用潜力。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [21] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS是一个研究导向的架构，利用多个专用AI代理实现物理黄金的自动化、安全代币化与交易，结合区块链智能合约和AI决策，满足合规性、流动性和风险管理需求。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界替代资产（如黄金）在去中心化交易中如何满足合规性、流动性和风险管理的严格需求。

Method: 采用区块链智能合约与链下AI代理结合的方式，设计四个协作代理（合规、代币发行、做市、风险控制）和一个协调核心。

Result: 原型系统实现1.2秒内按需代币发行，做市代理在波动条件下保持0.5%以内的点差，系统可扩展至每秒5000笔交易。

Conclusion: AI代理驱动的去中心化交易所可满足高性能与安全性需求，为传统非流动性资产提供民主化访问途径。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [22] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 本文提出了一种形式化定义，用于统一神经符号AI的核心要素，将其推理定义为逻辑函数与置信函数的积分计算。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI领域缺乏公认的形式化定义，阻碍了系统间的比较与发展。

Method: 引入一种形式化定义，抽象出神经符号AI的关键成分，定义其推理为逻辑函数与置信函数的积分。

Result: 该定义能够抽象出代表性神经符号AI系统的核心特征。

Conclusion: 提出的形式化定义有助于统一神经符号AI领域，促进系统间的比较与进一步发展。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [23] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 结合代码文本和结构化建模的新方法，弥补了现有LLMs在代码分析和生成能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的代码LLMs在分析代码结构化属性（如控制流和数据流）方面能力有限，而传统结构化方法又缺乏生成能力和规模。

Method: 提出一种新方法，结合代码文本建模和结构化建模的优势。

Result: 未明确提及具体结果，但旨在提升代码分析和生成能力。

Conclusion: 新方法有望弥补现有技术的不足，提升代码LLMs的综合能力。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [24] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 提出了一种基于协作的信任增强方法，用于提高自主系统的决策可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 自主系统在动态复杂环境中确保安全和正确行为仍具挑战性，尤其是在存在冲突信息时，如何聚合数据以支持可信决策是关键问题。

Method: 利用自主系统的不同质量属性（如感知质量）确定可信度，并借鉴社会认识论概念定义聚合和传播规则，使用BDD进行信念聚合和传播。

Result: 通过BDD的形式化模型和简化规则，实现了高效的协作自动推理计算结构。

Conclusion: 该方法通过协作和信任评估，提升了自主系统的决策可信度和可靠性。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [25] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 本文提出了一种使用答案集编程（ASP）计算组合电路实际最大延迟的方法，以替代传统的静态时序分析，从而提高处理器性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态时序分析虽然计算速度快，但只能提供延迟的上界，可能导致处理器性能未达最优。本文旨在通过精确计算实际最大延迟来优化性能。

Method: 将问题建模为答案集编程（ASP），并提出非平凡的编码方法，利用ASP的高效求解器解决这一计算难题。

Result: 实验结果表明，ASP能够有效解决硬件设计中的复杂问题，验证了其可行性。

Conclusion: ASP为硬件设计中的延迟计算提供了一种可行且高效的解决方案，有助于提升处理器性能。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [26] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph提出了一种双路径全局-局部融合的知识图谱推理方法，通过分离全局和局部信息的处理路径，解决了现有方法中的分数过平滑问题，显著提升了推理质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱推理方法在处理全局和局部信息时容易导致分数过平滑，模糊正确答案与错误答案的区分，影响推理效果。

Method: DuetGraph采用双路径机制，分别处理局部信息（通过消息传递）和全局信息（通过注意力机制），避免相互干扰；并通过粗到细的优化策略，将实体分为高、低分两个子集，缩小候选空间并增强分数差异。

Result: 实验表明，DuetGraph在多个数据集上实现了最先进的性能，推理质量提升高达8.7%，训练效率加速1.8倍。

Conclusion: DuetGraph通过双路径融合和粗到细优化，有效解决了知识图谱推理中的过平滑问题，显著提升了推理效果和效率。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [27] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: 论文介绍了AgentOps框架，用于观察、分析和优化基于LLM的智能代理系统，强调自动化在管理不确定性中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能代理系统广泛应用，其不确定性（如概率推理、动态执行路径）带来新挑战，传统方法难以应对。

Method: 提出AgentOps框架，包含六阶段自动化流程：行为观察、指标收集、问题检测、根因分析、优化建议和运行时自动化。

Result: 框架支持开发者、测试者、SRE和业务用户，通过自动化管理不确定性，提升系统安全性和适应性。

Conclusion: AgentOps通过自动化“驯服”不确定性，为智能代理系统的安全高效运行提供支持。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [28] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: Opus Prompt Intention Framework通过引入中间意图捕捉层，提升基于LLM的复杂工作流生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决直接根据用户查询生成工作流时逻辑性和扩展性不足的问题。

Method: 提出Opus Workflow Intention Framework，包括从查询中提取信号、解析为结构化意图对象，并基于意图生成工作流。

Result: 在1000对多意图查询-工作流测试中，语义相似度指标显著提升。

Conclusion: 该框架显著提高了工作流生成质量，尤其在混合意图场景下表现优异。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [29] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 本文探讨了如何利用边加权定量双极论证框架（EW-QBAFs）实现可争议AI决策，提出了一种基于梯度关系归因解释（G-RAEs）的迭代算法，以调整边权重达到目标论证强度。


<details>
  <summary>Details</summary>
Motivation: 研究如何使AI决策与人类偏好一致，支持可争议性，但EW-QBAFs在此领域的研究较少。

Method: 提出G-RAEs量化边权重变化对目标论证强度的影响，并开发迭代算法调整权重。

Result: 在模拟推荐系统和多层感知器的合成EW-QBAFs上实验验证了方法的有效性。

Conclusion: G-RAEs和迭代算法能有效解决EW-QBAFs中的可争议性问题。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [30] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN是一种基于视觉语言模型（VLM）的框架，通过模拟人类认知和学习机制，提升机器人在未知环境中的导航和交互能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的需求导航（DDN）方法依赖预收集数据，泛化能力有限，无法适应未知场景。

Method: CogDDN整合快速和慢速思维系统，语义对齐检测对象与指令，并采用双过程决策模块（启发式和分析式）及思维链推理。

Result: 在AI2Thor模拟器和ProcThor数据集上的评估显示，CogDDN比单视角相机方法性能提升15%。

Conclusion: CogDDN显著提高了导航准确性和适应性，为未知环境中的机器人导航提供了新思路。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [31] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 提出了一种结合自然语言对话与可验证保证的神经符号框架，用于复杂物流决策，提升了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决物流决策中传统方法（如整数规划）速度慢且无法处理不确定性，以及大语言模型（LLMs）易误解和幻觉的问题。

Method: 开发了一个神经符号框架，将用户请求转换为结构化规划规范，量化不确定性，并在置信度低时触发交互式澄清循环。

Result: 轻量级模型在100个不确定性过滤示例上微调后，性能超越GPT-4.1，推理延迟降低近50%。

Conclusion: 该框架为复杂物流提供了可验证、实时且用户对齐的决策路径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [32] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI系统通过人类语言“思考”为AI安全提供了新机会，可通过监控其思维链（CoT）检测不良意图。尽管不完美，但CoT监控有潜力，建议进一步研究和投资。


<details>
  <summary>Details</summary>
Motivation: 探索通过监控AI的思维链来增强AI安全性，弥补现有方法的不足。

Method: 提出监控AI的思维链（CoT）以检测潜在的不良意图。

Result: CoT监控虽不完美，但显示出潜力，建议进一步研究和开发。

Conclusion: 建议前沿模型开发者考虑开发决策对CoT可监控性的影响，并投资于相关研究。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [33] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR框架通过整合Perspective-Aware AI（PAi）与XR，提供基于用户身份的上下文感知体验，解决了现有系统用户建模浅层和认知上下文有限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI增强的XR系统因用户建模浅层和认知上下文有限，无法提供充分的沉浸式体验。

Method: PAiR基于Chronicles（多模态数字足迹学习到的身份模型）构建，采用闭环系统动态链接用户状态与沉浸式环境。

Result: 通过Unity-based OpenDome引擎实现的两个概念验证场景展示了PAiR的实用性。

Conclusion: PAiR通过将基于视角的身份模型嵌入沉浸式系统，为人机交互开辟了新方向。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [34] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 论文提出一个基于开放进化理论的框架，重新审视强化学习的三个核心假设，并探讨其在理论和应用中的意义。


<details>
  <summary>Details</summary>
Motivation: 针对强化学习中关于代理定义、学习目标和奖励假设的三个核心假设进行概念性修订，以推动理论和应用的发展。

Method: 借鉴开放进化理论，重新审视这三个假设，并结合进化动力学和起源生命理论进行分析。

Result: 提出进化动力学可以在个体生命周期内运作，丰富了学习视角，并探讨了奖励假设的局限性。

Conclusion: 进化范式虽无法完全解决代理问题，但为理解生物系统中的强化学习提供了新方向，建议结合起源生命理论进一步研究。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [35] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench是一个用于评估LLM代理在土木工程图纸修订任务中的开源基准，包含12类任务、46个定制功能和1920个任务，旨在全面测试代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从工业角度（如土木工程）系统评估LLM代理自动化能力的基准，DrafterBench填补了这一空白。

Method: 通过总结实际图纸文件中的任务，设计包含多种功能和任务的基准，测试代理在复杂指令理解、知识利用和动态适应等方面的能力。

Result: DrafterBench提供了任务准确性和错误统计的详细分析，帮助深入了解代理能力并确定改进目标。

Conclusion: DrafterBench为LLM代理在工程应用中的集成提供了评估工具，并开源了测试集，促进进一步研究。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale是一个评估LLM在高密度指令下性能的基准测试，发现即使最先进的模型在500条指令时准确率仅为68%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估单条或少量指令的任务，无法反映生产级LLM系统需同时遵循大量指令的需求。

Method: 引入IFScale基准测试，包含500条关键词包含指令，用于商业报告写作任务，评估20个前沿模型。

Result: 最佳模型在500条指令时准确率为68%，模型规模和推理能力与性能下降模式相关。

Conclusion: 研究结果有助于设计高密度指令提示，并揭示了性能与延迟的权衡，基准测试已开源。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 论文提出了一种AI驱动的系统，通过两个主要代理（Truth Sleuth和Trend Bender）来检测YouTube视频中的错误信息并主动在评论区进行干预。


<details>
  <summary>Details</summary>
Motivation: 错误信息在数字世界中快速传播，对公众认知造成威胁，需要有效手段进行干预。

Method: 系统采用RAG方法（Truth Sleuth）提取并验证视频中的声明，Trend Bender则生成有说服力的评论以引导讨论。

Result: 实验表明系统在事实核查和用户互动方面表现优异，能有效影响观点。

Conclusion: AI驱动的干预措施在打击错误信息和促进在线空间理性讨论方面具有潜力。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [38] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp是一款基于智能手机的离线对话应用，利用量化的大型语言模型（LLMs）为心理健康提供支持，解决了用户可访问性、网络连接和数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 数字平台在心理健康支持中存在用户可访问性、网络连接和数据隐私的挑战，需要一种离线解决方案。

Method: 通过微调、量化的LLaMA-3.2-1B-Instruct模型，结合自定义的心理健康知识数据集，开发了EmoSApp。

Result: 定性评估显示EmoSApp能提供连贯、共情的对话和实用建议；定量评估表明模型在低资源环境下表现优异。

Conclusion: EmoSApp为便携、安全、定制化的AI心理健康解决方案提供了范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [39] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 该论文提出了一种模块化工具链，用于处理法律、医疗和行政文本数据，解决隐私和异构性问题，支持本地硬件运行的开源模型。


<details>
  <summary>Details</summary>
Motivation: 利用未结构化的法律、医疗和行政文本进行研究，但面临隐私和语言异构性的挑战。

Method: 采用大型语言模型（LLM）进行文本标准化、摘要和翻译，结合命名实体识别和基于规则的方法进行匿名化。

Result: 在瑞典法院判决数据集上验证了工具链的有效性，成功匿名化并保留语义内容。

Conclusion: 该工具链为隐私敏感的大规模文本分析提供了新可能。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [40] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 论文提出了一种针对自然语言解释（NLEs）的更新版XAI分类法，以支持透明AI系统的治理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，NLEs成为解释模型行为的关键，需要系统研究其特性和治理影响。

Method: 基于可解释AI（XAI）文献，构建了一个针对提示型NLEs的三维分类法，涵盖上下文、生成与展示、评估。

Result: 分类法为研究者、审计者和政策制定者提供了描述、设计和改进NLEs的框架。

Conclusion: 该分类法有助于提升AI系统的透明度和治理效果。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [41] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA是一个模块化框架，通过轻量级LoRA适配器和KL正则化训练减少大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在自然语言任务中的幻觉问题，提高真实世界部署的可信度。

Method: 结合自动提示重写、混合检索和低秩适配器调优，通过幻觉检测模块和反馈校正循环实现事实对齐。

Result: 显著减少事实漂移，同时保持模型的效率和模块化。

Conclusion: AutoRAG-LoRA有效提升了大语言模型的事实准确性，适用于实际应用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [42] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 论文探讨了如何通过语言模型表达不确定性以增强用户信任，提出模仿人类沟通的“拟人化不确定性”概念，并分析了现有研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LLMs）在输出时往往过于自信，即使准确性存疑，这削弱了其可信度。需要通过语言表达不确定性以优化人机协作。

Method: 提出“拟人化不确定性”概念，模仿人类沟通方式表达不确定性，并综述了相关研究及数据偏见分析。

Result: 研究发现现有NLP研究忽视了人类不确定性沟通的细微差别及数据偏见，需进一步个性化与语言真实性。

Conclusion: 未来研究应关注人机不确定性沟通的独特因素，并分解“拟人化不确定性”为具体方向。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [43] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: PLEX是一种无需扰动的局部解释方法，利用LLM的上下文嵌入和Siamese网络，显著提高了解释效率。


<details>
  <summary>Details</summary>
Motivation: LLM在文本分类中表现优异，但其复杂性导致解释性差，现有方法（如LIME和SHAP）计算成本高。

Method: PLEX通过提取LLM的上下文嵌入，并训练Siamese网络与特征重要性分数对齐，避免了后续扰动。

Result: 在四个分类任务中，PLEX与LIME和SHAP的一致性超过92%，且计算效率显著提升。

Conclusion: PLEX为LLM的文本分类提供了一种高效且可靠的解释方法。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [44] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在对话代理中的情感建模能力研究，发现其情感层次结构与人类心理模型一致，但也存在对社会经济群体的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何建模用户情感状态，以支持其伦理部署。

Method: 基于情感轮（心理学框架）分析模型输出中情感状态的概率依赖关系。

Result: LLMs自然形成与人类心理模型一致的情感层次结构，且模型越大层次越复杂；同时发现对社会经济群体的系统性情感识别偏见。

Conclusion: LLMs内化了社会感知的某些方面，研究为基于认知理论的模型评估提供了潜在方向。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [45] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 研究探讨了成人服务网站（ASW）广告文本分析在打击性交易中的作用，提出了一种高效的定制Transformer模型，优于现有模型，并展示了其在多个任务中的应用。


<details>
  <summary>Details</summary>
Motivation: ASW广告文本分析对识别性交易受害者至关重要，但文本的复杂性（如表情符号、语法混乱）增加了分析难度。

Method: 研究比较了多种语言建模方法，包括信息检索、预训练Transformer和定制Transformer模型，并验证了定制模型的高效性。

Result: 定制Transformer模型在准确性、召回率、F1分数和ROC AUC上优于BERT-base、RoBERTa和ModernBERT。

Conclusion: 定制模型为ASW文本分析提供了显著进步，可用于多种下游应用和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [46] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练文本嵌入模型增强属性图的语义分析，提升节点分类和关系预测任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 属性图中丰富的文本属性未被充分利用，希望通过预训练模型增强语义分析能力。

Method: 将文本节点和边的属性嵌入到预训练语言模型中，不改变图结构，支持下游任务。

Result: 文本语义显著提升了属性图分析的准确性和可解释性。

Conclusion: 预训练文本嵌入模型能有效增强属性图的语义分析能力。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [47] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA是首个评估模型解读科学文献中示意图能力的基准，包含1,500个专家标注示例。评估18种前沿多模态模型，发现与人类专家存在显著差距，并分析了模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的示意图是重要信息载体，但现有模型对其解读能力不足，需专门基准评估和改进。

Method: 构建包含1,500个示例的MISS-QA基准，测试18种多模态模型对示意图的解读能力，并进行错误分析。

Result: 模型表现显著低于人类专家，尤其在无法回答的问题上，分析揭示了当前模型的局限性。

Conclusion: MISS-QA为提升多模态模型对科学文献的理解提供了关键见解，未来需进一步优化模型能力。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [48] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 研究发现，社交平台上的仇恨言论并未因获得社会认可而显著增加或极端化。


<details>
  <summary>Details</summary>
Motivation: 探讨社交认可对在线仇恨言论的激励作用，验证社会认可理论的两个假设。

Method: 分析2018-2021年间Parler平台的1.1亿条帖子，观察点赞数与后续仇恨言论的关系。

Result: 点赞数与后续仇恨言论无显著关联，且在不同时间尺度上关系复杂。

Conclusion: 社交认可对仇恨言论的强化机制在特定平台上可能表现不同。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [49] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM是一个基于大型语言模型（LLM）的框架，用于从RFC文档中提取准确的有限状态机（FSM），解决了现有技术的可扩展性和覆盖不全问题。


<details>
  <summary>Details</summary>
Motivation: 现有FSM提取技术存在可扩展性、覆盖不全和自然语言规范模糊等问题，限制了协议分析和漏洞发现的效率。

Method: FlowFSM结合LLM、提示链和思维链推理，系统处理协议规范，识别状态转换并构建结构化规则书。

Result: 在FTP和RTSP协议上的实验表明，FlowFSM提取精度高，且减少了虚假转换。

Conclusion: 基于代理的LLM系统在协议分析和FSM推断方面具有潜力，适用于网络安全和逆向工程。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [50] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在司法系统中的公平性，提出了一个评估框架，发现LLMs普遍存在不一致性、偏见和不平衡的准确性，并开发了一个公开工具包支持未来研究。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险领域（如司法）的应用日益增多，但其公平性和对社会正义的影响尚未充分研究。

Method: 基于司法公平理论构建评估框架，开发三个指标（不一致性、偏见、不平衡准确性），并在16个LLMs上进行实验。

Result: LLMs普遍存在不公平现象，尤其在人口统计标签上偏见更明显。调整温度参数可影响公平性，但模型大小、发布时间和来源国无显著影响。

Conclusion: LLMs在司法系统中的公平性存在严重问题，需进一步研究和改进。提供的工具包支持未来工作。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [51] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 论文研究了对话生成中主观与客观风格相似性的区别及其对用户偏好的影响，发现主观相似性与用户偏好强相关，且与客观相似性不同。


<details>
  <summary>Details</summary>
Motivation: 探讨用户与系统风格相似性对用户印象的影响，区分主观与客观相似性。

Method: 构建包含用户偏好、主观风格相似性和客观风格相似性的新数据集，并在开放域对话设置中分析。

Result: 主观风格相似性与用户偏好强相关，且与第三方评估的客观相似性不同。

Conclusion: 区分主观与客观评价对理解风格相似性与用户偏好的关系至关重要。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [52] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 论文提出HanjaBridge方法，通过注入汉字的语义信息解决韩语中的同音异义问题，显著提升了韩语语言理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在韩语等低资源语言中表现不佳的问题，尤其是同音异义汉字词在韩文脚本中无法区分的问题。

Method: 提出HanjaBridge方法，通过持续预训练框架注入所有可能的汉字候选，结合知识蒸馏避免灾难性遗忘。

Result: 在KoBALT基准上相对提升21%，并观察到中韩跨语言迁移的积极效果。

Conclusion: HanjaBridge有效提升韩语理解能力，且无需推理时额外成本，具有实用性和高效性。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [53] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在类比推理任务中与人类表现的对比，探讨了语义表示和显式提示的效果，并考察了模型规模和架构的影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在类比推理任务中是否具备类似人类的推理能力，填补现有研究的空白。

Method: 通过故事类比任务，评估LLMs的语义表示和显式提示效果，比较不同模型规模和架构的表现。

Result: LLMs在类比推理任务中表现接近人类，但推理能力仍有差距；模型规模和架构对性能有显著影响。

Conclusion: LLMs在类比推理方面有潜力，但仍需改进以更接近人类推理能力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [54] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: DS@GT团队在eRisk 2025挑战中采用提示工程策略，利用LLMs进行抑郁检测，取得第二名成绩。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在对话式抑郁检测中的应用，并通过BDI-II评估标准提升模型输出的一致性。

Method: 采用提示工程策略，设计多样化的LLMs提示，生成结构化JSON输出，并评估模型间一致性和内部一致性。

Result: 最佳提交在官方排行榜上排名第二，指标为DCHR=0.50、ADODL=0.89和ASHR=0.27。

Conclusion: 提示设计方法有效对齐模型输出与BDI-II标准，为分析对话线索对症状预测的影响提供了可能。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [55] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign通过微调大语言模型（LLM），将其视为另一种自然语言来处理手语生成，利用逐步提示策略解决手语与口语的差异。


<details>
  <summary>Details</summary>
Motivation: 手语生成因复杂性和独特规则而受限，现有大语言模型对其影响有限。

Method: 微调LLM，采用逐步提示策略提取其内在手语知识，支持学习和生成过程。

Result: 在How2Sign和Phoenix14T数据集上实验表明，该方法有效结合LLM的手语知识和推理能力，对齐手语与口语的分布和语法规则。

Conclusion: TEAM-Sign成功利用LLM能力，为手语生成提供了新思路。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [56] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于分层低秩适应（LoRA）的方法，用于检测英语和西班牙语推文中的性别歧视，通过条件适配器路由显式建模标签依赖关系，并在多语言训练中实现高效参数微调。


<details>
  <summary>Details</summary>
Motivation: 解决文本性别歧视检测任务，特别是在多语言环境下，同时减少计算和存储资源的需求。

Method: 采用分层LoRA适配器，显式建模三个子任务的标签依赖关系，并对所有线性变换进行适应，而非仅注意力层。

Result: 在多语言训练中，性能提升1.7-2.4% F1，训练时间减少75%，模型存储减少98%，且在所有子任务中表现优异。

Conclusion: 该方法通过参数高效微调和多语言训练策略，实现了高性能和资源效率的平衡。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [57] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2是Team HUMANE在FEVER-25研讨会上提出的系统，通过文档摘要和答案重构提升证据质量，优化真实性预测，并集成更新的语言模型，实现了高效的事实验证。


<details>
  <summary>Details</summary>
Motivation: 改进去年的最佳开源模型HerO，提升证据质量和系统性能，以适应真实世界的事实验证需求。

Method: 采用文档摘要和答案重构优化证据质量，通过后训练量化优化真实性预测，并集成更新的语言模型。

Result: 在排行榜上排名第二，运行时间最短，表现出高效性和实际应用潜力。

Conclusion: HerO 2在事实验证任务中表现出色，兼具高效性和实用性，代码已开源。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [58] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 论文介绍了首个韩语新闻立场检测数据集K-News-Stance和框架JoA-ICL，用于长文本立场检测，提升新闻推荐多样性和媒体偏见分析。


<details>
  <summary>Details</summary>
Motivation: 在线新闻消费增长导致推荐系统可能加剧信息茧房和政治极化，需要立场检测技术来引入多元视角。

Method: 提出K-News-Stance数据集和JoA-ICL框架，通过语言模型代理分析关键段落立场并聚合推断全文立场。

Result: JoA-ICL优于现有方法，能有效捕捉长新闻立场，案例研究显示其在推荐多样性和媒体偏见分析中的实用性。

Conclusion: 研究填补了韩语长文本立场检测的空白，JoA-ICL为新闻推荐和媒体分析提供了新工具。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [59] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 研究提出了一种基于LLM增强的临床NLP流程，用于从非结构化临床笔记中提取心血管疾病早期指标，提升风险分层和预测性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病的及时识别和准确风险分层对降低全球死亡率至关重要，现有模型主要依赖结构化数据，而临床笔记中的非结构化信息具有潜在价值。

Method: 采用领域适应的大型语言模型，结合心血管特异性微调、提示推理和实体感知推理，从自由文本中提取症状并进行上下文分析。

Result: 在MIMIC-III和CARDIO-NLP数据集上表现优异，精确度、召回率、F1分数和AUROC均提升，临床相关性高（kappa=0.82）。

Conclusion: 该研究展示了LLM在临床决策支持系统中的潜力，可推进早期预警系统并将患者叙述转化为可操作的风险评估。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [60] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 论文提出了一种基于混合Transformer的情感分析框架，用于解码孟加拉国七月革命期间社交媒体评论中的公众情绪，并在低资源语言（如孟加拉语）中取得了83.7%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析社交媒体评论，揭示孟加拉国七月革命期间公众的情绪和观点，以支持社会正义和系统性改革的诉求。

Method: 采用混合Transformer框架（包括BanglaBERT、mBERT、XLM-RoBERTa和提出的XMB-BERT）进行特征提取，结合PCA降维和11种机器学习分类器进行情感分析。

Result: 提出的XMB-BERT与投票分类器组合取得了83.7%的准确率，优于其他模型组合。

Conclusion: 研究表明机器学习技术在低资源语言（如孟加拉语）的社会情感分析中具有巨大潜力。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [61] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 论文探讨了在跨境金融活动中使用大型语言模型（LLMs）改进实体匹配的准确性和效率，相比传统方法，LLMs表现更优。


<details>
  <summary>Details</summary>
Motivation: 跨境金融活动增加，需要准确识别和分类外国实体以支持风险管理、合规和防欺诈。传统匹配方法因语言和语义问题效果有限。

Method: 比较传统方法（如Jaccard、余弦、Levenshtein距离）、Hugging Face的LLMs和接口型LLMs（如Microsoft Copilot、Qwen 2.5），使用65个葡萄牙公司案例数据集。

Result: 传统方法准确率超92%，但假阳性率高（20-40%）；接口型LLMs准确率超93%，F1分数超96%，假阳性率更低（40-80%）。

Conclusion: LLMs在实体匹配任务中优于传统方法，尤其在处理语言和语义复杂性时表现更佳。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [62] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: DIJA是一种针对扩散式大语言模型（dLLMs）的越狱攻击框架，利用其双向建模和并行解码机制，揭示了现有对齐机制的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管dLLMs在代码生成和文本填充方面表现优异，但其安全性问题未被充分研究，尤其是针对上下文感知的对抗性提示的防护不足。

Method: DIJA通过构造交错掩码-文本提示，利用dLLMs的双向建模和并行解码机制，绕过标准对齐机制，生成有害内容。

Result: DIJA在多个基准测试中显著优于现有越狱方法，最高可达100%的关键词攻击成功率，并大幅超越基线方法。

Conclusion: 研究揭示了dLLMs的安全漏洞，呼吁重新思考此类模型的安全对齐机制。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [63] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 该论文提出了一种研究大型语言模型（LLM）中毒的框架，揭示了多个后门触发器可以共存且互不干扰，并展示了其鲁棒性。同时，提出了一种基于分层权重差异分析的选择性重训练防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM中毒攻击的触发机制和多触发器交互理解有限，论文旨在填补这一空白。

Method: 提出框架研究LLM中毒，展示多触发器的共存与鲁棒性，并通过分层权重差异分析提出防御方法。

Result: 发现多触发器可共存且鲁棒激活，揭示了LLM更广泛的漏洞。防御方法能高效移除触发器行为。

Conclusion: 论文揭示了LLM的多触发器中毒漏洞，并提出了一种高效的防御方法。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [64] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 提出了一种基于集成系统的多语言多模态推理方法，在ImageCLEF 2025 EXAMS V挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多语言教育场景下的高精度推理需求，通过轻量级OCR-VLM集成和精确提示策略提升性能。

Method: 集成Gemini 2.5 Flash、Gemini 1.5 Pro和Gemini 2.5 Pro，通过少样本和零样本提示协调，并进行广泛的消融研究。

Result: 在官方排行榜上，系统以81.4%的准确率在多语言赛道排名第一，并在13个语言赛道中领先11个。

Conclusion: 轻量级OCR-VLM集成结合精确提示策略和多语言增强，在高风险多语言教育场景中优于端到端模型。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [65] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 论文提出了一种新方法WikiMem，用于量化大型语言模型（LLM）中个人事实关联的记忆，并支持动态构建遗忘集以满足GDPR的“被遗忘权”要求。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLM可能记忆并泄露个人信息的问题，特别是在GDPR的RTBF要求下，现有方法无法有效识别模型中的个体级数据关联。

Method: 方法包括引入WikiMem数据集（包含5000多个自然语言测试用例）和一种模型无关的度量标准，通过校准负对数似然对真实值与反事实进行排序。

Result: 评估了15个LLM（参数从410M到70B），发现记忆与主题网络存在和模型规模相关。

Conclusion: 研究为识别LLM中个体级记忆数据提供了基础，支持动态构建遗忘集以满足RTBF请求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [66] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 研究探讨了多智能体系统（MAS）在定性研究中的表现，发现温度和智能体角色对共识达成有显著影响，但并未显著提升编码准确性。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统（MAS）在定性研究中的潜力，尤其是其在编码和数据标注任务中是否优于单智能体。

Method: 通过实验研究，使用6种开源LLM和18种配置，分析了77,000多个编码决策，比较了不同温度和角色对共识和编码准确性的影响。

Result: 温度显著影响共识达成，多角色延迟共识，但未显著提升准确性。仅特定模型和配置下MAS表现优于单智能体。

Conclusion: 挑战了多角色MAS优于单智能体的观点，指出其局限性，并开源了实验代码。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [67] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 该论文介绍了西班牙语和加泰罗尼亚语的社会偏见评估数据集EsBBQ和CaBBQ，用于评估大型语言模型在西班牙社会背景下的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏非英语语言和美国以外社会背景的社会偏见评估资源，研究旨在填补这一空白。

Method: 基于原始BBQ数据集，设计了西班牙语和加泰罗尼亚语的平行数据集，采用多选题QA设置评估10类社会偏见。

Result: 评估结果显示，模型在模糊场景中易选错答案，且高QA准确率常与依赖社会偏见相关。

Conclusion: 研究强调了在多语言和社会背景下评估和减少模型偏见的重要性。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [68] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的方法SAE-LAPE，用于识别大语言模型（LLM）中的语言特定特征，并发现这些特征主要分布在模型的中间到最终层，对多语言性能和输出有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究多语言大语言模型（LLM）中语言特定特征的机制，以解决现有方法难以从跨语言表示中分离语言特定单元的问题。

Method: 使用稀疏自编码器（SAE）学习单义特征，并引入SAE-LAPE方法，基于特征激活概率识别语言特定特征。

Result: 发现语言特定特征主要分布在模型的中间到最终层，这些特征可解释且影响模型的多语言性能和输出，同时可用于语言识别。

Conclusion: SAE-LAPE方法有效识别了语言特定特征，为理解LLM的多语言机制提供了新视角，并具有实际应用潜力。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [69] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: KV-Latent通过降采样Key-Value向量维度到潜在空间，显著减少KV Cache占用并提升推理速度，仅需少量额外训练。


<details>
  <summary>Details</summary>
Motivation: Decoder架构的Key-Value (KV)缓存逐渐增加成为推理效率瓶颈，影响内存消耗和数据传输带宽。

Method: 提出KV-Latent范式，降采样KV向量维度至潜在空间，改进Rotary Positional Embedding的频率采样机制以增强稳定性。

Result: 实验显示KV-Latent在减少KV Cache占用和提升推理速度方面效果显著，且对模型性能影响小。

Conclusion: KV-Latent为构建高效语言模型系统提供了新思路，并在KV Cache节省和LLM效率方面开辟了新可能。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [70] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的自动形式化方法，通过错误反馈实现无需训练的形式化流程，并构建了一个高质量的数学问题数据集。


<details>
  <summary>Details</summary>
Motivation: 推动形式数学推理的发展，需要高效准确的自动形式化方法，利用大规模自然语言数学问题数据集构建形式语言数据集。

Method: 提出基于大语言模型的自动形式化流程，结合错误反馈机制，无需训练即可实现形式化。构建了一个奥林匹克级别的数学问题数据集。

Result: 生成了包含3,922个自然语言问题和9,787个Lean形式化问题的数据集，64.46%被评为高质量。实验表明，少样本学习、错误反馈和增加采样数量能提升形式化效果。

Conclusion: 该数据集适合作为自动定理证明器的基准，实验验证了其挑战性和在形式推理任务中的价值。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [71] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 论文提出了首个中文细粒度仇恨言论数据集（STATE ToxiCN），并研究了编码仇恨术语及大语言模型（LLM）的解释能力，提出了一种整合标注词典的方法以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 中文仇恨言论检测研究滞后，且缺乏细粒度标注数据集和编码仇恨术语的解释性研究，限制了模型的深度语义理解和解释能力。

Method: 1. 引入首个中文细粒度仇恨言论数据集STATE ToxiCN；2. 研究编码仇恨术语及LLM的解释能力；3. 提出整合标注词典的方法。

Result: 1. 评估了现有模型对仇恨语义的理解；2. 显著提升了仇恨言论检测性能。

Conclusion: 为中文仇恨言论检测的解释性研究提供了宝贵资源和见解。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [72] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot是一个多代理大型语言模型系统，旨在提升罗马尼亚语医生在文本远程医疗中的沟通质量，而非临床准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医生在远程医疗中书面回复的呈现质量问题，而非临床准确性。

Method: 使用三个LLM代理，通过DSPy自动优化提示，基于低资源罗马尼亚语数据设计，部署开放权重模型。

Result: 实证评估和41名医生的实际部署显示用户评价和回复质量显著提升。

Conclusion: Dr.Copilot是罗马尼亚医疗环境中首批实际部署的LLM系统之一，效果显著。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [73] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种名为ConVA的方法，通过直接调整大语言模型（LLM）的内部值向量，确保其与人类价值观一致，同时保持模型性能和流畅性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，如何使其与人类价值观保持一致变得尤为重要，以确保其透明性和适应性。

Method: 采用上下文控制的值向量识别方法，结合门控值向量激活技术，实现对LLM内部值的精确和无偏调整。

Result: 实验表明，该方法在10种基本价值观上实现了最高的控制成功率，且不影响模型性能和流畅性。

Conclusion: ConVA方法有效且最小化了对模型性能的影响，确保LLM在输入提示对立或恶意时仍能保持目标价值观。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [74] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合人类专家和大型语言模型（LLM）的方法，用于评估学术论文的方法新颖性，通过提取同行评审报告中的新颖性相关句子和LLM总结的方法部分来微调预训练语言模型（PLM），并设计了文本引导融合模块以提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性评估方法（专家判断或引用组合）存在局限性，如专家知识有限或引用组合有效性不确定。结合LLM的知识和人类专家的判断能力可以弥补这些不足。

Method: 从同行评审报告中提取新颖性相关句子，用LLM总结论文方法部分，用于微调PLM；设计文本引导融合模块（Sparse-Attention）整合人类和LLM知识。

Result: 实验表明，该方法在评估方法新颖性方面优于大量基线模型。

Conclusion: 结合人类和LLM知识的方法能有效评估学术论文的方法新颖性，性能优于传统方法。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [75] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次系统评估了多种过程模型表示（PMR）在大型语言模型（LLM）中的应用，提出PMo数据集，并比较了不同PMR在过程建模（PMo）和过程模型生成（PMG）中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有PMR在结构、复杂性和可用性上差异大，且缺乏系统比较，导致LLM在PMo任务中的应用难以评估。

Method: 引入PMo数据集（含55个过程描述和9种PMR模型），从LLM适用性和PMG性能两个维度评估PMR。

Result: Mermaid在六项PMo标准中得分最高，BPMN text在过程元素相似性上表现最佳。

Conclusion: 研究为LLM在PMo任务中的PMR选择提供了实证依据，Mermaid和BPMN text各有优势。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [76] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 本文研究了在Transformer模型中应用加权损失函数进行多标签情感检测，通过动态调整类别权重解决数据不平衡问题，并在BRIGHTER数据集上评估了BERT、RoBERTa和BART模型。


<details>
  <summary>Details</summary>
Motivation: 解决多标签情感检测中的数据不平衡问题，避免传统重采样方法的计算负担。

Method: 使用动态调整类别的加权损失函数，评估BERT、RoBERTa和BART模型在BRIGHTER数据集上的表现。

Result: 加权损失函数提高了高频情感类别的性能，但对少数类别的效果有限。

Conclusion: 该方法在多标签情感检测中有效，但仍面临处理少数类别的挑战。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [77] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级、可解释的数据污染风险（DCR）框架，用于检测和量化大型语言模型（LLMs）在评估数据中的污染问题，并通过调整准确率提供更真实的性能评估。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，评估数据污染（BDC）问题日益突出，导致性能指标虚高，影响模型的真实泛化能力评估。

Method: DCR框架通过四个粒度级别（语义、信息、数据和标签）检测污染，并利用模糊推理系统合成污染分数，生成统一的DCR因子来调整准确率。

Result: 在9个LLM（0.5B-72B）上的验证表明，DCR能可靠诊断污染严重程度，调整后的准确率与未污染基准相比平均误差在4%以内。

Conclusion: DCR框架计算高效且透明，为日常评估提供了实用的污染检测工具，有助于更公平的比较和提升LLM基准测试的可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [78] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0整合了非推理和推理模式，提升了可用性和推理能力，支持多语言，并推出了两种规模的模型，性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为迎接智能代理时代，EXAONE 4.0增强了工具使用和多语言支持，同时优化了模型性能。

Method: EXAONE 4.0结合了非推理模式（类似EXAONE 3.5）和推理模式（类似EXAONE Deep），并扩展了多语言支持（包括西班牙语）。提供了32B和1.2B两种规模的模型。

Result: EXAONE 4.0在性能上优于同类开源模型，甚至能与前沿模型竞争。

Conclusion: EXAONE 4.0为研究提供了高性能且易于获取的模型，推动了智能代理和多语言AI的发展。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [79] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 论文通过引入因果链式思维图（CCGs）分析大语言模型推理过程，发现推理节点是最终答案的中介，且模型内部结构与CCGs相似。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维（CoT）如何提升大语言模型的推理能力，缺乏共识机制。

Method: 提出因果链式思维图（CCGs），从推理轨迹中自动提取有向无环图，建模语言模型输出的细粒度因果关系。

Result: 实验表明推理节点是答案的中介，且模型内部结构与CCGs相似。

Conclusion: KisMATH数据集支持可控的图对齐干预，为研究CoT在LLM推理中的作用提供了新途径。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [80] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 论文介绍了Ettin模型套件，比较了编码器-解码器架构在分类、检索和生成任务中的表现，发现各自擅长的任务不同，且互相转换效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究编码器-解码器架构在不同任务中的表现差异，避免以往研究中参数、训练技术和数据集不一致的问题。

Method: 使用相同训练方法训练不同规模的编码器和解码器模型（1700万到10亿参数），并在分类、检索和生成任务中比较其性能。

Result: 编码器在分类和检索任务中表现更好，解码器在生成任务中更优；互相转换效果不如直接使用对应架构。

Conclusion: 编码器-解码器架构各有优势，直接使用对应架构优于互相转换，开源所有研究资料以供进一步分析。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [81] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 研究探讨如何通过提示控制大型语言模型（LLMs）的推理策略，并评估其对逻辑问题解决的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs通常倾向于单一推理策略，可能限制其在多样化推理任务中的表现。

Method: 通过实验测试不同提示对LLMs推理策略的影响，并提出方法指导模型选择最优策略。

Result: 实验显示单一策略无法持续提升准确性，但适应性选择策略可改善性能。

Conclusion: 研究提出了优化LLMs推理能力的新方法，强调策略选择的重要性。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [82] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: HKGAI-V1是一个为香港定制的多语言大语言模型，通过深度调整和检索增强生成技术，解决了本地文化、法律和价值观需求，并在敏感查询和数字主权方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 为香港建立符合本地多语言环境、法律框架和文化价值观的AI基础设施，确保AI应用在关键领域的可控性。

Method: 基于DeepSeek架构，通过全参数微调和检索增强生成（RAG）技术，开发了HKGAI-V1，并设计了对抗性香港价值观基准测试。

Result: HKGAI-V1在本地敏感查询处理上优于通用模型，并实现了数字主权；开发了评估工具Adversarial HK Value Benchmark。

Conclusion: 论文提供了技术成果和可复制的蓝图，支持开发符合区域特色的高级AI系统。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [83] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 研究评估了LLM生成的酒店摘要的忠实性，发现简单指标（如词重叠）与人类判断相关性高，而LLM作为评估工具不可靠。


<details>
  <summary>Details</summary>
Motivation: 探讨如何有效评估LLM生成的酒店摘要对输入数据的忠实性，以解决实际业务中的风险和挑战。

Method: 通过人类评估活动（分类错误评估和细粒度标注），比较传统指标、可训练方法和LLM作为评估工具的效果。

Result: 简单指标（如词重叠）与人类判断相关性高（Spearman秩相关系数0.63），而LLM评估工具表现不稳定。

Conclusion: LLM生成的摘要质量高，但评估不可靠；简单指标在跨领域数据中表现优异，需注意业务风险和众包评估的挑战。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet是一种基于小波变换和因果推理的低光图像增强方法，通过全局和局部因果分析提升语义信息恢复能力。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽视实例级语义信息和特征特性，CWNet旨在解决这一问题。

Method: 结合因果推理和小波变换，包括全局度量学习和局部CLIP语义损失，优化频率信息恢复。

Result: CWNet在多个数据集上显著优于现有方法，表现鲁棒。

Conclusion: CWNet通过因果推理和小波变换有效提升低光图像增强性能。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [85] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 提出了一种整合外部生物知识的新框架，用于增强显微镜图像分析模型，以解决细胞系异质性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 高通量筛选技术在药物发现中至关重要，但细胞系的形态和生物学异质性使得新细胞系的扰动筛选具有挑战性。

Method: 通过构建知识图谱（利用STRING和Hetionet数据库）和整合单细胞转录组特征，明确解耦扰动特异性和细胞系特异性表征。

Result: 在RxRx数据库上评估，通过少量样本微调，显著提升了新细胞系的图像分析性能。

Conclusion: 该方法有效提升了显微镜图像分析在新细胞系中的泛化能力，适用于表型药物发现。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [86] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该研究审计了两个先进的面部表情识别数据集，发现数据集中的许多图像是摆拍的，而非自然表情。同时，模型对非白人或深色皮肤的人存在偏见，倾向于预测负面情绪。


<details>
  <summary>Details</summary>
Motivation: 解决面部表情识别算法在自然表情和不同种族/肤色人群上表现不佳的问题，揭示数据集收集方法对模型性能的影响。

Method: 随机抽样审计数据集中的图像，区分自然与摆拍表情，并测试模型在不同种族和肤色人群上的表现。

Result: 发现数据集中存在大量摆拍图像，且模型对非白人或深色皮肤的人存在负面情绪预测偏见。

Conclusion: 数据集的质量和多样性影响模型性能，当前模型存在偏见，可能在实际应用中造成伤害。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [87] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 提出一种无需描述符的兴趣点匹配方法，显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖描述符进行兴趣点匹配，需计算、存储和传输描述符，效率较低。

Method: 在兴趣点检测阶段直接关联匹配点，避免使用描述符。

Result: 匹配精度略低于传统方法，但内存使用大幅减少。

Conclusion: 该方法为几何计算机视觉任务提供了一种高效的无描述符匹配方案。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [88] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一种新的航天器图像分割数据集，用于训练和评估实时自主检测系统，并在YOLOv8和YOLOv11模型上进行了性能测试。


<details>
  <summary>Details</summary>
Motivation: 航天器在太空环境中易受损坏，人工或机器人维修成本高且风险大，因此需要开发可靠的自主检测系统。

Method: 创建了一个包含64k标注图像的航天器数据集，结合真实和合成背景，并添加噪声和失真以模拟真实环境。使用YOLOv8和YOLOv11模型进行微调，并在特定硬件和时间约束下评估性能。

Result: 模型在测试中达到了Dice分数0.92，Hausdorff距离0.69，推理时间约0.5秒。

Conclusion: 该数据集和模型为航天器实时图像分割提供了有效工具，可用于太空自主检测应用。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [89] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种高效的数据方法，通过LLM代理系统增强空间推理能力，解决复杂室内仓库场景中的空间问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间理解任务上表现不足，需要更高效的方法提升其能力。

Method: 设计了一个LLM代理系统，集成多种工具进行空间推理和API交互。

Result: 在AI City Challenge数据集上表现出高准确性和效率。

Conclusion: 该系统为复杂空间任务提供了一种高效解决方案。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [90] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT是一种嵌套ViT架构，通过动态调整计算资源提升效率，优于现有嵌套Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 解决固定计算预算和输入复杂度不匹配导致的效率问题。

Method: 采用渐进式思维阶段和Token Recycling机制动态调整计算资源。

Result: 在相同吞吐量下精度提升2.0 p.p.，相同计算量下提升2.9 p.p.。

Conclusion: ThinkingViT高效且兼容现有ViT模型，为动态推理提供了新思路。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [91] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）的自主目标检测框架（LAOD），通过动态生成场景特定对象名称，实现无需标签的零样本检测。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖固定类别集，灵活性不足；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。

Method: 利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，动态调整检测目标。

Result: 在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，能够检测并命名新对象。

Conclusion: LAOD框架提升了开放世界理解的自主性和适应性。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [92] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM是一种改进的Grad-CAM方法，通过跨层聚合信息和Winsorization技术生成更鲁棒和可解释的热图。


<details>
  <summary>Details</summary>
Motivation: 解释CNN决策过程对高风险领域至关重要，但现有方法如Grad-CAM可能掩盖重要语义或放大噪声。

Method: 提出Winsor-CAM，通过Winsorization技术衰减异常值，并允许用户调节阈值，实现多层级语义探索。

Result: 在PASCAL VOC 2012数据集上，Winsor-CAM比Grad-CAM和均匀层平均方法表现更优，定位指标更佳。

Conclusion: Winsor-CAM通过多层级解释和人机交互控制，推动了可信AI的发展。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [93] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来改进模型的可解释性和任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法难以解释参数更新的贡献，稀疏编码框架旨在解决这一问题。

Method: 使用稀疏编码表示微调特征，特征字典原子作为基本构建块，稀疏系数指示原子重要性。

Result: 在图像编辑和文本到图像概念定制任务中表现优于基线方法。

Conclusion: 稀疏编码框架提高了模型的可解释性和任务适应性，并在实际应用中表现优异。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [94] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 提出了一种结合LOF算法和YOLO-v11n的轻量级结直肠息肉检测框架，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要死因之一，及时准确的息肉检测对诊断和预防至关重要。

Method: 使用LOF算法过滤噪声数据，结合YOLO-v11n模型，通过5折交叉验证和数据增强优化性能。

Result: 精度95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%，mAP@0.5:0.95为77.75%。

Conclusion: 该方法适合临床实时结肠镜检查，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [95] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super是一种改进的3D医学图像中心线追踪模型，解决了原始模型的重复分支和过早终止问题，并在新开发的数据集上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 准确追踪管状树结构（如血管和气道）对医学任务至关重要，但现有模型存在重复分支和过早终止的问题，且缺乏公开数据集进行评估。

Method: 提出Trexplorer Super模型，通过新技术改进性能，并开发了一个合成和两个真实数据集用于评估。

Result: Trexplorer Super在所有数据集上优于现有技术，但合成数据的强表现不一定适用于真实数据。

Conclusion: Trexplorer Super显著提升了中心线追踪性能，数据集和代码已开源。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [96] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于CNN的轻量级全球天气预报模型KAI-a，在保持高精度的同时显著降低计算需求，并通过案例验证了其在极端天气事件中的实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的AI天气预报模型精度高，但其训练复杂度和资源需求大。本研究旨在通过现代CNN架构实现高效且准确的天气预报。

Method: 采用改进的CNN架构，结合尺度不变设计和InceptionNeXt模块，针对地球系统数据优化。模型在ERA5数据集上训练，参数仅700万。

Result: KAI-a在中期天气预报中表现与最先进模型相当，计算需求显著降低，且在极端天气事件（如2018年欧洲热浪）中表现稳健。

Conclusion: KAI-a展示了CNN架构在天气预报中的潜力，为高效且实用的数据驱动预测提供了新方向。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [97] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文提出两种正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度依赖标签不一致问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度依赖标签不一致（TsDLI）问题，提升模型泛化性和可解释性。

Method: 提出局部变异损失（LVL）和局部-全局一致性损失（LGCL），结合有界变异函数和交换时间距离的数学原理。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL优于现有方法，LVL表现最佳。

Conclusion: 提出的方法在标签不一致情况下平衡了预测性能和可解释性，LVL和LGCL表现突出。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [98] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill提出了一种几何引导的弱监督自蒸馏框架，通过教师-学生学习和基于视场的掩码，提升跨视图定位的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨视图定位方法依赖高成本的全监督学习，需要精确的姿态标注。GeoDistill旨在通过弱监督减少标注需求，同时提升性能。

Method: 采用教师-学生学习框架，教师模型定位全景图像，学生模型处理有限视场图像，通过预测对齐和掩码机制优化特征学习。

Result: 实验表明GeoDistill显著提升了定位性能，并引入了一种无需精确平面位置标注的方向估计网络。

Conclusion: GeoDistill为跨视图定位提供了一种高效、可扩展的解决方案，适用于实际应用场景。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [99] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为GAPL-SCD的图聚合原型学习方法，用于解决语义变化检测中的多任务优化问题，通过自适应权重分配和梯度旋转提升性能。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）需要同时优化多个任务，容易因任务间冲突导致负迁移。本文旨在解决这一问题。

Method: 设计了多任务联合优化框架，包括语义分割、变化检测和图聚合原型学习。采用自适应权重分配和梯度旋转方法缓解任务冲突。

Result: 在SECOND和Landsat-SCD数据集上实现了最先进的性能，显著提升了SCD任务的准确性和鲁棒性。

Conclusion: GAPL-SCD方法通过多任务优化和特征增强，有效提升了语义变化检测的性能。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [100] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，显著提升了身份保真度和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复方法在身份保真度上存在不足，尤其是面对身份模糊输入和随机生成过程时。

Method: RIDFR利用预训练扩散模型，结合内容注入模块和身份注入模块，并通过对齐学习抑制ID无关语义干扰。

Result: 实验表明，RIDFR在身份保真度和视觉质量上优于现有方法，表现出强鲁棒性。

Conclusion: RIDFR通过创新设计解决了身份不确定性问题，为人脸修复提供了高保真解决方案。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [101] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 本文提出一个名为WomenSports的新数据集，用于女性运动分类，并提出一种基于CNN的深度特征提取方法，结合通道注意力机制，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有女性运动动作数据集不足，缺乏足够的类别内和类别间变化，限制了相关研究的发展。

Method: 提出一个CNN模型，结合通道注意力机制优化特征提取，并在多个数据集上进行实验验证。

Result: 在WomenSports数据集上，使用ResNet-50达到89.15%的分类准确率。

Conclusion: 提出的数据集和方法为女性运动分类研究提供了新资源，并在实验中表现出色。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [102] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种基于小波注意力机制和射线编码器的新型HOI检测架构，旨在解决现有方法效率低和资源消耗高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测器在高效性和可靠性上表现不佳，需要资源密集型训练和低效架构。

Method: 设计小波注意力机制主干网络和射线编码器，分别用于提取多阶交互特征和优化多尺度注意力。

Result: 在ImageNet和HICO-DET等基准数据集上验证了架构的有效性。

Conclusion: 提出的架构显著提升了HOI检测的效率和准确性，代码已开源。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [103] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait提出了一种通过残差学习解决步态识别中遮挡问题的方法，同时保持对完整步态的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前步态识别方法未解决遮挡问题，且现有方法需要不切实际的数据收集或牺牲完整步态性能。

Method: 将遮挡步态建模为完整步态的残差偏差，通过残差学习网络自适应整合残差。

Result: 在Gait3D、GREW和BRIAR数据集上验证，显著提升遮挡步态识别性能且不影响完整步态准确性。

Conclusion: 残差学习是解决遮挡步态识别并保留完整步态性能的有效方法。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [104] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN是一种轻量级架构设计，通过多阶空间特征和通道聚合模块提升性能，参数效率高。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer在视觉任务中的简单性偏差和信息冗余问题。

Method: 采用可变感受野的核和波基通道聚合模块，优化空间和通道信息处理。

Result: 在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数）。

Conclusion: SpaRTAN通过高效设计实现高性能，参数效率显著。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [105] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP利用CLIP模型进行零样本异常检测，通过特征匹配和跨模态对齐提升性能，并在批次测试中利用其他图像作为参考，结合文本信息过滤噪声。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决零样本异常检测中罕见类别的识别问题，并适应工业需求。

Method: 结合特征匹配与跨模态对齐，利用批次内图像作为参考，并通过文本信息过滤噪声，同时恢复CLIP的局部语义相关性。

Result: 在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，超越现有方法。

Conclusion: FiSeCLIP为零样本异常检测提供了更强基线，展示了CLIP模型在细粒度任务中的潜力。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [106] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于语义显著区域的放射报告生成方法（SISRNet），通过聚焦医学关键区域，生成更准确的报告。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据偏差生成不准确的报告，限制了临床应用。

Method: 利用细粒度跨模态语义识别关键区域，并在图像建模和报告生成中系统关注这些区域。

Result: 在IU-Xray和MIMIC-CXR数据集上表现优于现有方法。

Conclusion: SISRNet能有效捕捉细微异常，减少数据偏差影响，生成临床准确的报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [107] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge的CBCT-to-MDCT翻译框架，结合GAN先验和人类引导的扩散模型，确保解剖保真度和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN或扩散模型在医学图像翻译中边界一致性和临床偏好对齐的不足。

Method: 采用Schrodinger Bridge框架，结合GAN先验和人类反馈（通过CFG），通过迭代优化和锦标赛选择内化人类偏好。

Result: 在临床数据集上，方法在RMSE、SSIM、LPIPS和Dice指标上表现优异，仅需10步采样。

Conclusion: 该框架高效且有效，适用于实时、偏好对齐的医学图像翻译。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [108] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种个性化开放词汇语义分割任务，通过文本提示调优和负掩码提案解决现有方法无法识别用户特定文本（如‘我的杯子’）的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割（OVSS）无法处理用户个性化文本（如‘我的杯子’），导致无法分割用户感兴趣的特定区域。

Method: 提出基于文本提示调优的插件方法，结合负掩码提案减少错误预测，并通过视觉嵌入增强文本提示表示。

Result: 在新建立的基准测试（FSS$^\text{per}$, CUB$^\text{per}$, ADE$^\text{per}$）上验证了方法的优越性。

Conclusion: 该方法在保持原始OVSS性能的同时，显著提升了个性化分割能力。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [109] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: DGFDNet提出了一种双域去雾网络，结合空间和频域特征，通过物理引导的退化对齐提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在单幅图像去雾中表现优异但计算成本高，现有方法依赖空间域特征或频域线索但耦合不足。

Method: DGFDNet包含HAFM模块（自适应增强雾相关频域成分）和MGAM模块（多尺度特征融合），并引入PCGB分支迭代优化先验。

Result: 在四个基准数据集上，DGFDNet实现了最优性能，兼具鲁棒性和实时性。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了复杂雾况下的去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [110] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D是一个专注于足踝区域的高分辨率点云数据集，用于3D点云补全任务，支持生物力学研究和临床应用。


<details>
  <summary>Details</summary>
Motivation: 足踝在步态中的运动分析对生物力学研究和临床评估至关重要，但现有数据集通常关注全身或下肢运动，缺乏足踝区域的详细数据。

Method: FootGait3D包含8,403帧点云数据，来自46名受试者，使用五摄像头深度传感系统采集，提供完整和部分遮挡的点云数据。

Result: 数据集支持单模态和多模态点云补全方法的评估，为足踝几何恢复提供基准。

Conclusion: FootGait3D有望推动生物力学和多段足建模研究，适用于临床步态分析、假肢设计和机器人应用。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [111] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，通过Swin Transformer和新型模块实现高效特征提取和多尺度融合，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率卫星图像中目标检测的挑战，利用Transformer架构提升性能。

Method: 采用Swin Transformer替代CNN主干，结合UpConvMixer块和Fusion Blocks实现多尺度特征提取与融合。

Result: 在xView数据集上达到32.95%的准确率，比现有方法提升11.46%。

Conclusion: GLOD通过创新设计和优化，显著提升了卫星图像目标检测的性能和效率。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [112] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn框架通过原型驱动的语义近似模块（PSA）减少对文本输入的依赖，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学语言引导分割方法依赖成对的图像-文本输入，限制了数据利用和临床应用。

Method: 引入PSA模块，通过原型空间和查询-响应机制近似语义引导，减少对文本的依赖。

Result: 在多个数据集上，ProLearn在文本有限的情况下优于现有方法。

Conclusion: ProLearn有效解决了文本依赖问题，提升了语言引导分割的实用性。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [113] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP是一个新的局部3D高斯编辑框架，通过3D-GALP生成准确的3D掩码，并结合正则化SDS损失，实现了精确且大幅度的局部编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经表示和实例级编辑模型在实现精确局部3D编辑时面临挑战，尤其是高斯泼溅技术因多视角2D分割不一致和SDS损失的模糊性而受限。

Method: RoMaP提出3D-GALP模块生成鲁棒的3D掩码，并结合正则化SDS损失（包括L1锚定损失和额外正则化器）实现精确编辑。

Result: 实验表明，RoMaP在重建和生成的高斯场景中实现了最先进的局部3D编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更鲁棒和灵活的局部编辑能力。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [114] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于关节角度的无标记人体姿态估计（HPE）优化方法，通过傅里叶级数和高阶双向循环网络提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有HPE方法在关键点识别和轨迹平滑性上存在误差，且受限于人工标注数据的不准确性。

Method: 采用关节角度建模，结合高阶傅里叶级数近似和双向循环网络优化HRNet的输出。

Result: 在花样滑冰和街舞等挑战性场景中，JAR方法优于现有HPE优化网络。

Conclusion: 基于关节角度的建模和高质量数据集显著提升了HPE的精度和稳定性。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [115] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了结构对称性和部分遮挡问题，并发布了SKD数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器的单目姿态估计对在轨服务任务至关重要，但现有关键点检测器对结构对称性和部分遮挡敏感。

Method: 设计了基于图的关键点网络（GKNet），利用关键点图的几何约束。

Result: 实验表明GKNet在精度和有效性上优于现有方法。

Conclusion: GKNet和SKD数据集为非合作航天器的姿态估计提供了高效解决方案。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [116] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于交叉验证策略的深度学习模型，用于从GPR图像中自动识别道路地下病害（RSD），显著提高了识别准确率并减少了人工工作量。


<details>
  <summary>Details</summary>
Motivation: GPR图像中的RSD识别依赖人工且效率低，现有深度学习方法受限于数据稀缺和网络区分能力不足。

Method: 构建了高质量的3D GPR数据集，并提出基于YOLO模型的交叉验证策略。

Result: 在实地测试中，召回率超过98.6%，检测系统可减少约90%的人工工作量。

Conclusion: 该方法为RSD自动识别提供了高效解决方案，具有实际应用价值。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [117] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 提出了首个3D大气基准Atmos-Bench和新型网络FourCastX，用于恢复大气结构，无需辅助输入且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助输入和简化物理近似，缺乏标准化3D基准，可能引入不确定性且未能充分捕捉真实辐射传输和大气散射-吸收效应。

Method: 通过耦合WRF和增强的COSP模拟器生成921,600张图像切片，构建3D散射体积；在模型中嵌入ATB-BC物理约束，确保能量一致性。

Result: 在Atmos-Bench数据集上，355 nm和532 nm波段均表现优异，超越现有基线模型。

Conclusion: Atmos-Bench为卫星3D大气结构恢复设定了新标准，有助于更深入的气候研究。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [118] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统回顾了视觉识别模型的可解释性研究，提出了一种以人为中心的分类法，并探讨了评估指标和新技术的机遇。


<details>
  <summary>Details</summary>
Motivation: 随着视觉识别模型在关键领域的应用增加，理解其机制和失败原因的需求推动了可解释性研究的发展。

Method: 提出了一种基于意图、对象、呈现和方法的分类法，系统化整理了可解释性方法。

Result: 建立了视觉识别模型可解释性研究的系统性分类标准，并总结了评估指标的需求。

Conclusion: 本文旨在组织现有研究并启发未来对视觉识别模型可解释性的探索。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [119] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++是一种新型多模态大语言模型，专注于通用关键点理解，通过用户指令整合多种输入模态，实现了高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在捕捉细粒度语义信息（如关键点）方面表现不足，而关键点对细粒度图像分析等应用至关重要。

Method: 提出KptLLM++，采用‘识别-检测’范式，结合结构化思维链推理机制，并扩展训练数据集至50万样本。

Result: 在多个关键点检测基准测试中表现优异，展示了其作为统一解决方案的潜力。

Conclusion: KptLLM++在细粒度图像理解和人机交互方面具有变革性意义。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [120] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于水母物种的检测和分类，结合多种特征提取技术和分类器，最高准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 水母在海洋生态系统中扮演重要角色，但其快速繁殖和生态影响对生物多样性保护构成挑战，准确识别物种对生态监测和管理至关重要。

Method: 整合MobileNetV3、ResNet50等特征提取技术，结合传统机器学习分类器和前馈神经网络分类器，并使用softmax函数直接分类。

Result: MobileNetV3与人工神经网络的组合表现最佳，准确率达98%。

Conclusion: 深度学习与混合框架在解决生物多样性挑战和海洋物种检测方面具有显著效果。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [121] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 论文提出了一种多模态引导的硬样本生成与学习框架（HSGL），通过结合文本和视觉模态来定义、生成和优化硬样本，提升了服装变化行人重识别（CC-ReID）的性能。


<details>
  <summary>Details</summary>
Motivation: 硬样本在行人重识别任务中具有挑战性，尤其是在服装变化场景下。其模糊性和缺乏明确定义限制了学习策略的设计和模型的鲁棒性。

Method: HSGL框架包含两个核心组件：双粒度硬样本生成（DGHSG）和硬样本自适应学习（HSAL），分别通过多模态生成样本和硬度感知优化策略提升模型性能。

Result: 在多个CC-ReID基准测试中，HSGL表现出色，显著加速了学习过程，并在PRCC和LTCC数据集上达到了最先进性能。

Conclusion: 多模态引导的硬样本生成与学习为CC-ReID提供了新的解决方案，显著提升了模型的判别能力和鲁棒性。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [122] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MMOne的通用框架，用于解决多模态场景表示中的模态冲突问题，通过模态建模模块和多模态分解机制，实现了更紧凑和高效的多模态表示。


<details>
  <summary>Details</summary>
Motivation: 人类通过多模态线索感知世界，但不同模态间的固有差异（如属性差异和粒度差异）带来了挑战，需要一种能够统一表示多模态场景的方法。

Method: 提出MMOne框架，包括模态建模模块（使用模态指示器捕捉模态特性）和多模态分解机制（将多模态高斯分布分解为单模态高斯分布）。

Result: 实验表明，该方法能显著提升各模态的表示能力，并支持扩展到更多模态。

Conclusion: MMOne框架有效解决了多模态场景表示中的模态冲突问题，提供了一种紧凑且高效的解决方案。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [123] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，通过检测和分割任务验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于极端天气和人类活动导致滑坡灾害频发，但传统观测方法在大范围和复杂地形下难以实现自动化。

Method: 采用遥感图像作为输入数据，设计了一种新型神经网络架构，用于滑坡检测和分割任务。

Result: 在三个基准数据集上测试，检测任务的F1分数分别为98.23和93.83，分割任务的mIoU分数为63.74和76.88。

Conclusion: 实验结果证明了模型在实际滑坡观测系统中的潜在应用价值。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [124] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型的色彩视觉能力，提出了测试任务和数据集，并分析了错误类型及优化策略。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的色彩视觉能力尚未被充分探索，需要填补这一研究空白。

Method: 定义了色彩视觉测试任务，构建了多类别、多难度级别的数据集，并分析了模型的错误类型。

Result: 提出了微调策略以提升模型在色彩视觉测试中的表现。

Conclusion: 通过测试和优化，可以提升大型视觉语言模型的色彩视觉能力。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [125] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为CMCRL的自监督多层对比表示学习算法，通过聚类引导和对比训练，显著提升了柑橘病害检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 柑橘病害严重影响产量，传统深度学习方法依赖大量标注数据，而CMCRL旨在利用无标注数据优化性能。

Method: 引入聚类中心对比和多层对比训练（MCT）设计，实现自监督学习，适应症状相似性和分层特征表示。

Result: 在公开数据集CDD上，CMCRL比现有方法准确率提升4.5%-30.1%，并在F1分数、精确率和召回率上表现优异。

Conclusion: CMCRL不仅缩小了与全监督方法的性能差距，还解决了类别不平衡问题，展现了强大的鲁棒性。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [126] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 本文评估了开源通用和医学专用视觉语言模型（VLMs）在医疗任务中的表现，发现通用大模型在部分任务上已优于医学专用模型，但推理能力仍是瓶颈，且性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在医疗任务中的能力，填补现有研究的空白。

Method: 评估了3B至72B参数的VLMs在八个医疗基准测试中的表现，分为理解和推理两部分。

Result: 通用大模型在部分任务上表现优于医学专用模型，但推理能力不足，且性能因任务设计、标注质量和知识需求差异显著。

Conclusion: 当前模型尚未达到临床部署的可靠性标准，需加强多模态对齐和更严格的评估协议。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [127] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: 提出了一种名为MCULoRA的新方法，通过解耦模态组合的共享信息和动态调整训练比例，解决了多模态情感识别中模态不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 实际应用中多模态数据常因传感器故障或隐私问题不完整，现有方法因模态组合训练梯度冲突导致性能下降。

Method: MCULoRA包含两个模块：MCLA（解耦模态组合信息）和DPFT（动态调整训练比例）。

Result: 在多个基准数据集上，MCULoRA显著优于现有方法。

Conclusion: MCULoRA为不完整多模态学习提供了一种高效参数训练框架。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [128] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 论文提出了NarrLV，首个全面评估长视频生成模型叙事表达能力的基准，通过Temporal Narrative Atom（TNA）和自动提示生成管道量化叙事丰富度，并设计了基于MLLM的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成模型的评估缺乏专门针对叙事表达能力的基准，现有基准（如VBench）仅支持简单叙事提示。

Method: 1. 引入TNA作为基本叙事单元；2. 构建自动提示生成管道；3. 设计基于MLLM的评估指标。

Result: 实验表明，该指标与人类判断高度一致，揭示了当前视频生成模型在叙事表达上的能力边界。

Conclusion: NarrLV为长视频生成模型的叙事能力评估提供了有效工具，填补了研究空白。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [129] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出了一种基于公平性的分组方法，用于连续敏感属性，以识别关键子群体并优化公平性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将连续敏感属性（如肤色）划分为默认组可能忽略少数群体的歧视问题，需更精细的分组方法。

Method: 通过观察歧视水平分组数据，提出最大化组间歧视方差的新标准，识别关键子群体。

Result: 在合成数据集和真实数据集（CelebA、FFHQ）上验证了方法的鲁棒性，揭示了更细微的歧视模式，并用于去偏。

Conclusion: 该方法在提升公平性的同时保持准确性，适用于工业部署。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [130] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的框架，通过改进修复模型和引入新损失函数，生成高质量烟雾图像，提升烟雾检测模型性能。


<details>
  <summary>Details</summary>
Motivation: 森林火灾烟雾图像数据稀缺，现有修复模型生成烟雾图像质量不足，需改进。

Method: 结合预训练分割模型和多模态模型获取烟雾掩码和图像描述，设计掩码和掩码图像特征引导的网络架构，提出掩码随机差异损失函数。

Result: 生成的烟雾图像真实多样，有效提升烟雾检测模型性能。

Conclusion: 提出的框架解决了烟雾图像生成问题，为烟雾检测任务提供了高质量数据集。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [131] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD框架通过结构化多视角分解和跨模态一致性视图标记，显著提升了3D视觉定位在复杂查询中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理多锚点查询中的目标解耦和视角变化导致的空间描述不一致问题。

Method: 提出ViewSRD框架，包含简单关系解耦模块（SRD）、多视角文本-场景交互模块（Multi-TSI）和文本-场景推理模块。

Result: 在3D视觉定位数据集上，ViewSRD显著优于现有方法，尤其在需要精确空间区分的复杂查询中。

Conclusion: ViewSRD通过结构化多视角分解和跨模态一致性，有效解决了3D视觉定位中的复杂查询问题。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [132] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测与识别，解决了该领域的多项挑战，并取得了99.6%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在国防和监控领域的目标检测与识别面临诸多挑战，如数据集有限、硬件限制、天气影响等，导致现有深度学习方法表现不佳。

Method: 基于改进的YOLOv5s，优化了检测头、特征融合和自定义数据增强，提出了YOLOatr模型。

Result: 在DSIAC MWIR数据集上测试，YOLOatr在实时ATR任务中达到了99.6%的SOTA性能。

Conclusion: YOLOatr在热红外图像目标识别中表现出色，为解决该领域的复杂问题提供了有效方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [133] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP是一个基于物联网的番茄植物表型数据集，通过标准化数据采集协议提供64,464张RGB图像，包含7个区域的标注和50个生长阶段分类。验证表明，基于该数据集的深度学习模型在精度和速度上与专家相当。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法存在观察者偏见和不一致性问题，影响精细植物分析的准确性和可重复性。

Method: 开发了TomatoMAP数据集，使用物联网成像系统采集标准化数据，并通过MobileNetv3、YOLOv11和MaskRCNN组成的深度学习框架进行验证。

Result: 模型在精度和速度上与专家相当，Cohen's Kappa和评分者一致性热图验证了自动化方法的可靠性。

Conclusion: TomatoMAP为精细植物表型分析提供了可靠的数据集和自动化解决方案。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [134] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 论文提出了一种任务导向的人体抓取合成方法，通过任务感知接触图结合场景和任务信息，显著提升了抓取质量和任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统抓取合成方法仅关注物体与手的关系，缺乏对场景和任务的考虑，导致抓取姿势与任务需求不匹配。

Method: 采用两阶段流程：首先生成任务感知接触图，随后利用该图合成任务导向的抓取姿势。

Result: 实验验证了结合场景和任务信息的重要性，新方法在抓取质量和任务性能上显著优于现有方法。

Conclusion: 任务感知接触图是实现高效任务导向抓取的关键，新方法为相关领域提供了有效解决方案。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [135] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 论文提出了一种基于人工智能的方法，利用YOLOv11模型自动识别侵蚀区域并估算面积，开发了交互式网页应用EROSCAN。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专业知识和手动处理，效率低，需自动化解决方案。

Method: 使用YOLOv11模型，结合照片和LiDAR图像，通过Roboflow平台进行数据标注和训练。

Result: 模型准确率70%，能精确识别侵蚀区域并计算面积，开发了EROSCAN应用。

Conclusion: 该方法优化了侵蚀检测和量化，支持风险管理和土地规划决策。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [136] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新的高斯泼溅框架，首次引入多种几何基元以提高表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一基元（椭圆或椭球）表示复杂多样的物体表面，限制了重建质量。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的高斯泼溅流程。

Result: 实验证明该框架能显著提升表面重建的准确性。

Conclusion: 通过引入多种基元，框架有效提升了高斯泼溅的表面重建质量。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [137] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet结合单目深度估计与多视角立体视觉，通过单目特征和深度引导改进MVS在纹理缺失和反射区域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MVS方法在纹理缺失和反射区域表现不佳，而单目深度估计无需特征匹配，能提供鲁棒的相对深度。

Method: 通过注意力机制整合单目特征，动态更新深度候选，并设计相对一致性损失监督深度预测。

Result: 在DTU和Tanks-and-Temples数据集上达到SOTA，排名第一。

Conclusion: MonoMVSNet通过单目引导显著提升了MVS在挑战性区域的性能。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [138] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 论文提出了UGC-VideoCap，一个专注于音频和视觉平衡整合的短视频字幕生成基准和模型框架，以解决现有方法忽视音频重要性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成方法过于依赖视觉信息，忽视了音频在传达场景动态和叙事背景中的关键作用，缺乏多模态数据集和高效模型。

Method: 提出了UGC-VideoCap基准，包含1000个TikTok视频和4000个QA对，并开发了UGC-VideoCaptioner(3B)模型，采用两阶段训练策略（监督微调+GRPO）。

Result: UGC-VideoCap提供了高质量的多模态数据集，模型在有限数据下表现高效且性能优异。

Conclusion: UGC-VideoCap为无约束的真实用户生成视频提供了高质量基准和数据高效解决方案，推动了多模态视频字幕生成的发展。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [139] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 本文提出了一种几何方法来描述人脸识别模型对可解释面部和图像属性的依赖性或不变性，并引入了一种物理启发的对齐度量。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在人脸识别任务中取得了显著进展，但现有方法仅关注身份信息，忽略了嵌入空间中多尺度几何结构的存在。本文旨在揭示这些结构及其对模型性能的影响。

Method: 提出了一种几何方法，结合物理启发的对齐度量，评估模型对不同属性的依赖性或不变性。通过控制实验和合成数据微调的模型进行验证。

Result: 研究发现模型对不同属性表现出不同程度的依赖性或不变性，揭示了模型的优势和局限性。

Conclusion: 该方法为模型提供了更深的可解释性，有助于理解其性能和行为。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [140] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR模型在图像生成领域作为扩散模型的替代方案，本文研究其适应性和差分隐私（DP）适应性问题，发现VAR在非DP任务中表现优于扩散模型，但DP性能较差。


<details>
  <summary>Details</summary>
Motivation: 探索VAR模型的适应性和差分隐私适应性问题，填补现有研究中VAR适应性和DP解决方案的空白。

Method: 实现并比较多种VAR适应策略，并与扩散模型的最先进适应策略进行对比。

Result: VAR在非DP适应任务中表现优于扩散模型，但在DP适应中性能较差。

Conclusion: 需要进一步研究VAR的隐私适应策略，以提升其DP性能。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [141] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI框架利用神经表示视频（NeRV）技术，通过预训练-微调、混合精度训练和并行化目标加速INR压缩，同时引入超压缩技术提升压缩比，在医学影像数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化能力有限，INR虽具潜力但面临压缩速度慢和压缩比不足的问题。

Method: 提出COLI框架，结合NeRV技术，采用预训练-微调、混合精度训练和并行化目标加速INR压缩，并引入超压缩技术提升压缩比。

Result: 在医学影像数据集上，COLI显著降低比特率（bpp），PSNR和SSIM指标优于或接近现有方法，同时训练速度提升4倍。

Conclusion: COLI通过创新技术解决了INR压缩的效率和压缩比问题，为大规模图像压缩提供了高效解决方案。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [142] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和扩散生成模型的血管几何合成方法，用于生成高保真度的主动脉几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法依赖线性假设，难以处理复杂血管拓扑结构，因此需要更灵活的方法。

Method: HUG-VAS结合NURBS参数化和分层扩散模型，生成中心线和径向轮廓，支持零样本条件生成。

Result: 生成的主动脉几何结构与原始数据集生物标志物分布高度匹配，适用于多种实际应用。

Conclusion: HUG-VAS首次将图像先验与生成形状建模统一结合，为心血管诊断和治疗规划提供了新工具。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [143] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI算法通过结合组合边缘像素采样和卷积密度估计，实现了在模糊图像中高精度且实时的圆检测与拟合。


<details>
  <summary>Details</summary>
Motivation: 解决在退化成像条件下鲁棒的圆检测与拟合这一基础计算机视觉挑战。

Method: 结合高效的组合边缘像素采样和参数空间中的卷积密度估计。

Result: 在多种实验框架下表现优异，包括真实医疗数据、合成数据及不同分辨率和异常值条件下的测试，实现了高精度（Jaccard指数0.896）和实时性能（40.3 fps）。

Conclusion: 3C-FBI在精度、速度和鲁棒性上的优异表现使其适用于医疗影像、机器人和工业检测等挑战性场景。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [144] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类感知的模糊颜色模型COLIBRI，旨在弥合计算机颜色表示与人类视觉感知之间的差距。通过实验和调查，模型生成模糊分区和隶属函数，并展示出优于传统颜色模型的性能。


<details>
  <summary>Details</summary>
Motivation: 人类对颜色的感知与计算机的颜色表示存在差距，需要一种更贴近人类感知的颜色模型。

Method: 采用三阶段实验方法：初步实验确定可区分的颜色刺激，大规模人类分类调查（1000+受试者），提取模糊分区和生成隶属函数。

Result: 模型在人类感知对齐方面优于RGB、HSV和LAB等传统颜色模型。

Conclusion: COLIBRI模型在设计和人机交互等领域具有重要意义，为颜色表示提供了更贴近人类感知的解决方案。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [145] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出了一种新颖的5阶段框架，用于从EEG信号解码视觉表示，通过跨模态对齐和重新排序实现上下文感知的EEG到图像生成，实验结果显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码视觉表示具有挑战性，因其复杂和噪声特性，需要一种更有效的方法。

Method: 5阶段框架：EEG编码器、跨模态对齐、标题重新排序、加权插值和图像生成。

Result: 生成高质量图像，分类准确率提升13.43%，生成准确率提升15.21%，FID降低36.61%。

Conclusion: 该方法在语义对齐和图像质量上优于现有技术，为EEG到图像生成提供了有效解决方案。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [146] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist通过点跟踪注意力和自适应令牌合并，解决了文本到图像生成中身份和背景一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持背景细节和身份一致性方面表现不佳，限制了实际应用。

Method: 采用点跟踪注意力和自适应令牌合并，结合前景和背景的解耦控制。

Result: CharaConsist能生成高质量且一致的图像，适用于连续或离散场景。

Conclusion: CharaConsist是首个针对DiT模型的文本到图像一致性生成方法，扩展了实际应用范围。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [147] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种流式4D视觉几何变换器，用于实时感知和重建视频中的4D时空几何，采用因果变换器架构和高效注意力机制，实现了高速推理和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 从视频中实时感知和重建4D时空几何是计算机视觉中的基础但具有挑战性的任务，需要支持交互式应用。

Method: 采用因果变换器架构，利用时间因果注意力和历史键值缓存作为隐式记忆，实现流式长期4D重建。训练时从双向视觉几何变换器（VGGT）中蒸馏知识，推理时支持高效注意力操作（如FlashAttention）。

Result: 在多个4D几何感知基准测试中，模型在在线场景下显著提高了推理速度，同时保持了竞争力性能。

Conclusion: 该模型为可扩展和交互式的4D视觉系统铺平了道路，代码已开源。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [148] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 综述了深度估计在3D计算机视觉中的重要性，探讨了传统硬件方法的局限性和基于视觉方法的挑战，并提出了深度基础模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度估计方法的高成本、低分辨率和环境敏感性，以及基于视觉方法的泛化和稳定性问题。

Method: 调查了单目、立体、多视图和单目视频设置下的深度学习架构和范式，探讨了大规模数据集的作用。

Result: 提出了深度基础模型的概念，强调了其在零样本泛化能力上的潜力。

Conclusion: 通过识别关键架构和训练策略，为未来研究和应用提供了方向。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [149] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Main category: cs.DB

TL;DR: SQLord是一个企业级NL2SQL框架，通过数据反向生成和复杂查询分解方法，结合GPT-Judge评估框架，显著提升了复杂业务场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL框架在复杂业务逻辑和领域特定数据上表现不佳，且评估方法依赖稀缺的标注数据和数据库环境。

Method: SQLord采用数据反向生成方法生成标注数据，并通过自动化工作流分解复杂查询，同时提出GPT-Judge评估框架。

Result: 离线测试显著优于现有基准，在线准确率持续超过90%，并在全球最大B2B电商平台成功应用。

Conclusion: SQLord在复杂现实场景中表现出色，具有显著优势。

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [150] [LLMATCH: A Unified Schema Matching Framework with Large Language Models](https://arxiv.org/abs/2507.10897)
*Sha Wang,Yuchen Li,Hanhua Xiao,Bing Tian Dai,Roy Ka-Wei Lee,Yanfei Dong,Lambert Deng*

Main category: cs.DB

TL;DR: LLMatch是一个模块化的模式匹配框架，通过三阶段分解和两阶段优化策略，显著提升了复杂模式匹配的准确性和工程师效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理复杂多表模式匹配时表现不佳，需要一种更高效、模块化的解决方案。

Method: LLMatch将模式匹配分为三个阶段：模式准备、表候选选择和列级对齐，并采用Rollup和Drilldown两阶段优化策略。

Result: 实验表明，LLMatch在复杂模式匹配中显著提高了准确性，并提升了工程师的生产力。

Conclusion: LLMatch为复杂模式匹配提供了一种高效、模块化的解决方案，并通过SchemaNet基准验证了其有效性。

Abstract: Schema matching is a foundational task in enterprise data integration, aiming
to align disparate data sources. While traditional methods handle simple
one-to-one table mappings, they often struggle with complex multi-table schema
matching in real-world applications. We present LLMatch, a unified and modular
schema matching framework. LLMatch decomposes schema matching into three
distinct stages: schema preparation, table-candidate selection, and
column-level alignment, enabling component-level evaluation and future-proof
compatibility. It includes a novel two-stage optimization strategy: a Rollup
module that consolidates semantically related columns into higher-order
concepts, followed by a Drilldown module that re-expands these concepts for
fine-grained column mapping. To address the scarcity of complex semantic
matching benchmarks, we introduce SchemaNet, a benchmark derived from
real-world schema pairs across three enterprise domains, designed to capture
the challenges of multi-table schema alignment in practical settings.
Experiments demonstrate that LLMatch significantly improves matching accuracy
in complex schema matching settings and substantially boosts engineer
productivity in real-world data integration.

</details>


### [151] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Main category: cs.DB

TL;DR: TableEG框架利用大语言模型生成真实的表格数据错误，填补了合成与真实错误之间的差距，并提供了错误检测与校正的基准。


<details>
  <summary>Details</summary>
Motivation: 数据质量问题严重影响下游分析，但缺乏多样化的真实错误数据集限制了评估，手动标注耗时且不一致，因此探索合成错误生成。

Method: 采用表格微调策略和三元组表示$(I, T, O)$建模错误生成、检测与校正任务，捕获表格中的复杂依赖关系。

Result: TableEG生成的错误在模式和分布相似性上优于规则方法和未经微调的LLM生成错误，且性能指标与真实错误高度一致。

Conclusion: TableEG不仅填补了合成与真实错误之间的差距，还为后续错误检测与校正任务提供了可靠基准。

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


### [152] [TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column Search](https://arxiv.org/abs/2507.11505)
*Harsha Kokel,Aamod Khatiwada,Tejaswini Pedapati,Haritha Ananthakrishnan,Oktie Hassanzadeh,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.DB

TL;DR: 论文提出了一种多标准方法TOPJoin，用于解决企业数据湖中基于上下文感知的列可连接性问题，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于列相似性的方法在企业数据湖中不足以识别可连接列和表，需要结合查询列的上下文。

Method: 定义了上下文感知的列可连接性，并提出多标准方法TOPJoin进行可连接列搜索。

Result: TOPJoin在学术和真实世界基准测试中均优于现有基线方法。

Conclusion: TOPJoin通过结合上下文信息，显著提升了企业数据湖中可连接列的搜索效果。

Abstract: One of the major challenges in enterprise data analysis is the task of
finding joinable tables that are conceptually related and provide meaningful
insights. Traditionally, joinable tables have been discovered through a search
for similar columns, where two columns are considered similar syntactically if
there is a set overlap or they are considered similar semantically if either
the column embeddings or value embeddings are closer in the embedding space.
However, for enterprise data lakes, column similarity is not sufficient to
identify joinable columns and tables. The context of the query column is
important. Hence, in this work, we first define context-aware column
joinability. Then we propose a multi-criteria approach, called TOPJoin, for
joinable column search. We evaluate TOPJoin against existing join search
baselines over one academic and one real-world join search benchmark. Through
experiments, we find that TOPJoin performs better on both benchmarks than the
baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [153] [FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block](https://arxiv.org/abs/2507.10757)
*Ryan Zarick,Isaac Zhang,Daniel Wong,Thomas Kim,Bryan Pellegrino,Mignon Li,Kelvin Wong*

Main category: cs.DC

TL;DR: FAFO是一种区块链交易调度器，通过重新排序交易以提高并行性，显著提升执行吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前区块链执行吞吐量受数据竞争限制，FAFO旨在解决这一问题。

Method: 使用CPU优化的Bloom过滤器检测冲突，并行调度交易执行。

Result: 单节点实现每秒110万次ETH转账和50万次ERC20转账，成本降低91%。

Conclusion: FAFO证明通过优化执行层和交易调度设计，可实现高吞吐量支持未来去中心化应用。

Abstract: Current blockchain execution throughput is limited by data contention,
reducing execution layer parallelism. Fast Ahead-of-Formation Optimization
(FAFO) is the first blockchain transaction scheduler to address this problem by
reordering transactions before block formation for maximum concurrency. FAFO
uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts
and schedule parallel transaction execution at high throughput and low
overhead.
  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1
million native ETH transfers per second and over half a million ERC20 transfers
per second on a single node (Table 1), with 91% lower cost compared to
state-of-the-art sharded execution. Unlike many other existing high throughput
blockchain execution clients, FAFO uses QMDB to Merkleize world state after
every block, enabling light clients and stateless validation for ZK-based
vApps. FAFO scales with minimal synchronization overhead, scaling linearly with
additional CPU resources until it fully exploits the maximum parallelism of the
underlying transaction flow. FAFO proves that the high throughput necessary to
support future decentralized applications can be achieved with a streamlined
execution layer and innovations in blockchain transaction scheduler design.
FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.

</details>


### [154] [Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks](https://arxiv.org/abs/2507.10789)
*Aaron Jarmusch,Nathan Graddon,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文对NVIDIA Blackwell架构进行了微架构分析，通过微基准测试揭示了其关键子系统，并与Hopper架构进行了比较。


<details>
  <summary>Details</summary>
Motivation: 科学研究的快速发展对计算能力提出了更高需求，GPU成为解决方案之一。本文旨在深入分析Blackwell架构的性能特征。

Method: 通过微基准测试研究Blackwell架构的延迟、吞吐量、缓存行为和调度细节，并与Hopper架构（使用GeForce RTX 5080和H100 PCIe）进行比较。

Result: 揭示了Blackwell架构的关键子系统（如内存层次、SM执行管道）及其性能改进与退步，同时分析了功耗效率。

Conclusion: 研究结果为开发者和性能工程师优化Blackwell平台上的工作负载提供了实用见解，并为GPU架构研究贡献了新数据。

Abstract: The rapid development in scientific research provides a need for more compute
power, which is partly being solved by GPUs. This paper presents a
microarchitectural analysis of the modern NVIDIA Blackwell architecture by
studying GPU performance
  features with thought through microbenchmarks. We unveil key subsystems,
including the memory hierarchy, SM execution
  pipelines, and the SM sub-core units, including the 5th generation tensor
cores supporting FP4 and FP6 precisions.
  To understand the different key features of the NVIDIA GPU, we study latency,
throughput, cache behavior, and scheduling
  details, revealing subtle tuning metrics in the design of Blackwell. To
develop a comprehensive analysis, we compare the
  Blackwell architecture with the previous Hopper architecture by using the
GeForce RTX 5080 and H100 PCIe, respectively. We
  evaluate and compare results, presenting both generational improvements and
performance regressions. Additionally, we
  investigate the role of power efficiency and energy consumption under varied
workloads. Our findings provide actionable insights
  for application developers, compiler writers, and performance engineers to
optimize workloads on Blackwell-based platforms,
  and contribute new data to the growing research on GPU architectures.

</details>


### [155] [MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit](https://arxiv.org/abs/2507.11067)
*Yinuo Wang,Tianqi Mao,Lin Gan,Wubing Wan,Zeyu Song,Jiayu Fu,Lanke He,Wenqiang Wang,Zekun Yin,Wei Xue,Guangwen Yang*

Main category: cs.DC

TL;DR: 论文探讨了矩阵加速的三维高阶模板计算，提出了基于SIMD和矩阵单元的优化方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 三维高阶模板计算在HPC中的应用尚未充分探索，尤其是在多核CPU上利用矩阵单元加速的策略。

Method: 结合SIMD和矩阵单元的算法优化，改进内存效率，并提出多线程并行范式以解决数据共享问题。

Result: MMStencil在Nvidia A100上性能提升2.1倍，实际HPC应用中速度提升1.8倍。

Conclusion: MMStencil通过综合优化显著提升了三维高阶模板计算的性能，适用于实际HPC应用。

Abstract: Matrix-accelerated stencil computation is a hot research topic, yet its
application to three-dimensional (3D) high-order stencils and HPC remains
underexplored. With the emergence of matrix units on multicore CPUs, we analyze
matrix-based acceleration strategies and tailor an optimal approach for 3D
high-order stencils. We introduce algorithmic optimizations based on SIMD and
matrix units to address strided memory accesses, alignment conflicts, and
redundant accesses. We propose memory optimizations to boost on-package memory
efficiency, and a novel multi-thread parallelism paradigm to overcome
data-sharing challenges caused by the absence of shared data caches. MMStencil
sustains consistently high hardware utilization across diverse stencil shapes
and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA
effects and MPI limitations in hybrid parallelism. Combining all the
innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100
GPGPU by up to 2.1x. Moreover, the performance improvements translate directly
to real-world HPC applications and enable RTM applications to yield 1.8x
speedup versus a highly optimized industrial Nvidia A100 GPGPU version.

</details>


### [156] [Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL](https://arxiv.org/abs/2507.11094)
*Nibedita Behera,Ashwina Kumar,Atharva Chougule,Mohammed Shan P S,Rushabh Nirdosh Lalwani,Rupesh Nasre*

Main category: cs.DC

TL;DR: 本文提出了一种抽象方案和运行时优化方法，用于高效处理动态图算法，通过DSL自动生成并行代码，并在多种环境中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于图算法的计算、内存访问和通信的固有不规则性，并行化图算法非常困难，尤其是在动态图场景下。现有框架在处理动态图时存在局限性，因此需要新的解决方案。

Method: 引入一种抽象方案和运行时优化，通过DSL表达动态处理逻辑，并自动生成针对多核、分布式和众核环境的并行代码。

Result: 在十个大型图和三种常用算法（最短路径、PageRank和三角形计数）上验证了方法的有效性。

Conclusion: 该方法为动态图算法的高效并行处理提供了可行的解决方案。

Abstract: With the rapid growth of unstructured and semistructured data, parallelizing
graph algorithms has become essential for efficiency. However, due to the
inherent irregularity in computation, memory access patterns, and
communication, graph algorithms are notoriously difficult to parallelize. To
address this challenge, several libraries, frameworks, and domain-specific
languages (DSLs) have been proposed to ease the parallel programming burden for
domain experts. Existing frameworks partially or fully abstract away
parallelism intricacies, provide intuitive scheduling mnemonics, and employ
program analysis to identify data races and generate synchronization code.
Despite these advances, most frameworks are limited in their abstractions and
runtime optimizations, especially when dealing with static graphs. In contrast,
many real-world graphs are inherently dynamic, with evolving structures over
time through insertions, deletions, and modifications of vertices, edges, and
attributes. Generating efficient and correctly synchronized code for such
dynamic graph algorithms remains a significant challenge.
  In this work, we introduce an abstraction scheme and runtime optimizations
for the efficient processing of morph algorithms. Specifically, given an
initial graph G and a set of updates $\Delta$G involving edge insertions and
deletions, we express the dynamic processing logic through a DSL and
automatically generate parallel code targeting multicore, distributed, and
many-core environments. We demonstrate the effectiveness of our approach by
applying the DSL-generated code to ten large graphs with diverse
characteristics and three widely used algorithms: Shortest Paths, PageRank, and
Triangle Counting.

</details>


### [157] [Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration](https://arxiv.org/abs/2507.11165)
*Shixun Wu,Jinwen Pan,Jinyang Liu,Jiannan Tian,Ziwei Qiu,Jiajun Huang,Kai Zhao,Xin Liang,Sheng Di,Zizhong Chen,Franck Cappello*

Main category: cs.DC

TL;DR: cuSZ-Hi是一种优化的GPU科学数据压缩器，提供高压缩比和低延迟，支持多样化数据特性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高性能计算架构的发展需要高吞吐量数据压缩方案，现有方法在压缩比和延迟上不足。

Method: 优化并行插值数据预测方案，探索最佳无损编码技术，系统评估性能。

Result: cuSZ-Hi在相同误差限制下压缩比提升249%，相同PSNR下提升215%。

Conclusion: cuSZ-Hi是高效、灵活且开源的GPU科学数据压缩解决方案。

Abstract: As high-performance computing architectures evolve, more scientific computing
workflows are being deployed on advanced computing platforms such as GPUs.
These workflows can produce raw data at extremely high throughputs, requiring
urgent high-ratio and low-latency error-bounded data compression solutions. In
this paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific
error-bounded lossy compressor with a flexible, domain-irrelevant, and fully
open-source framework design. Our novel contributions are: 1) We maximally
optimize the parallelized interpolation-based data prediction scheme on GPUs,
enabling the full functionalities of interpolation-based scientific data
prediction that are adaptive to diverse data characteristics; 2) We thoroughly
explore and investigate lossless data encoding techniques, then craft and
incorporate the best-fit lossless encoding pipelines for maximizing the
compression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on
benchmarking datasets together with representative baselines. Compared to
existing state-of-the-art scientific lossy compressors, with comparative or
better throughput than existing high-ratio scientific error-bounded lossy
compressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio
improvement under the same error bound, and up to 215% compression ratio
improvement under the same decompression data PSNR.

</details>


### [158] [Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics](https://arxiv.org/abs/2507.11289)
*Martin Rose,Simon Homes,Lukas Ramsperger,Jose Gracia,Christoph Niethammer,Jadran Vrabec*

Main category: cs.DC

TL;DR: 提出了一种基于GPU集群的高带宽通信框架，支持显式算法的线性性能扩展，性能仅受数据集大小和GPU数量限制。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中追求最高性能，通过高带宽通信实现GPU集群的高效协作。

Method: 采用环形进程（GPU）间的数据切片传递，实现时间并行化，用户只需编写GPU内核算法，无需了解底层并行策略。

Result: 以Lennard-Jones势为基础的分子动力学模拟为例，该框架在强扩展情况下性能优于LAMMPS。

Conclusion: 该框架为高性能科学计算提供了一种高效且易于使用的解决方案。

Abstract: In the quest for highest performance in scientific computing, we present a
novel framework that relies on high-bandwidth communication between GPUs in a
compute cluster. The framework offers linear scaling of performance for
explicit algorithms that is only limited by the size of the dataset and the
number of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)
from one GPU, where they are processed, to the next, which results in a
parallel-in-time parallelization. The user of the framework has to write GPU
kernels that implement the algorithm and provide slices of the dataset.
Knowledge about the underlying parallelization strategy is not required because
the communication between processes is carried out by the framework. As a case
study, molecular dynamics simulation based on the Lennard-Jones potential is
implemented to measure the performance for a homogeneous fluid. Single node
performance and strong scaling behavior of this framework is compared to
LAMMPS, which is outperformed in the strong scaling case.

</details>


### [159] [A new Dune grid for scalable dynamic adaptivity based on the p4est software library](https://arxiv.org/abs/2507.11386)
*Carsten Burstedde,Mikhail Kirilin,Robert Klöfkorn*

Main category: cs.DC

TL;DR: 扩展Dune求解器库以支持p4est网格接口，提升MPI可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 继承p4est的高MPI可扩展性、轻量数据结构和多块网格拓扑支持。

Method: 实现Dune与p4est的耦合，并与Dune-ALUGrid进行性能对比。

Result: 新实现优于Dune-ALUGrid，尤其在可扩展性方面；提出改进的平衡策略。

Conclusion: 新接口和平衡策略显著提升性能，适用于并行环境。

Abstract: In this work we extend the Dune solver library with another grid interface to
the open-source p4est software. While Dune already supports about a dozen
different mesh implementations through its mesh interface Dune-Grid, we
undertake this new coupling effort in order to inherit p4est's practically
unlimited MPI scalability as well as its relatively thin data structures, and
its native support for multi-block (forest) mesh topologies in both 2D and 3D.
  The presented implementation is compared to an existing implementation based
on Dune-ALUGrid for a variety of challenging test examples in a parallel
environment. The numerical experiments show that the implementation presented
here is outperforming Dune-ALUGrid in terms of scalability. In addition, an
alternative balancing strategy is presented to ensure 2:1 balancing across
element faces showing improved performance compared to the existing p4est
balance strategy in the numerical examples considered in this work.

</details>


### [160] [Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations](https://arxiv.org/abs/2507.11417)
*Miray Özcan,Philipp Wiesner,Philipp Weiß,Odej Kao*

Main category: cs.DC

TL;DR: 论文提出了一种模拟框架，用于评估大型语言模型（LLM）推理在不同部署设置下的能源和碳排放影响。


<details>
  <summary>Details</summary>
Motivation: 现有模拟框架缺乏对能源消耗的考虑，无法准确估计LLM推理相关的碳排放。

Method: 扩展了高保真LLM推理模拟器，集成GPU功耗模型，并结合能源系统协同模拟环境，量化碳排放。

Result: 框架揭示了推理参数对能源需求和碳足迹的影响，展示了可再生能源抵消潜力高达69.2%。

Conclusion: 该框架为未来碳感知推理基础设施设计提供了基础。

Abstract: The environmental impact of Large Language Models (LLMs) is rising
significantly, with inference now accounting for more than half of their total
lifecycle carbon emissions. However, existing simulation frameworks, which are
increasingly used to determine efficient LLM deployments, lack any concept of
power and, therefore, cannot accurately estimate inference-related emissions.
We present a simulation framework to assess the energy and carbon implications
of LLM inference under varying deployment setups. First, we extend a
high-fidelity LLM inference simulator with a GPU power model that estimates
power consumption based on utilization metrics, enabling analysis across
configurations like batch size, sequence length, and model parallelism. Second,
we integrate simulation outputs into an energy system co-simulation environment
to quantify carbon emissions under specific grid conditions and explore the
potential of carbon-aware scheduling. Through scenario-based analysis, our
framework reveals how inference parameters affect energy demand and carbon
footprint, demonstrates a renewable offset potential of up to 69.2% in an
illustrative deployment case, and provides a foundation for future carbon-aware
inference infrastructure design.

</details>


### [161] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: FLsim是一个模块化、可扩展且高效的联邦学习模拟框架，支持多样化的实验需求，包括数据分布、学习算法、网络拓扑等自定义配置。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的研究和基准测试面临挑战，需要一个统一的框架来简化实验流程。

Method: FLsim提供模块化设计，支持自定义数据分布、学习算法、网络拓扑等，并通过实验验证其有效性。

Result: FLsim在模拟多种联邦学习实验中表现出高效性和灵活性。

Conclusion: FLsim为联邦学习研究提供了前所未有的灵活性和功能性，是联邦学习模拟框架的重要进步。

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


### [162] [Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications](https://arxiv.org/abs/2507.11437)
*Sagar Bharadwaj,Srinivasan Seshan,Anthony Rowe*

Main category: cs.DC

TL;DR: 论文提出了一种联邦空间命名系统，以解决现有集中式地图基础设施在空间Web应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 空间Web（内容与现实世界位置关联的Web）缺乏一个空间命名系统，阻碍了其发展。现有地图服务由少数大公司控制，且主要覆盖户外公共空间，无法满足新兴应用（如增强现实）对室内外详细地图的需求。

Method: 提出联邦空间命名系统，允许多方管理和提供自己的地图，实现地图管理的可扩展性和隐私保护。

Result: 讨论了如何在联邦地图上重新设计地址到位置映射、基于位置的搜索和路由等基本服务。

Conclusion: 联邦空间命名系统能够解决集中式地图基础设施的不足，支持空间Web的进一步发展。

Abstract: The emergence of the Spatial Web -- the Web where content is tied to
real-world locations has the potential to improve and enable many applications
such as augmented reality, navigation, robotics, and more. The Spatial Web is
missing a key ingredient that is impeding its growth -- a spatial naming system
to resolve real-world locations to names. Today's spatial naming systems are
digital maps such as Google and Apple maps. These maps and the location-based
services provided on top of these maps are primarily controlled by a few large
corporations and mostly cover outdoor public spaces. Emerging classes of
applications, such as persistent world-scale augmented reality, require
detailed maps of both outdoor and indoor spaces. Existing centralized mapping
infrastructures are proving insufficient for such applications because of the
scale of cartography efforts required and the privacy of indoor map data.
  In this paper, we present a case for a federated spatial naming system, or in
other words, a federated mapping infrastructure. This enables disparate parties
to manage and serve their own maps of physical regions and unlocks scalability
of map management, isolation and privacy of maps. Map-related services such as
address-to-location mapping, location-based search, and routing needs
re-architecting to work on federated maps. We discuss some essential services
and practicalities of enabling these services.

</details>


### [163] [Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine](https://arxiv.org/abs/2507.11512)
*Aditya Kashi,Nicholson Koukpaizan,Hao Lu,Michael Matheson,Sarp Oral,Feiyi Wang*

Main category: cs.DC

TL;DR: 混合精度算法在科学计算中应用，HPG-MxP基准测试首次在GPU超算上实现1.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 探索混合精度算法在HPC系统中的实际性能增益，尤其是针对稀疏矩阵应用。

Method: 开发并优化HPG-MxP基准测试，结合双精度和单精度计算。

Result: 在现代GPU超算上实现1.6倍的加速。

Conclusion: 混合精度算法在特定HPC系统中具有实际性能优势。

Abstract: Mixed-precision algorithms have been proposed as a way for scientific
computing to benefit from some of the gains seen for artificial intelligence
(AI) on recent high performance computing (HPC) platforms. A few applications
dominated by dense matrix operations have seen substantial speedups by
utilizing low precision formats such as FP16. However, a majority of scientific
simulation applications are memory bandwidth limited. Beyond preliminary
studies, the practical gain from using mixed-precision algorithms on a given
HPC system is largely unclear.
  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been
proposed to measure the useful performance of a HPC system on sparse
matrix-based mixed-precision applications. In this work, we present a highly
optimized implementation of the HPG-MxP benchmark for an exascale system and
describe our algorithm enhancements. We show for the first time a speedup of
1.6x using a combination of double- and single-precision on modern GPU-based
supercomputers.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [164] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 本文提出LiLM-RDB-SFC方法，结合轻量级语言模型（LiLM）和关系数据库（RDB），通过指导深度强化学习（DRL）模型优化服务功能链（SFC）配置。FLAN-T5模型表现优于BART和SQLCoder。


<details>
  <summary>Details</summary>
Motivation: 现代SDN和NFV环境中，SFC管理和VNF配置是重要挑战，传统DRL方法因依赖结构化数据和固定规则，适应性不足。

Method: 结合LiLM（BART和FLAN-T5）与RDB，解析网络状态查询，指导DRL模型进行SFC配置。

Result: FLAN-T5在测试损失（0.00161 vs. 0.00734）、准确率（94.79% vs. 80.2%）和处理时间（2h 2min vs. 2h 38min）上优于BART，且处理时间比SQLCoder减少96%。

Conclusion: LiLM-RDB-SFC方法显著提升SFC配置效率，FLAN-T5在性能和速度上表现优异。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [165] [Arcturus: A Cloud Overlay Network for Global Accelerator with Enhanced Performance and Stability](https://arxiv.org/abs/2507.10928)
*Matthew Yang Liu,Chuang Chen,Pengcheng Lv,Hui Guo,Yanan Zhang,Cong Wang,Yusen Li,Zhenyu Li,Yu-Chu Tian*

Main category: cs.NI

TL;DR: Arcturus是一个云原生全球加速器框架，通过利用低成本、异构的云资源，解决了现有GA服务的高成本和灵活性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有全球加速器服务绑定特定云提供商，导致高成本、部署不灵活，Arcturus旨在解决这些问题。

Method: 采用双平面设计：转发平面构建自适应代理网络，调度平面通过轻量级优化协调负载和路由。

Result: 在百万RPS测试中，Arcturus性能提升1.7倍，成本降低71%，资源效率超过80%。

Conclusion: Arcturus展示了在云资源大规模使用中的高效性，为全球加速器服务提供了更优解决方案。

Abstract: Global Accelerator (GA) services play a vital role in ensuring low-latency,
high-reliability communication for real-time interactive applications. However,
existing GA offerings are tightly bound to specific cloud providers, resulting
in high costs, rigid deployment, and limited flexibility, especially for
large-scale or budget-sensitive deployments. Arcturus is a cloud-native GA
framework that revisits the design of GA systems by leveraging low-cost,
heterogeneous cloud resources across multiple providers. Rather than relying on
fixed, high-end infrastructure, Arcturus dynamically constructs its
acceleration network and balances performance, stability, and resource
efficiency. To achieve this, Arcturus introduces a two-plane design: a
forwarding plane that builds a proxy network with adaptive control, and a
scheduling plane that coordinates load and routing through lightweight,
quantitative optimization. Evaluations under millions of RPS show that Arcturus
outperforms commercial GA services by up to 1.7X in acceleration performance,
reduces cost by 71%, and maintains over 80% resource efficiency--demonstrating
efficient use of cloud resources at scale.

</details>


### [166] [SIMCODE: A Benchmark for Natural Language to ns-3 Network Simulation Code Generation](https://arxiv.org/abs/2507.11014)
*Tasnim Ahmed,Mirza Mohammad Azwad,Salimur Choudhury*

Main category: cs.NI

TL;DR: SIMCODE是首个评估LLM生成ns-3仿真代码能力的基准，包含400个任务，测试了三种LLM的性能，发现GPT-4.1表现最佳但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成方面表现出色，但在特定领域（如ns-3仿真脚本生成）的效果尚未充分研究，且现有工具缺乏系统性评估。

Method: 引入SIMCODE基准，包含400个任务，评估三种LLM（Gemini-2.0、GPT-4.1、Qwen-3）在六种提示技术下的表现，并研究任务特定微调的影响。

Result: GPT-4.1表现最佳，但执行准确率仍有限，主要错误为缺少头文件和API不匹配。

Conclusion: SIMCODE为评估LLM和领域感知生成系统提供了基础，但LLM在生成ns-3仿真代码方面仍有提升空间。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation across various domains. However, their effectiveness in
generating simulation scripts for domain-specific environments like ns-3
remains underexplored. Despite the growing interest in automating network
simulations, existing tools primarily focus on interactive automation over
rigorous evaluation. To facilitate systematic evaluation, we introduce SIMCODE,
the first benchmark to evaluate LLMs' ability to generate ns-3 simulation code
from natural language. SIMCODE includes 400 tasks across introductory,
intermediate, and advanced levels, with solutions and test cases. Using
SIMCODE, we evaluate three prominent LLMs, Gemini-2.0, GPT-4.1, and Qwen-3,
across six prompt techniques. Furthermore, investigating task-specific
fine-tuning's impact reveals that while GPT-4.1 outperforms others, execution
accuracy remains modest, with substantial room for improvement. Error analysis
identifies missing headers and API mismatches as dominant failures.
Nevertheless, SIMCODE provides a foundational step toward evaluating LLMs and
research in domain-aware generative systems.

</details>


### [167] [Graph-based Fingerprint Update Using Unlabelled WiFi Signals](https://arxiv.org/abs/2507.11038)
*Ka Ho Chiu,Handi Yin,Weipeng Zhuo,Chul-Ho Lee,S. -H. Gary Chan*

Main category: cs.NI

TL;DR: GUFU是一种基于图神经网络的WiFi指纹更新方法，利用未标记信号和新AP有效更新现有指纹数据库，显著提升了适应性和准确性。


<details>
  <summary>Details</summary>
Motivation: WiFi信号环境随时间变化，现有方法未能充分利用新信号或新AP的特征，需要一种更有效的更新方法。

Method: GUFU通过图神经网络和链接预测算法，利用新信号和AP重新训练增量网络，并更新指定位置的信号向量。

Result: 在四个大型代表性站点实验中，GUFU在RSS值和位置预测上分别减少了21.4%和29.8%的误差。

Conclusion: GUFU显著提升了WiFi指纹的适应性和准确性，优于现有方法。

Abstract: WiFi received signal strength (RSS) environment evolves over time due to
movement of access points (APs), AP power adjustment, installation and removal
of APs, etc. We study how to effectively update an existing database of
fingerprints, defined as the RSS values of APs at designated locations, using a
batch of newly collected unlabelled (possibly crowdsourced) WiFi signals. Prior
art either estimates the locations of the new signals without updating the
existing fingerprints or filters out the new APs without sufficiently embracing
their features. To address that, we propose GUFU, a novel effective graph-based
approach to update WiFi fingerprints using unlabelled signals with possibly new
APs. Based on the observation that similar signal vectors likely imply physical
proximity, GUFU employs a graph neural network (GNN) and a link prediction
algorithm to retrain an incremental network given the new signals and APs.
After the retraining, it then updates the signal vectors at the designated
locations. Through extensive experiments in four large representative sites,
GUFU is shown to achieve remarkably higher fingerprint adaptivity as compared
with other state-of-the-art approaches, with error reduction of 21.4% and 29.8%
in RSS values and location prediction, respectively.

</details>


### [168] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: 论文提出利用机器学习预测Wi-Fi网络的信道质量（帧交付率），以优化工业应用中的网络操作。


<details>
  <summary>Details</summary>
Motivation: 工业与关键任务应用对无线网络的鲁棒性、可靠性和确定性需求增加，推动了新方法的创新。

Method: 采用卷积神经网络和长短期记忆网络，基于真实Wi-Fi数据集分析预测效果。

Result: 帧交付率可被可靠预测，卷积神经网络在计算复杂度和资源消耗上更优。

Conclusion: 卷积神经网络在嵌入式与工业系统中更具实用性。

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


### [169] [Resilient Time-Sensitive Networking for Industrial IoT: Configuration and Fault-Tolerance Evaluation](https://arxiv.org/abs/2507.11250)
*Mohamed Seliem,Dirk Pesch,Utz Roedig,Cormac Sreenan*

Main category: cs.NI

TL;DR: IN2C是一个基于OMNeT++/INET的仿真框架，用于评估TSN在故障条件下的性能，结果表明FRER能消除丢包但增加链路利用率。


<details>
  <summary>Details</summary>
Motivation: 评估TSN在工业系统中的容错能力，尤其是在现实故障条件下的表现。

Method: 开发IN2C框架，集成TSN核心功能（如时间同步、流量整形、FRER）和XML驱动的故障注入，模拟四种故障场景。

Result: FRER消除了丢包并实现亚毫秒级恢复，但链路利用率增加2-3倍。

Conclusion: IN2C为带宽受限的工业环境中部署TSN提供了实用指导。

Abstract: Time-Sensitive Networking (TSN) is increasingly adopted in industrial systems
to meet strict latency, jitter, and reliability requirements. However,
evaluating TSN's fault tolerance under realistic failure conditions remains
challenging. This paper presents IN2C, a modular OMNeT++/INET-based simulation
framework that models two synchronized production cells connected to
centralized infrastructure. IN2C integrates core TSN features, including time
synchronization, traffic shaping, per-stream filtering, and Frame Replication
and Elimination for Redundancy (FRER), alongside XML-driven fault injection for
link and node failures. Four fault scenarios are evaluated to compare TSN
performance with and without redundancy. Results show that FRER eliminates
packet loss and achieves submillisecond recovery, though with 2-3x higher link
utilization. These findings offer practical guidance for deploying TSN in
bandwidth-constrained industrial environments.

</details>


### [170] [JamShield: A Machine Learning Detection System for Over-the-Air Jamming Attacks](https://arxiv.org/abs/2507.11483)
*Ioannis Panitsas,Yagmur Yigit,Leandros Tassiulas,Leandros Maglaras,Berk Canberk*

Main category: cs.NI

TL;DR: JamShield是一种动态干扰检测系统，通过混合特征选择和实时分类算法调整，显著提高了检测率和准确性。


<details>
  <summary>Details</summary>
Motivation: 无线网络易受干扰攻击，现有检测方法依赖模拟数据或有限数据集，难以在真实场景中有效工作。

Method: JamShield利用混合特征选择和自动分类模块，动态调整算法以适应实时网络条件。

Result: 实验结果显示，JamShield在检测率、精确度和召回率上优于现有算法，同时减少了误报和漏检。

Conclusion: JamShield为真实无线网络中的干扰攻击提供了可靠且高效的检测解决方案。

Abstract: Wireless networks are vulnerable to jamming attacks due to the shared
communication medium, which can severely degrade performance and disrupt
services. Despite extensive research, current jamming detection methods often
rely on simulated data or proprietary over-the-air datasets with limited
cross-layer features, failing to accurately represent the real state of a
network and thus limiting their effectiveness in real-world scenarios. To
address these challenges, we introduce JamShield, a dynamic jamming detection
system trained on our own collected over-the-air and publicly available
dataset. It utilizes hybrid feature selection to prioritize relevant features
for accurate and efficient detection. Additionally, it includes an
auto-classification module that dynamically adjusts the classification
algorithm in real-time based on current network conditions. Our experimental
results demonstrate significant improvements in detection rate, precision, and
recall, along with reduced false alarms and misdetections compared to
state-of-the-art detection algorithms, making JamShield a robust and reliable
solution for detecting jamming attacks in real-world wireless networks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [171] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: 论文定义了一类确定性流函数，并证明其可作为状态幺半群的同态实现。同态定律比之前的流程序优化语义框架更简单，同时支持丰富的等式推理。


<details>
  <summary>Details</summary>
Motivation: 旨在简化流程序优化的语义框架，同时保留对复杂数据流程序（如并行组合和反馈）的等式推理支持。

Method: 通过将确定性流函数实现为状态幺半群的同态，简化了同态定律。

Result: 展示了该方法在分区数据库连接、分层否定和简化TCP模型中的有效性。

Conclusion: 该方法简化了流程序优化的语义框架，同时保留了丰富的推理能力。

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


### [172] [The downgrading semantics of memory safety](https://arxiv.org/abs/2507.11282)
*René Rydhof Hansen,Andreas Stenbæk Larsen,Aslan Askarov*

Main category: cs.PL

TL;DR: 本文提出了一种称为“渐进分配器独立性”的概念，用于准确捕捉内存安全的分配器特定方面。通过低级别语言中的malloc和free操作，结合非干扰性，解决了内存安全语义的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的内存安全定义以负面事件为基础，被认为缺乏原则性。本文旨在填补这一空白，提出一种更精确的语义定义。

Method: 使用低级别语言模型，结合malloc和free操作，通过非干扰性定义渐进分配器独立性，并处理内存不足和指针到整数的转换。

Result: 提出了一种新的内存安全定义，能够处理分配器的影响，并通过信息流技术解决了技术挑战。

Conclusion: 渐进分配器独立性为内存安全提供了一种更精确的语义定义，填补了现有理论的不足。

Abstract: Memory safety is traditionally characterized in terms of bad things that
cannot happen, an approach that is often criticized as unprincipled. Prior work
suggest a connection between memory safety and noninterference, but no
satisfactory semantic notion of memory safety is currently known.
  This work proposes a notion of gradual allocator independence that accurately
captures many allocator-specific aspects of memory safety. We consider a
low-level language with access to an allocator that provides malloc and free
primitives in a flat memory model. Pointers are just integers, and as such it
is trivial to write memory-unsafe programs. The basic intuition of gradual
allocator independence is that of noninterference, namely that allocators must
not influence program execution. This intuition is refined in two important
ways to account for the allocators running out-of-memory and for programs to
have pointer-to-integer casts. The key insight of the definition is to treat
these extensions as forms of downgrading and give them satisfactory technical
treatment using the state-of-the-art information flow machinery.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [173] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: 论文介绍了$	exttt{DroidCollection}$，一个包含百万代码样本、七种编程语言、43种编码模型输出的数据集，用于训练和评估机器生成代码检测器。同时开发了$	exttt{DroidDetect}$检测器，实验表明现有检测器泛化能力不足，但通过对抗数据训练可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成代码检测器在多样编程语言和领域泛化能力不足，且易受对抗样本攻击。

Method: 构建$	exttt{DroidCollection}$数据集，开发$	exttt{DroidDetect}$检测器，采用多任务目标训练，并实验验证对抗数据和噪声分布处理方法。

Result: 现有检测器泛化能力差，但对抗数据训练可显著提升性能；度量学习和不确定性重采样有效。

Conclusion: $	exttt{DroidCollection}$和$	exttt{DroidDetect}$为机器生成代码检测提供了全面解决方案，对抗数据和噪声处理方法可提升检测器性能。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [174] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino是一个结合LLM、RAG和工具验证的系统，用于自动化生成和验证Policy as Code规则，提升IaC环境中的合规性。


<details>
  <summary>Details</summary>
Motivation: Policy as Code的采用受限于政策语言的复杂性和配置错误的风险，需要一种自动化解决方案。

Method: ARPaCCino利用LLM、RAG和工具验证，从自然语言描述生成Rego规则，并验证和优化IaC配置。

Result: 实验证明ARPaCCino能生成正确规则，识别不合规基础设施并进行修正，即使使用小型LLM。

Conclusion: ARPaCCino展示了基于RAG的架构在提升PaC工作流自动化、可靠性和可访问性方面的潜力。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [175] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: Meta Self-Refining框架通过元修正层解决语言模型管道在软约束竞争中的低效回溯问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型管道在面对竞争性软约束时效率低下，导致回溯循环。

Method: 引入元修正层，监控执行历史，检测振荡失败，并通过元修复器合成策略性指令。

Result: Meta Self-Refining能有效修复回溯循环，提升语言模型程序的效率。

Conclusion: 该框架为语言模型管道提供了一种动态修复竞争约束的方法。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [176] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry是一个协议无关的工具管理库，简化了工具注册、执行和生命周期管理，显著减少了代码量和提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前工具集成方法存在碎片化、协议限制和实现复杂性问题，增加了开发负担。

Method: 提出Toolregistry，通过统一接口管理工具注册、表示、执行和生命周期。

Result: Toolregistry减少了60-80%的集成代码，性能提升3.1倍，100%兼容OpenAI标准。

Conclusion: Toolregistry显著提升了开发效率和代码可维护性，已在开源社区发布。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [177] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具SENSOR，用于分类用户评论中的隐私相关请求和错误报告，采用GRACE模型，性能优异。


<details>
  <summary>Details</summary>
Motivation: 社交媒体用户评论中隐私问题突出，但手动分类困难，缺乏针对隐私相关请求和错误报告的分类工具。

Method: 提出GRACE模型（基于GRU、CBOW和注意力机制），分析16000条用户评论，手动标注并训练模型。

Result: GRACE模型表现最佳（F1-score: 0.9434, ROC-AUC: 0.9934, 准确率: 95.10%）。

Conclusion: SENSOR工具能有效帮助开发者提取隐私相关反馈，提升用户隐私和信任。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [178] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 论文探讨了大语言模型在代码任务中的表现，指出其虽擅长语法模式匹配，但缺乏代码语义理解能力。通过针对代码理解任务的微调，模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成和补全任务中表现优异，但缺乏对代码语义的深层理解，导致在调试和优化等任务中表现不佳。

Method: 提出通过大规模数据集对模型进行微调，专门针对代码理解任务，以增强其语义理解能力。

Result: 微调后，模型在代码理解任务中表现显著提升，特别是QWQ-32B模型准确率从70%提升至83.47%，Codestral-22B模型达到最高准确率87.66%。

Conclusion: 针对代码理解任务的微调能有效提升大语言模型的语义理解能力，尤其在复杂任务中表现更优。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [179] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench (CAB) 是首个用于评估多轮编程辅助的基准框架，基于真实代码库问题自动生成数据集，并揭示大型语言模型在复杂项目环境中的能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有编程助手基准主要关注代码生成任务，缺乏对多轮交互和真实项目环境的评估。CAB 旨在填补这一空白，提供更贴近实际开发场景的评估标准。

Method: CAB 通过从 GitHub 问题自动生成可扩展数据集，并利用容器化代码库和模拟用户进行多轮交互评估。

Result: 在 3,286 个真实编程问题上的测试显示，模型在 Stack Overflow 上表现良好（70-83% 成功率），但在 CAB 的复杂问题中仅解决 16.49%。

Conclusion: CAB 揭示了模型在项目特定上下文中的能力不足，强调了复杂场景下编程辅助的挑战。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [180] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: 论文研究了即时漏洞预测（JIT-VP）在现实场景中的有效性，发现当前评估方法依赖理想化数据集，导致预测性能显著下降。通过大规模数据集和实验分析，指出数据不平衡问题，并探讨了现有解决方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统复杂性增加，质量保证面临挑战。JIT-VP是一种主动识别漏洞的方法，但现有评估依赖理想化数据集，缺乏现实性。

Method: 研究引入包含漏洞相关和漏洞无关提交的大规模数据集（FFmpeg和Linux内核），评估八种先进JIT-VP技术的性能，并探讨处理数据不平衡的方法。

Result: 现实条件下JIT-VP性能显著下降（如PR-AUC从0.805降至0.016），现有数据不平衡处理技术（如过采样、欠采样）效果不佳。

Conclusion: 研究强调现实评估的重要性，并指出需要领域特定技术解决数据不平衡问题。

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [181] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: 研究探讨了使用生成式AI助手自动管理敏捷项目中的待办事项，通过开发Jira插件实现高精度且高效的待办事项整理。


<details>
  <summary>Details</summary>
Motivation: 随着产品待办事项的规模和复杂性增加，冗余、过时或定义不清的任务堆积，导致优先级和决策过程复杂化。

Method: 通过设计科学周期开发Jira插件，利用向量数据库嵌入待办事项，通过余弦相似度检测重复项，并借助GPT-4o模型提出合并、删除或新增建议。

Result: AI辅助的待办事项整理实现了100%的精确度，并将完成时间缩短了45%。

Conclusion: 该工具能够优化待办事项细化流程，同时提升用户体验。

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [182] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: 本文总结了敏捷软件开发中研究与实际应用之间的差距，并探讨了国际研讨会的发现，提出了缩小差距的策略和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 敏捷软件开发虽广泛应用，但研究与实际实施之间存在显著差距，需促进两者的协作。

Method: 通过首次国际研讨会，收集参与者对差距的主要主题和因素的分析，并提出策略。

Result: 识别了导致差距的关键因素，提出了缩小差距的策略，并指出需进一步研究的挑战。

Conclusion: 研讨会为促进敏捷软件开发中研究与实践的协作提供了重要见解，未来需更多研究解决挑战。

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [183] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 论文研究了六种先进的大型语言模型（LLMs）在解决真实Python问题时推荐的库的类型、特征及可用性，发现它们倾向于推荐成熟、流行的第三方库，但也存在可用性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着开发者越来越多地使用LLMs辅助编程任务，了解这些模型如何推荐库对代码功能、安全性和可维护性至关重要。

Method: 通过从Stack Overflow获取真实Python问题，对六种LLMs进行实证研究，分析其推荐的库的类型、特征及可用性。

Result: LLMs主要推荐第三方库，且多为成熟、流行和宽松许可的依赖项，但4.6%的库因导入名称与可安装包不匹配而无法自动解析，仅两种模型提供安装指导。

Conclusion: 研究为开发者和研究人员提供了实用见解，并指出了改进LLM生成代码在依赖项方面的可靠性和可用性的机会。

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [184] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: 本文探讨了自适应AI驱动的对话代理在软件开发中的作用，强调其动态、上下文感知的辅助能力，并分析了其优势、挑战及未来潜力。


<details>
  <summary>Details</summary>
Motivation: 研究自适应AI对话代理如何通过机器学习和自然语言处理提升开发效率，解决传统规则系统的局限性。

Method: 通过分析现有工具（如GitHub Copilot和Microsoft Teams机器人）的演变，探讨自适应AI在软件开发中的应用与挑战。

Result: 自适应AI对话代理能提供个性化、实时支持，显著提升开发效率，但也面临数据隐私和伦理问题。

Conclusion: 自适应AI对话代理有望通过定制化支持和高效开发周期彻底改变软件开发领域。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [185] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: 该研究探索了使用大型语言模型（LLMs）作为自动化评估工具来评估提交消息质量，发现其性能接近人类水平，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 提交消息质量在软件开发中至关重要，但传统自动评估指标（如BLEU、ROUGE-L）存在局限性，导致依赖人工评估。

Method: 通过系统实验，结合Chain-of-Thought推理和少量示例，测试不同LLMs的评估能力。

Result: LLMs在评估提交消息质量上显著优于传统指标，且具有可接受的重复性、鲁棒性和公平性。

Conclusion: LLMs为提交消息评估提供了可扩展的高质量替代方案，减少了人工评估的需求。

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [186] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: SWE-MERA是一个动态更新的基准测试，旨在解决SWE-bench中的数据污染问题，通过自动化收集GitHub问题并验证质量，评估了多个LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SWE-bench基准存在数据污染问题（如解决方案泄漏和测试用例不足），限制了LLM在软件工程中的评估效果。

Method: 开发了SWE-MERA，通过自动化收集真实GitHub问题并实施严格质量验证，构建了约10,000个潜在任务。

Result: 评估显示SWE-MERA对最新LLM具有强区分能力，并报告了2024年9月至2025年6月收集任务上的性能。

Conclusion: SWE-MERA解决了SWE-bench的局限性，为LLM在软件工程中的评估提供了更可靠的基准。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [187] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: MT4DP是一种基于变形测试的数据投毒攻击检测框架，用于深度学习代码搜索模型，通过语义等效变形关系显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 数据投毒攻击对深度学习代码搜索模型构成严重安全威胁，现有检测方法效果不足。

Method: MT4DP通过识别高频词作为潜在投毒目标，生成语义等效的后续查询，并重新排序代码片段以检测投毒攻击。

Result: 实验显示MT4DP在F1分数和精确率上分别比最佳基线提升191%和265%。

Conclusion: MT4DP有效提升数据投毒攻击检测能力，推动相关安全研究。

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [188] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: 提出了一种基于自动机学习和测试技术的调试方法，通过失败解释（FE）、最终失败解释（EFE）和早期检测（ED）来生成简洁且信息丰富的错误描述。


<details>
  <summary>Details</summary>
Motivation: 调试复杂系统耗时且困难，需要更高效的方法来生成有意义的错误总结。

Method: 利用自动机学习和测试技术，提取关键测试模式并过滤无关信息。

Result: 在多种测试模式和实际基准测试中验证了方法的有效性，能生成紧凑且信息丰富的错误描述。

Conclusion: 该方法显著提升了错误检测和理解的效率，适用于复杂系统的调试。

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [189] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 提出了一种基于Fisher精确检验的统计突变杀死标准，解决了DeepCrime违反单调性的问题。


<details>
  <summary>Details</summary>
Motivation: DeepCrime的统计突变杀死标准存在违反单调性的问题，即扩大测试集可能导致先前杀死的突变不再被分类为杀死。

Method: 采用Fisher精确检验重新定义统计突变杀死标准，保持统计严谨性的同时确保单调性。

Result: 新方法在保持统计严谨性的同时解决了单调性问题。

Conclusion: 基于Fisher精确检验的新方法是一种更可靠的统计突变杀死标准。

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [190] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: MARAUS是一个结合多代理和检索增强的大学招生系统，通过实际部署在越南高等教育招生咨询中，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在自动化咨询任务中多为原型或合成基准，MARAUS旨在填补这一空白，为实际招生场景提供解决方案。

Method: 结合混合检索、多代理协调和基于LLM的生成，开发了一个针对实际招生需求的系统，并与越南运输技术大学合作进行技术开发和实际评估。

Result: 系统处理了6000多次用户交互，平均准确率达92%，幻觉率从15%降至1.45%，响应时间低于4秒，成本效益高。

Conclusion: MARAUS为低资源教育环境中部署代理式RAG系统提供了实用见解。

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [191] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: 研究探索了使用基础模型（如Phi4-14B、Claude 3.5 Sonnet等）检测代码重构的可行性，开发了工具RefModel，并在性能和通用性上优于传统静态分析工具。


<details>
  <summary>Details</summary>
Motivation: 传统重构检测工具依赖复杂规则和静态分析，难以扩展和通用化，因此研究探索了基础模型的替代方案。

Method: 使用多种基础模型（如Phi4-14B、Claude 3.5 Sonnet等）在人工生成的Java程序和真实开源项目数据集上进行评估，并与传统工具（如RefactoringMiner、RefDiff等）对比。

Result: RefModel在性能上与传统工具相当甚至更优，Claude 3.5 Sonnet和Gemini 2.5 Pro在真实场景中识别了97%的重构，且能泛化到Python和Golang。

Conclusion: 基础模型在重构检测中具有潜力，性能优越且易于扩展，为未来工具开发提供了新方向。

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [192] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: 论文通过定性实证研究探讨了软件从业者对安全债务（SDs）的认知、管理和沟通方式，强调了在软件开发周期中加强安全实践的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件和自动化的普及，时间、资源限制以及对功能的优先考虑导致不安全编码实践，形成安全债务（SDs）。缺乏实证研究了解从业者如何应对SDs。

Method: 通过半结构化访谈22名来自不同角色、组织和国家的软件从业者，研究其对SDs的认知、行为、工具策略及风险沟通。

Result: 发现从业者对SDs的认知和管理存在差异，部分优先交付速度而非安全。需加强安全实践、平衡资源与安全任务。

Conclusion: 强调在SDLC中整合安全实践，更一致地使用缓解策略，并关注CIA三要素。

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [193] [Forecasting NYC Yellow Taxi Ridership Decline: A Time Series Analysis of Daily Passenger Counts (2017-2019)](https://arxiv.org/abs/2507.10588)
*Gaurav Singh*

Main category: econ.GN

TL;DR: 研究分析了2017-2019年纽约市黄色出租车的每日乘客量，使用时间序列模型预测乘客量，发现线性下降趋势和季节性模式，最佳模型为自回归模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解和预测纽约市黄色出租车乘客量的下降趋势，为政策制定者和利益相关者提供决策依据。

Method: 使用纽约市出租车和豪华轿车委员会的数据集，采用ARIMA等时间序列模型，结合去趋势和周期去除技术。

Result: 研究发现每日乘客量线性下降约200人，最佳模型的测试RMSE为34,880（日均乘客量438,000）。

Conclusion: 研究为理解纽约市黄色出租车乘客量下降提供了重要见解，并展示了自回归模型在预测中的有效性。

Abstract: This study analyzes and forecasts daily passenger counts for New York City's
iconic yellow taxis during 2017-2019, a period of significant decline in
ridership. Using a comprehensive dataset from the NYC Taxi and Limousine
Commission, we employ various time series modeling approaches, including ARIMA
models, to predict daily passenger volumes. Our analysis reveals strong
seasonal patterns, with a consistent linear decline of approximately 200
passengers per day throughout the study period. After comparing multiple
modeling approaches, we find that a first-order autoregressive model, combined
with careful detrending and cycle removal, provides the most accurate
predictions, achieving a test RMSE of 34,880 passengers on a mean ridership of
438,000 daily passengers. The research provides valuable insights for
policymakers and stakeholders in understanding and potentially addressing the
declining trajectory of NYC's yellow taxi service.

</details>


### [194] [Artificial Finance: How AI Thinks About Money](https://arxiv.org/abs/2507.10933)
*Orhan Erdem,Ragavi Pobbathi Ashok*

Main category: econ.GN

TL;DR: 论文比较了大型语言模型（LLMs）与全球人类参与者在金融决策问题上的表现，发现LLMs倾向于风险中性决策，偶尔出现与规范推理不一致的行为，且其响应与坦桑尼亚参与者最相似。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在金融决策中如何模拟人类行为，揭示其潜在的文化和训练影响。

Method: 向七个领先的LLMs（如GPT系列、Gemini 2.0 Flash等）提出金融决策问题，并与来自53个国家的人类响应进行比较。

Result: LLMs表现出风险中性决策模式，偶尔与规范推理不一致，且其响应与坦桑尼亚参与者最相似。

Conclusion: 研究揭示了LLMs模拟人类决策行为的方式，并强调了其输出中潜在的文化和训练影响。

Abstract: In this paper, we explore how large language models (LLMs) approach financial
decision-making by systematically comparing their responses to those of human
participants across the globe. We posed a set of commonly used financial
decision-making questions to seven leading LLMs, including five models from the
GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We
then compared their outputs to human responses drawn from a dataset covering 53
nations. Our analysis reveals three main results. First, LLMs generally exhibit
a risk-neutral decision-making pattern, favoring choices aligned with expected
value calculations when faced with lottery-type questions. Second, when
evaluating trade-offs between present and future, LLMs occasionally produce
responses that appear inconsistent with normative reasoning. Third, when we
examine cross-national similarities, we find that the LLMs' aggregate responses
most closely resemble those of participants from Tanzania. These findings
contribute to the understanding of how LLMs emulate human-like decision
behaviors and highlight potential cultural and training influences embedded
within their outputs.

</details>


### [195] [Propagation of carbon price shocks through the value chain: the mean-field game of defaults](https://arxiv.org/abs/2507.11353)
*Zorana Grbac,Simone Pavarana,Thorsten Schmidt,Peter Tankov*

Main category: econ.GN

TL;DR: 论文提出了一种新的平均场博弈框架，用于分析碳定价在多部门经济中的影响，重点关注可违约企业。


<details>
  <summary>Details</summary>
Motivation: 研究碳定价对多部门经济的影响，特别是企业如何通过调整投入和违约时机来应对碳价格变化。

Method: 采用最优停止平均场博弈模型，通过线性规划方法描述纳什均衡，并证明其存在性和价格系统的唯一性。

Result: 数值模拟显示，碳价格冲击会导致排放与劳动力之间的替代效应，并在多部门经济中产生显著的溢出效应。

Conclusion: 碳价格冲击对价值链有重要影响，部门间的相互依赖性是制定有效脱碳路径的关键。

Abstract: We introduce a new mean-field game framework to analyze the impact of carbon
pricing in a multi-sector economy with defaultable firms. Each sector produces
a homogeneous good, with its price endogenously determined through market
clearing. Firms act as price takers and maximize profits by choosing an optimal
allocation of inputs-including labor, emissions, and intermediate goods from
other sectors-while interacting through the endogenous sectoral price. Firms
also choose their default timing to maximize shareholder value.
  Formally, we model the economy as an optimal stopping mean-field game within
each sector. The resulting system of coupled mean-field games admits a linear
programming formulation that characterizes Nash equilibria in terms of
population measure flows. We prove the existence of a linear programming Nash
equilibrium and establish uniqueness of the associated price system.
  Numerical illustrations are presented for firms with constant elasticity of
substitution (CES) production functions. In a stylized single-sector economy,
carbon price shocks induce substitution between emissions and labor. In a
three-sector economy, the manufacturing sector faces consumer demand and
requires inputs from a brown sector, which can be increasingly replaced by
green-sector goods as carbon prices rise. These experiments reveal that carbon
price shocks can generate substantial spillover effects along the value chain,
underscoring the importance of sectoral interdependencies in shaping effective
decarbonization pathways.

</details>


### [196] [Adaptive Robust Optimization for European Electricity System Planning Considering Regional Dunkelflaute Events](https://arxiv.org/abs/2507.11361)
*Maximilian Bernecker,Smaranda Sgarciu,Xiaoming Kan,Mehrnaz Anvari,Iegor Riepin,Felix Müsgens*

Main category: econ.GN

TL;DR: 研究开发了一个基于自适应鲁棒优化（ARO）的欧洲电力系统脱碳容量扩展模型，分析了极端天气对系统成本和技术选择的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决欧洲电力系统在完全脱碳背景下，如何应对极端天气事件（如Dunkelflaute）带来的挑战。

Method: 采用自适应鲁棒优化（ARO）框架，内生识别最坏区域Dunkelflaute事件，并整合多种极端天气情景。

Result: 结果显示系统成本随极端天气范围非线性增加，最大成本增加71%。技术选择随天气压力变化，大规模中断需氢储能和负荷削减。

Conclusion: 结论强调需要欧洲协调政策，支持跨境基础设施投资和灵活技术部署，以缓解Dunkelflaute事件的系统性风险。

Abstract: This study develops a capacity expansion model for a fully decarbonized
European electricity system using an Adaptive Robust Optimization (ARO)
framework. The model endogenously identifies the worst regional Dunkelflaute
events, prolonged periods of low wind and solar availability, and incorporates
multiple extreme weather realizations within a single optimization run. Results
show that system costs rise nonlinearly with the geographic extent of these
events: a single worst-case regional disruption increases costs by 9%, but
broader disruptions across multiple regions lead to much sharper increases, up
to 51%. As Dunkelflaute conditions extend across most of Europe, additional
cost impacts level off, with a maximum increase of 71%. The optimal technology
mix evolves with the severity of weather stress: while renewables, batteries,
and interregional transmission are sufficient to manage localized events,
large-scale disruptions require long-term hydrogen storage and load shedding to
maintain system resilience. Central European regions, especially Germany and
France, emerge as systemic bottlenecks, while peripheral regions bear the cost
of compensatory overbuilding. These findings underscore the need for a
coordinated European policy strategy that goes beyond national planning to
support cross-border infrastructure investment, scale up flexible technologies
such as long-duration storage, and promote a geographically balanced deployment
of renewables to mitigate systemic risks associated with Dunkelflaute events.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [197] [Incentivizing Knowledge Transfers](https://arxiv.org/abs/2507.11018)
*Zhonghong Kuang,Yi Liu,Dong Wei*

Main category: econ.TH

TL;DR: 研究如何设计关系合同以激励专家向新手分享专业知识，通过第三方主导的资源分配实现知识转移。


<details>
  <summary>Details</summary>
Motivation: 专家担心知识转移会削弱其未来收益，而第三方主导者希望通过合同促进知识共享。

Method: 设计利润最大化的合同，专家初期免费培训新手，随后逐步补偿其未来损失。

Result: 知识转移逐步进行，但长期可能无法完全实现。

Conclusion: 模型扩展到代际重叠情景，考虑了专家退休和新人职业发展。

Abstract: We study the optimal design of relational contracts that incentivize an
expert to share specialized knowledge with a novice. While the expert fears
that a more knowledgeable novice may later erode his future rents, a
third-party principal is willing to allocate her resources to facilitate
knowledge transfer. In the unique profit-maximizing contract between the
principal and the expert, the expert is asked to train the novice as much as
possible, for free, in the initial period; knowledge transfers then proceed
gradually and perpetually, with the principal always compensating the expert
for his future losses immediately upon verifying the training he provided; even
in the long run, a complete knowledge transfer might not be attainable. We
further extend our analysis to an overlapping-generation model, accounting for
the retirement of experts and the career progression of novices.

</details>


### [198] [Value of History in Social Learning: Applications to Markets for History](https://arxiv.org/abs/2507.11029)
*Hiroto Sato,Konan Shimizu*

Main category: econ.TH

TL;DR: 论文研究了社交学习环境中历史信息的价值，分析了最大化历史价值的信息结构，并将其应用于历史市场的垄断定价模型。


<details>
  <summary>Details</summary>
Motivation: 探讨在社交学习中，历史信息对个体决策的价值，以及如何设计信息结构以最大化这种价值。

Method: 通过定义历史信息的价值，分析其最大化条件，并构建垄断数据卖方的市场模型，研究其定价策略和信息设计。

Result: 发现历史信息的价值在完全信息与无信息混合时最高；垄断卖方会设计低于社会最优水平的信息披露。

Conclusion: 垄断数据卖方倾向于减少信息披露以增加历史信息的价值，导致社会次优结果。

Abstract: In social learning environments, agents acquire information from both private
signals and the observed actions of predecessors, referred to as history. We
define the value of history as the gain in expected payoff from accessing both
the private signal and history, compared to relying on the signal alone. We
first characterize the information structures that maximize this value, showing
that it is highest under a mixture of full information and no information. We
then apply these insights to a model of markets for history, where a
monopolistic data seller collects and sells access to history. In equilibrium,
the seller's dynamic pricing becomes the value of history for each agent. This
gives the seller incentives to increase the value of history by designing the
information structure. The seller optimal information discloses less
information than the socially optimal level.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [199] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 论文提出了一种名为CLS-DM的模型，通过跨模态特征对比学习，解决了2D X射线图像与3D CT图像在潜在空间对齐的问题，提升了稀疏视图CT重建的效果。


<details>
  <summary>Details</summary>
Motivation: 传统CT成像存在时间消耗大和辐射暴露高的问题，稀疏视图CT重建方法可以降低成本与风险，但现有方法在潜在空间对齐上效果不佳。

Method: 提出了CLS-DM模型，利用跨模态特征对比学习，从2D X射线图像中提取3D潜在信息，并实现模态间的潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM指标上优于经典和最新生成模型。

Conclusion: CLS-DM不仅提升了稀疏X射线重建CT的效果和经济性，还可推广至其他跨模态转换任务，如文本到图像合成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [200] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 论文提出了一种名为3D MIR的新方法，结合深度学习和物理约束，用于从磁场图像中恢复半导体封装中的3D电流信息。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，准确恢复3D信息对于无损检测和电路缺陷定位至关重要。

Method: 3D MIR分为三个阶段：1) CNN处理磁场图像预测参数；2) 利用空间物理约束提供初始估计；3) 优化器调整参数以最小化重建与实际磁场的差异。

Result: 3D MIR方法能够高精度恢复3D信息，为半导体封装中的磁场图像重建设定了新标准。

Conclusion: 该方法展示了深度学习和物理驱动优化在实际应用中的潜力。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [201] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net是一种新型肝脏和肿瘤分割框架，结合双曲卷积、多尺度纹理学习和生物启发的可塑性机制，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决腹部CT图像中肝脏和肿瘤分割的挑战，如复杂解剖结构、肿瘤外观多变和标注数据有限。

Method: 结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性感知和轻量级时间注意力。

Result: 在LiTS数据集上Dice分数93.26%，IoU 88.09%；在3D-IRCADb-01数据集上Dice 87.45%，IoU 80.30%。

Conclusion: HANS-Net在肝脏和肿瘤分割中表现出高效、鲁棒和泛化能力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>
