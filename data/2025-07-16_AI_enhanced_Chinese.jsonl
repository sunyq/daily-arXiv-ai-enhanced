{"id": "2507.10577", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.10577", "abs": "https://arxiv.org/abs/2507.10577", "authors": ["Log\u00e9 C\u00e9cile", "Ghori Rehan"], "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "comment": null, "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u4ee3\u7406\uff08Truth Sleuth\u548cTrend Bender\uff09\u6765\u68c0\u6d4bYouTube\u89c6\u9891\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\u5e76\u4e3b\u52a8\u5728\u8bc4\u8bba\u533a\u8fdb\u884c\u5e72\u9884\u3002", "motivation": "\u9519\u8bef\u4fe1\u606f\u5728\u6570\u5b57\u4e16\u754c\u4e2d\u5feb\u901f\u4f20\u64ad\uff0c\u5bf9\u516c\u4f17\u8ba4\u77e5\u9020\u6210\u5a01\u80c1\uff0c\u9700\u8981\u6709\u6548\u624b\u6bb5\u8fdb\u884c\u5e72\u9884\u3002", "method": "\u7cfb\u7edf\u91c7\u7528RAG\u65b9\u6cd5\uff08Truth Sleuth\uff09\u63d0\u53d6\u5e76\u9a8c\u8bc1\u89c6\u9891\u4e2d\u7684\u58f0\u660e\uff0cTrend Bender\u5219\u751f\u6210\u6709\u8bf4\u670d\u529b\u7684\u8bc4\u8bba\u4ee5\u5f15\u5bfc\u8ba8\u8bba\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u5728\u4e8b\u5b9e\u6838\u67e5\u548c\u7528\u6237\u4e92\u52a8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u5f71\u54cd\u89c2\u70b9\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u5e72\u9884\u63aa\u65bd\u5728\u6253\u51fb\u9519\u8bef\u4fe1\u606f\u548c\u4fc3\u8fdb\u5728\u7ebf\u7a7a\u95f4\u7406\u6027\u8ba8\u8bba\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.10580", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10580", "abs": "https://arxiv.org/abs/2507.10580", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.", "AI": {"tldr": "EmoSApp\u662f\u4e00\u6b3e\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u79bb\u7ebf\u5bf9\u8bdd\u5e94\u7528\uff0c\u5229\u7528\u91cf\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u5fc3\u7406\u5065\u5eb7\u63d0\u4f9b\u652f\u6301\uff0c\u89e3\u51b3\u4e86\u7528\u6237\u53ef\u8bbf\u95ee\u6027\u3001\u7f51\u7edc\u8fde\u63a5\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u5b58\u5728\u7528\u6237\u53ef\u8bbf\u95ee\u6027\u3001\u7f51\u7edc\u8fde\u63a5\u548c\u6570\u636e\u9690\u79c1\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u79bb\u7ebf\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u3001\u91cf\u5316\u7684LLaMA-3.2-1B-Instruct\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u7684\u5fc3\u7406\u5065\u5eb7\u77e5\u8bc6\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86EmoSApp\u3002", "result": "\u5b9a\u6027\u8bc4\u4f30\u663e\u793aEmoSApp\u80fd\u63d0\u4f9b\u8fde\u8d2f\u3001\u5171\u60c5\u7684\u5bf9\u8bdd\u548c\u5b9e\u7528\u5efa\u8bae\uff1b\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EmoSApp\u4e3a\u4fbf\u643a\u3001\u5b89\u5168\u3001\u5b9a\u5236\u5316\u7684AI\u5fc3\u7406\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002"}}
{"id": "2507.10582", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.10582", "abs": "https://arxiv.org/abs/2507.10582", "authors": ["Anders Ledberg", "Anna Thal\u00e9n"], "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis", "comment": null, "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u5de5\u5177\u94fe\uff0c\u7528\u4e8e\u5904\u7406\u6cd5\u5f8b\u3001\u533b\u7597\u548c\u884c\u653f\u6587\u672c\u6570\u636e\uff0c\u89e3\u51b3\u9690\u79c1\u548c\u5f02\u6784\u6027\u95ee\u9898\uff0c\u652f\u6301\u672c\u5730\u786c\u4ef6\u8fd0\u884c\u7684\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u5229\u7528\u672a\u7ed3\u6784\u5316\u7684\u6cd5\u5f8b\u3001\u533b\u7597\u548c\u884c\u653f\u6587\u672c\u8fdb\u884c\u7814\u7a76\uff0c\u4f46\u9762\u4e34\u9690\u79c1\u548c\u8bed\u8a00\u5f02\u6784\u6027\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6587\u672c\u6807\u51c6\u5316\u3001\u6458\u8981\u548c\u7ffb\u8bd1\uff0c\u7ed3\u5408\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u8fdb\u884c\u533f\u540d\u5316\u3002", "result": "\u5728\u745e\u5178\u6cd5\u9662\u5224\u51b3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5de5\u5177\u94fe\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u533f\u540d\u5316\u5e76\u4fdd\u7559\u8bed\u4e49\u5185\u5bb9\u3002", "conclusion": "\u8be5\u5de5\u5177\u94fe\u4e3a\u9690\u79c1\u654f\u611f\u7684\u5927\u89c4\u6a21\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2507.10585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10585", "abs": "https://arxiv.org/abs/2507.10585", "authors": ["Isar Nejadgholi", "Mona Omidyeganeh", "Marc-Antoine Drouin", "Jonathan Boisvert"], "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations", "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2\n  figures", "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff08NLEs\uff09\u7684\u66f4\u65b0\u7248XAI\u5206\u7c7b\u6cd5\uff0c\u4ee5\u652f\u6301\u900f\u660eAI\u7cfb\u7edf\u7684\u6cbb\u7406\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0cNLEs\u6210\u4e3a\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u7684\u5173\u952e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u7279\u6027\u548c\u6cbb\u7406\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6587\u732e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9\u63d0\u793a\u578bNLEs\u7684\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4e0a\u4e0b\u6587\u3001\u751f\u6210\u4e0e\u5c55\u793a\u3001\u8bc4\u4f30\u3002", "result": "\u5206\u7c7b\u6cd5\u4e3a\u7814\u7a76\u8005\u3001\u5ba1\u8ba1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u63cf\u8ff0\u3001\u8bbe\u8ba1\u548c\u6539\u8fdbNLEs\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u6cbb\u7406\u6548\u679c\u3002"}}
{"id": "2507.10629", "categories": ["cs.DB", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.10629", "abs": "https://arxiv.org/abs/2507.10629", "authors": ["Song Cheng", "Qiannan Cheng", "Linbo Jin", "Lei Yi", "Guannan Zhang"], "title": "SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition", "comment": "WWW '25: Companion Proceedings of the ACM on Web Conference 2025\n  Pages 919 - 923 https://doi.org/10.1145/3701716.3715541", "summary": "Transforming natural language into SQL queries (NL2SQL) is crucial for\ndata-driven business applications. Existing frameworks, trained on open-source\ndatasets, struggle with complex business logic and lack domain-specific data\nfor fine-tuning. Additionally, evaluation methods often require annotated data\nand executable database environments, which are scarce in real-world scenarios.\nTo address these challenges, we propose SQLord, an enterprise-level NL2SQL\nframework. First, SQLord introduces a data reverse generation approach to\nconvert raw SQL statements into annotated data for supervised fine-tuning\n(SFT). Second, it proposes a decomposition method for complex queries using an\nautomated workflow generator. Additionally, SQLord features a comprehensive\nGPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL\nEvaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.\nOffline tests significantly outperform state of the art baselines, and online\naccuracy consistently exceeds 90, highlighting SQLord's advantages and\neffectiveness in complex real world scenarios. SQLord has been successfully\napplied across multiple scenarios on the world's largest B2B e-commerce\nplatform.", "AI": {"tldr": "SQLord\u662f\u4e00\u4e2a\u4f01\u4e1a\u7ea7NL2SQL\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u53cd\u5411\u751f\u6210\u548c\u590d\u6742\u67e5\u8be2\u5206\u89e3\u65b9\u6cd5\uff0c\u7ed3\u5408GPT-Judge\u8bc4\u4f30\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4e1a\u52a1\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709NL2SQL\u6846\u67b6\u5728\u590d\u6742\u4e1a\u52a1\u903b\u8f91\u548c\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u7a00\u7f3a\u7684\u6807\u6ce8\u6570\u636e\u548c\u6570\u636e\u5e93\u73af\u5883\u3002", "method": "SQLord\u91c7\u7528\u6570\u636e\u53cd\u5411\u751f\u6210\u65b9\u6cd5\u751f\u6210\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5206\u89e3\u590d\u6742\u67e5\u8be2\uff0c\u540c\u65f6\u63d0\u51faGPT-Judge\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u79bb\u7ebf\u6d4b\u8bd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5728\u7ebf\u51c6\u786e\u7387\u6301\u7eed\u8d85\u8fc790%\uff0c\u5e76\u5728\u5168\u7403\u6700\u5927B2B\u7535\u5546\u5e73\u53f0\u6210\u529f\u5e94\u7528\u3002", "conclusion": "SQLord\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.10757", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.10757", "abs": "https://arxiv.org/abs/2507.10757", "authors": ["Ryan Zarick", "Isaac Zhang", "Daniel Wong", "Thomas Kim", "Bryan Pellegrino", "Mignon Li", "Kelvin Wong"], "title": "FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block", "comment": null, "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.", "AI": {"tldr": "FAFO\u662f\u4e00\u79cd\u533a\u5757\u94fe\u4ea4\u6613\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u4ea4\u6613\u4ee5\u63d0\u9ad8\u5e76\u884c\u6027\uff0c\u663e\u8457\u63d0\u5347\u6267\u884c\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u533a\u5757\u94fe\u6267\u884c\u541e\u5410\u91cf\u53d7\u6570\u636e\u7ade\u4e89\u9650\u5236\uff0cFAFO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528CPU\u4f18\u5316\u7684Bloom\u8fc7\u6ee4\u5668\u68c0\u6d4b\u51b2\u7a81\uff0c\u5e76\u884c\u8c03\u5ea6\u4ea4\u6613\u6267\u884c\u3002", "result": "\u5355\u8282\u70b9\u5b9e\u73b0\u6bcf\u79d2110\u4e07\u6b21ETH\u8f6c\u8d26\u548c50\u4e07\u6b21ERC20\u8f6c\u8d26\uff0c\u6210\u672c\u964d\u4f4e91%\u3002", "conclusion": "FAFO\u8bc1\u660e\u901a\u8fc7\u4f18\u5316\u6267\u884c\u5c42\u548c\u4ea4\u6613\u8c03\u5ea6\u8bbe\u8ba1\uff0c\u53ef\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u652f\u6301\u672a\u6765\u53bb\u4e2d\u5fc3\u5316\u5e94\u7528\u3002"}}
{"id": "2507.11018", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2507.11018", "abs": "https://arxiv.org/abs/2507.11018", "authors": ["Zhonghong Kuang", "Yi Liu", "Dong Wei"], "title": "Incentivizing Knowledge Transfers", "comment": null, "summary": "We study the optimal design of relational contracts that incentivize an\nexpert to share specialized knowledge with a novice. While the expert fears\nthat a more knowledgeable novice may later erode his future rents, a\nthird-party principal is willing to allocate her resources to facilitate\nknowledge transfer. In the unique profit-maximizing contract between the\nprincipal and the expert, the expert is asked to train the novice as much as\npossible, for free, in the initial period; knowledge transfers then proceed\ngradually and perpetually, with the principal always compensating the expert\nfor his future losses immediately upon verifying the training he provided; even\nin the long run, a complete knowledge transfer might not be attainable. We\nfurther extend our analysis to an overlapping-generation model, accounting for\nthe retirement of experts and the career progression of novices.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1\u5173\u7cfb\u5408\u540c\u4ee5\u6fc0\u52b1\u4e13\u5bb6\u5411\u65b0\u624b\u5206\u4eab\u4e13\u4e1a\u77e5\u8bc6\uff0c\u901a\u8fc7\u7b2c\u4e09\u65b9\u4e3b\u5bfc\u7684\u8d44\u6e90\u5206\u914d\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u4e13\u5bb6\u62c5\u5fc3\u77e5\u8bc6\u8f6c\u79fb\u4f1a\u524a\u5f31\u5176\u672a\u6765\u6536\u76ca\uff0c\u800c\u7b2c\u4e09\u65b9\u4e3b\u5bfc\u8005\u5e0c\u671b\u901a\u8fc7\u5408\u540c\u4fc3\u8fdb\u77e5\u8bc6\u5171\u4eab\u3002", "method": "\u8bbe\u8ba1\u5229\u6da6\u6700\u5927\u5316\u7684\u5408\u540c\uff0c\u4e13\u5bb6\u521d\u671f\u514d\u8d39\u57f9\u8bad\u65b0\u624b\uff0c\u968f\u540e\u9010\u6b65\u8865\u507f\u5176\u672a\u6765\u635f\u5931\u3002", "result": "\u77e5\u8bc6\u8f6c\u79fb\u9010\u6b65\u8fdb\u884c\uff0c\u4f46\u957f\u671f\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u5b9e\u73b0\u3002", "conclusion": "\u6a21\u578b\u6269\u5c55\u5230\u4ee3\u9645\u91cd\u53e0\u60c5\u666f\uff0c\u8003\u8651\u4e86\u4e13\u5bb6\u9000\u4f11\u548c\u65b0\u4eba\u804c\u4e1a\u53d1\u5c55\u3002"}}
{"id": "2507.10903", "categories": ["cs.NI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10903", "abs": "https://arxiv.org/abs/2507.10903", "authors": ["Parisa Fard Moshiri", "Xinyu Zhu", "Poonam Lohan", "Burak Kantarci", "Emil Janulewicz"], "title": "LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning", "comment": "9 pages, 6 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "summary": "Effective management of Service Function Chains (SFCs) and optimal Virtual\nNetwork Function (VNF) placement are critical challenges in modern\nSoftware-Defined Networking (SDN) and Network Function Virtualization (NFV)\nenvironments. Although Deep Reinforcement Learning (DRL) is widely adopted for\ndynamic network decision-making, its inherent dependency on structured data and\nfixed action rules often limits adaptability and responsiveness, particularly\nunder unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a\nnovel approach combining Lightweight Language Model (LiLM) with Relational\nDatabase (RDB) to answer network state queries to guide DRL model for efficient\nSFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and\nAuto-Regressive Transformers (BART) and the Fine-tuned Language Net T5\n(FLAN-T5), to interpret network data and support diverse query types related to\nSFC demands, data center resources, and VNF availability. Results demonstrate\nthat FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to\n0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time\n(2h 2min compared to 2h 38min). Moreover, when compared to the large language\nmodel SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting\nprocessing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLiLM-RDB-SFC\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff08LiLM\uff09\u548c\u5173\u7cfb\u6570\u636e\u5e93\uff08RDB\uff09\uff0c\u901a\u8fc7\u6307\u5bfc\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6a21\u578b\u4f18\u5316\u670d\u52a1\u529f\u80fd\u94fe\uff08SFC\uff09\u914d\u7f6e\u3002FLAN-T5\u6a21\u578b\u8868\u73b0\u4f18\u4e8eBART\u548cSQLCoder\u3002", "motivation": "\u73b0\u4ee3SDN\u548cNFV\u73af\u5883\u4e2d\uff0cSFC\u7ba1\u7406\u548cVNF\u914d\u7f6e\u662f\u91cd\u8981\u6311\u6218\uff0c\u4f20\u7edfDRL\u65b9\u6cd5\u56e0\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\u548c\u56fa\u5b9a\u89c4\u5219\uff0c\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408LiLM\uff08BART\u548cFLAN-T5\uff09\u4e0eRDB\uff0c\u89e3\u6790\u7f51\u7edc\u72b6\u6001\u67e5\u8be2\uff0c\u6307\u5bfcDRL\u6a21\u578b\u8fdb\u884cSFC\u914d\u7f6e\u3002", "result": "FLAN-T5\u5728\u6d4b\u8bd5\u635f\u5931\uff080.00161 vs. 0.00734\uff09\u3001\u51c6\u786e\u7387\uff0894.79% vs. 80.2%\uff09\u548c\u5904\u7406\u65f6\u95f4\uff082h 2min vs. 2h 38min\uff09\u4e0a\u4f18\u4e8eBART\uff0c\u4e14\u5904\u7406\u65f6\u95f4\u6bd4SQLCoder\u51cf\u5c1196%\u3002", "conclusion": "LiLM-RDB-SFC\u65b9\u6cd5\u663e\u8457\u63d0\u5347SFC\u914d\u7f6e\u6548\u7387\uff0cFLAN-T5\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.10799", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10799", "abs": "https://arxiv.org/abs/2507.10799", "authors": ["Tyler Hou", "Michael Arntzenius", "Max Willsey"], "title": "Stream programs are monoid homomorphisms with state", "comment": null, "summary": "We define a broad class of deterministic stream functions and show they can\nbe implemented as homomorphisms into a \"state\" monoid. The homomorphism laws\nare simpler than the conditions of previous semantic frameworks for stream\nprogram optimization, yet retain support for rich equational reasoning over\nexpressive dataflow programs, including sequential composition, parallel\ncomposition, and feedback. We demonstrate this using examples of partitioned\ndatabase joins, stratified negation, and a simplified model of TCP.", "AI": {"tldr": "\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u7c7b\u786e\u5b9a\u6027\u6d41\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u4f5c\u4e3a\u72b6\u6001\u5e7a\u534a\u7fa4\u7684\u540c\u6001\u5b9e\u73b0\u3002\u540c\u6001\u5b9a\u5f8b\u6bd4\u4e4b\u524d\u7684\u6d41\u7a0b\u5e8f\u4f18\u5316\u8bed\u4e49\u6846\u67b6\u66f4\u7b80\u5355\uff0c\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u7b49\u5f0f\u63a8\u7406\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u6d41\u7a0b\u5e8f\u4f18\u5316\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u590d\u6742\u6570\u636e\u6d41\u7a0b\u5e8f\uff08\u5982\u5e76\u884c\u7ec4\u5408\u548c\u53cd\u9988\uff09\u7684\u7b49\u5f0f\u63a8\u7406\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5c06\u786e\u5b9a\u6027\u6d41\u51fd\u6570\u5b9e\u73b0\u4e3a\u72b6\u6001\u5e7a\u534a\u7fa4\u7684\u540c\u6001\uff0c\u7b80\u5316\u4e86\u540c\u6001\u5b9a\u5f8b\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5206\u533a\u6570\u636e\u5e93\u8fde\u63a5\u3001\u5206\u5c42\u5426\u5b9a\u548c\u7b80\u5316TCP\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6d41\u7a0b\u5e8f\u4f18\u5316\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e30\u5bcc\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.10562", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10562", "abs": "https://arxiv.org/abs/2507.10562", "authors": ["Hari Masoor"], "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents", "comment": "7 pages, 4 figures, 3 implementation examples. Original work\n  submitted as a preprint", "summary": "Current AI agent architectures suffer from ephemeral memory limitations,\npreventing effective collaboration and knowledge sharing across sessions and\nagent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a\nnovel framework that enables persistent, secure, and semantically searchable\nmemory sharing among AI agents. Our protocol addresses three critical\nchallenges: (1) persistent context preservation across agent sessions, (2)\nsecure multi-agent collaboration with fine-grained access control, and (3)\nefficient semantic discovery of relevant historical context. SAMEP implements a\ndistributed memory repository with vector-based semantic search, cryptographic\naccess controls (AES-256-GCM), and standardized APIs compatible with existing\nagent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness\nacross diverse domains including multi-agent software development, healthcare\nAI with HIPAA compliance, and multi-modal processing pipelines. Experimental\nresults show 73% reduction in redundant computations, 89% improvement in\ncontext relevance scores, and complete compliance with regulatory requirements\nincluding audit trail generation. SAMEP enables a new paradigm of persistent,\ncollaborative AI agent ecosystems while maintaining security and privacy\nguarantees.", "AI": {"tldr": "SAMEP\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u4e45\u3001\u5b89\u5168\u4e14\u53ef\u8bed\u4e49\u641c\u7d22\u7684\u5185\u5b58\u5171\u4eab\u89e3\u51b3AI\u4ee3\u7406\u7684\u77ed\u6682\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u67b6\u6784\u5b58\u5728\u77ed\u6682\u5185\u5b58\u9650\u5236\uff0c\u963b\u788d\u8de8\u4f1a\u8bdd\u548c\u4ee3\u7406\u8fb9\u754c\u7684\u534f\u4f5c\u4e0e\u77e5\u8bc6\u5171\u4eab\u3002", "method": "SAMEP\u91c7\u7528\u5206\u5e03\u5f0f\u5185\u5b58\u5b58\u50a8\u5e93\uff0c\u7ed3\u5408\u5411\u91cf\u8bed\u4e49\u641c\u7d22\u3001\u52a0\u5bc6\u8bbf\u95ee\u63a7\u5236\uff08AES-256-GCM\uff09\u548c\u6807\u51c6\u5316API\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSAMEP\u51cf\u5c11\u4e8673%\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u534789%\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u5e76\u5b8c\u5168\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u3002", "conclusion": "SAMEP\u4e3a\u6301\u4e45\u534f\u4f5c\u7684AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u540c\u65f6\u4fdd\u969c\u5b89\u5168\u4e0e\u9690\u79c1\u3002"}}
{"id": "2507.10583", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.10583", "abs": "https://arxiv.org/abs/2507.10583", "authors": ["Daniil Orel", "Indraneil Paul", "Iryna Gurevych", "Preslav Nakov"], "title": "$\\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection", "comment": null, "summary": "In this work, we compile $\\textbf{$\\texttt{DroidCollection}$}$, the most\nextensive open data suite for training and evaluating machine-generated code\ndetectors, comprising over a million code samples, seven programming languages,\noutputs from 43 coding models, and over three real-world coding domains.\nAlongside fully AI-generated samples, our collection includes human-AI\nco-authored code, as well as adversarial samples explicitly crafted to evade\ndetection. Subsequently, we develop $\\textbf{$\\texttt{DroidDetect}$}$, a suite\nof encoder-only detectors trained using a multi-task objective over\n$\\texttt{DroidCollection}$. Our experiments show that existing detectors'\nperformance fails to generalise to diverse coding domains and programming\nlanguages outside of their narrow training data. Additionally, we demonstrate\nthat while most detectors are easily compromised by humanising the output\ndistributions using superficial prompting and alignment approaches, this\nproblem can be easily amended by training on a small amount of adversarial\ndata. Finally, we demonstrate the effectiveness of metric learning and\nuncertainty-based resampling as means to enhance detector training on possibly\nnoisy distributions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86$\texttt{DroidCollection}$\uff0c\u4e00\u4e2a\u5305\u542b\u767e\u4e07\u4ee3\u7801\u6837\u672c\u3001\u4e03\u79cd\u7f16\u7a0b\u8bed\u8a00\u300143\u79cd\u7f16\u7801\u6a21\u578b\u8f93\u51fa\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u673a\u5668\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u5668\u3002\u540c\u65f6\u5f00\u53d1\u4e86$\texttt{DroidDetect}$\u68c0\u6d4b\u5668\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u68c0\u6d4b\u5668\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u5bf9\u6297\u6570\u636e\u8bad\u7ec3\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u5668\u5728\u591a\u6837\u7f16\u7a0b\u8bed\u8a00\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u6613\u53d7\u5bf9\u6297\u6837\u672c\u653b\u51fb\u3002", "method": "\u6784\u5efa$\texttt{DroidCollection}$\u6570\u636e\u96c6\uff0c\u5f00\u53d1$\texttt{DroidDetect}$\u68c0\u6d4b\u5668\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u76ee\u6807\u8bad\u7ec3\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u5bf9\u6297\u6570\u636e\u548c\u566a\u58f0\u5206\u5e03\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u68c0\u6d4b\u5668\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4f46\u5bf9\u6297\u6570\u636e\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b\u5ea6\u91cf\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u91cd\u91c7\u6837\u6709\u6548\u3002", "conclusion": "$\texttt{DroidCollection}$\u548c$\texttt{DroidDetect}$\u4e3a\u673a\u5668\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u6297\u6570\u636e\u548c\u566a\u58f0\u5904\u7406\u65b9\u6cd5\u53ef\u63d0\u5347\u68c0\u6d4b\u5668\u6027\u80fd\u3002"}}
{"id": "2507.10588", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.10588", "abs": "https://arxiv.org/abs/2507.10588", "authors": ["Gaurav Singh"], "title": "Forecasting NYC Yellow Taxi Ridership Decline: A Time Series Analysis of Daily Passenger Counts (2017-2019)", "comment": null, "summary": "This study analyzes and forecasts daily passenger counts for New York City's\niconic yellow taxis during 2017-2019, a period of significant decline in\nridership. Using a comprehensive dataset from the NYC Taxi and Limousine\nCommission, we employ various time series modeling approaches, including ARIMA\nmodels, to predict daily passenger volumes. Our analysis reveals strong\nseasonal patterns, with a consistent linear decline of approximately 200\npassengers per day throughout the study period. After comparing multiple\nmodeling approaches, we find that a first-order autoregressive model, combined\nwith careful detrending and cycle removal, provides the most accurate\npredictions, achieving a test RMSE of 34,880 passengers on a mean ridership of\n438,000 daily passengers. The research provides valuable insights for\npolicymakers and stakeholders in understanding and potentially addressing the\ndeclining trajectory of NYC's yellow taxi service.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e862017-2019\u5e74\u7ebd\u7ea6\u5e02\u9ec4\u8272\u51fa\u79df\u8f66\u7684\u6bcf\u65e5\u4e58\u5ba2\u91cf\uff0c\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u9884\u6d4b\u4e58\u5ba2\u91cf\uff0c\u53d1\u73b0\u7ebf\u6027\u4e0b\u964d\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u6a21\u5f0f\uff0c\u6700\u4f73\u6a21\u578b\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u548c\u9884\u6d4b\u7ebd\u7ea6\u5e02\u9ec4\u8272\u51fa\u79df\u8f66\u4e58\u5ba2\u91cf\u7684\u4e0b\u964d\u8d8b\u52bf\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u7ebd\u7ea6\u5e02\u51fa\u79df\u8f66\u548c\u8c6a\u534e\u8f7f\u8f66\u59d4\u5458\u4f1a\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528ARIMA\u7b49\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u7ed3\u5408\u53bb\u8d8b\u52bf\u548c\u5468\u671f\u53bb\u9664\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6bcf\u65e5\u4e58\u5ba2\u91cf\u7ebf\u6027\u4e0b\u964d\u7ea6200\u4eba\uff0c\u6700\u4f73\u6a21\u578b\u7684\u6d4b\u8bd5RMSE\u4e3a34,880\uff08\u65e5\u5747\u4e58\u5ba2\u91cf438,000\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u7ebd\u7ea6\u5e02\u9ec4\u8272\u51fa\u79df\u8f66\u4e58\u5ba2\u91cf\u4e0b\u964d\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10689", "abs": "https://arxiv.org/abs/2507.10689", "authors": ["Tongshun Zhang", "Pingping Liu", "Yubing Lu", "Mengen Cai", "Zijian Zhang", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement", "comment": "Accepted by ICCV 2025", "summary": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on\nuniform brightness adjustment, often neglecting instance-level semantic\ninformation and the inherent characteristics of different features. To address\nthese limitations, we propose CWNet (Causal Wavelet Network), a novel\narchitecture that leverages wavelet transforms for causal reasoning.\nSpecifically, our approach comprises two key components: 1) Inspired by the\nconcept of intervention in causality, we adopt a causal reasoning perspective\nto reveal the underlying causal relationships in low-light enhancement. From a\nglobal perspective, we employ a metric learning strategy to ensure causal\nembeddings adhere to causal principles, separating them from non-causal\nconfounding factors while focusing on the invariance of causal factors. At the\nlocal level, we introduce an instance-level CLIP semantic loss to precisely\nmaintain causal factor consistency. 2) Based on our causal analysis, we present\na wavelet transform-based backbone network that effectively optimizes the\nrecovery of frequency information, ensuring precise enhancement tailored to the\nspecific attributes of wavelet transforms. Extensive experiments demonstrate\nthat CWNet significantly outperforms current state-of-the-art methods across\nmultiple datasets, showcasing its robust performance across diverse scenes.\nCode is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.", "AI": {"tldr": "CWNet\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u548c\u56e0\u679c\u63a8\u7406\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u56e0\u679c\u5206\u6790\u63d0\u5347\u8bed\u4e49\u4fe1\u606f\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5ffd\u89c6\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u4fe1\u606f\u548c\u7279\u5f81\u7279\u6027\uff0cCWNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u5c0f\u6ce2\u53d8\u6362\uff0c\u5305\u62ec\u5168\u5c40\u5ea6\u91cf\u5b66\u4e60\u548c\u5c40\u90e8CLIP\u8bed\u4e49\u635f\u5931\uff0c\u4f18\u5316\u9891\u7387\u4fe1\u606f\u6062\u590d\u3002", "result": "CWNet\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "CWNet\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u548c\u5c0f\u6ce2\u53d8\u6362\u6709\u6548\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u3002"}}
{"id": "2507.10586", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10586", "abs": "https://arxiv.org/abs/2507.10586", "authors": ["Kaushik Dwivedi", "Padmanabh Patanjali Mishra"], "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.", "AI": {"tldr": "AutoRAG-LoRA\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\u548cKL\u6b63\u5219\u5316\u8bad\u7ec3\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u63d0\u793a\u91cd\u5199\u3001\u6df7\u5408\u68c0\u7d22\u548c\u4f4e\u79e9\u9002\u914d\u5668\u8c03\u4f18\uff0c\u901a\u8fc7\u5e7b\u89c9\u68c0\u6d4b\u6a21\u5757\u548c\u53cd\u9988\u6821\u6b63\u5faa\u73af\u5b9e\u73b0\u4e8b\u5b9e\u5bf9\u9f50\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e8b\u5b9e\u6f02\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6548\u7387\u548c\u6a21\u5757\u5316\u3002", "conclusion": "AutoRAG-LoRA\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.10897", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10897", "abs": "https://arxiv.org/abs/2507.10897", "authors": ["Sha Wang", "Yuchen Li", "Hanhua Xiao", "Bing Tian Dai", "Roy Ka-Wei Lee", "Yanfei Dong", "Lambert Deng"], "title": "LLMATCH: A Unified Schema Matching Framework with Large Language Models", "comment": "Accepted at APWeb 2025, Schema Matching, LLM, Data Management", "summary": "Schema matching is a foundational task in enterprise data integration, aiming\nto align disparate data sources. While traditional methods handle simple\none-to-one table mappings, they often struggle with complex multi-table schema\nmatching in real-world applications. We present LLMatch, a unified and modular\nschema matching framework. LLMatch decomposes schema matching into three\ndistinct stages: schema preparation, table-candidate selection, and\ncolumn-level alignment, enabling component-level evaluation and future-proof\ncompatibility. It includes a novel two-stage optimization strategy: a Rollup\nmodule that consolidates semantically related columns into higher-order\nconcepts, followed by a Drilldown module that re-expands these concepts for\nfine-grained column mapping. To address the scarcity of complex semantic\nmatching benchmarks, we introduce SchemaNet, a benchmark derived from\nreal-world schema pairs across three enterprise domains, designed to capture\nthe challenges of multi-table schema alignment in practical settings.\nExperiments demonstrate that LLMatch significantly improves matching accuracy\nin complex schema matching settings and substantially boosts engineer\nproductivity in real-world data integration.", "AI": {"tldr": "LLMatch\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u6a21\u5f0f\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5206\u89e3\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6a21\u5f0f\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u5de5\u7a0b\u5e08\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u8868\u6a21\u5f0f\u5339\u914d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LLMatch\u5c06\u6a21\u5f0f\u5339\u914d\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u6a21\u5f0f\u51c6\u5907\u3001\u8868\u5019\u9009\u9009\u62e9\u548c\u5217\u7ea7\u5bf9\u9f50\uff0c\u5e76\u91c7\u7528Rollup\u548cDrilldown\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMatch\u5728\u590d\u6742\u6a21\u5f0f\u5339\u914d\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u5de5\u7a0b\u5e08\u7684\u751f\u4ea7\u529b\u3002", "conclusion": "LLMatch\u4e3a\u590d\u6742\u6a21\u5f0f\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7SchemaNet\u57fa\u51c6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.10789", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10789", "abs": "https://arxiv.org/abs/2507.10789", "authors": ["Aaron Jarmusch", "Nathan Graddon", "Sunita Chandrasekaran"], "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks", "comment": null, "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.", "AI": {"tldr": "\u672c\u6587\u5bf9NVIDIA Blackwell\u67b6\u6784\u8fdb\u884c\u4e86\u5fae\u67b6\u6784\u5206\u6790\uff0c\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5176\u5173\u952e\u5b50\u7cfb\u7edf\uff0c\u5e76\u4e0eHopper\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u8ba1\u7b97\u80fd\u529b\u63d0\u51fa\u4e86\u66f4\u9ad8\u9700\u6c42\uff0cGPU\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u5206\u6790Blackwell\u67b6\u6784\u7684\u6027\u80fd\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u7814\u7a76Blackwell\u67b6\u6784\u7684\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u7f13\u5b58\u884c\u4e3a\u548c\u8c03\u5ea6\u7ec6\u8282\uff0c\u5e76\u4e0eHopper\u67b6\u6784\uff08\u4f7f\u7528GeForce RTX 5080\u548cH100 PCIe\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u63ed\u793a\u4e86Blackwell\u67b6\u6784\u7684\u5173\u952e\u5b50\u7cfb\u7edf\uff08\u5982\u5185\u5b58\u5c42\u6b21\u3001SM\u6267\u884c\u7ba1\u9053\uff09\u53ca\u5176\u6027\u80fd\u6539\u8fdb\u4e0e\u9000\u6b65\uff0c\u540c\u65f6\u5206\u6790\u4e86\u529f\u8017\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u8005\u548c\u6027\u80fd\u5de5\u7a0b\u5e08\u4f18\u5316Blackwell\u5e73\u53f0\u4e0a\u7684\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u4e3aGPU\u67b6\u6784\u7814\u7a76\u8d21\u732e\u4e86\u65b0\u6570\u636e\u3002"}}
{"id": "2507.11029", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2507.11029", "abs": "https://arxiv.org/abs/2507.11029", "authors": ["Hiroto Sato", "Konan Shimizu"], "title": "Value of History in Social Learning: Applications to Markets for History", "comment": null, "summary": "In social learning environments, agents acquire information from both private\nsignals and the observed actions of predecessors, referred to as history. We\ndefine the value of history as the gain in expected payoff from accessing both\nthe private signal and history, compared to relying on the signal alone. We\nfirst characterize the information structures that maximize this value, showing\nthat it is highest under a mixture of full information and no information. We\nthen apply these insights to a model of markets for history, where a\nmonopolistic data seller collects and sells access to history. In equilibrium,\nthe seller's dynamic pricing becomes the value of history for each agent. This\ngives the seller incentives to increase the value of history by designing the\ninformation structure. The seller optimal information discloses less\ninformation than the socially optimal level.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u5b66\u4e60\u73af\u5883\u4e2d\u5386\u53f2\u4fe1\u606f\u7684\u4ef7\u503c\uff0c\u5206\u6790\u4e86\u6700\u5927\u5316\u5386\u53f2\u4ef7\u503c\u7684\u4fe1\u606f\u7ed3\u6784\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5386\u53f2\u5e02\u573a\u7684\u5784\u65ad\u5b9a\u4ef7\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u5728\u793e\u4ea4\u5b66\u4e60\u4e2d\uff0c\u5386\u53f2\u4fe1\u606f\u5bf9\u4e2a\u4f53\u51b3\u7b56\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u4fe1\u606f\u7ed3\u6784\u4ee5\u6700\u5927\u5316\u8fd9\u79cd\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u5386\u53f2\u4fe1\u606f\u7684\u4ef7\u503c\uff0c\u5206\u6790\u5176\u6700\u5927\u5316\u6761\u4ef6\uff0c\u5e76\u6784\u5efa\u5784\u65ad\u6570\u636e\u5356\u65b9\u7684\u5e02\u573a\u6a21\u578b\uff0c\u7814\u7a76\u5176\u5b9a\u4ef7\u7b56\u7565\u548c\u4fe1\u606f\u8bbe\u8ba1\u3002", "result": "\u53d1\u73b0\u5386\u53f2\u4fe1\u606f\u7684\u4ef7\u503c\u5728\u5b8c\u5168\u4fe1\u606f\u4e0e\u65e0\u4fe1\u606f\u6df7\u5408\u65f6\u6700\u9ad8\uff1b\u5784\u65ad\u5356\u65b9\u4f1a\u8bbe\u8ba1\u4f4e\u4e8e\u793e\u4f1a\u6700\u4f18\u6c34\u5e73\u7684\u4fe1\u606f\u62ab\u9732\u3002", "conclusion": "\u5784\u65ad\u6570\u636e\u5356\u65b9\u503e\u5411\u4e8e\u51cf\u5c11\u4fe1\u606f\u62ab\u9732\u4ee5\u589e\u52a0\u5386\u53f2\u4fe1\u606f\u7684\u4ef7\u503c\uff0c\u5bfc\u81f4\u793e\u4f1a\u6b21\u4f18\u7ed3\u679c\u3002"}}
{"id": "2507.10928", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10928", "abs": "https://arxiv.org/abs/2507.10928", "authors": ["Matthew Yang Liu", "Chuang Chen", "Pengcheng Lv", "Hui Guo", "Yanan Zhang", "Cong Wang", "Yusen Li", "Zhenyu Li", "Yu-Chu Tian"], "title": "Arcturus: A Cloud Overlay Network for Global Accelerator with Enhanced Performance and Stability", "comment": null, "summary": "Global Accelerator (GA) services play a vital role in ensuring low-latency,\nhigh-reliability communication for real-time interactive applications. However,\nexisting GA offerings are tightly bound to specific cloud providers, resulting\nin high costs, rigid deployment, and limited flexibility, especially for\nlarge-scale or budget-sensitive deployments. Arcturus is a cloud-native GA\nframework that revisits the design of GA systems by leveraging low-cost,\nheterogeneous cloud resources across multiple providers. Rather than relying on\nfixed, high-end infrastructure, Arcturus dynamically constructs its\nacceleration network and balances performance, stability, and resource\nefficiency. To achieve this, Arcturus introduces a two-plane design: a\nforwarding plane that builds a proxy network with adaptive control, and a\nscheduling plane that coordinates load and routing through lightweight,\nquantitative optimization. Evaluations under millions of RPS show that Arcturus\noutperforms commercial GA services by up to 1.7X in acceleration performance,\nreduces cost by 71%, and maintains over 80% resource efficiency--demonstrating\nefficient use of cloud resources at scale.", "AI": {"tldr": "Arcturus\u662f\u4e00\u4e2a\u4e91\u539f\u751f\u5168\u7403\u52a0\u901f\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u4f4e\u6210\u672c\u3001\u5f02\u6784\u7684\u4e91\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GA\u670d\u52a1\u7684\u9ad8\u6210\u672c\u548c\u7075\u6d3b\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5168\u7403\u52a0\u901f\u5668\u670d\u52a1\u7ed1\u5b9a\u7279\u5b9a\u4e91\u63d0\u4f9b\u5546\uff0c\u5bfc\u81f4\u9ad8\u6210\u672c\u3001\u90e8\u7f72\u4e0d\u7075\u6d3b\uff0cArcturus\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5e73\u9762\u8bbe\u8ba1\uff1a\u8f6c\u53d1\u5e73\u9762\u6784\u5efa\u81ea\u9002\u5e94\u4ee3\u7406\u7f51\u7edc\uff0c\u8c03\u5ea6\u5e73\u9762\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4f18\u5316\u534f\u8c03\u8d1f\u8f7d\u548c\u8def\u7531\u3002", "result": "\u5728\u767e\u4e07RPS\u6d4b\u8bd5\u4e2d\uff0cArcturus\u6027\u80fd\u63d0\u53471.7\u500d\uff0c\u6210\u672c\u964d\u4f4e71%\uff0c\u8d44\u6e90\u6548\u7387\u8d85\u8fc780%\u3002", "conclusion": "Arcturus\u5c55\u793a\u4e86\u5728\u4e91\u8d44\u6e90\u5927\u89c4\u6a21\u4f7f\u7528\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u5168\u7403\u52a0\u901f\u5668\u670d\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11282", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.11282", "abs": "https://arxiv.org/abs/2507.11282", "authors": ["Ren\u00e9 Rydhof Hansen", "Andreas Stenb\u00e6k Larsen", "Aslan Askarov"], "title": "The downgrading semantics of memory safety", "comment": "56 pages, 27 figures", "summary": "Memory safety is traditionally characterized in terms of bad things that\ncannot happen, an approach that is often criticized as unprincipled. Prior work\nsuggest a connection between memory safety and noninterference, but no\nsatisfactory semantic notion of memory safety is currently known.\n  This work proposes a notion of gradual allocator independence that accurately\ncaptures many allocator-specific aspects of memory safety. We consider a\nlow-level language with access to an allocator that provides malloc and free\nprimitives in a flat memory model. Pointers are just integers, and as such it\nis trivial to write memory-unsafe programs. The basic intuition of gradual\nallocator independence is that of noninterference, namely that allocators must\nnot influence program execution. This intuition is refined in two important\nways to account for the allocators running out-of-memory and for programs to\nhave pointer-to-integer casts. The key insight of the definition is to treat\nthese extensions as forms of downgrading and give them satisfactory technical\ntreatment using the state-of-the-art information flow machinery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\u201d\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u51c6\u786e\u6355\u6349\u5185\u5b58\u5b89\u5168\u7684\u5206\u914d\u5668\u7279\u5b9a\u65b9\u9762\u3002\u901a\u8fc7\u4f4e\u7ea7\u522b\u8bed\u8a00\u4e2d\u7684malloc\u548cfree\u64cd\u4f5c\uff0c\u7ed3\u5408\u975e\u5e72\u6270\u6027\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u5b89\u5168\u8bed\u4e49\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684\u5185\u5b58\u5b89\u5168\u5b9a\u4e49\u4ee5\u8d1f\u9762\u4e8b\u4ef6\u4e3a\u57fa\u7840\uff0c\u88ab\u8ba4\u4e3a\u7f3a\u4e4f\u539f\u5219\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u8bed\u4e49\u5b9a\u4e49\u3002", "method": "\u4f7f\u7528\u4f4e\u7ea7\u522b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408malloc\u548cfree\u64cd\u4f5c\uff0c\u901a\u8fc7\u975e\u5e72\u6270\u6027\u5b9a\u4e49\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\uff0c\u5e76\u5904\u7406\u5185\u5b58\u4e0d\u8db3\u548c\u6307\u9488\u5230\u6574\u6570\u7684\u8f6c\u6362\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5185\u5b58\u5b89\u5168\u5b9a\u4e49\uff0c\u80fd\u591f\u5904\u7406\u5206\u914d\u5668\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u6d41\u6280\u672f\u89e3\u51b3\u4e86\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\u4e3a\u5185\u5b58\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u8bed\u4e49\u5b9a\u4e49\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.10566", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA", "cs.NE", "68T07, 68T40, 91A20", "I.2.6; I.2.11; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10566", "abs": "https://arxiv.org/abs/2507.10566", "authors": ["Hung Ming Liu"], "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems", "comment": "30 pages, 4 figures", "summary": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development\nof Emergent Communication has long been constrained by the ``Joint Exploration\nDilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .\nTraditional methods address this by introducing inductive biases to facilitate\ncommunication emergence . This study fundamentally questions whether such\nartificial inductive biases are, in fact, over-engineering. Through experiments\nwith the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized\nVariational Autoencoder (VQ-VAE), we demonstrate that when agents possess an\nendogenous symbol system, their neural representations naturally exhibit\nspontaneous semantic compression and Nash equilibrium-driven semantic\nconvergence, achieving effective symbolic communication without external\ninductive biases. This aligns with recent neuroscience findings suggesting that\nthe human brain does not directly use human language for internal thought , and\nresonates with research on ``soft thinking'' capabilities in Large Language\nModels (LLMs) . Compared to traditional explicit communication methods, AIM\ndemonstrates stronger generality and efficiency. The interpretable analysis\ntoolkit developed in this study confirms that symbol usage exhibits a\nsignificant power-law distribution, leading to three major theoretical\ninsights: the ``Neural Communication Hypothesis'', the ``Tool-First\nPrinciple'', and the ``Semantic Interpretability Paradigm''. Future research\nwill explore the integration of Hierarchical Quantized Variational Autoencoders\n(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the\npotential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This\ndiscovery offers new avenues for bridging symbolism and connectionism.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVQ-VAE\u7684\"AI\u6bcd\u8bed\"\uff08AIM\uff09\u6846\u67b6\uff0c\u8bc1\u660e\u65e0\u9700\u5916\u90e8\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ee3\u7406\u7684\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u53ef\u5b9e\u73b0\u81ea\u7136\u8bed\u4e49\u538b\u7f29\u548c\u7eb3\u4ec0\u5747\u8861\u9a71\u52a8\u7684\u8bed\u4e49\u6536\u655b\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7b26\u53f7\u901a\u4fe1\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\"\u8054\u5408\u63a2\u7d22\u56f0\u5883\"\u5bfc\u81f4\u7684\"\u901a\u4fe1\u771f\u7a7a\u5747\u8861\"\u95ee\u9898\uff0c\u8d28\u7591\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4eba\u5de5\u5f52\u7eb3\u504f\u7f6e\u7684\u5fc5\u8981\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eVQ-VAE\u7684AIM\u6846\u67b6\uff0c\u7814\u7a76\u4ee3\u7406\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u7684\u81ea\u53d1\u8bed\u4e49\u538b\u7f29\u548c\u8bed\u4e49\u6536\u655b\u3002", "result": "AIM\u6846\u67b6\u5728\u65e0\u9700\u5916\u90e8\u504f\u7f6e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u901a\u4fe1\uff0c\u7b26\u53f7\u4f7f\u7528\u5448\u73b0\u5e42\u5f8b\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u5927\u7406\u8bba\u89c1\u89e3\u3002", "conclusion": "AIM\u6846\u67b6\u4e3a\u7b26\u53f7\u4e3b\u4e49\u4e0e\u8fde\u63a5\u4e3b\u4e49\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u672a\u6765\u5c06\u63a2\u7d22HQ-VAE\u548cRL\u4f4e\u5c42\u9884\u8bad\u7ec3\u4ee5\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2507.10584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10584", "abs": "https://arxiv.org/abs/2507.10584", "authors": ["Francesco Romeo", "Luigi Arena", "Francesco Blefari", "Francesco Aurelio Pironti", "Matteo Lupinacci", "Angelo Furfaro"], "title": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance", "comment": null, "summary": "Policy as Code (PaC) is a paradigm that encodes security and compliance\npolicies into machine-readable formats, enabling automated enforcement in\nInfrastructure as Code (IaC) environments. However, its adoption is hindered by\nthe complexity of policy languages and the risk of misconfigurations. In this\nwork, we present ARPaCCino, an agentic system that combines Large Language\nModels (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation\nto automate the generation and verification of PaC rules. Given natural\nlanguage descriptions of the desired policies, ARPaCCino generates formal Rego\nrules, assesses IaC compliance, and iteratively refines the IaC configurations\nto ensure conformance. Thanks to its modular agentic architecture and\nintegration with external tools and knowledge bases, ARPaCCino supports policy\nvalidation across a wide range of technologies, including niche or emerging IaC\nframeworks. Experimental evaluation involving a Terraform-based case study\ndemonstrates ARPaCCino's effectiveness in generating syntactically and\nsemantically correct policies, identifying non-compliant infrastructures, and\napplying corrective modifications, even when using smaller, open-weight LLMs.\nOur results highlight the potential of agentic RAG architectures to enhance the\nautomation, reliability, and accessibility of PaC workflows.", "AI": {"tldr": "ARPaCCino\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u3001RAG\u548c\u5de5\u5177\u9a8c\u8bc1\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u548c\u9a8c\u8bc1Policy as Code\u89c4\u5219\uff0c\u63d0\u5347IaC\u73af\u5883\u4e2d\u7684\u5408\u89c4\u6027\u3002", "motivation": "Policy as Code\u7684\u91c7\u7528\u53d7\u9650\u4e8e\u653f\u7b56\u8bed\u8a00\u7684\u590d\u6742\u6027\u548c\u914d\u7f6e\u9519\u8bef\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "ARPaCCino\u5229\u7528LLM\u3001RAG\u548c\u5de5\u5177\u9a8c\u8bc1\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210Rego\u89c4\u5219\uff0c\u5e76\u9a8c\u8bc1\u548c\u4f18\u5316IaC\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eARPaCCino\u80fd\u751f\u6210\u6b63\u786e\u89c4\u5219\uff0c\u8bc6\u522b\u4e0d\u5408\u89c4\u57fa\u7840\u8bbe\u65bd\u5e76\u8fdb\u884c\u4fee\u6b63\uff0c\u5373\u4f7f\u4f7f\u7528\u5c0f\u578bLLM\u3002", "conclusion": "ARPaCCino\u5c55\u793a\u4e86\u57fa\u4e8eRAG\u7684\u67b6\u6784\u5728\u63d0\u5347PaC\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u3001\u53ef\u9760\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10933", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.10933", "abs": "https://arxiv.org/abs/2507.10933", "authors": ["Orhan Erdem", "Ragavi Pobbathi Ashok"], "title": "Artificial Finance: How AI Thinks About Money", "comment": null, "summary": "In this paper, we explore how large language models (LLMs) approach financial\ndecision-making by systematically comparing their responses to those of human\nparticipants across the globe. We posed a set of commonly used financial\ndecision-making questions to seven leading LLMs, including five models from the\nGPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We\nthen compared their outputs to human responses drawn from a dataset covering 53\nnations. Our analysis reveals three main results. First, LLMs generally exhibit\na risk-neutral decision-making pattern, favoring choices aligned with expected\nvalue calculations when faced with lottery-type questions. Second, when\nevaluating trade-offs between present and future, LLMs occasionally produce\nresponses that appear inconsistent with normative reasoning. Third, when we\nexamine cross-national similarities, we find that the LLMs' aggregate responses\nmost closely resemble those of participants from Tanzania. These findings\ncontribute to the understanding of how LLMs emulate human-like decision\nbehaviors and highlight potential cultural and training influences embedded\nwithin their outputs.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5168\u7403\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u91d1\u878d\u51b3\u7b56\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u503e\u5411\u4e8e\u98ce\u9669\u4e2d\u6027\u51b3\u7b56\uff0c\u5076\u5c14\u51fa\u73b0\u4e0e\u89c4\u8303\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u4e14\u5176\u54cd\u5e94\u4e0e\u5766\u6851\u5c3c\u4e9a\u53c2\u4e0e\u8005\u6700\u76f8\u4f3c\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u7684\u6587\u5316\u548c\u8bad\u7ec3\u5f71\u54cd\u3002", "method": "\u5411\u4e03\u4e2a\u9886\u5148\u7684LLMs\uff08\u5982GPT\u7cfb\u5217\u3001Gemini 2.0 Flash\u7b49\uff09\u63d0\u51fa\u91d1\u878d\u51b3\u7b56\u95ee\u9898\uff0c\u5e76\u4e0e\u6765\u81ea53\u4e2a\u56fd\u5bb6\u7684\u4eba\u7c7b\u54cd\u5e94\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLMs\u8868\u73b0\u51fa\u98ce\u9669\u4e2d\u6027\u51b3\u7b56\u6a21\u5f0f\uff0c\u5076\u5c14\u4e0e\u89c4\u8303\u63a8\u7406\u4e0d\u4e00\u81f4\uff0c\u4e14\u5176\u54cd\u5e94\u4e0e\u5766\u6851\u5c3c\u4e9a\u53c2\u4e0e\u8005\u6700\u76f8\u4f3c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u884c\u4e3a\u7684\u65b9\u5f0f\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u8f93\u51fa\u4e2d\u6f5c\u5728\u7684\u6587\u5316\u548c\u8bad\u7ec3\u5f71\u54cd\u3002"}}
{"id": "2507.10737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10737", "abs": "https://arxiv.org/abs/2507.10737", "authors": ["Jiayuan Chen", "Thai-Hoang Pham", "Yuanlong Wang", "Ping Zhang"], "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines", "comment": "ICCV 2025", "summary": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u5916\u90e8\u751f\u7269\u77e5\u8bc6\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u7ec6\u80de\u7cfb\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u9ad8\u901a\u91cf\u7b5b\u9009\u6280\u672f\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7ec6\u80de\u7cfb\u7684\u5f62\u6001\u548c\u751f\u7269\u5b66\u5f02\u8d28\u6027\u4f7f\u5f97\u65b0\u7ec6\u80de\u7cfb\u7684\u6270\u52a8\u7b5b\u9009\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff08\u5229\u7528STRING\u548cHetionet\u6570\u636e\u5e93\uff09\u548c\u6574\u5408\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u7279\u5f81\uff0c\u660e\u786e\u89e3\u8026\u6270\u52a8\u7279\u5f02\u6027\u548c\u7ec6\u80de\u7cfb\u7279\u5f02\u6027\u8868\u5f81\u3002", "result": "\u5728RxRx\u6570\u636e\u5e93\u4e0a\u8bc4\u4f30\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u7ec6\u80de\u7cfb\u7684\u56fe\u50cf\u5206\u6790\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u5728\u65b0\u7ec6\u80de\u7cfb\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u8868\u578b\u836f\u7269\u53d1\u73b0\u3002"}}
{"id": "2507.10587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10587", "abs": "https://arxiv.org/abs/2507.10587", "authors": ["Dennis Ulmer", "Alexandra Lorson", "Ivan Titov", "Christian Hardmeier"], "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing", "comment": null, "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u4ee5\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\uff0c\u63d0\u51fa\u6a21\u4eff\u4eba\u7c7b\u6c9f\u901a\u7684\u201c\u62df\u4eba\u5316\u4e0d\u786e\u5b9a\u6027\u201d\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f93\u51fa\u65f6\u5f80\u5f80\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u5373\u4f7f\u51c6\u786e\u6027\u5b58\u7591\uff0c\u8fd9\u524a\u5f31\u4e86\u5176\u53ef\u4fe1\u5ea6\u3002\u9700\u8981\u901a\u8fc7\u8bed\u8a00\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u63d0\u51fa\u201c\u62df\u4eba\u5316\u4e0d\u786e\u5b9a\u6027\u201d\u6982\u5ff5\uff0c\u6a21\u4eff\u4eba\u7c7b\u6c9f\u901a\u65b9\u5f0f\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u7efc\u8ff0\u4e86\u76f8\u5173\u7814\u7a76\u53ca\u6570\u636e\u504f\u89c1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709NLP\u7814\u7a76\u5ffd\u89c6\u4e86\u4eba\u7c7b\u4e0d\u786e\u5b9a\u6027\u6c9f\u901a\u7684\u7ec6\u5fae\u5dee\u522b\u53ca\u6570\u636e\u504f\u89c1\uff0c\u9700\u8fdb\u4e00\u6b65\u4e2a\u6027\u5316\u4e0e\u8bed\u8a00\u771f\u5b9e\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u4eba\u673a\u4e0d\u786e\u5b9a\u6027\u6c9f\u901a\u7684\u72ec\u7279\u56e0\u7d20\uff0c\u5e76\u5206\u89e3\u201c\u62df\u4eba\u5316\u4e0d\u786e\u5b9a\u6027\u201d\u4e3a\u5177\u4f53\u65b9\u5411\u3002"}}
{"id": "2507.10934", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10934", "abs": "https://arxiv.org/abs/2507.10934", "authors": ["Xinyuan Liu", "Jiahui Chen", "Bocheng Hu", "Yu Sun", "Xinyang Chen", "Shaoxu Song"], "title": "Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models", "comment": null, "summary": "Data quality remains an important challenge in data-driven systems, as errors\nin tabular data can severely compromise downstream analytics and machine\nlearning performance. Although numerous error detection algorithms have been\nproposed, the lack of diverse, real-world error datasets limits comprehensive\nevaluation. Manual error annotation is both time-consuming and inconsistent,\nmotivating the exploration of synthetic error generation as an alternative. In\nthis work, we introduce TableEG, a framework that leverages large language\nmodels (LLMs) to generate authentic errors. By employing a table fine-tuning\nstrategy and a triplet representation $(I, T, O)$ to model error generation,\ndetection, and correction tasks, TableEG captures the complex dependencies\ninherent in two-dimensional tables. Trained on 12 real-world datasets spanning\n10 diverse domains, TableEG ensures that the synthesized errors faithfully\nreflect authentic error distributions. Experimental results indicate that\nerrors generated by TableEG exhibit superior pattern and distribution\nsimilarity compared to both rule-based methods and LLM-generated errors without\nfine-tuning. Furthermore, performance metrics on TableEG-generated errors\nclosely align with those on real-world errors across nearly all datasets and\ndetection algorithms, particularly for machine learning based detection\ntechniques. Overall, TableEG not only bridges the gap between synthetic and\nreal-world errors but also establishes a robust benchmark for subsequent error\ndetection and correction tasks.", "AI": {"tldr": "TableEG\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u771f\u5b9e\u7684\u8868\u683c\u6570\u636e\u9519\u8bef\uff0c\u586b\u8865\u4e86\u5408\u6210\u4e0e\u771f\u5b9e\u9519\u8bef\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9519\u8bef\u68c0\u6d4b\u4e0e\u6821\u6b63\u7684\u57fa\u51c6\u3002", "motivation": "\u6570\u636e\u8d28\u91cf\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\uff0c\u4f46\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u771f\u5b9e\u9519\u8bef\u6570\u636e\u96c6\u9650\u5236\u4e86\u8bc4\u4f30\uff0c\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u63a2\u7d22\u5408\u6210\u9519\u8bef\u751f\u6210\u3002", "method": "\u91c7\u7528\u8868\u683c\u5fae\u8c03\u7b56\u7565\u548c\u4e09\u5143\u7ec4\u8868\u793a$(I, T, O)$\u5efa\u6a21\u9519\u8bef\u751f\u6210\u3001\u68c0\u6d4b\u4e0e\u6821\u6b63\u4efb\u52a1\uff0c\u6355\u83b7\u8868\u683c\u4e2d\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "TableEG\u751f\u6210\u7684\u9519\u8bef\u5728\u6a21\u5f0f\u548c\u5206\u5e03\u76f8\u4f3c\u6027\u4e0a\u4f18\u4e8e\u89c4\u5219\u65b9\u6cd5\u548c\u672a\u7ecf\u5fae\u8c03\u7684LLM\u751f\u6210\u9519\u8bef\uff0c\u4e14\u6027\u80fd\u6307\u6807\u4e0e\u771f\u5b9e\u9519\u8bef\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "TableEG\u4e0d\u4ec5\u586b\u8865\u4e86\u5408\u6210\u4e0e\u771f\u5b9e\u9519\u8bef\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8fd8\u4e3a\u540e\u7eed\u9519\u8bef\u68c0\u6d4b\u4e0e\u6821\u6b63\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\u3002"}}
{"id": "2507.11067", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11067", "abs": "https://arxiv.org/abs/2507.11067", "authors": ["Yinuo Wang", "Tianqi Mao", "Lin Gan", "Wubing Wan", "Zeyu Song", "Jiayu Fu", "Lanke He", "Wenqiang Wang", "Zekun Yin", "Wei Xue", "Guangwen Yang"], "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit", "comment": "Yinuo Wang and Tianqi Mao contributed equally to this work", "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u77e9\u9635\u52a0\u901f\u7684\u4e09\u7ef4\u9ad8\u9636\u6a21\u677f\u8ba1\u7b97\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eSIMD\u548c\u77e9\u9635\u5355\u5143\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4e09\u7ef4\u9ad8\u9636\u6a21\u677f\u8ba1\u7b97\u5728HPC\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u591a\u6838CPU\u4e0a\u5229\u7528\u77e9\u9635\u5355\u5143\u52a0\u901f\u7684\u7b56\u7565\u3002", "method": "\u7ed3\u5408SIMD\u548c\u77e9\u9635\u5355\u5143\u7684\u7b97\u6cd5\u4f18\u5316\uff0c\u6539\u8fdb\u5185\u5b58\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u591a\u7ebf\u7a0b\u5e76\u884c\u8303\u5f0f\u4ee5\u89e3\u51b3\u6570\u636e\u5171\u4eab\u95ee\u9898\u3002", "result": "MMStencil\u5728Nvidia A100\u4e0a\u6027\u80fd\u63d0\u53472.1\u500d\uff0c\u5b9e\u9645HPC\u5e94\u7528\u4e2d\u901f\u5ea6\u63d0\u53471.8\u500d\u3002", "conclusion": "MMStencil\u901a\u8fc7\u7efc\u5408\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u4e09\u7ef4\u9ad8\u9636\u6a21\u677f\u8ba1\u7b97\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645HPC\u5e94\u7528\u3002"}}
{"id": "2507.11014", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11014", "abs": "https://arxiv.org/abs/2507.11014", "authors": ["Tasnim Ahmed", "Mirza Mohammad Azwad", "Salimur Choudhury"], "title": "SIMCODE: A Benchmark for Natural Language to ns-3 Network Simulation Code Generation", "comment": "This paper has been accepted for presentation at the 50th IEEE\n  Conference on Local Computer Networks (LCN) - special track on Large Language\n  Models and Networking", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation across various domains. However, their effectiveness in\ngenerating simulation scripts for domain-specific environments like ns-3\nremains underexplored. Despite the growing interest in automating network\nsimulations, existing tools primarily focus on interactive automation over\nrigorous evaluation. To facilitate systematic evaluation, we introduce SIMCODE,\nthe first benchmark to evaluate LLMs' ability to generate ns-3 simulation code\nfrom natural language. SIMCODE includes 400 tasks across introductory,\nintermediate, and advanced levels, with solutions and test cases. Using\nSIMCODE, we evaluate three prominent LLMs, Gemini-2.0, GPT-4.1, and Qwen-3,\nacross six prompt techniques. Furthermore, investigating task-specific\nfine-tuning's impact reveals that while GPT-4.1 outperforms others, execution\naccuracy remains modest, with substantial room for improvement. Error analysis\nidentifies missing headers and API mismatches as dominant failures.\nNevertheless, SIMCODE provides a foundational step toward evaluating LLMs and\nresearch in domain-aware generative systems.", "AI": {"tldr": "SIMCODE\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u751f\u6210ns-3\u4eff\u771f\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b400\u4e2a\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cdLLM\u7684\u6027\u80fd\uff0c\u53d1\u73b0GPT-4.1\u8868\u73b0\u6700\u4f73\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982ns-3\u4eff\u771f\u811a\u672c\u751f\u6210\uff09\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u4e14\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u5f15\u5165SIMCODE\u57fa\u51c6\uff0c\u5305\u542b400\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e09\u79cdLLM\uff08Gemini-2.0\u3001GPT-4.1\u3001Qwen-3\uff09\u5728\u516d\u79cd\u63d0\u793a\u6280\u672f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u5f71\u54cd\u3002", "result": "GPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6267\u884c\u51c6\u786e\u7387\u4ecd\u6709\u9650\uff0c\u4e3b\u8981\u9519\u8bef\u4e3a\u7f3a\u5c11\u5934\u6587\u4ef6\u548cAPI\u4e0d\u5339\u914d\u3002", "conclusion": "SIMCODE\u4e3a\u8bc4\u4f30LLM\u548c\u9886\u57df\u611f\u77e5\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f46LLM\u5728\u751f\u6210ns-3\u4eff\u771f\u4ee3\u7801\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2507.10571", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10571", "abs": "https://arxiv.org/abs/2507.10571", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6a21\u5757\u5316AI\u89c6\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4ee3\u7406\u4e0e\u975e\u89c6\u89c9\u63a8\u7406\u534f\u8c03\u5668\uff0c\u901a\u8fc7\u4fe1\u4efb\u6821\u51c6\u63d0\u5347\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001AI\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u786e\u4fdd\u4ee3\u7406\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5305\u62ec\u591a\u6a21\u6001\u4ee3\u7406\u3001\u975e\u89c6\u89c9\u63a8\u7406\u534f\u8c03\u5668\u548cRAG\u6a21\u5757\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u56fe\u50cf\u68c0\u7d22\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u4fe1\u4efb\u611f\u77e5\u534f\u8c03\u548cRAG\u4f7f\u51c6\u786e\u7387\u63d0\u534777.94%\uff0c\u603b\u4f53\u8fbe\u523085.63%\u3002GPT-4o\u6821\u51c6\u6548\u679c\u66f4\u597d\uff0c\u800cQwen-2.5-VL\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5206\u79bb\u611f\u77e5\u4e0e\u5143\u63a8\u7406\uff0c\u53ef\u6269\u5c55\u81f3\u8bca\u65ad\u3001\u751f\u7269\u5b66\u7b49\u4fe1\u4efb\u5173\u952e\u9886\u57df\uff0c\u5e76\u5f00\u6e90\u4e86\u6240\u6709\u6a21\u578b\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u590d\u73b0\u548c\u793e\u533a\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2507.10590", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10590", "abs": "https://arxiv.org/abs/2507.10590", "authors": ["Mojtaba Eshghie"], "title": "Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime", "comment": null, "summary": "Language Model (LM) pipelines can dynamically refine their outputs against\nprogrammatic constraints. However, their effectiveness collapses when faced\nwith competing soft constraints, leading to inefficient backtracking loops\nwhere satisfying one constraint violates another. We introduce Meta\nSelf-Refining, a framework that equips LM pipelines with a meta-corrective\nlayer to repair these competitions at runtime/inference-time. Our approach\nmonitors the pipeline's execution history to detect oscillatory failures. Upon\ndetection, it invokes a meta-repairer LM that analyzes the holistic state of\nthe backtracking attempts and synthesizes a strategic instruction to balance\nthe competing requirements. This self-repair instruction guides the original LM\nout of a failing refining loop towards a successful output. Our results show\nMeta Self-Refining can successfully repair these loops, leading to more\nefficient LM programs.", "AI": {"tldr": "Meta Self-Refining\u6846\u67b6\u901a\u8fc7\u5143\u4fee\u6b63\u5c42\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\u5728\u8f6f\u7ea6\u675f\u7ade\u4e89\u4e2d\u7684\u4f4e\u6548\u56de\u6eaf\u95ee\u9898\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\u5728\u9762\u5bf9\u7ade\u4e89\u6027\u8f6f\u7ea6\u675f\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u56de\u6eaf\u5faa\u73af\u3002", "method": "\u5f15\u5165\u5143\u4fee\u6b63\u5c42\uff0c\u76d1\u63a7\u6267\u884c\u5386\u53f2\uff0c\u68c0\u6d4b\u632f\u8361\u5931\u8d25\uff0c\u5e76\u901a\u8fc7\u5143\u4fee\u590d\u5668\u5408\u6210\u7b56\u7565\u6027\u6307\u4ee4\u3002", "result": "Meta Self-Refining\u80fd\u6709\u6548\u4fee\u590d\u56de\u6eaf\u5faa\u73af\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7a0b\u5e8f\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u4fee\u590d\u7ade\u4e89\u7ea6\u675f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.11353", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.11353", "abs": "https://arxiv.org/abs/2507.11353", "authors": ["Zorana Grbac", "Simone Pavarana", "Thorsten Schmidt", "Peter Tankov"], "title": "Propagation of carbon price shocks through the value chain: the mean-field game of defaults", "comment": null, "summary": "We introduce a new mean-field game framework to analyze the impact of carbon\npricing in a multi-sector economy with defaultable firms. Each sector produces\na homogeneous good, with its price endogenously determined through market\nclearing. Firms act as price takers and maximize profits by choosing an optimal\nallocation of inputs-including labor, emissions, and intermediate goods from\nother sectors-while interacting through the endogenous sectoral price. Firms\nalso choose their default timing to maximize shareholder value.\n  Formally, we model the economy as an optimal stopping mean-field game within\neach sector. The resulting system of coupled mean-field games admits a linear\nprogramming formulation that characterizes Nash equilibria in terms of\npopulation measure flows. We prove the existence of a linear programming Nash\nequilibrium and establish uniqueness of the associated price system.\n  Numerical illustrations are presented for firms with constant elasticity of\nsubstitution (CES) production functions. In a stylized single-sector economy,\ncarbon price shocks induce substitution between emissions and labor. In a\nthree-sector economy, the manufacturing sector faces consumer demand and\nrequires inputs from a brown sector, which can be increasingly replaced by\ngreen-sector goods as carbon prices rise. These experiments reveal that carbon\nprice shocks can generate substantial spillover effects along the value chain,\nunderscoring the importance of sectoral interdependencies in shaping effective\ndecarbonization pathways.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e73\u5747\u573a\u535a\u5f08\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u78b3\u5b9a\u4ef7\u5728\u591a\u90e8\u95e8\u7ecf\u6d4e\u4e2d\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u8fdd\u7ea6\u4f01\u4e1a\u3002", "motivation": "\u7814\u7a76\u78b3\u5b9a\u4ef7\u5bf9\u591a\u90e8\u95e8\u7ecf\u6d4e\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4f01\u4e1a\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u6295\u5165\u548c\u8fdd\u7ea6\u65f6\u673a\u6765\u5e94\u5bf9\u78b3\u4ef7\u683c\u53d8\u5316\u3002", "method": "\u91c7\u7528\u6700\u4f18\u505c\u6b62\u5e73\u5747\u573a\u535a\u5f08\u6a21\u578b\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u65b9\u6cd5\u63cf\u8ff0\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u8bc1\u660e\u5176\u5b58\u5728\u6027\u548c\u4ef7\u683c\u7cfb\u7edf\u7684\u552f\u4e00\u6027\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u78b3\u4ef7\u683c\u51b2\u51fb\u4f1a\u5bfc\u81f4\u6392\u653e\u4e0e\u52b3\u52a8\u529b\u4e4b\u95f4\u7684\u66ff\u4ee3\u6548\u5e94\uff0c\u5e76\u5728\u591a\u90e8\u95e8\u7ecf\u6d4e\u4e2d\u4ea7\u751f\u663e\u8457\u7684\u6ea2\u51fa\u6548\u5e94\u3002", "conclusion": "\u78b3\u4ef7\u683c\u51b2\u51fb\u5bf9\u4ef7\u503c\u94fe\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u90e8\u95e8\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u662f\u5236\u5b9a\u6709\u6548\u8131\u78b3\u8def\u5f84\u7684\u5173\u952e\u3002"}}
{"id": "2507.10755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10755", "abs": "https://arxiv.org/abs/2507.10755", "authors": ["Rina Khan", "Catherine Stinson"], "title": "Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias", "comment": null, "summary": "Facial expression recognition (FER) algorithms classify facial expressions\ninto emotions such as happy, sad, or angry. An evaluative challenge facing FER\nalgorithms is the fall in performance when detecting spontaneous expressions\ncompared to posed expressions. An ethical (and evaluative) challenge facing FER\nalgorithms is that they tend to perform poorly for people of some races and\nskin colors. These challenges are linked to the data collection practices\nemployed in the creation of FER datasets. In this study, we audit two\nstate-of-the-art FER datasets. We take random samples from each dataset and\nexamine whether images are spontaneous or posed. In doing so, we propose a\nmethodology for identifying spontaneous or posed images. We discover a\nsignificant number of images that were posed in the datasets purporting to\nconsist of in-the-wild images. Since performance of FER models vary between\nspontaneous and posed images, the performance of models trained on these\ndatasets will not represent the true performance if such models were to be\ndeployed in in-the-wild applications. We also observe the skin color of\nindividuals in the samples, and test three models trained on each of the\ndatasets to predict facial expressions of people from various races and skin\ntones. We find that the FER models audited were more likely to predict people\nlabeled as not white or determined to have dark skin as showing a negative\nemotion such as anger or sadness even when they were smiling. This bias makes\nsuch models prone to perpetuate harm in real life applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5ba1\u8ba1\u4e86\u4e24\u4e2a\u5148\u8fdb\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u4e2d\u7684\u8bb8\u591a\u56fe\u50cf\u662f\u6446\u62cd\u7684\uff0c\u800c\u975e\u81ea\u7136\u8868\u60c5\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5bf9\u975e\u767d\u4eba\u6216\u6df1\u8272\u76ae\u80a4\u7684\u4eba\u5b58\u5728\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u9884\u6d4b\u8d1f\u9762\u60c5\u7eea\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7b97\u6cd5\u5728\u81ea\u7136\u8868\u60c5\u548c\u4e0d\u540c\u79cd\u65cf/\u80a4\u8272\u4eba\u7fa4\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u63ed\u793a\u6570\u636e\u96c6\u6536\u96c6\u65b9\u6cd5\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u968f\u673a\u62bd\u6837\u5ba1\u8ba1\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\uff0c\u533a\u5206\u81ea\u7136\u4e0e\u6446\u62cd\u8868\u60c5\uff0c\u5e76\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u79cd\u65cf\u548c\u80a4\u8272\u4eba\u7fa4\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u6446\u62cd\u56fe\u50cf\uff0c\u4e14\u6a21\u578b\u5bf9\u975e\u767d\u4eba\u6216\u6df1\u8272\u76ae\u80a4\u7684\u4eba\u5b58\u5728\u8d1f\u9762\u60c5\u7eea\u9884\u6d4b\u504f\u89c1\u3002", "conclusion": "\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u5f53\u524d\u6a21\u578b\u5b58\u5728\u504f\u89c1\uff0c\u53ef\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9020\u6210\u4f24\u5bb3\u3002"}}
{"id": "2507.10596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10596", "abs": "https://arxiv.org/abs/2507.10596", "authors": ["Yogachandran Rahulamathavan", "Misbah Farooq", "Varuna De Silva"], "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification", "comment": null, "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification.", "AI": {"tldr": "PLEX\u662f\u4e00\u79cd\u65e0\u9700\u6270\u52a8\u7684\u5c40\u90e8\u89e3\u91ca\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u548cSiamese\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u91ca\u6548\u7387\u3002", "motivation": "LLM\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u590d\u6742\u6027\u5bfc\u81f4\u89e3\u91ca\u6027\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982LIME\u548cSHAP\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "PLEX\u901a\u8fc7\u63d0\u53d6LLM\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5e76\u8bad\u7ec3Siamese\u7f51\u7edc\u4e0e\u7279\u5f81\u91cd\u8981\u6027\u5206\u6570\u5bf9\u9f50\uff0c\u907f\u514d\u4e86\u540e\u7eed\u6270\u52a8\u3002", "result": "\u5728\u56db\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cPLEX\u4e0eLIME\u548cSHAP\u7684\u4e00\u81f4\u6027\u8d85\u8fc792%\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "PLEX\u4e3aLLM\u7684\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u91ca\u65b9\u6cd5\u3002"}}
{"id": "2507.11505", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.11505", "abs": "https://arxiv.org/abs/2507.11505", "authors": ["Harsha Kokel", "Aamod Khatiwada", "Tejaswini Pedapati", "Haritha Ananthakrishnan", "Oktie Hassanzadeh", "Horst Samulowitz", "Kavitha Srinivas"], "title": "TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column Search", "comment": "VLDB 2025 Workshop: Tabular Data Analysis (TaDA); The source code,\n  data, and/or other artifacts have been made available at\n  https://github.com/IBM/ContextAwareJoin", "summary": "One of the major challenges in enterprise data analysis is the task of\nfinding joinable tables that are conceptually related and provide meaningful\ninsights. Traditionally, joinable tables have been discovered through a search\nfor similar columns, where two columns are considered similar syntactically if\nthere is a set overlap or they are considered similar semantically if either\nthe column embeddings or value embeddings are closer in the embedding space.\nHowever, for enterprise data lakes, column similarity is not sufficient to\nidentify joinable columns and tables. The context of the query column is\nimportant. Hence, in this work, we first define context-aware column\njoinability. Then we propose a multi-criteria approach, called TOPJoin, for\njoinable column search. We evaluate TOPJoin against existing join search\nbaselines over one academic and one real-world join search benchmark. Through\nexperiments, we find that TOPJoin performs better on both benchmarks than the\nbaselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6807\u51c6\u65b9\u6cd5TOPJoin\uff0c\u7528\u4e8e\u89e3\u51b3\u4f01\u4e1a\u6570\u636e\u6e56\u4e2d\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5217\u53ef\u8fde\u63a5\u6027\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5217\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u5728\u4f01\u4e1a\u6570\u636e\u6e56\u4e2d\u4e0d\u8db3\u4ee5\u8bc6\u522b\u53ef\u8fde\u63a5\u5217\u548c\u8868\uff0c\u9700\u8981\u7ed3\u5408\u67e5\u8be2\u5217\u7684\u4e0a\u4e0b\u6587\u3002", "method": "\u5b9a\u4e49\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5217\u53ef\u8fde\u63a5\u6027\uff0c\u5e76\u63d0\u51fa\u591a\u6807\u51c6\u65b9\u6cd5TOPJoin\u8fdb\u884c\u53ef\u8fde\u63a5\u5217\u641c\u7d22\u3002", "result": "TOPJoin\u5728\u5b66\u672f\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TOPJoin\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u6570\u636e\u6e56\u4e2d\u53ef\u8fde\u63a5\u5217\u7684\u641c\u7d22\u6548\u679c\u3002"}}
{"id": "2507.11094", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11094", "abs": "https://arxiv.org/abs/2507.11094", "authors": ["Nibedita Behera", "Ashwina Kumar", "Atharva Chougule", "Mohammed Shan P S", "Rushabh Nirdosh Lalwani", "Rupesh Nasre"], "title": "Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL", "comment": null, "summary": "With the rapid growth of unstructured and semistructured data, parallelizing\ngraph algorithms has become essential for efficiency. However, due to the\ninherent irregularity in computation, memory access patterns, and\ncommunication, graph algorithms are notoriously difficult to parallelize. To\naddress this challenge, several libraries, frameworks, and domain-specific\nlanguages (DSLs) have been proposed to ease the parallel programming burden for\ndomain experts. Existing frameworks partially or fully abstract away\nparallelism intricacies, provide intuitive scheduling mnemonics, and employ\nprogram analysis to identify data races and generate synchronization code.\nDespite these advances, most frameworks are limited in their abstractions and\nruntime optimizations, especially when dealing with static graphs. In contrast,\nmany real-world graphs are inherently dynamic, with evolving structures over\ntime through insertions, deletions, and modifications of vertices, edges, and\nattributes. Generating efficient and correctly synchronized code for such\ndynamic graph algorithms remains a significant challenge.\n  In this work, we introduce an abstraction scheme and runtime optimizations\nfor the efficient processing of morph algorithms. Specifically, given an\ninitial graph G and a set of updates $\\Delta$G involving edge insertions and\ndeletions, we express the dynamic processing logic through a DSL and\nautomatically generate parallel code targeting multicore, distributed, and\nmany-core environments. We demonstrate the effectiveness of our approach by\napplying the DSL-generated code to ten large graphs with diverse\ncharacteristics and three widely used algorithms: Shortest Paths, PageRank, and\nTriangle Counting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62bd\u8c61\u65b9\u6848\u548c\u8fd0\u884c\u65f6\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u52a8\u6001\u56fe\u7b97\u6cd5\uff0c\u901a\u8fc7DSL\u81ea\u52a8\u751f\u6210\u5e76\u884c\u4ee3\u7801\uff0c\u5e76\u5728\u591a\u79cd\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u56fe\u7b97\u6cd5\u7684\u8ba1\u7b97\u3001\u5185\u5b58\u8bbf\u95ee\u548c\u901a\u4fe1\u7684\u56fa\u6709\u4e0d\u89c4\u5219\u6027\uff0c\u5e76\u884c\u5316\u56fe\u7b97\u6cd5\u975e\u5e38\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u56fe\u573a\u666f\u4e0b\u3002\u73b0\u6709\u6846\u67b6\u5728\u5904\u7406\u52a8\u6001\u56fe\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u62bd\u8c61\u65b9\u6848\u548c\u8fd0\u884c\u65f6\u4f18\u5316\uff0c\u901a\u8fc7DSL\u8868\u8fbe\u52a8\u6001\u5904\u7406\u903b\u8f91\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u9488\u5bf9\u591a\u6838\u3001\u5206\u5e03\u5f0f\u548c\u4f17\u6838\u73af\u5883\u7684\u5e76\u884c\u4ee3\u7801\u3002", "result": "\u5728\u5341\u4e2a\u5927\u578b\u56fe\u548c\u4e09\u79cd\u5e38\u7528\u7b97\u6cd5\uff08\u6700\u77ed\u8def\u5f84\u3001PageRank\u548c\u4e09\u89d2\u5f62\u8ba1\u6570\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u56fe\u7b97\u6cd5\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11038", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11038", "abs": "https://arxiv.org/abs/2507.11038", "authors": ["Ka Ho Chiu", "Handi Yin", "Weipeng Zhuo", "Chul-Ho Lee", "S. -H. Gary Chan"], "title": "Graph-based Fingerprint Update Using Unlabelled WiFi Signals", "comment": "Published in Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies, Volume 9, Issue 1, Article No. 3, Pages 1 - 26", "summary": "WiFi received signal strength (RSS) environment evolves over time due to\nmovement of access points (APs), AP power adjustment, installation and removal\nof APs, etc. We study how to effectively update an existing database of\nfingerprints, defined as the RSS values of APs at designated locations, using a\nbatch of newly collected unlabelled (possibly crowdsourced) WiFi signals. Prior\nart either estimates the locations of the new signals without updating the\nexisting fingerprints or filters out the new APs without sufficiently embracing\ntheir features. To address that, we propose GUFU, a novel effective graph-based\napproach to update WiFi fingerprints using unlabelled signals with possibly new\nAPs. Based on the observation that similar signal vectors likely imply physical\nproximity, GUFU employs a graph neural network (GNN) and a link prediction\nalgorithm to retrain an incremental network given the new signals and APs.\nAfter the retraining, it then updates the signal vectors at the designated\nlocations. Through extensive experiments in four large representative sites,\nGUFU is shown to achieve remarkably higher fingerprint adaptivity as compared\nwith other state-of-the-art approaches, with error reduction of 21.4% and 29.8%\nin RSS values and location prediction, respectively.", "AI": {"tldr": "GUFU\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684WiFi\u6307\u7eb9\u66f4\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u4fe1\u53f7\u548c\u65b0AP\u6709\u6548\u66f4\u65b0\u73b0\u6709\u6307\u7eb9\u6570\u636e\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "WiFi\u4fe1\u53f7\u73af\u5883\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u65b0\u4fe1\u53f7\u6216\u65b0AP\u7684\u7279\u5f81\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u66f4\u65b0\u65b9\u6cd5\u3002", "method": "GUFU\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u94fe\u63a5\u9884\u6d4b\u7b97\u6cd5\uff0c\u5229\u7528\u65b0\u4fe1\u53f7\u548cAP\u91cd\u65b0\u8bad\u7ec3\u589e\u91cf\u7f51\u7edc\uff0c\u5e76\u66f4\u65b0\u6307\u5b9a\u4f4d\u7f6e\u7684\u4fe1\u53f7\u5411\u91cf\u3002", "result": "\u5728\u56db\u4e2a\u5927\u578b\u4ee3\u8868\u6027\u7ad9\u70b9\u5b9e\u9a8c\u4e2d\uff0cGUFU\u5728RSS\u503c\u548c\u4f4d\u7f6e\u9884\u6d4b\u4e0a\u5206\u522b\u51cf\u5c11\u4e8621.4%\u548c29.8%\u7684\u8bef\u5dee\u3002", "conclusion": "GUFU\u663e\u8457\u63d0\u5347\u4e86WiFi\u6307\u7eb9\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.10624", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10624", "abs": "https://arxiv.org/abs/2507.10624", "authors": ["Zheng Zhang"], "title": "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning", "comment": "Substantial change to previous version (experiments, theorem,\n  analysis and related work); currently under review at TMLR", "summary": "Large Language Models (LLMs) display striking surface fluency yet\nsystematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,\nand logical consistency. This paper offers a structural diagnosis of such\nfailures, revealing a persistent gap between \\textit{comprehension} and\n\\textit{competence}. Through controlled experiments and architectural analysis,\nwe demonstrate that LLMs often articulate correct principles without reliably\napplying them--a failure rooted not in knowledge access, but in computational\nexecution. We term this phenomenon the computational \\textit{split-brain\nsyndrome}, where instruction and action pathways are geometrically and\nfunctionally dissociated. This core limitation recurs across domains, from\nmathematical operations to relational inferences, and explains why model\nbehavior remains brittle even under idealized prompting. We argue that LLMs\nfunction as powerful pattern completion engines, but lack the architectural\nscaffolding for principled, compositional reasoning. Our findings delineate the\nboundary of current LLM capabilities and motivate future models with\nmetacognitive control, principle lifting, and structurally grounded execution.\nThis diagnosis also clarifies why mechanistic interpretability findings may\nreflect training-specific pattern coordination rather than universal\ncomputational principles, and why the geometric separation between instruction\nand execution pathways suggests limitations in neural introspection and\nmechanistic analysis.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7b97\u672f\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5176\u7406\u89e3\u4e0e\u80fd\u529b\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u8ba1\u7b97\u5206\u88c2\u8111\u7efc\u5408\u5f81\u201d\u7684\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u63ed\u793a\u5176\u8868\u9762\u6d41\u7545\u6027\u4e0e\u5b9e\u9645\u80fd\u529b\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u548c\u67b6\u6784\u5206\u6790\uff0c\u7814\u7a76LLMs\u5728\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u6267\u884c\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0LLMs\u80fd\u591f\u8868\u8fbe\u6b63\u786e\u539f\u5219\u4f46\u65e0\u6cd5\u53ef\u9760\u5e94\u7528\uff0c\u539f\u56e0\u662f\u8ba1\u7b97\u6267\u884c\u4e2d\u7684\u529f\u80fd\u5206\u79bb\uff0c\u800c\u975e\u77e5\u8bc6\u83b7\u53d6\u95ee\u9898\u3002", "conclusion": "LLMs\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u672a\u6765\u6a21\u578b\u9700\u5f15\u5165\u5143\u8ba4\u77e5\u63a7\u5236\u548c\u539f\u5219\u63d0\u5347\u673a\u5236\u3002"}}
{"id": "2507.10593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10593", "abs": "https://arxiv.org/abs/2507.10593", "authors": ["Peng Ding"], "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs", "comment": null, "summary": "Large Language Model (LLM) applications are increasingly relying on external\ntools to extend their capabilities beyond text generation. However, current\ntool integration approaches suffer from fragmentation, protocol limitations,\nand implementation complexity, leading to substantial development overhead.\nThis paper presents Toolregistry, a protocol-agnostic tool management library\nthat simplifies tool registration, representation, execution, and lifecycle\nmanagement via a unified interface. Our evaluation demonstrates that\n\\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x\nperformance improvements through concurrent execution, and 100% compatibility\nwith OpenAI function calling standards. Real-world case studies show\nsignificant improvements in development efficiency and code maintainability\nacross diverse integration scenarios. \\toolregistry is open-source and\navailable at https://github.com/Oaklight/ToolRegistry, with comprehensive\ndocumentation at https://toolregistry.readthedocs.io/.", "AI": {"tldr": "Toolregistry\u662f\u4e00\u4e2a\u534f\u8bae\u65e0\u5173\u7684\u5de5\u5177\u7ba1\u7406\u5e93\uff0c\u7b80\u5316\u4e86\u5de5\u5177\u6ce8\u518c\u3001\u6267\u884c\u548c\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4ee3\u7801\u91cf\u548c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u3001\u534f\u8bae\u9650\u5236\u548c\u5b9e\u73b0\u590d\u6742\u6027\u95ee\u9898\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u8d1f\u62c5\u3002", "method": "\u63d0\u51faToolregistry\uff0c\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u7ba1\u7406\u5de5\u5177\u6ce8\u518c\u3001\u8868\u793a\u3001\u6267\u884c\u548c\u751f\u547d\u5468\u671f\u3002", "result": "Toolregistry\u51cf\u5c11\u4e8660-80%\u7684\u96c6\u6210\u4ee3\u7801\uff0c\u6027\u80fd\u63d0\u53473.1\u500d\uff0c100%\u517c\u5bb9OpenAI\u6807\u51c6\u3002", "conclusion": "Toolregistry\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\uff0c\u5df2\u5728\u5f00\u6e90\u793e\u533a\u53d1\u5e03\u3002"}}
{"id": "2507.11361", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.11361", "abs": "https://arxiv.org/abs/2507.11361", "authors": ["Maximilian Bernecker", "Smaranda Sgarciu", "Xiaoming Kan", "Mehrnaz Anvari", "Iegor Riepin", "Felix M\u00fcsgens"], "title": "Adaptive Robust Optimization for European Electricity System Planning Considering Regional Dunkelflaute Events", "comment": "Code and data can be found on github:\n  https://github.com/bernemax/ARO_Dunkelflaute_Europe", "summary": "This study develops a capacity expansion model for a fully decarbonized\nEuropean electricity system using an Adaptive Robust Optimization (ARO)\nframework. The model endogenously identifies the worst regional Dunkelflaute\nevents, prolonged periods of low wind and solar availability, and incorporates\nmultiple extreme weather realizations within a single optimization run. Results\nshow that system costs rise nonlinearly with the geographic extent of these\nevents: a single worst-case regional disruption increases costs by 9%, but\nbroader disruptions across multiple regions lead to much sharper increases, up\nto 51%. As Dunkelflaute conditions extend across most of Europe, additional\ncost impacts level off, with a maximum increase of 71%. The optimal technology\nmix evolves with the severity of weather stress: while renewables, batteries,\nand interregional transmission are sufficient to manage localized events,\nlarge-scale disruptions require long-term hydrogen storage and load shedding to\nmaintain system resilience. Central European regions, especially Germany and\nFrance, emerge as systemic bottlenecks, while peripheral regions bear the cost\nof compensatory overbuilding. These findings underscore the need for a\ncoordinated European policy strategy that goes beyond national planning to\nsupport cross-border infrastructure investment, scale up flexible technologies\nsuch as long-duration storage, and promote a geographically balanced deployment\nof renewables to mitigate systemic risks associated with Dunkelflaute events.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u9002\u5e94\u9c81\u68d2\u4f18\u5316\uff08ARO\uff09\u7684\u6b27\u6d32\u7535\u529b\u7cfb\u7edf\u8131\u78b3\u5bb9\u91cf\u6269\u5c55\u6a21\u578b\uff0c\u5206\u6790\u4e86\u6781\u7aef\u5929\u6c14\u5bf9\u7cfb\u7edf\u6210\u672c\u548c\u6280\u672f\u9009\u62e9\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u6b27\u6d32\u7535\u529b\u7cfb\u7edf\u5728\u5b8c\u5168\u8131\u78b3\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u5e94\u5bf9\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\uff08\u5982Dunkelflaute\uff09\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u9c81\u68d2\u4f18\u5316\uff08ARO\uff09\u6846\u67b6\uff0c\u5185\u751f\u8bc6\u522b\u6700\u574f\u533a\u57dfDunkelflaute\u4e8b\u4ef6\uff0c\u5e76\u6574\u5408\u591a\u79cd\u6781\u7aef\u5929\u6c14\u60c5\u666f\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u6210\u672c\u968f\u6781\u7aef\u5929\u6c14\u8303\u56f4\u975e\u7ebf\u6027\u589e\u52a0\uff0c\u6700\u5927\u6210\u672c\u589e\u52a071%\u3002\u6280\u672f\u9009\u62e9\u968f\u5929\u6c14\u538b\u529b\u53d8\u5316\uff0c\u5927\u89c4\u6a21\u4e2d\u65ad\u9700\u6c22\u50a8\u80fd\u548c\u8d1f\u8377\u524a\u51cf\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u6b27\u6d32\u534f\u8c03\u653f\u7b56\uff0c\u652f\u6301\u8de8\u5883\u57fa\u7840\u8bbe\u65bd\u6295\u8d44\u548c\u7075\u6d3b\u6280\u672f\u90e8\u7f72\uff0c\u4ee5\u7f13\u89e3Dunkelflaute\u4e8b\u4ef6\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u3002"}}
{"id": "2507.10770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10770", "abs": "https://arxiv.org/abs/2507.10770", "authors": ["Ionu\u0163 Grigore", "C\u0103lin-Adrian Popa", "Claudiu Leoveanu-Condrei"], "title": "FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching", "comment": null, "summary": "The extraction and matching of interest points are fundamental to many\ngeometric computer vision tasks. Traditionally, matching is performed by\nassigning descriptors to interest points and identifying correspondences based\non descriptor similarity. This work introduces a technique where interest\npoints are inherently associated during detection, eliminating the need for\ncomputing, storing, transmitting, or matching descriptors. Although the\nmatching accuracy is marginally lower than that of conventional approaches, our\nmethod completely eliminates the need for descriptors, leading to a drastic\nreduction in memory usage for localization systems. We assess its effectiveness\nby comparing it against both classical handcrafted methods and modern learned\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u63cf\u8ff0\u7b26\u7684\u5174\u8da3\u70b9\u5339\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u63cf\u8ff0\u7b26\u8fdb\u884c\u5174\u8da3\u70b9\u5339\u914d\uff0c\u9700\u8ba1\u7b97\u3001\u5b58\u50a8\u548c\u4f20\u8f93\u63cf\u8ff0\u7b26\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u5728\u5174\u8da3\u70b9\u68c0\u6d4b\u9636\u6bb5\u76f4\u63a5\u5173\u8054\u5339\u914d\u70b9\uff0c\u907f\u514d\u4f7f\u7528\u63cf\u8ff0\u7b26\u3002", "result": "\u5339\u914d\u7cbe\u5ea6\u7565\u4f4e\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51e0\u4f55\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u63cf\u8ff0\u7b26\u5339\u914d\u65b9\u6848\u3002"}}
{"id": "2507.10599", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10599", "abs": "https://arxiv.org/abs/2507.10599", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "comment": null, "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u7684\u60c5\u611f\u5efa\u6a21\u80fd\u529b\u7814\u7a76\uff0c\u53d1\u73b0\u5176\u60c5\u611f\u5c42\u6b21\u7ed3\u6784\u4e0e\u4eba\u7c7b\u5fc3\u7406\u6a21\u578b\u4e00\u81f4\uff0c\u4f46\u4e5f\u5b58\u5728\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u7fa4\u4f53\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u5efa\u6a21\u7528\u6237\u60c5\u611f\u72b6\u6001\uff0c\u4ee5\u652f\u6301\u5176\u4f26\u7406\u90e8\u7f72\u3002", "method": "\u57fa\u4e8e\u60c5\u611f\u8f6e\uff08\u5fc3\u7406\u5b66\u6846\u67b6\uff09\u5206\u6790\u6a21\u578b\u8f93\u51fa\u4e2d\u60c5\u611f\u72b6\u6001\u7684\u6982\u7387\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "LLMs\u81ea\u7136\u5f62\u6210\u4e0e\u4eba\u7c7b\u5fc3\u7406\u6a21\u578b\u4e00\u81f4\u7684\u60c5\u611f\u5c42\u6b21\u7ed3\u6784\uff0c\u4e14\u6a21\u578b\u8d8a\u5927\u5c42\u6b21\u8d8a\u590d\u6742\uff1b\u540c\u65f6\u53d1\u73b0\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u7fa4\u4f53\u7684\u7cfb\u7edf\u6027\u60c5\u611f\u8bc6\u522b\u504f\u89c1\u3002", "conclusion": "LLMs\u5185\u5316\u4e86\u793e\u4f1a\u611f\u77e5\u7684\u67d0\u4e9b\u65b9\u9762\uff0c\u7814\u7a76\u4e3a\u57fa\u4e8e\u8ba4\u77e5\u7406\u8bba\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2507.11165", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11165", "abs": "https://arxiv.org/abs/2507.11165", "authors": ["Shixun Wu", "Jinwen Pan", "Jinyang Liu", "Jiannan Tian", "Ziwei Qiu", "Jiajun Huang", "Kai Zhao", "Xin Liang", "Sheng Di", "Zizhong Chen", "Franck Cappello"], "title": "Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration", "comment": "accepted by SC '25", "summary": "As high-performance computing architectures evolve, more scientific computing\nworkflows are being deployed on advanced computing platforms such as GPUs.\nThese workflows can produce raw data at extremely high throughputs, requiring\nurgent high-ratio and low-latency error-bounded data compression solutions. In\nthis paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific\nerror-bounded lossy compressor with a flexible, domain-irrelevant, and fully\nopen-source framework design. Our novel contributions are: 1) We maximally\noptimize the parallelized interpolation-based data prediction scheme on GPUs,\nenabling the full functionalities of interpolation-based scientific data\nprediction that are adaptive to diverse data characteristics; 2) We thoroughly\nexplore and investigate lossless data encoding techniques, then craft and\nincorporate the best-fit lossless encoding pipelines for maximizing the\ncompression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on\nbenchmarking datasets together with representative baselines. Compared to\nexisting state-of-the-art scientific lossy compressors, with comparative or\nbetter throughput than existing high-ratio scientific error-bounded lossy\ncompressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio\nimprovement under the same error bound, and up to 215% compression ratio\nimprovement under the same decompression data PSNR.", "AI": {"tldr": "cuSZ-Hi\u662f\u4e00\u79cd\u4f18\u5316\u7684GPU\u79d1\u5b66\u6570\u636e\u538b\u7f29\u5668\uff0c\u63d0\u4f9b\u9ad8\u538b\u7f29\u6bd4\u548c\u4f4e\u5ef6\u8fdf\uff0c\u652f\u6301\u591a\u6837\u5316\u6570\u636e\u7279\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u7684\u53d1\u5c55\u9700\u8981\u9ad8\u541e\u5410\u91cf\u6570\u636e\u538b\u7f29\u65b9\u6848\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u538b\u7f29\u6bd4\u548c\u5ef6\u8fdf\u4e0a\u4e0d\u8db3\u3002", "method": "\u4f18\u5316\u5e76\u884c\u63d2\u503c\u6570\u636e\u9884\u6d4b\u65b9\u6848\uff0c\u63a2\u7d22\u6700\u4f73\u65e0\u635f\u7f16\u7801\u6280\u672f\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6027\u80fd\u3002", "result": "cuSZ-Hi\u5728\u76f8\u540c\u8bef\u5dee\u9650\u5236\u4e0b\u538b\u7f29\u6bd4\u63d0\u5347249%\uff0c\u76f8\u540cPSNR\u4e0b\u63d0\u5347215%\u3002", "conclusion": "cuSZ-Hi\u662f\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u5f00\u6e90\u7684GPU\u79d1\u5b66\u6570\u636e\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11168", "categories": ["cs.NI", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11168", "abs": "https://arxiv.org/abs/2507.11168", "authors": ["Gabriele Formis", "Amanda Ericson", "Stefan Forsstrom", "Kyi Thar", "Gianluca Cena", "Stefano Scanzio"], "title": "Improving Wi-Fi Network Performance Prediction with Deep Learning Models", "comment": "preprint accepted, 8 pages, 2025", "summary": "The increasing need for robustness, reliability, and determinism in wireless\nnetworks for industrial and mission-critical applications is the driver for the\ngrowth of new innovative methods. The study presented in this work makes use of\nmachine learning techniques to predict channel quality in a Wi-Fi network in\nterms of the frame delivery ratio. Predictions can be used proactively to\nadjust communication parameters at runtime and optimize network operations for\nindustrial applications. Methods including convolutional neural networks and\nlong short-term memory were analyzed on datasets acquired from a real Wi-Fi\nsetup across multiple channels. The models were compared in terms of prediction\naccuracy and computational complexity. Results show that the frame delivery\nratio can be reliably predicted, and convolutional neural networks, although\nslightly less effective than other models, are more efficient in terms of CPU\nusage and memory consumption. This enhances the model's usability on embedded\nand industrial systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4bWi-Fi\u7f51\u7edc\u7684\u4fe1\u9053\u8d28\u91cf\uff08\u5e27\u4ea4\u4ed8\u7387\uff09\uff0c\u4ee5\u4f18\u5316\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u7f51\u7edc\u64cd\u4f5c\u3002", "motivation": "\u5de5\u4e1a\u4e0e\u5173\u952e\u4efb\u52a1\u5e94\u7528\u5bf9\u65e0\u7ebf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u3001\u53ef\u9760\u6027\u548c\u786e\u5b9a\u6027\u9700\u6c42\u589e\u52a0\uff0c\u63a8\u52a8\u4e86\u65b0\u65b9\u6cd5\u7684\u521b\u65b0\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff0c\u57fa\u4e8e\u771f\u5b9eWi-Fi\u6570\u636e\u96c6\u5206\u6790\u9884\u6d4b\u6548\u679c\u3002", "result": "\u5e27\u4ea4\u4ed8\u7387\u53ef\u88ab\u53ef\u9760\u9884\u6d4b\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5d4c\u5165\u5f0f\u4e0e\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.10630", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10630", "abs": "https://arxiv.org/abs/2507.10630", "authors": ["Ye Yang", "Xue Xiao", "Ping Yin", "Taotao Xie"], "title": "Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs", "comment": null, "summary": "API calls by large language models (LLMs) offer a cutting-edge approach for\ndata analysis. However, their ability to effectively utilize tools via API\ncalls remains underexplored in knowledge-intensive domains like meteorology.\nThis paper introduces KG2data, a system that integrates knowledge graphs, LLMs,\nReAct agents, and tool-use technologies to enable intelligent data acquisition\nand query handling in the meteorological field. Using a virtual API, we\nevaluate API call accuracy across three metrics: name recognition failure,\nhallucination failure, and call correctness. KG2data achieves superior\nperformance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and\nchat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based\nsystems by addressing their limited access to domain-specific knowledge, which\nhampers performance on complex or terminology-rich queries. By using a\nknowledge graph as persistent memory, our system enhances content retrieval,\ncomplex query handling, domain-specific reasoning, semantic relationship\nresolution, and heterogeneous data integration. It also mitigates the high cost\nof fine-tuning LLMs, making the system more adaptable to evolving domain\nknowledge and API structures. In summary, KG2data provides a novel solution for\nintelligent, knowledge-based question answering and data analysis in domains\nwith high knowledge demands.", "AI": {"tldr": "KG2data\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u3001LLM\u3001ReAct\u4ee3\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6280\u672f\uff0c\u63d0\u5347\u6c14\u8c61\u9886\u57df\u6570\u636e\u67e5\u8be2\u548c\u5206\u6790\u80fd\u529b\uff0c\u6027\u80fd\u4f18\u4e8eRAG2data\u548cchat2data\u3002", "motivation": "\u63a2\u7d22LLM\u901a\u8fc7API\u8c03\u7528\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u6c14\u8c61\u5b66\uff09\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edfLLM\u7cfb\u7edf\u5728\u590d\u6742\u67e5\u8be2\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165KG2data\u7cfb\u7edf\uff0c\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u3001LLM\u3001ReAct\u4ee3\u7406\u548c\u5de5\u5177\u6280\u672f\uff0c\u901a\u8fc7\u865a\u62dfAPI\u8bc4\u4f30API\u8c03\u7528\u51c6\u786e\u6027\u3002", "result": "KG2data\u5728\u540d\u79f0\u8bc6\u522b\u5931\u8d25\uff081.43%\uff09\u3001\u5e7b\u89c9\u5931\u8d25\uff080%\uff09\u548c\u8c03\u7528\u6b63\u786e\u6027\uff0888.57%\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u5bf9\u6bd4\u7cfb\u7edf\u3002", "conclusion": "KG2data\u4e3a\u9ad8\u77e5\u8bc6\u9700\u6c42\u9886\u57df\u63d0\u4f9b\u4e86\u667a\u80fd\u95ee\u7b54\u548c\u6570\u636e\u5206\u6790\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u5fae\u8c03\u6210\u672c\u5e76\u9002\u5e94\u77e5\u8bc6\u6f14\u8fdb\u3002"}}
{"id": "2507.10640", "categories": ["cs.SE", "cs.LG", "cs.SI", "D.2.2"], "pdf": "https://arxiv.org/pdf/2507.10640", "abs": "https://arxiv.org/abs/2507.10640", "authors": ["Labiba Farah", "Mohammad Ridwan Kabir", "Shohel Ahmed", "MD Mohaymen Ul Anam", "Md. Sakibul Islam"], "title": "SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications", "comment": "26 pages, 9 figures, 5 tables", "summary": "The widespread use of social media applications has raised significant\nprivacy concerns, often highlighted in user reviews. These reviews also provide\ndevelopers with valuable insights into improving apps by addressing issues and\nintroducing better features. However, the sheer volume and nuanced nature of\nreviews make manual identification and prioritization of privacy-related\nconcerns challenging for developers. Previous studies have developed software\nutilities to automatically classify user reviews as privacy-relevant,\nprivacy-irrelevant, bug reports, feature requests, etc., using machine\nlearning. Notably, there is a lack of focus on classifying reviews specifically\nas privacy-related feature requests, privacy-related bug reports, or\nprivacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated\nonline annotation tool designed to help developers annotate and classify user\nreviews into these categories. For automating the annotation of such reviews,\nthis paper introduces the annotation model, GRACE (GRU-based Attention with\nCBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words\n(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven\npopular social media apps on Google Play Store, including Instagram, Facebook,\nWhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were\nanalyzed. Two annotators manually labelled the reviews, achieving a Cohen's\nKappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement\nfor training machine learning models. Among the models tested, GRACE\ndemonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:\n0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates\nsignificant potential to assist developers with extracting and addressing\nprivacy-related feature requests or bug reports from user reviews, enhancing\nuser privacy and trust.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177SENSOR\uff0c\u7528\u4e8e\u5206\u7c7b\u7528\u6237\u8bc4\u8bba\u4e2d\u7684\u9690\u79c1\u76f8\u5173\u8bf7\u6c42\u548c\u9519\u8bef\u62a5\u544a\uff0c\u91c7\u7528GRACE\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u8bc4\u8bba\u4e2d\u9690\u79c1\u95ee\u9898\u7a81\u51fa\uff0c\u4f46\u624b\u52a8\u5206\u7c7b\u56f0\u96be\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9690\u79c1\u76f8\u5173\u8bf7\u6c42\u548c\u9519\u8bef\u62a5\u544a\u7684\u5206\u7c7b\u5de5\u5177\u3002", "method": "\u63d0\u51faGRACE\u6a21\u578b\uff08\u57fa\u4e8eGRU\u3001CBOW\u548c\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u5206\u679016000\u6761\u7528\u6237\u8bc4\u8bba\uff0c\u624b\u52a8\u6807\u6ce8\u5e76\u8bad\u7ec3\u6a21\u578b\u3002", "result": "GRACE\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08F1-score: 0.9434, ROC-AUC: 0.9934, \u51c6\u786e\u7387: 95.10%\uff09\u3002", "conclusion": "SENSOR\u5de5\u5177\u80fd\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u63d0\u53d6\u9690\u79c1\u76f8\u5173\u53cd\u9988\uff0c\u63d0\u5347\u7528\u6237\u9690\u79c1\u548c\u4fe1\u4efb\u3002"}}
{"id": "2507.10775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10775", "abs": "https://arxiv.org/abs/2507.10775", "authors": ["Jeffrey Joan Sam", "Janhavi Sathe", "Nikhil Chigali", "Naman Gupta", "Radhey Ruparel", "Yicheng Jiang", "Janmajay Singh", "James W. Berck", "Arko Barman"], "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers", "comment": null, "summary": "Spacecraft deployed in outer space are routinely subjected to various forms\nof damage due to exposure to hazardous environments. In addition, there are\nsignificant risks to the subsequent process of in-space repairs through human\nextravehicular activity or robotic manipulation, incurring substantial\noperational costs. Recent developments in image segmentation could enable the\ndevelopment of reliable and cost-effective autonomous inspection systems. While\nthese models often require large amounts of training data to achieve\nsatisfactory results, publicly available annotated spacecraft segmentation data\nare very scarce. Here, we present a new dataset of nearly 64k annotated\nspacecraft images that was created using real spacecraft models, superimposed\non a mixture of real and synthetic backgrounds generated using NASA's TTALOS\npipeline. To mimic camera distortions and noise in real-world image\nacquisition, we also added different types of noise and distortion to the\nimages. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to\ngenerate performance benchmarks for the dataset under well-defined hardware and\ninference time constraints to mimic real-world image segmentation challenges\nfor real-time onboard applications in space on NASA's inspector spacecraft. The\nresulting models, when tested under these constraints, achieved a Dice score of\n0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.\nThe dataset and models for performance benchmark are available at\nhttps://github.com/RiceD2KLab/SWiM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u822a\u5929\u5668\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u5b9e\u65f6\u81ea\u4e3b\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u5728YOLOv8\u548cYOLOv11\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6027\u80fd\u6d4b\u8bd5\u3002", "motivation": "\u822a\u5929\u5668\u5728\u592a\u7a7a\u73af\u5883\u4e2d\u6613\u53d7\u635f\u574f\uff0c\u4eba\u5de5\u6216\u673a\u5668\u4eba\u7ef4\u4fee\u6210\u672c\u9ad8\u4e14\u98ce\u9669\u5927\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u4e3b\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b64k\u6807\u6ce8\u56fe\u50cf\u7684\u822a\u5929\u5668\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u771f\u5b9e\u548c\u5408\u6210\u80cc\u666f\uff0c\u5e76\u6dfb\u52a0\u566a\u58f0\u548c\u5931\u771f\u4ee5\u6a21\u62df\u771f\u5b9e\u73af\u5883\u3002\u4f7f\u7528YOLOv8\u548cYOLOv11\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u7279\u5b9a\u786c\u4ef6\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86Dice\u5206\u65700.92\uff0cHausdorff\u8ddd\u79bb0.69\uff0c\u63a8\u7406\u65f6\u95f4\u7ea60.5\u79d2\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u822a\u5929\u5668\u5b9e\u65f6\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u592a\u7a7a\u81ea\u4e3b\u68c0\u6d4b\u5e94\u7528\u3002"}}
{"id": "2507.10743", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10743", "abs": "https://arxiv.org/abs/2507.10743", "authors": ["Nickolas Freeman", "Thanh Nguyen", "Gregory Bott", "Jason Parton", "Collin Francel"], "title": "Language Models for Adult Service Website Text Analysis", "comment": "32 pages, 12 figures, 1 table", "summary": "Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6210\u4eba\u670d\u52a1\u7f51\u7ad9\uff08ASW\uff09\u5e7f\u544a\u6587\u672c\u5206\u6790\u5728\u6253\u51fb\u6027\u4ea4\u6613\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5b9a\u5236Transformer\u6a21\u578b\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "ASW\u5e7f\u544a\u6587\u672c\u5206\u6790\u5bf9\u8bc6\u522b\u6027\u4ea4\u6613\u53d7\u5bb3\u8005\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6587\u672c\u7684\u590d\u6742\u6027\uff08\u5982\u8868\u60c5\u7b26\u53f7\u3001\u8bed\u6cd5\u6df7\u4e71\uff09\u589e\u52a0\u4e86\u5206\u6790\u96be\u5ea6\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\uff0c\u5305\u62ec\u4fe1\u606f\u68c0\u7d22\u3001\u9884\u8bad\u7ec3Transformer\u548c\u5b9a\u5236Transformer\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5b9a\u5236\u6a21\u578b\u7684\u9ad8\u6548\u6027\u3002", "result": "\u5b9a\u5236Transformer\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cROC AUC\u4e0a\u4f18\u4e8eBERT-base\u3001RoBERTa\u548cModernBERT\u3002", "conclusion": "\u5b9a\u5236\u6a21\u578b\u4e3aASW\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u53ef\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u548c\u7814\u7a76\u3002"}}
{"id": "2507.11289", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.11289", "abs": "https://arxiv.org/abs/2507.11289", "authors": ["Martin Rose", "Simon Homes", "Lukas Ramsperger", "Jose Gracia", "Christoph Niethammer", "Jadran Vrabec"], "title": "Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics", "comment": "Accepted for publication at HeteroPar 2025 co-located with Euro-Par\n  2025", "summary": "In the quest for highest performance in scientific computing, we present a\nnovel framework that relies on high-bandwidth communication between GPUs in a\ncompute cluster. The framework offers linear scaling of performance for\nexplicit algorithms that is only limited by the size of the dataset and the\nnumber of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)\nfrom one GPU, where they are processed, to the next, which results in a\nparallel-in-time parallelization. The user of the framework has to write GPU\nkernels that implement the algorithm and provide slices of the dataset.\nKnowledge about the underlying parallelization strategy is not required because\nthe communication between processes is carried out by the framework. As a case\nstudy, molecular dynamics simulation based on the Lennard-Jones potential is\nimplemented to measure the performance for a homogeneous fluid. Single node\nperformance and strong scaling behavior of this framework is compared to\nLAMMPS, which is outperformed in the strong scaling case.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u96c6\u7fa4\u7684\u9ad8\u5e26\u5bbd\u901a\u4fe1\u6846\u67b6\uff0c\u652f\u6301\u663e\u5f0f\u7b97\u6cd5\u7684\u7ebf\u6027\u6027\u80fd\u6269\u5c55\uff0c\u6027\u80fd\u4ec5\u53d7\u6570\u636e\u96c6\u5927\u5c0f\u548cGPU\u6570\u91cf\u9650\u5236\u3002", "motivation": "\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u8ffd\u6c42\u6700\u9ad8\u6027\u80fd\uff0c\u901a\u8fc7\u9ad8\u5e26\u5bbd\u901a\u4fe1\u5b9e\u73b0GPU\u96c6\u7fa4\u7684\u9ad8\u6548\u534f\u4f5c\u3002", "method": "\u91c7\u7528\u73af\u5f62\u8fdb\u7a0b\uff08GPU\uff09\u95f4\u7684\u6570\u636e\u5207\u7247\u4f20\u9012\uff0c\u5b9e\u73b0\u65f6\u95f4\u5e76\u884c\u5316\uff0c\u7528\u6237\u53ea\u9700\u7f16\u5199GPU\u5185\u6838\u7b97\u6cd5\uff0c\u65e0\u9700\u4e86\u89e3\u5e95\u5c42\u5e76\u884c\u7b56\u7565\u3002", "result": "\u4ee5Lennard-Jones\u52bf\u4e3a\u57fa\u7840\u7684\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e3a\u4f8b\uff0c\u8be5\u6846\u67b6\u5728\u5f3a\u6269\u5c55\u60c5\u51b5\u4e0b\u6027\u80fd\u4f18\u4e8eLAMMPS\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11250", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11250", "abs": "https://arxiv.org/abs/2507.11250", "authors": ["Mohamed Seliem", "Dirk Pesch", "Utz Roedig", "Cormac Sreenan"], "title": "Resilient Time-Sensitive Networking for Industrial IoT: Configuration and Fault-Tolerance Evaluation", "comment": "(c) 2025 IEEE. This is the author's version of a paper accepted for\n  presentation at the IEEE ETFA 2025 conference. The final version will appear\n  in the conference proceedings", "summary": "Time-Sensitive Networking (TSN) is increasingly adopted in industrial systems\nto meet strict latency, jitter, and reliability requirements. However,\nevaluating TSN's fault tolerance under realistic failure conditions remains\nchallenging. This paper presents IN2C, a modular OMNeT++/INET-based simulation\nframework that models two synchronized production cells connected to\ncentralized infrastructure. IN2C integrates core TSN features, including time\nsynchronization, traffic shaping, per-stream filtering, and Frame Replication\nand Elimination for Redundancy (FRER), alongside XML-driven fault injection for\nlink and node failures. Four fault scenarios are evaluated to compare TSN\nperformance with and without redundancy. Results show that FRER eliminates\npacket loss and achieves submillisecond recovery, though with 2-3x higher link\nutilization. These findings offer practical guidance for deploying TSN in\nbandwidth-constrained industrial environments.", "AI": {"tldr": "IN2C\u662f\u4e00\u4e2a\u57fa\u4e8eOMNeT++/INET\u7684\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30TSN\u5728\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660eFRER\u80fd\u6d88\u9664\u4e22\u5305\u4f46\u589e\u52a0\u94fe\u8def\u5229\u7528\u7387\u3002", "motivation": "\u8bc4\u4f30TSN\u5728\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1IN2C\u6846\u67b6\uff0c\u96c6\u6210TSN\u6838\u5fc3\u529f\u80fd\uff08\u5982\u65f6\u95f4\u540c\u6b65\u3001\u6d41\u91cf\u6574\u5f62\u3001FRER\uff09\u548cXML\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\uff0c\u6a21\u62df\u56db\u79cd\u6545\u969c\u573a\u666f\u3002", "result": "FRER\u6d88\u9664\u4e86\u4e22\u5305\u5e76\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7\u6062\u590d\uff0c\u4f46\u94fe\u8def\u5229\u7528\u7387\u589e\u52a02-3\u500d\u3002", "conclusion": "IN2C\u4e3a\u5e26\u5bbd\u53d7\u9650\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72TSN\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.10644", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10644", "abs": "https://arxiv.org/abs/2507.10644", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86Web of Agents (WoA)\u7684\u6f14\u53d8\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u8f74\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u4ece\u8bed\u4e49Web\u5230\u73b0\u4ee3Agentic AI\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5e76\u547c\u5401\u89e3\u51b3\u793e\u4f1a\u6280\u672f\u6311\u6218\u3002", "motivation": "\u7814\u7a76WoA\u7684\u6f14\u53d8\uff0c\u63ed\u793a\u73b0\u4ee3\u534f\u8bae\u4e0e\u65e9\u671f\u6807\u51c6\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u5f15\u5165\u56db\u8f74\u5206\u7c7b\u6cd5\uff08\u8bed\u4e49\u57fa\u7840\u3001\u901a\u4fe1\u8303\u5f0f\u3001\u667a\u80fd\u4e2d\u5fc3\u3001\u53d1\u73b0\u673a\u5236\uff09\u6765\u6bd4\u8f83\u4e0d\u540c\u4e16\u4ee3\u7684\u4ee3\u7406\u67b6\u6784\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4e2d\u5fc3\u4ece\u5916\u90e8\u6570\u636e\u6216\u5e73\u53f0\u8f6c\u79fb\u5230\u4ee3\u7406\u6838\u5fc3\u6a21\u578b\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u652f\u6301\u4e86\u73b0\u4ee3Agentic AI\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u65b0\u534f\u8bae\u867d\u91cd\u8981\uff0c\u4f46\u4e0d\u8db3\u4ee5\u6784\u5efa\u5f3a\u5927\u3001\u5f00\u653e\u3001\u53ef\u4fe1\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u672a\u6765\u7814\u7a76\u9700\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\u3001\u7ecf\u6d4e\u6a21\u578b\u3001\u5b89\u5168\u4e0e\u6cbb\u7406\u7b49\u6311\u6218\u3002"}}
{"id": "2507.10641", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10641", "abs": "https://arxiv.org/abs/2507.10641", "authors": ["Jayant Havare", "Saurav Chaudhary", "Ganesh Ramakrishnan", "Kaushik Maharajan", "Srikanth Tamilselvam"], "title": "A Code Comprehension Benchmark for Large Language Models for Code", "comment": "10 Pages, 5 Figures", "summary": "Large Language Models have shown impressive capabilities in coding tasks like\ncode generation and code completion, as they have been trained on a large\namount of code data. Also, since one of the core pretraining objectives is Next\nToken Prediction, these models tends to learn surface-level syntactic patterns\nin code. However, this does not guarantee code comprehension ability i.e. the\nability to capture the semantics of the code. In our opinion, this is the\nreason why these models often underperform on tasks that require deeper\nsemantic understanding, such as code debugging and code optimization. To\naddress this, we propose fine-tuning these models specifically for code\ncomprehension tasks using large-scale datasets, enabling them to develop a more\nrobust understanding of code semantics. We evaluate three code models of\nvarying sizes on a suite of code comprehension tasks designed to assess\nsemantic understanding beyond surface-level syntactic pattern matching. In\nparticular, we analyze performance on the Subjectivity Grading Task and observe\nthat model performance improves after fine-tuning on relevant downstream tasks.\nThe most significant improvement is seen in the QWQ-32B model, where accuracy\nincreases from 70% to 83.47%. A similar or explainable trend is observed across\nother models, clearly indicating an enhancement in code comprehension ability.\nAmong the models studied, the DPO-fine-tuned Codestral-22B achieves the highest\nmicro-accuracy of 87.66% on the Subjectivity Grading Task.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6307\u51fa\u5176\u867d\u64c5\u957f\u8bed\u6cd5\u6a21\u5f0f\u5339\u914d\uff0c\u4f46\u7f3a\u4e4f\u4ee3\u7801\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u9488\u5bf9\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u8bed\u4e49\u7684\u6df1\u5c42\u7406\u89e3\uff0c\u5bfc\u81f4\u5728\u8c03\u8bd5\u548c\u4f18\u5316\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\uff0c\u4ee5\u589e\u5f3a\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662fQWQ-32B\u6a21\u578b\u51c6\u786e\u7387\u4ece70%\u63d0\u5347\u81f383.47%\uff0cCodestral-22B\u6a21\u578b\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u738787.66%\u3002", "conclusion": "\u9488\u5bf9\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u7684\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.10778", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10778", "abs": "https://arxiv.org/abs/2507.10778", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Warehouse Spatial Question Answering with LLM Agent", "comment": "1st Place Solution of the 9th AI City Challenge Track 3", "summary": "Spatial understanding has been a challenging task for existing Multi-modal\nLarge Language Models~(MLLMs). Previous methods leverage large-scale MLLM\nfinetuning to enhance MLLM's spatial understanding ability. In this paper, we\npresent a data-efficient approach. We propose a LLM agent system with strong\nand advanced spatial reasoning ability, which can be used to solve the\nchallenging spatial question answering task in complex indoor warehouse\nscenarios. Our system integrates multiple tools that allow the LLM agent to\nconduct spatial reasoning and API tools interaction to answer the given\ncomplicated spatial question. Extensive evaluations on the 2025 AI City\nChallenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that\nour system achieves high accuracy and efficiency in tasks such as object\nretrieval, counting, and distance estimation. The code is available at:\nhttps://github.com/hsiangwei0903/SpatialAgent", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u7cfb\u7edf\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u590d\u6742\u5ba4\u5185\u4ed3\u5e93\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u63d0\u5347\u5176\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aLLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u96c6\u6210\u591a\u79cd\u5de5\u5177\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\u548cAPI\u4ea4\u4e92\u3002", "result": "\u5728AI City Challenge\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u590d\u6742\u7a7a\u95f4\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10772", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10772", "abs": "https://arxiv.org/abs/2507.10772", "authors": ["Michal Podstawski"], "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u5d4c\u5165\u6a21\u578b\u589e\u5f3a\u5c5e\u6027\u56fe\u7684\u8bed\u4e49\u5206\u6790\uff0c\u63d0\u5347\u8282\u70b9\u5206\u7c7b\u548c\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c5e\u6027\u56fe\u4e2d\u4e30\u5bcc\u7684\u6587\u672c\u5c5e\u6027\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u5e0c\u671b\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u589e\u5f3a\u8bed\u4e49\u5206\u6790\u80fd\u529b\u3002", "method": "\u5c06\u6587\u672c\u8282\u70b9\u548c\u8fb9\u7684\u5c5e\u6027\u5d4c\u5165\u5230\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4e0d\u6539\u53d8\u56fe\u7ed3\u6784\uff0c\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u6587\u672c\u8bed\u4e49\u663e\u8457\u63d0\u5347\u4e86\u5c5e\u6027\u56fe\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6587\u672c\u5d4c\u5165\u6a21\u578b\u80fd\u6709\u6548\u589e\u5f3a\u5c5e\u6027\u56fe\u7684\u8bed\u4e49\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2507.11386", "categories": ["cs.DC", "65M50, 65N50"], "pdf": "https://arxiv.org/pdf/2507.11386", "abs": "https://arxiv.org/abs/2507.11386", "authors": ["Carsten Burstedde", "Mikhail Kirilin", "Robert Kl\u00f6fkorn"], "title": "A new Dune grid for scalable dynamic adaptivity based on the p4est software library", "comment": "27 pages, 8 figures, 2 algorithms", "summary": "In this work we extend the Dune solver library with another grid interface to\nthe open-source p4est software. While Dune already supports about a dozen\ndifferent mesh implementations through its mesh interface Dune-Grid, we\nundertake this new coupling effort in order to inherit p4est's practically\nunlimited MPI scalability as well as its relatively thin data structures, and\nits native support for multi-block (forest) mesh topologies in both 2D and 3D.\n  The presented implementation is compared to an existing implementation based\non Dune-ALUGrid for a variety of challenging test examples in a parallel\nenvironment. The numerical experiments show that the implementation presented\nhere is outperforming Dune-ALUGrid in terms of scalability. In addition, an\nalternative balancing strategy is presented to ensure 2:1 balancing across\nelement faces showing improved performance compared to the existing p4est\nbalance strategy in the numerical examples considered in this work.", "AI": {"tldr": "\u6269\u5c55Dune\u6c42\u89e3\u5668\u5e93\u4ee5\u652f\u6301p4est\u7f51\u683c\u63a5\u53e3\uff0c\u63d0\u5347MPI\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u7ee7\u627fp4est\u7684\u9ad8MPI\u53ef\u6269\u5c55\u6027\u3001\u8f7b\u91cf\u6570\u636e\u7ed3\u6784\u548c\u591a\u5757\u7f51\u683c\u62d3\u6251\u652f\u6301\u3002", "method": "\u5b9e\u73b0Dune\u4e0ep4est\u7684\u8026\u5408\uff0c\u5e76\u4e0eDune-ALUGrid\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u65b0\u5b9e\u73b0\u4f18\u4e8eDune-ALUGrid\uff0c\u5c24\u5176\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\uff1b\u63d0\u51fa\u6539\u8fdb\u7684\u5e73\u8861\u7b56\u7565\u3002", "conclusion": "\u65b0\u63a5\u53e3\u548c\u5e73\u8861\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5e76\u884c\u73af\u5883\u3002"}}
{"id": "2507.11483", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11483", "abs": "https://arxiv.org/abs/2507.11483", "authors": ["Ioannis Panitsas", "Yagmur Yigit", "Leandros Tassiulas", "Leandros Maglaras", "Berk Canberk"], "title": "JamShield: A Machine Learning Detection System for Over-the-Air Jamming Attacks", "comment": "Accepted for presentation at IEEE International Conference on\n  Communications (ICC), 2025", "summary": "Wireless networks are vulnerable to jamming attacks due to the shared\ncommunication medium, which can severely degrade performance and disrupt\nservices. Despite extensive research, current jamming detection methods often\nrely on simulated data or proprietary over-the-air datasets with limited\ncross-layer features, failing to accurately represent the real state of a\nnetwork and thus limiting their effectiveness in real-world scenarios. To\naddress these challenges, we introduce JamShield, a dynamic jamming detection\nsystem trained on our own collected over-the-air and publicly available\ndataset. It utilizes hybrid feature selection to prioritize relevant features\nfor accurate and efficient detection. Additionally, it includes an\nauto-classification module that dynamically adjusts the classification\nalgorithm in real-time based on current network conditions. Our experimental\nresults demonstrate significant improvements in detection rate, precision, and\nrecall, along with reduced false alarms and misdetections compared to\nstate-of-the-art detection algorithms, making JamShield a robust and reliable\nsolution for detecting jamming attacks in real-world wireless networks.", "AI": {"tldr": "JamShield\u662f\u4e00\u79cd\u52a8\u6001\u5e72\u6270\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u7279\u5f81\u9009\u62e9\u548c\u5b9e\u65f6\u5206\u7c7b\u7b97\u6cd5\u8c03\u6574\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u6613\u53d7\u5e72\u6270\u653b\u51fb\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u6216\u6709\u9650\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u5de5\u4f5c\u3002", "method": "JamShield\u5229\u7528\u6df7\u5408\u7279\u5f81\u9009\u62e9\u548c\u81ea\u52a8\u5206\u7c7b\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u7b97\u6cd5\u4ee5\u9002\u5e94\u5b9e\u65f6\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cJamShield\u5728\u68c0\u6d4b\u7387\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bef\u62a5\u548c\u6f0f\u68c0\u3002", "conclusion": "JamShield\u4e3a\u771f\u5b9e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e72\u6270\u653b\u51fb\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10740", "categories": ["cs.AI", "cs.NE", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10740", "abs": "https://arxiv.org/abs/2507.10740", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "title": "Parsing Musical Structure to Enable Meaningful Variations", "comment": null, "summary": "This paper presents a novel rule-based approach for generating music by\nvarying existing tunes. We parse each tune to find the Pathway Assembly (PA) [\n1], that is a structure representing all repetitions in the tune. The Sequitur\nalgorithm [2 ] is used for this. The result is a grammar. We then carry out\nmutation on the grammar, rather than on a tune directly. There are potentially\n19 types of mutations such as adding, removing, swapping or reversing parts of\nthe grammar that can be applied to the grammars. The system employs one of the\nmutations randomly in this step to automatically manipulate the grammar.\nFollowing the mutation, we need to expand the grammar which returns a new tune.\nThe output after 1 or more mutations will be a new tune related to the original\ntune. Our study examines how tunes change gradually over the course of multiple\nmutations. Edit distances, structural complexity and length of the tunes are\nused to show how a tune is changed after multiple mutations. In addition, the\nsize of effect of each mutation type is analyzed. As a final point, we review\nthe musical aspect of the output tunes. It should be noted that the study only\nfocused on generating new pitch sequences. The study is based on an Irish\ntraditional tune dataset and a list of integers has been used to represent each\ntune's pitch values.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u97f3\u4e50\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5f02\u73b0\u6709\u66f2\u8c03\u751f\u6210\u65b0\u97f3\u4e50\u3002\u5229\u7528Sequitur\u7b97\u6cd5\u89e3\u6790\u66f2\u8c03\u4e3a\u8bed\u6cd5\u7ed3\u6784\uff0c\u968f\u673a\u5e94\u752819\u79cd\u53d8\u5f02\u7c7b\u578b\uff0c\u751f\u6210\u65b0\u66f2\u8c03\uff0c\u5e76\u5206\u6790\u5176\u53d8\u5316\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u89c4\u5219\u548c\u53d8\u5f02\u65b9\u6cd5\u4ece\u73b0\u6709\u66f2\u8c03\u751f\u6210\u65b0\u97f3\u4e50\uff0c\u5e76\u5206\u6790\u53d8\u5f02\u5bf9\u66f2\u8c03\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Sequitur\u7b97\u6cd5\u89e3\u6790\u66f2\u8c03\u4e3a\u8bed\u6cd5\u7ed3\u6784\uff08PA\uff09\uff0c\u968f\u673a\u5e94\u752819\u79cd\u53d8\u5f02\u7c7b\u578b\uff08\u5982\u6dfb\u52a0\u3001\u5220\u9664\u3001\u4ea4\u6362\u7b49\uff09\uff0c\u751f\u6210\u65b0\u66f2\u8c03\u3002", "result": "\u901a\u8fc7\u7f16\u8f91\u8ddd\u79bb\u3001\u7ed3\u6784\u590d\u6742\u5ea6\u548c\u66f2\u8c03\u957f\u5ea6\u5206\u6790\u53d8\u5f02\u6548\u679c\uff0c\u5e76\u8bc4\u4f30\u6bcf\u79cd\u53d8\u5f02\u7c7b\u578b\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u4e0e\u539f\u59cb\u66f2\u8c03\u76f8\u5173\u7684\u65b0\u97f3\u4e50\uff0c\u4f46\u4ec5\u5173\u6ce8\u97f3\u9ad8\u5e8f\u5217\u751f\u6210\uff0c\u672a\u6d89\u53ca\u5176\u4ed6\u97f3\u4e50\u5143\u7d20\u3002"}}
{"id": "2507.10646", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10646", "abs": "https://arxiv.org/abs/2507.10646", "authors": ["Myeongsoo Kim", "Shweta Garg", "Baishakhi Ray", "Varun Kumar", "Anoop Deoras"], "title": "CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance", "comment": null, "summary": "Programming assistants powered by large language models have transformed\nsoftware development, yet most benchmarks focus narrowly on code generation\ntasks. Recent efforts like InfiBench and StackEval attempt to address this gap\nusing Stack Overflow data but remain limited to single-turn interactions in\nisolated contexts, require significant manual curation, and fail to represent\ncomplete project environments. We introduce CodeAssistBench (CAB), the first\nbenchmark framework for evaluating multi-turn programming assistance in\nrealistic settings that address real-world questions about actual codebases.\nUnlike existing programming Q&A benchmarks, CAB automatically generates\nscalable datasets from question-related GitHub issues using configurable\nparameters (e.g., repository creation date, star count, programming languages),\nand includes automatic containerization of codebases for evaluation. It then\nevaluates models through simulated users in these containerized environments\nwith full codebase access. Using this framework, we constructed a test set of\n3,286 real-world programming questions across 231 repositories, spanning seven\nprogramming languages and diverse problem domains. Our evaluation of leading\nLLMs reveals a substantial capability gap: while models perform well on Stack\nOverflow questions with success rates of 70-83%, they resolve only up to 16.49%\nof CAB's recent issues. This discrepancy highlights the challenges of providing\nassistance in complex, project-specific contexts versus answering standalone\nquestions.", "AI": {"tldr": "CodeAssistBench (CAB) \u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u7f16\u7a0b\u8f85\u52a9\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4ee3\u7801\u5e93\u95ee\u9898\u81ea\u52a8\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u9879\u76ee\u73af\u5883\u4e2d\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7f16\u7a0b\u52a9\u624b\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8f6e\u4ea4\u4e92\u548c\u771f\u5b9e\u9879\u76ee\u73af\u5883\u7684\u8bc4\u4f30\u3002CAB \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u8d34\u8fd1\u5b9e\u9645\u5f00\u53d1\u573a\u666f\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "CAB \u901a\u8fc7\u4ece GitHub \u95ee\u9898\u81ea\u52a8\u751f\u6210\u53ef\u6269\u5c55\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u5bb9\u5668\u5316\u4ee3\u7801\u5e93\u548c\u6a21\u62df\u7528\u6237\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u8bc4\u4f30\u3002", "result": "\u5728 3,286 \u4e2a\u771f\u5b9e\u7f16\u7a0b\u95ee\u9898\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u6a21\u578b\u5728 Stack Overflow \u4e0a\u8868\u73b0\u826f\u597d\uff0870-83% \u6210\u529f\u7387\uff09\uff0c\u4f46\u5728 CAB \u7684\u590d\u6742\u95ee\u9898\u4e2d\u4ec5\u89e3\u51b3 16.49%\u3002", "conclusion": "CAB \u63ed\u793a\u4e86\u6a21\u578b\u5728\u9879\u76ee\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7f16\u7a0b\u8f85\u52a9\u7684\u6311\u6218\u3002"}}
{"id": "2507.10800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10800", "abs": "https://arxiv.org/abs/2507.10800", "authors": ["Ali Hojjat", "Janek Haberer", "Soren Pirk", "Olaf Landsiedel"], "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference", "comment": "Under Review", "summary": "Vision Transformers deliver state-of-the-art performance, yet their fixed\ncomputational budget prevents scalable deployment across heterogeneous\nhardware. Recent nested Transformer architectures mitigate this by embedding\nnested subnetworks within a single model to enable scalable inference. However,\nthese models allocate the same amount of compute to all inputs, regardless of\ntheir complexity, which leads to inefficiencies. To address this, we introduce\nThinkingViT, a nested ViT architecture that employs progressive thinking stages\nto dynamically adjust inference computation based on input difficulty.\nThinkingViT initiates inference by activating a small subset of the most\nimportant attention heads and terminates early if predictions reach sufficient\ncertainty. Otherwise, it activates additional attention heads and re-evaluates\nthe input. At the core of ThinkingViT is our Token Recycling mechanism, which\nconditions each subsequent inference stage on the embeddings from the previous\nstage, enabling progressive improvement. Due to its backbone-preserving design,\nThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show\nthat ThinkingViT surpasses nested baselines by up to 2.0 percentage points\n(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs\non ImageNet-1K. The source code is available at\nhttps://github.com/ds-kiel/ThinkingViT.", "AI": {"tldr": "ThinkingViT\u662f\u4e00\u79cd\u5d4c\u5957ViT\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u5d4c\u5957Transformer\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u548c\u8f93\u5165\u590d\u6742\u5ea6\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u601d\u7ef4\u9636\u6bb5\u548cToken Recycling\u673a\u5236\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728\u76f8\u540c\u541e\u5410\u91cf\u4e0b\u7cbe\u5ea6\u63d0\u53472.0 p.p.\uff0c\u76f8\u540c\u8ba1\u7b97\u91cf\u4e0b\u63d0\u53472.9 p.p.\u3002", "conclusion": "ThinkingViT\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709ViT\u6a21\u578b\uff0c\u4e3a\u52a8\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10787", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10787", "abs": "https://arxiv.org/abs/2507.10787", "authors": ["Yilun Zhao", "Chengye Wang", "Chuhan Li", "Arman Cohan"], "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers", "comment": "ACL 2025 Findings", "summary": "This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature.", "AI": {"tldr": "MISS-QA\u662f\u9996\u4e2a\u8bc4\u4f30\u6a21\u578b\u89e3\u8bfb\u79d1\u5b66\u6587\u732e\u4e2d\u793a\u610f\u56fe\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b1,500\u4e2a\u4e13\u5bb6\u6807\u6ce8\u793a\u4f8b\u3002\u8bc4\u4f3018\u79cd\u524d\u6cbf\u591a\u6a21\u6001\u6a21\u578b\uff0c\u53d1\u73b0\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u793a\u610f\u56fe\u662f\u91cd\u8981\u4fe1\u606f\u8f7d\u4f53\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5bf9\u5176\u89e3\u8bfb\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u4e13\u95e8\u57fa\u51c6\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u5305\u542b1,500\u4e2a\u793a\u4f8b\u7684MISS-QA\u57fa\u51c6\uff0c\u6d4b\u8bd518\u79cd\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u793a\u610f\u56fe\u7684\u89e3\u8bfb\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u9519\u8bef\u5206\u6790\u3002", "result": "\u6a21\u578b\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5c24\u5176\u5728\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u4e0a\uff0c\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MISS-QA\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u79d1\u5b66\u6587\u732e\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2507.11417", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11417", "abs": "https://arxiv.org/abs/2507.11417", "authors": ["Miray \u00d6zcan", "Philipp Wiesner", "Philipp Wei\u00df", "Odej Kao"], "title": "Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations", "comment": "Presented at the Workshop on Performance and Energy Efficiency in\n  Concurrent and Distributed Systems (PECS) at Euro-PAR'25", "summary": "The environmental impact of Large Language Models (LLMs) is rising\nsignificantly, with inference now accounting for more than half of their total\nlifecycle carbon emissions. However, existing simulation frameworks, which are\nincreasingly used to determine efficient LLM deployments, lack any concept of\npower and, therefore, cannot accurately estimate inference-related emissions.\nWe present a simulation framework to assess the energy and carbon implications\nof LLM inference under varying deployment setups. First, we extend a\nhigh-fidelity LLM inference simulator with a GPU power model that estimates\npower consumption based on utilization metrics, enabling analysis across\nconfigurations like batch size, sequence length, and model parallelism. Second,\nwe integrate simulation outputs into an energy system co-simulation environment\nto quantify carbon emissions under specific grid conditions and explore the\npotential of carbon-aware scheduling. Through scenario-based analysis, our\nframework reveals how inference parameters affect energy demand and carbon\nfootprint, demonstrates a renewable offset potential of up to 69.2% in an\nillustrative deployment case, and provides a foundation for future carbon-aware\ninference infrastructure design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u5728\u4e0d\u540c\u90e8\u7f72\u8bbe\u7f6e\u4e0b\u7684\u80fd\u6e90\u548c\u78b3\u6392\u653e\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u6846\u67b6\u7f3a\u4e4f\u5bf9\u80fd\u6e90\u6d88\u8017\u7684\u8003\u8651\uff0c\u65e0\u6cd5\u51c6\u786e\u4f30\u8ba1LLM\u63a8\u7406\u76f8\u5173\u7684\u78b3\u6392\u653e\u3002", "method": "\u6269\u5c55\u4e86\u9ad8\u4fdd\u771fLLM\u63a8\u7406\u6a21\u62df\u5668\uff0c\u96c6\u6210GPU\u529f\u8017\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u80fd\u6e90\u7cfb\u7edf\u534f\u540c\u6a21\u62df\u73af\u5883\uff0c\u91cf\u5316\u78b3\u6392\u653e\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86\u63a8\u7406\u53c2\u6570\u5bf9\u80fd\u6e90\u9700\u6c42\u548c\u78b3\u8db3\u8ff9\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u62b5\u6d88\u6f5c\u529b\u9ad8\u8fbe69.2%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u78b3\u611f\u77e5\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.10750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10750", "abs": "https://arxiv.org/abs/2507.10750", "authors": ["Pandu Devarakota", "Nicolas Tsesmetzis", "Faruk O. Alpak", "Apurva Gala", "Detlef Hohl"], "title": "AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition", "comment": "Technical article to be submitted to Data Centric Engineering Journal", "summary": "Thanks to the availability of massive amounts of data, computing resources,\nand advanced algorithms, AI has entered nearly every sector. This has sparked\nsignificant investment and interest, particularly in building data centers with\nthe necessary hardware and software to develop and operate AI models and\nAI-based workflows. In this technical review article, we present energy\nconsumption scenarios of data centers and impact on GHG emissions, considering\nboth near-term projections (up to 2030) and long-term outlook (2035 and\nbeyond). We address the quintessential question of whether AI will have a net\npositive, neutral, or negative impact on CO2 emissions by 2035. Additionally,\nwe discuss AI's potential to automate, create efficient and disruptive\nworkflows across various fields related to energy production, supply and\nconsumption. In the near-term scenario, the growing demand for AI will likely\nstrain computing resources, lead to increase in electricity consumption and\ntherefore associated CO2 emissions. This is due to the power-hungry nature of\nbig data centers and the requirements for training and running of large and\ncomplex AI models, as well as the penetration of AI assistant search and\napplications for public use. However, the long-term outlook could be more\npromising. AI has the potential to be a game-changer in CO2 reduction. Its\nability to further automate and optimize processes across industries, from\nenergy production to logistics, could significantly decrease our carbon\nfootprint. This positive impact is anticipated to outweigh the initial\nemissions bump, creating value for businesses and society in areas where\ntraditional solutions have fallen short. In essence, AI might cause some\ninitial growing pains for the environment, but it has the potential to support\nclimate mitigation efforts.", "AI": {"tldr": "AI\u7684\u53d1\u5c55\u77ed\u671f\u5185\u53ef\u80fd\u589e\u52a0\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u6e90\u6d88\u8017\u548cCO2\u6392\u653e\uff0c\u4f46\u957f\u671f\u6765\u770b\uff0cAI\u6709\u671b\u901a\u8fc7\u4f18\u5316\u80fd\u6e90\u751f\u4ea7\u548c\u7269\u6d41\u7b49\u884c\u4e1a\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u78b3\u8db3\u8ff9\u3002", "motivation": "\u63a2\u8ba8AI\u5bf9\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u6d88\u8017\u548c\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u5176\u77ed\u671f\u548c\u957f\u671f\u5bf9CO2\u6392\u653e\u7684\u51c0\u6548\u5e94\u3002", "method": "\u5206\u6790\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u6e90\u6d88\u8017\u60c5\u666f\uff0c\u7ed3\u5408\u8fd1\u8fdc\u671f\uff082030\u5e74\u53ca\u4ee5\u540e\uff09\u7684\u9884\u6d4b\uff0c\u8bc4\u4f30AI\u5bf9CO2\u6392\u653e\u7684\u5f71\u54cd\u3002", "result": "\u77ed\u671f\u5185AI\u9700\u6c42\u589e\u957f\u53ef\u80fd\u5bfc\u81f4CO2\u6392\u653e\u589e\u52a0\uff0c\u4f46\u957f\u671f\u6765\u770bAI\u6709\u671b\u901a\u8fc7\u4f18\u5316\u6d41\u7a0b\u663e\u8457\u51cf\u5c11\u78b3\u6392\u653e\u3002", "conclusion": "AI\u521d\u671f\u53ef\u80fd\u5bf9\u73af\u5883\u9020\u6210\u538b\u529b\uff0c\u4f46\u957f\u671f\u5c06\u6210\u4e3a\u6c14\u5019\u7f13\u89e3\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.10729", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10729", "abs": "https://arxiv.org/abs/2507.10729", "authors": ["Duong Nguyen", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "title": "Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction", "comment": null, "summary": "Modern software systems are increasingly complex, presenting significant\nchallenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)\nis a proactive approach to identifying vulnerable commits and providing early\nwarnings about potential security risks. However, we observe that current\nJIT-VP evaluations rely on an idealized setting, where the evaluation datasets\nare artificially balanced, consisting exclusively of vulnerability-introducing\nand vulnerability-fixing commits.\n  To address this limitation, this study assesses the effectiveness of JIT-VP\ntechniques under a more realistic setting that includes both\nvulnerability-related and vulnerability-neutral commits. To enable a reliable\nevaluation, we introduce a large-scale public dataset comprising over one\nmillion commits from FFmpeg and the Linux kernel. Our empirical analysis of\neight state-of-the-art JIT-VP techniques reveals a significant decline in\npredictive performance when applied to real-world conditions; for example, the\naverage PR-AUC on Linux drops 98\\% from 0.805 to 0.016. This discrepancy is\nmainly attributed to the severe class imbalance in real-world datasets, where\nvulnerability-introducing commits constitute only a small fraction of all\ncommits.\n  To mitigate this issue, we explore the effectiveness of widely adopted\ntechniques for handling dataset imbalance, including customized loss functions,\noversampling, and undersampling. Surprisingly, our experimental results\nindicate that these techniques are ineffective in addressing the imbalance\nproblem in JIT-VP. These findings underscore the importance of realistic\nevaluations of JIT-VP and the need for domain-specific techniques to address\ndata imbalance in such scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5373\u65f6\u6f0f\u6d1e\u9884\u6d4b\uff08JIT-VP\uff09\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u7406\u60f3\u5316\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u5206\u6790\uff0c\u6307\u51fa\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\uff0c\u8d28\u91cf\u4fdd\u8bc1\u9762\u4e34\u6311\u6218\u3002JIT-VP\u662f\u4e00\u79cd\u4e3b\u52a8\u8bc6\u522b\u6f0f\u6d1e\u7684\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u4f9d\u8d56\u7406\u60f3\u5316\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u73b0\u5b9e\u6027\u3002", "method": "\u7814\u7a76\u5f15\u5165\u5305\u542b\u6f0f\u6d1e\u76f8\u5173\u548c\u6f0f\u6d1e\u65e0\u5173\u63d0\u4ea4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08FFmpeg\u548cLinux\u5185\u6838\uff09\uff0c\u8bc4\u4f30\u516b\u79cd\u5148\u8fdbJIT-VP\u6280\u672f\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u7684\u65b9\u6cd5\u3002", "result": "\u73b0\u5b9e\u6761\u4ef6\u4e0bJIT-VP\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5982PR-AUC\u4ece0.805\u964d\u81f30.016\uff09\uff0c\u73b0\u6709\u6570\u636e\u4e0d\u5e73\u8861\u5904\u7406\u6280\u672f\uff08\u5982\u8fc7\u91c7\u6837\u3001\u6b20\u91c7\u6837\uff09\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u73b0\u5b9e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u9700\u8981\u9886\u57df\u7279\u5b9a\u6280\u672f\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2507.10844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10844", "abs": "https://arxiv.org/abs/2507.10844", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "LLM-Guided Agentic Object Detection for Open-World Understanding", "comment": null, "summary": "Object detection traditionally relies on fixed category sets, requiring\ncostly re-training to handle novel objects. While Open-World and\nOpen-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD\nlacks semantic labels for unknowns, and OVOD depends on user prompts, limiting\nautonomy. We propose an LLM-guided agentic object detection (LAOD) framework\nthat enables fully label-free, zero-shot detection by prompting a Large\nLanguage Model (LLM) to generate scene-specific object names. These are passed\nto an open-vocabulary detector for localization, allowing the system to adapt\nits goals dynamically. We introduce two new metrics, Class-Agnostic Average\nPrecision (CAAP) and Semantic Naming Average Precision (SNAP), to separately\nevaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD\nvalidate our approach, showing strong performance in detecting and naming novel\nobjects. Our method offers enhanced autonomy and adaptability for open-world\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff08LAOD\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u573a\u666f\u7279\u5b9a\u5bf9\u8c61\u540d\u79f0\uff0c\u5b9e\u73b0\u65e0\u9700\u6807\u7b7e\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u4f9d\u8d56\u56fa\u5b9a\u7c7b\u522b\u96c6\uff0c\u7075\u6d3b\u6027\u4e0d\u8db3\uff1b\u73b0\u6709\u5f00\u653e\u4e16\u754c\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u6807\u7b7e\u7f3a\u5931\u6216\u4f9d\u8d56\u7528\u6237\u63d0\u793a\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u573a\u666f\u7279\u5b9a\u5bf9\u8c61\u540d\u79f0\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u8fdb\u884c\u5b9a\u4f4d\uff0c\u52a8\u6001\u8c03\u6574\u68c0\u6d4b\u76ee\u6807\u3002", "result": "\u5728LVIS\u3001COCO\u548cCOCO-OOD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u68c0\u6d4b\u5e76\u547d\u540d\u65b0\u5bf9\u8c61\u3002", "conclusion": "LAOD\u6846\u67b6\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u7406\u89e3\u7684\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.10810", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.10810", "abs": "https://arxiv.org/abs/2507.10810", "authors": ["David M. Markowitz", "Samuel Hardman Taylor"], "title": "Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler", "comment": null, "summary": "In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u793e\u4ea4\u5e73\u53f0\u4e0a\u7684\u4ec7\u6068\u8a00\u8bba\u5e76\u672a\u56e0\u83b7\u5f97\u793e\u4f1a\u8ba4\u53ef\u800c\u663e\u8457\u589e\u52a0\u6216\u6781\u7aef\u5316\u3002", "motivation": "\u63a2\u8ba8\u793e\u4ea4\u8ba4\u53ef\u5bf9\u5728\u7ebf\u4ec7\u6068\u8a00\u8bba\u7684\u6fc0\u52b1\u4f5c\u7528\uff0c\u9a8c\u8bc1\u793e\u4f1a\u8ba4\u53ef\u7406\u8bba\u7684\u4e24\u4e2a\u5047\u8bbe\u3002", "method": "\u5206\u67902018-2021\u5e74\u95f4Parler\u5e73\u53f0\u76841.1\u4ebf\u6761\u5e16\u5b50\uff0c\u89c2\u5bdf\u70b9\u8d5e\u6570\u4e0e\u540e\u7eed\u4ec7\u6068\u8a00\u8bba\u7684\u5173\u7cfb\u3002", "result": "\u70b9\u8d5e\u6570\u4e0e\u540e\u7eed\u4ec7\u6068\u8a00\u8bba\u65e0\u663e\u8457\u5173\u8054\uff0c\u4e14\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5173\u7cfb\u590d\u6742\u3002", "conclusion": "\u793e\u4ea4\u8ba4\u53ef\u5bf9\u4ec7\u6068\u8a00\u8bba\u7684\u5f3a\u5316\u673a\u5236\u5728\u7279\u5b9a\u5e73\u53f0\u4e0a\u53ef\u80fd\u8868\u73b0\u4e0d\u540c\u3002"}}
{"id": "2507.11430", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11430", "abs": "https://arxiv.org/abs/2507.11430", "authors": ["Arnab Mukherjee", "Raju Halder", "Joydeep Chandra"], "title": "FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning", "comment": null, "summary": "Federated Learning (FL) has undergone significant development since its\ninception in 2016, advancing from basic algorithms to complex methodologies\ntailored to address diverse challenges and use cases. However, research and\nbenchmarking of novel FL techniques against a plethora of established\nstate-of-the-art solutions remain challenging. To streamline this process, we\nintroduce FLsim, a comprehensive FL simulation framework designed to meet the\ndiverse requirements of FL workflows in the literature. FLsim is characterized\nby its modularity, scalability, resource efficiency, and controlled\nreproducibility of experimental outcomes. Its easy to use interface allows\nusers to specify customized FL requirements through job configuration, which\nsupports: (a) customized data distributions, ranging from non-independent and\nidentically distributed (non-iid) data to independent and identically\ndistributed (iid) data, (b) selection of local learning algorithms according to\nuser preferences, with complete agnosticism to ML libraries, (c) choice of\nnetwork topology illustrating communication patterns among nodes, (d)\ndefinition of model aggregation and consensus algorithms, and (e) pluggable\nblockchain support for enhanced robustness. Through a series of experimental\nevaluations, we demonstrate the effectiveness and versatility of FLsim in\nsimulating a diverse range of state-of-the-art FL experiments. We envisage that\nFLsim would mark a significant advancement in FL simulation frameworks,\noffering unprecedented flexibility and functionality for researchers and\npractitioners alike.", "AI": {"tldr": "FLsim\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u5b9e\u9a8c\u9700\u6c42\uff0c\u5305\u62ec\u6570\u636e\u5206\u5e03\u3001\u5b66\u4e60\u7b97\u6cd5\u3001\u7f51\u7edc\u62d3\u6251\u7b49\u81ea\u5b9a\u4e49\u914d\u7f6e\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u7814\u7a76\u548c\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7b80\u5316\u5b9e\u9a8c\u6d41\u7a0b\u3002", "method": "FLsim\u63d0\u4f9b\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6570\u636e\u5206\u5e03\u3001\u5b66\u4e60\u7b97\u6cd5\u3001\u7f51\u7edc\u62d3\u6251\u7b49\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "FLsim\u5728\u6a21\u62df\u591a\u79cd\u8054\u90a6\u5b66\u4e60\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "FLsim\u4e3a\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u7075\u6d3b\u6027\u548c\u529f\u80fd\u6027\uff0c\u662f\u8054\u90a6\u5b66\u4e60\u6a21\u62df\u6846\u67b6\u7684\u91cd\u8981\u8fdb\u6b65\u3002"}}
{"id": "2507.11222", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11222", "abs": "https://arxiv.org/abs/2507.11222", "authors": ["Fares Wael", "Youssef Maklad", "Ali Hamdi", "Wael Elsersy"], "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "comment": null, "summary": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.", "AI": {"tldr": "FlowFSM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4eceRFC\u6587\u6863\u4e2d\u63d0\u53d6\u51c6\u786e\u7684\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u53ef\u6269\u5c55\u6027\u548c\u8986\u76d6\u4e0d\u5168\u95ee\u9898\u3002", "motivation": "\u73b0\u6709FSM\u63d0\u53d6\u6280\u672f\u5b58\u5728\u53ef\u6269\u5c55\u6027\u3001\u8986\u76d6\u4e0d\u5168\u548c\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u534f\u8bae\u5206\u6790\u548c\u6f0f\u6d1e\u53d1\u73b0\u7684\u6548\u7387\u3002", "method": "FlowFSM\u7ed3\u5408LLM\u3001\u63d0\u793a\u94fe\u548c\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u7cfb\u7edf\u5904\u7406\u534f\u8bae\u89c4\u8303\uff0c\u8bc6\u522b\u72b6\u6001\u8f6c\u6362\u5e76\u6784\u5efa\u7ed3\u6784\u5316\u89c4\u5219\u4e66\u3002", "result": "\u5728FTP\u548cRTSP\u534f\u8bae\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlowFSM\u63d0\u53d6\u7cbe\u5ea6\u9ad8\uff0c\u4e14\u51cf\u5c11\u4e86\u865a\u5047\u8f6c\u6362\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u7cfb\u7edf\u5728\u534f\u8bae\u5206\u6790\u548cFSM\u63a8\u65ad\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u7f51\u7edc\u5b89\u5168\u548c\u9006\u5411\u5de5\u7a0b\u3002"}}
{"id": "2507.10758", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10758", "abs": "https://arxiv.org/abs/2507.10758", "authors": ["Nikesh Prajapati", "Bimal Karki", "Saroj Gopali", "Akbar Siami Namin"], "title": "IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models", "comment": null, "summary": "This paper intends to detect IoT malicious attacks through deep learning\nmodels and demonstrates a comprehensive evaluation of the deep learning and\ngraph-based models regarding malicious network traffic detection. The models\nparticularly are based on GraphSAGE, Bidirectional encoder representations from\ntransformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head\nAttention, together with Bidirectional Long Short-Term Memory (BI-LSTM)\nMulti-Head Attention and BI-LSTM and LSTM models. The chosen models\ndemonstrated great performance to model temporal patterns and detect feature\nsignificance. The observed performance are mainly due to the fact that IoT\nsystem traffic patterns are both sequential and diverse, leaving a rich set of\ntemporal patterns for the models to learn. Experimental results showed that\nBERT maintained the best performance. It achieved 99.94% accuracy rate\nalongside high precision and recall, F1-score and AUC-ROC score of 99.99% which\ndemonstrates its capabilities through temporal dependency capture. The\nMulti-Head Attention offered promising results by providing good detection\ncapabilities with interpretable results. On the other side, the Multi-Head\nAttention model required significant processing time like BI-LSTM variants. The\nGraphSAGE model achieved good accuracy while requiring the shortest training\ntime but yielded the lowest accuracy, precision, and F1 score compared to the\nother models", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u7269\u8054\u7f51\u6076\u610f\u653b\u51fb\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982GraphSAGE\u3001BERT\u3001TCN\u7b49\uff09\u5728\u6076\u610f\u6d41\u91cf\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5176\u4e2dBERT\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7269\u8054\u7f51\u6d41\u91cf\u5177\u6709\u65f6\u5e8f\u6027\u548c\u591a\u6837\u6027\uff0c\u9700\u8981\u9ad8\u6548\u6a21\u578b\u6765\u68c0\u6d4b\u6076\u610f\u653b\u51fb\u3002", "method": "\u4f7f\u7528GraphSAGE\u3001BERT\u3001TCN\u3001Multi-Head Attention\u3001BI-LSTM\u7b49\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "BERT\u8868\u73b0\u6700\u4f18\uff0c\u51c6\u786e\u7387\u8fbe99.94%\uff0c\u5176\u4ed6\u6a21\u578b\u5982Multi-Head Attention\u548cGraphSAGE\u5404\u6709\u4f18\u52a3\u3002", "conclusion": "BERT\u5728\u6355\u83b7\u65f6\u5e8f\u4f9d\u8d56\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u9700\u6743\u8861\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.10753", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10753", "abs": "https://arxiv.org/abs/2507.10753", "authors": ["Kasper Lien Oftebro", "Anh Nguyen-Duc", "Kai-Kristian Kemell"], "title": "GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study", "comment": null, "summary": "Effective backlog management is critical for ensuring that development teams\nremain aligned with evolving requirements and stakeholder expectations.\nHowever, as product backlogs consistently grow in scale and complexity, they\ntend to become cluttered with redundant, outdated, or poorly defined tasks,\ncomplicating prioritization and decision making processes. This study\ninvestigates whether a generative-AI (GenAI) assistant can automate backlog\ngrooming in Agile software projects without sacrificing accuracy or\ntransparency. Through Design Science cycles, we developed a Jira plug-in that\nembeds backlog issues with the vector database, detects duplicates via cosine\nsimilarity, and leverage the GPT-4o model to propose merges, deletions, or new\nissues. We found that AI-assisted backlog grooming achieved 100 percent\nprecision while reducing the time-to-completion by 45 percent. The findings\ndemonstrated the tool's potential to streamline backlog refinement processes\nwhile improving user experiences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u751f\u6210\u5f0fAI\u52a9\u624b\u81ea\u52a8\u7ba1\u7406\u654f\u6377\u9879\u76ee\u4e2d\u7684\u5f85\u529e\u4e8b\u9879\uff0c\u901a\u8fc7\u5f00\u53d1Jira\u63d2\u4ef6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e14\u9ad8\u6548\u7684\u5f85\u529e\u4e8b\u9879\u6574\u7406\u3002", "motivation": "\u968f\u7740\u4ea7\u54c1\u5f85\u529e\u4e8b\u9879\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u5197\u4f59\u3001\u8fc7\u65f6\u6216\u5b9a\u4e49\u4e0d\u6e05\u7684\u4efb\u52a1\u5806\u79ef\uff0c\u5bfc\u81f4\u4f18\u5148\u7ea7\u548c\u51b3\u7b56\u8fc7\u7a0b\u590d\u6742\u5316\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u79d1\u5b66\u5468\u671f\u5f00\u53d1Jira\u63d2\u4ef6\uff0c\u5229\u7528\u5411\u91cf\u6570\u636e\u5e93\u5d4c\u5165\u5f85\u529e\u4e8b\u9879\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u68c0\u6d4b\u91cd\u590d\u9879\uff0c\u5e76\u501f\u52a9GPT-4o\u6a21\u578b\u63d0\u51fa\u5408\u5e76\u3001\u5220\u9664\u6216\u65b0\u589e\u5efa\u8bae\u3002", "result": "AI\u8f85\u52a9\u7684\u5f85\u529e\u4e8b\u9879\u6574\u7406\u5b9e\u73b0\u4e86100%\u7684\u7cbe\u786e\u5ea6\uff0c\u5e76\u5c06\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u4e8645%\u3002", "conclusion": "\u8be5\u5de5\u5177\u80fd\u591f\u4f18\u5316\u5f85\u529e\u4e8b\u9879\u7ec6\u5316\u6d41\u7a0b\uff0c\u540c\u65f6\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.10846", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10846", "abs": "https://arxiv.org/abs/2507.10846", "authors": ["Casey Wall", "Longwei Wang", "Rodrigue Rizk", "KC Santosh"], "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization", "comment": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "summary": "Interpreting the decision-making process of Convolutional Neural Networks\n(CNNs) is critical for deploying models in high-stakes domains.\nGradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method\nfor visual explanations, yet it typically focuses on the final convolutional\nlayer or na\\\"ively averages across layers, strategies that can obscure\nimportant semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a\nnovel, human-tunable extension of Grad-CAM that generates robust and coherent\nsaliency maps by aggregating information across all convolutional layers. To\nmitigate the influence of noisy or extreme attribution values, Winsor-CAM\napplies Winsorization, a percentile-based outlier attenuation technique. A\nuser-controllable threshold allows for semantic-level tuning, enabling flexible\nexploration of model behavior across representational hierarchies. Evaluations\non standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the\nPASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable\nheatmaps and achieves superior performance in localization metrics, including\nintersection-over-union and center-of-mass alignment, when compared to Grad-CAM\nand uniform layer-averaging baselines. Winsor-CAM advances the goal of\ntrustworthy AI by offering interpretable, multi-layer insights with\nhuman-in-the-loop control.", "AI": {"tldr": "Winsor-CAM\u662f\u4e00\u79cd\u6539\u8fdb\u7684Grad-CAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u5c42\u805a\u5408\u4fe1\u606f\u548cWinsorization\u6280\u672f\u751f\u6210\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u70ed\u56fe\u3002", "motivation": "\u89e3\u91caCNN\u51b3\u7b56\u8fc7\u7a0b\u5bf9\u9ad8\u98ce\u9669\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982Grad-CAM\u53ef\u80fd\u63a9\u76d6\u91cd\u8981\u8bed\u4e49\u6216\u653e\u5927\u566a\u58f0\u3002", "method": "\u63d0\u51faWinsor-CAM\uff0c\u901a\u8fc7Winsorization\u6280\u672f\u8870\u51cf\u5f02\u5e38\u503c\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u8c03\u8282\u9608\u503c\uff0c\u5b9e\u73b0\u591a\u5c42\u7ea7\u8bed\u4e49\u63a2\u7d22\u3002", "result": "\u5728PASCAL VOC 2012\u6570\u636e\u96c6\u4e0a\uff0cWinsor-CAM\u6bd4Grad-CAM\u548c\u5747\u5300\u5c42\u5e73\u5747\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u5b9a\u4f4d\u6307\u6807\u66f4\u4f73\u3002", "conclusion": "Winsor-CAM\u901a\u8fc7\u591a\u5c42\u7ea7\u89e3\u91ca\u548c\u4eba\u673a\u4ea4\u4e92\u63a7\u5236\uff0c\u63a8\u52a8\u4e86\u53ef\u4fe1AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10852", "abs": "https://arxiv.org/abs/2507.10852", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53f8\u6cd5\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0LLMs\u666e\u904d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3001\u504f\u89c1\u548c\u4e0d\u5e73\u8861\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u516c\u5f00\u5de5\u5177\u5305\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "LLMs\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u53f8\u6cd5\uff09\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u516c\u5e73\u6027\u548c\u5bf9\u793e\u4f1a\u6b63\u4e49\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u53f8\u6cd5\u516c\u5e73\u7406\u8bba\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u5f00\u53d1\u4e09\u4e2a\u6307\u6807\uff08\u4e0d\u4e00\u81f4\u6027\u3001\u504f\u89c1\u3001\u4e0d\u5e73\u8861\u51c6\u786e\u6027\uff09\uff0c\u5e76\u572816\u4e2aLLMs\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "LLMs\u666e\u904d\u5b58\u5728\u4e0d\u516c\u5e73\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u4eba\u53e3\u7edf\u8ba1\u6807\u7b7e\u4e0a\u504f\u89c1\u66f4\u660e\u663e\u3002\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\u53ef\u5f71\u54cd\u516c\u5e73\u6027\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u3001\u53d1\u5e03\u65f6\u95f4\u548c\u6765\u6e90\u56fd\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLMs\u5728\u53f8\u6cd5\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u3002\u63d0\u4f9b\u7684\u5de5\u5177\u5305\u652f\u6301\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2507.11437", "categories": ["cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.11437", "abs": "https://arxiv.org/abs/2507.11437", "authors": ["Sagar Bharadwaj", "Srinivasan Seshan", "Anthony Rowe"], "title": "Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications", "comment": null, "summary": "The emergence of the Spatial Web -- the Web where content is tied to\nreal-world locations has the potential to improve and enable many applications\nsuch as augmented reality, navigation, robotics, and more. The Spatial Web is\nmissing a key ingredient that is impeding its growth -- a spatial naming system\nto resolve real-world locations to names. Today's spatial naming systems are\ndigital maps such as Google and Apple maps. These maps and the location-based\nservices provided on top of these maps are primarily controlled by a few large\ncorporations and mostly cover outdoor public spaces. Emerging classes of\napplications, such as persistent world-scale augmented reality, require\ndetailed maps of both outdoor and indoor spaces. Existing centralized mapping\ninfrastructures are proving insufficient for such applications because of the\nscale of cartography efforts required and the privacy of indoor map data.\n  In this paper, we present a case for a federated spatial naming system, or in\nother words, a federated mapping infrastructure. This enables disparate parties\nto manage and serve their own maps of physical regions and unlocks scalability\nof map management, isolation and privacy of maps. Map-related services such as\naddress-to-location mapping, location-based search, and routing needs\nre-architecting to work on federated maps. We discuss some essential services\nand practicalities of enabling these services.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u96c6\u4e2d\u5f0f\u5730\u56fe\u57fa\u7840\u8bbe\u65bd\u5728\u7a7a\u95f4Web\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u7a7a\u95f4Web\uff08\u5185\u5bb9\u4e0e\u73b0\u5b9e\u4e16\u754c\u4f4d\u7f6e\u5173\u8054\u7684Web\uff09\u7f3a\u4e4f\u4e00\u4e2a\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\uff0c\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u73b0\u6709\u5730\u56fe\u670d\u52a1\u7531\u5c11\u6570\u5927\u516c\u53f8\u63a7\u5236\uff0c\u4e14\u4e3b\u8981\u8986\u76d6\u6237\u5916\u516c\u5171\u7a7a\u95f4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65b0\u5174\u5e94\u7528\uff08\u5982\u589e\u5f3a\u73b0\u5b9e\uff09\u5bf9\u5ba4\u5185\u5916\u8be6\u7ec6\u5730\u56fe\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8054\u90a6\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\uff0c\u5141\u8bb8\u591a\u65b9\u7ba1\u7406\u548c\u63d0\u4f9b\u81ea\u5df1\u7684\u5730\u56fe\uff0c\u5b9e\u73b0\u5730\u56fe\u7ba1\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u8ba8\u8bba\u4e86\u5982\u4f55\u5728\u8054\u90a6\u5730\u56fe\u4e0a\u91cd\u65b0\u8bbe\u8ba1\u5730\u5740\u5230\u4f4d\u7f6e\u6620\u5c04\u3001\u57fa\u4e8e\u4f4d\u7f6e\u7684\u641c\u7d22\u548c\u8def\u7531\u7b49\u57fa\u672c\u670d\u52a1\u3002", "conclusion": "\u8054\u90a6\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\u80fd\u591f\u89e3\u51b3\u96c6\u4e2d\u5f0f\u5730\u56fe\u57fa\u7840\u8bbe\u65bd\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u7a7a\u95f4Web\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.10761", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10761", "abs": "https://arxiv.org/abs/2507.10761", "authors": ["Tyler King", "Nikolos Gurney", "John H. Miller", "Volkan Ustun"], "title": "Detecting AI Assistance in Abstract Complex Tasks", "comment": "Accepted to HCII 2025", "summary": "Detecting assistance from artificial intelligence is increasingly important\nas they become ubiquitous across complex tasks such as text generation, medical\ndiagnosis, and autonomous driving. Aid detection is challenging for humans,\nespecially when looking at abstract task data. Artificial neural networks excel\nat classification thanks to their ability to quickly learn from and process\nlarge amounts of data -- assuming appropriate preprocessing. We posit detecting\nhelp from AI as a classification task for such models. Much of the research in\nthis space examines the classification of complex but concrete data classes,\nsuch as images. Many AI assistance detection scenarios, however, result in data\nthat is not machine learning-friendly. We demonstrate that common models can\neffectively classify such data when it is appropriately preprocessed. To do so,\nwe construct four distinct neural network-friendly image formulations along\nwith an additional time-series formulation that explicitly encodes the\nexploration/exploitation of users, which allows for generalizability to other\nabstract tasks. We benchmark the quality of each image formulation across three\nclassical deep learning architectures, along with a parallel CNN-RNN\narchitecture that leverages the additional time series to maximize testing\nperformance, showcasing the importance of encoding temporal and spatial\nquantities for detecting AI aid in abstract tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06AI\u8f85\u52a9\u68c0\u6d4b\u89c6\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u5904\u7406\u6570\u636e\u5e76\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u53cb\u597d\u7684\u56fe\u50cf\u548c\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff0c\u8bc1\u660e\u4e86\u5e38\u89c1\u6a21\u578b\u80fd\u6709\u6548\u5206\u7c7b\u62bd\u8c61\u4efb\u52a1\u6570\u636e\u3002", "motivation": "\u968f\u7740AI\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u666e\u53ca\uff0c\u68c0\u6d4bAI\u8f85\u52a9\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u4eba\u7c7b\u96be\u4ee5\u5904\u7406\u62bd\u8c61\u4efb\u52a1\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u52bf\u3002", "method": "\u6784\u5efa\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u53cb\u597d\u7684\u56fe\u50cf\u8868\u793a\u548c\u4e00\u79cd\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff0c\u901a\u8fc7\u4e09\u79cd\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u4e00\u79cd\u5e76\u884cCNN-RNN\u67b6\u6784\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5f53\u9884\u5904\u7406\u7684\u6570\u636e\u548c\u7ed3\u5408\u65f6\u7a7a\u4fe1\u606f\u7684\u67b6\u6784\u80fd\u6709\u6548\u63d0\u5347AI\u8f85\u52a9\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "\u7f16\u7801\u65f6\u7a7a\u4fe1\u606f\u5bf9\u68c0\u6d4b\u62bd\u8c61\u4efb\u52a1\u4e2d\u7684AI\u8f85\u52a9\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u3002"}}
{"id": "2507.10785", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10785", "abs": "https://arxiv.org/abs/2507.10785", "authors": ["Michael Neumann", "Eva-Maria Sch\u00f6n", "Mali Senapathi", "Maria Rauschenberger", "Tiago Silva da Silva"], "title": "Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda", "comment": null, "summary": "Agile software development principles and values have been widely adopted\nacross various industries, influencing products and services globally. Despite\nits increasing popularity, a significant gap remains between research and\npractical implementation. This paper presents the findings of the first\ninternational workshop designed to foster collaboration between research and\npractice in agile software development. We discuss the main themes and factors\nidentified by the workshop participants that contribute to this gap, strategies\nto bridge it, and the challenges that require further research attention.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63a2\u8ba8\u4e86\u56fd\u9645\u7814\u8ba8\u4f1a\u7684\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u7f29\u5c0f\u5dee\u8ddd\u7684\u7b56\u7565\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u867d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7814\u7a76\u4e0e\u5b9e\u9645\u5b9e\u65bd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u4fc3\u8fdb\u4e24\u8005\u7684\u534f\u4f5c\u3002", "method": "\u901a\u8fc7\u9996\u6b21\u56fd\u9645\u7814\u8ba8\u4f1a\uff0c\u6536\u96c6\u53c2\u4e0e\u8005\u5bf9\u5dee\u8ddd\u7684\u4e3b\u8981\u4e3b\u9898\u548c\u56e0\u7d20\u7684\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u7b56\u7565\u3002", "result": "\u8bc6\u522b\u4e86\u5bfc\u81f4\u5dee\u8ddd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u4e86\u7f29\u5c0f\u5dee\u8ddd\u7684\u7b56\u7565\uff0c\u5e76\u6307\u51fa\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u8ba8\u4f1a\u4e3a\u4fc3\u8fdb\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u672a\u6765\u9700\u66f4\u591a\u7814\u7a76\u89e3\u51b3\u6311\u6218\u3002"}}
{"id": "2507.10855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10855", "abs": "https://arxiv.org/abs/2507.10855", "authors": ["Wei Chen", "Jingxi Yu", "Zichen Miao", "Qiang Qiu"], "title": "Sparse Fine-Tuning of Transformers for Generative Tasks", "comment": "Accepted by International Conference on Computer Vision 2025", "summary": "Large pre-trained transformers have revolutionized artificial intelligence\nacross various domains, and fine-tuning remains the dominant approach for\nadapting these models to downstream tasks due to the cost of training from\nscratch. However, in existing fine-tuning methods, the updated representations\nare formed as a dense combination of modified parameters, making it challenging\nto interpret their contributions and understand how the model adapts to new\ntasks. In this work, we introduce a fine-tuning framework inspired by sparse\ncoding, where fine-tuned features are represented as a sparse combination of\nbasic elements, i.e., feature dictionary atoms. The feature dictionary atoms\nfunction as fundamental building blocks of the representation, and tuning atoms\nallows for seamless adaptation to downstream tasks. Sparse coefficients then\nserve as indicators of atom importance, identifying the contribution of each\natom to the updated representation. Leveraging the atom selection capability of\nsparse coefficients, we first demonstrate that our method enhances image\nediting performance by improving text alignment through the removal of\nunimportant feature dictionary atoms. Additionally, we validate the\neffectiveness of our approach in the text-to-image concept customization task,\nwhere our method efficiently constructs the target concept using a sparse\ncombination of feature dictionary atoms, outperforming various baseline\nfine-tuning methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u7f16\u7801\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u7ec4\u5408\u7279\u5f81\u5b57\u5178\u539f\u5b50\u6765\u6539\u8fdb\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u89e3\u91ca\u53c2\u6570\u66f4\u65b0\u7684\u8d21\u732e\uff0c\u7a00\u758f\u7f16\u7801\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u7f16\u7801\u8868\u793a\u5fae\u8c03\u7279\u5f81\uff0c\u7279\u5f81\u5b57\u5178\u539f\u5b50\u4f5c\u4e3a\u57fa\u672c\u6784\u5efa\u5757\uff0c\u7a00\u758f\u7cfb\u6570\u6307\u793a\u539f\u5b50\u91cd\u8981\u6027\u3002", "result": "\u5728\u56fe\u50cf\u7f16\u8f91\u548c\u6587\u672c\u5230\u56fe\u50cf\u6982\u5ff5\u5b9a\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7a00\u758f\u7f16\u7801\u6846\u67b6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.10918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10918", "abs": "https://arxiv.org/abs/2507.10918", "authors": ["Ikumi Numaya", "Shoji Moriya", "Shiki Sato", "Reina Akama", "Jun Suzuki"], "title": "How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations", "comment": "Accepted to SIGDIAL 2025 (long)", "summary": "Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5bf9\u8bdd\u751f\u6210\u4e2d\u4e3b\u89c2\u4e0e\u5ba2\u89c2\u98ce\u683c\u76f8\u4f3c\u6027\u7684\u533a\u522b\u53ca\u5176\u5bf9\u7528\u6237\u504f\u597d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e3b\u89c2\u76f8\u4f3c\u6027\u4e0e\u7528\u6237\u504f\u597d\u5f3a\u76f8\u5173\uff0c\u4e14\u4e0e\u5ba2\u89c2\u76f8\u4f3c\u6027\u4e0d\u540c\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u4e0e\u7cfb\u7edf\u98ce\u683c\u76f8\u4f3c\u6027\u5bf9\u7528\u6237\u5370\u8c61\u7684\u5f71\u54cd\uff0c\u533a\u5206\u4e3b\u89c2\u4e0e\u5ba2\u89c2\u76f8\u4f3c\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b\u7528\u6237\u504f\u597d\u3001\u4e3b\u89c2\u98ce\u683c\u76f8\u4f3c\u6027\u548c\u5ba2\u89c2\u98ce\u683c\u76f8\u4f3c\u6027\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u8bbe\u7f6e\u4e2d\u5206\u6790\u3002", "result": "\u4e3b\u89c2\u98ce\u683c\u76f8\u4f3c\u6027\u4e0e\u7528\u6237\u504f\u597d\u5f3a\u76f8\u5173\uff0c\u4e14\u4e0e\u7b2c\u4e09\u65b9\u8bc4\u4f30\u7684\u5ba2\u89c2\u76f8\u4f3c\u6027\u4e0d\u540c\u3002", "conclusion": "\u533a\u5206\u4e3b\u89c2\u4e0e\u5ba2\u89c2\u8bc4\u4ef7\u5bf9\u7406\u89e3\u98ce\u683c\u76f8\u4f3c\u6027\u4e0e\u7528\u6237\u504f\u597d\u7684\u5173\u7cfb\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.11512", "categories": ["cs.DC", "cs.NA", "cs.PF", "math.NA", "65Y10", "G.4; C.4"], "pdf": "https://arxiv.org/pdf/2507.11512", "abs": "https://arxiv.org/abs/2507.11512", "authors": ["Aditya Kashi", "Nicholson Koukpaizan", "Hao Lu", "Michael Matheson", "Sarp Oral", "Feiyi Wang"], "title": "Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine", "comment": "Accepted for presentation at SC25, St. Louis, MO, USA", "summary": "Mixed-precision algorithms have been proposed as a way for scientific\ncomputing to benefit from some of the gains seen for artificial intelligence\n(AI) on recent high performance computing (HPC) platforms. A few applications\ndominated by dense matrix operations have seen substantial speedups by\nutilizing low precision formats such as FP16. However, a majority of scientific\nsimulation applications are memory bandwidth limited. Beyond preliminary\nstudies, the practical gain from using mixed-precision algorithms on a given\nHPC system is largely unclear.\n  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been\nproposed to measure the useful performance of a HPC system on sparse\nmatrix-based mixed-precision applications. In this work, we present a highly\noptimized implementation of the HPG-MxP benchmark for an exascale system and\ndescribe our algorithm enhancements. We show for the first time a speedup of\n1.6x using a combination of double- and single-precision on modern GPU-based\nsupercomputers.", "AI": {"tldr": "\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5e94\u7528\uff0cHPG-MxP\u57fa\u51c6\u6d4b\u8bd5\u9996\u6b21\u5728GPU\u8d85\u7b97\u4e0a\u5b9e\u73b01.6\u500d\u52a0\u901f\u3002", "motivation": "\u63a2\u7d22\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728HPC\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u6027\u80fd\u589e\u76ca\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7a00\u758f\u77e9\u9635\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u5e76\u4f18\u5316HPG-MxP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u53cc\u7cbe\u5ea6\u548c\u5355\u7cbe\u5ea6\u8ba1\u7b97\u3002", "result": "\u5728\u73b0\u4ee3GPU\u8d85\u7b97\u4e0a\u5b9e\u73b01.6\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728\u7279\u5b9aHPC\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.10798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10798", "abs": "https://arxiv.org/abs/2507.10798", "authors": ["Asim H. Gazi", "Bhanu T. Gullapalli", "Daiqi Gao", "Benjamin M. Marlin", "Vivek Shetty", "Susan A. Murphy"], "title": "Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions", "comment": "4 pages, 3 figures", "summary": "Timely decision making is critical to the effectiveness of mobile health\n(mHealth) interventions. At predefined timepoints called \"decision points,\"\nintelligent mHealth systems such as just-in-time adaptive interventions\n(JITAIs) estimate an individual's biobehavioral context from sensor or survey\ndata and determine whether and how to intervene. For interventions targeting\nhabitual behavior (e.g., oral hygiene), effectiveness often hinges on\ndelivering support shortly before the target behavior is likely to occur.\nCurrent practice schedules decision points at a fixed interval (e.g., one hour)\nbefore user-provided behavior times, and the fixed interval is kept the same\nfor all individuals. However, this one-size-fits-all approach performs poorly\nfor individuals with irregular routines, often scheduling decision points after\nthe target behavior has already occurred, rendering interventions ineffective.\nIn this paper, we propose SigmaScheduling, a method to dynamically schedule\ndecision points based on uncertainty in predicted behavior times. When behavior\ntiming is more predictable, SigmaScheduling schedules decision points closer to\nthe predicted behavior time; when timing is less certain, SigmaScheduling\nschedules decision points earlier, increasing the likelihood of timely\nintervention. We evaluated SigmaScheduling using real-world data from 68\nparticipants in a 10-week trial of Oralytics, a JITAI designed to improve daily\ntoothbrushing. SigmaScheduling increased the likelihood that decision points\npreceded brushing events in at least 70% of cases, preserving opportunities to\nintervene and impact behavior. Our results indicate that SigmaScheduling can\nadvance precision mHealth, particularly for JITAIs targeting time-sensitive,\nhabitual behaviors such as oral hygiene or dietary habits.", "AI": {"tldr": "SigmaScheduling\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u70b9\uff0c\u63d0\u9ad8\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u7684\u53ca\u65f6\u6027\u3002", "motivation": "\u56fa\u5b9a\u95f4\u9694\u7684\u51b3\u7b56\u70b9\u8c03\u5ea6\u5bf9\u4e60\u60ef\u6027\u884c\u4e3a\u5e72\u9884\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5bf9\u4f5c\u606f\u4e0d\u89c4\u5f8b\u7684\u7528\u6237\u3002", "method": "\u63d0\u51faSigmaScheduling\u65b9\u6cd5\uff0c\u6839\u636e\u884c\u4e3a\u65f6\u95f4\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u70b9\u3002", "result": "\u572868\u540d\u53c2\u4e0e\u8005\u7684\u8bd5\u9a8c\u4e2d\uff0cSigmaScheduling\u4f7f70%\u4ee5\u4e0a\u7684\u51b3\u7b56\u70b9\u6210\u529f\u5728\u5237\u7259\u884c\u4e3a\u524d\u89e6\u53d1\u3002", "conclusion": "SigmaScheduling\u63d0\u5347\u4e86\u7cbe\u51c6\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u7684\u4e60\u60ef\u6027\u884c\u4e3a\u3002"}}
{"id": "2507.10818", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10818", "abs": "https://arxiv.org/abs/2507.10818", "authors": ["Jasmine Latendresse", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow", "comment": null, "summary": "Software libraries are central to the functionality, security, and\nmaintainability of modern code. As developers increasingly turn to Large\nLanguage Models (LLMs) to assist with programming tasks, understanding how\nthese models recommend libraries is essential. In this paper, we conduct an\nempirical study of six state-of-the-art LLMs, both proprietary and open-source,\nby prompting them to solve real-world Python problems sourced from Stack\nOverflow. We analyze the types of libraries they import, the characteristics of\nthose libraries, and the extent to which the recommendations are usable out of\nthe box. Our results show that LLMs predominantly favour third-party libraries\nover standard ones, and often recommend mature, popular, and permissively\nlicensed dependencies. However, we also identify gaps in usability: 4.6% of the\nlibraries could not be resolved automatically due to structural mismatches\nbetween import names and installable packages, and only two models (out of six)\nprovided installation guidance. While the generated code is technically valid,\nthe lack of contextual support places the burden of manually resolving\ndependencies on the user. Our findings offer actionable insights for both\ndevelopers and researchers, and highlight opportunities to improve the\nreliability and usability of LLM-generated code in the context of software\ndependencies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u516d\u79cd\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u771f\u5b9ePython\u95ee\u9898\u65f6\u63a8\u8350\u7684\u5e93\u7684\u7c7b\u578b\u3001\u7279\u5f81\u53ca\u53ef\u7528\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u503e\u5411\u4e8e\u63a8\u8350\u6210\u719f\u3001\u6d41\u884c\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u4f46\u4e5f\u5b58\u5728\u53ef\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528LLMs\u8f85\u52a9\u7f16\u7a0b\u4efb\u52a1\uff0c\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u63a8\u8350\u5e93\u5bf9\u4ee3\u7801\u529f\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4eceStack Overflow\u83b7\u53d6\u771f\u5b9ePython\u95ee\u9898\uff0c\u5bf9\u516d\u79cdLLMs\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u5176\u63a8\u8350\u7684\u5e93\u7684\u7c7b\u578b\u3001\u7279\u5f81\u53ca\u53ef\u7528\u6027\u3002", "result": "LLMs\u4e3b\u8981\u63a8\u8350\u7b2c\u4e09\u65b9\u5e93\uff0c\u4e14\u591a\u4e3a\u6210\u719f\u3001\u6d41\u884c\u548c\u5bbd\u677e\u8bb8\u53ef\u7684\u4f9d\u8d56\u9879\uff0c\u4f464.6%\u7684\u5e93\u56e0\u5bfc\u5165\u540d\u79f0\u4e0e\u53ef\u5b89\u88c5\u5305\u4e0d\u5339\u914d\u800c\u65e0\u6cd5\u81ea\u52a8\u89e3\u6790\uff0c\u4ec5\u4e24\u79cd\u6a21\u578b\u63d0\u4f9b\u5b89\u88c5\u6307\u5bfc\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdbLLM\u751f\u6210\u4ee3\u7801\u5728\u4f9d\u8d56\u9879\u65b9\u9762\u7684\u53ef\u9760\u6027\u548c\u53ef\u7528\u6027\u7684\u673a\u4f1a\u3002"}}
{"id": "2507.10864", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10864", "abs": "https://arxiv.org/abs/2507.10864", "authors": ["Saadat Behzadi", "Danial Sharifrazi", "Bita Mesbahzadeh", "Javad Hassannataj Joloudarid", "Roohallah Alizadehsani"], "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n", "comment": null, "summary": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LOF\u7b97\u6cd5\u548cYOLO-v11n\u7684\u8f7b\u91cf\u7ea7\u7ed3\u76f4\u80a0\u606f\u8089\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u606f\u8089\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u9884\u9632\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528LOF\u7b97\u6cd5\u8fc7\u6ee4\u566a\u58f0\u6570\u636e\uff0c\u7ed3\u5408YOLO-v11n\u6a21\u578b\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6570\u636e\u589e\u5f3a\u4f18\u5316\u6027\u80fd\u3002", "result": "\u7cbe\u5ea695.83%\uff0c\u53ec\u56de\u738791.85%\uff0cF1\u5206\u657093.48%\uff0cmAP@0.5\u4e3a96.48%\uff0cmAP@0.5:0.95\u4e3a77.75%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u4e34\u5e8a\u5b9e\u65f6\u7ed3\u80a0\u955c\u68c0\u67e5\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u6a21\u578b\u6548\u7387\u5728\u533b\u5b66\u5f71\u50cfAI\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.10920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10920", "abs": "https://arxiv.org/abs/2507.10920", "authors": ["Seungho Choi"], "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHanjaBridge\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u6c49\u5b57\u7684\u8bed\u4e49\u4fe1\u606f\u89e3\u51b3\u97e9\u8bed\u4e2d\u7684\u540c\u97f3\u5f02\u4e49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97e9\u8bed\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u540c\u97f3\u5f02\u4e49\u6c49\u5b57\u8bcd\u5728\u97e9\u6587\u811a\u672c\u4e2d\u65e0\u6cd5\u533a\u5206\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHanjaBridge\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\u6ce8\u5165\u6240\u6709\u53ef\u80fd\u7684\u6c49\u5b57\u5019\u9009\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728KoBALT\u57fa\u51c6\u4e0a\u76f8\u5bf9\u63d0\u534721%\uff0c\u5e76\u89c2\u5bdf\u5230\u4e2d\u97e9\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u79ef\u6781\u6548\u679c\u3002", "conclusion": "HanjaBridge\u6709\u6548\u63d0\u5347\u97e9\u8bed\u7406\u89e3\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u63a8\u7406\u65f6\u989d\u5916\u6210\u672c\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.10803", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10803", "abs": "https://arxiv.org/abs/2507.10803", "authors": ["JaMor Hairston", "Ritvik Ranjan", "Sahithi Lakamana", "Anthony Spadaro", "Selen Bozkurt", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case", "comment": "Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2", "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health", "AI": {"tldr": "LLMs can automate thematic analysis with few-shot prompting, closely matching expert classifications for high-prevalence themes.", "motivation": "To evaluate if LLMs can replicate expert-driven thematic analysis of social media data, addressing challenges in inductive thematic analysis.", "method": "Used two Reddit datasets on xylazine, modeled as binary classifications with zero-, single-, and few-shot prompting, measuring performance via accuracy, precision, recall, and F1-score.", "result": "GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71), closely mirroring expert classifications for high-prevalence themes.", "conclusion": "Few-shot LLM-based approaches can automate thematic analyses, providing a scalable supplement for qualitative research."}}
{"id": "2507.10822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10822", "abs": "https://arxiv.org/abs/2507.10822", "authors": ["Omar Elsisi", "Glaucia Melo"], "title": "Past, Present and Future: Exploring Adaptive AI in Software Development Bots", "comment": null, "summary": "Conversational agents, such as chatbots and virtual assistants, have become\nessential in software development, boosting productivity, collaboration, and\nautomating various tasks. This paper examines the role of adaptive AI-powered\nconversational agents in software development, highlighting their ability to\noffer dynamic, context-aware assistance to developers. Unlike traditional\nrule-based systems, adaptive AI agents use machine learning and natural\nlanguage processing to learn from interactions and improve over time, providing\nmore personalized and responsive help. We look at how these tools have evolved\nfrom simple query-based systems to advanced AI-driven solutions like GitHub\nCopilot and Microsoft Teams bots. We also explore the challenges of integrating\nadaptive AI into software development processes. The study aims to assess the\nbenefits and limitations of these systems, address concerns like data privacy\nand ethical issues, and offer insights into their future use in the field.\nUltimately, adaptive AI chatbots have great potential to revolutionize software\ndevelopment by delivering real-time, customized support and enhancing the\nefficiency of development cycles.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u9002\u5e94AI\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4f5c\u7528\uff0c\u5f3a\u8c03\u5176\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8f85\u52a9\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u52bf\u3001\u6311\u6218\u53ca\u672a\u6765\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u81ea\u9002\u5e94AI\u5bf9\u8bdd\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u89c4\u5219\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u5de5\u5177\uff08\u5982GitHub Copilot\u548cMicrosoft Teams\u673a\u5668\u4eba\uff09\u7684\u6f14\u53d8\uff0c\u63a2\u8ba8\u81ea\u9002\u5e94AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u4e0e\u6311\u6218\u3002", "result": "\u81ea\u9002\u5e94AI\u5bf9\u8bdd\u4ee3\u7406\u80fd\u63d0\u4f9b\u4e2a\u6027\u5316\u3001\u5b9e\u65f6\u652f\u6301\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u4e5f\u9762\u4e34\u6570\u636e\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u81ea\u9002\u5e94AI\u5bf9\u8bdd\u4ee3\u7406\u6709\u671b\u901a\u8fc7\u5b9a\u5236\u5316\u652f\u6301\u548c\u9ad8\u6548\u5f00\u53d1\u5468\u671f\u5f7b\u5e95\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u9886\u57df\u3002"}}
{"id": "2507.10881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10881", "abs": "https://arxiv.org/abs/2507.10881", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alv\u00e9n", "Lennart Svensson", "Fredrik Kahl"], "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes", "comment": "Submitted Version. Accepted at MICCAI 2025", "summary": "Tubular tree structures, such as blood vessels and airways, are essential in\nhuman anatomy and accurately tracking them while preserving their topology is\ncrucial for various downstream tasks. Trexplorer is a recurrent model designed\nfor centerline tracking in 3D medical images but it struggles with predicting\nduplicate branches and terminating tracking prematurely. To address these\nissues, we present Trexplorer Super, an enhanced version that notably improves\nperformance through novel advancements. However, evaluating centerline tracking\nmodels is challenging due to the lack of public datasets. To enable thorough\nevaluation, we develop three centerline datasets, one synthetic and two real,\neach with increasing difficulty. Using these datasets, we conduct a\ncomprehensive evaluation of existing state-of-the-art (SOTA) models and compare\nthem with our approach. Trexplorer Super outperforms previous SOTA models on\nevery dataset. Our results also highlight that strong performance on synthetic\ndata does not necessarily translate to real datasets. The code and datasets are\navailable at https://github.com/RomStriker/Trexplorer-Super.", "AI": {"tldr": "Trexplorer Super\u662f\u4e00\u79cd\u6539\u8fdb\u76843D\u533b\u5b66\u56fe\u50cf\u4e2d\u5fc3\u7ebf\u8ffd\u8e2a\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u539f\u59cb\u6a21\u578b\u7684\u91cd\u590d\u5206\u652f\u548c\u8fc7\u65e9\u7ec8\u6b62\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u5f00\u53d1\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u51c6\u786e\u8ffd\u8e2a\u7ba1\u72b6\u6811\u7ed3\u6784\uff08\u5982\u8840\u7ba1\u548c\u6c14\u9053\uff09\u5bf9\u533b\u5b66\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5b58\u5728\u91cd\u590d\u5206\u652f\u548c\u8fc7\u65e9\u7ec8\u6b62\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u63d0\u51faTrexplorer Super\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u6280\u672f\u6539\u8fdb\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5408\u6210\u548c\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "Trexplorer Super\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4f46\u5408\u6210\u6570\u636e\u7684\u5f3a\u8868\u73b0\u4e0d\u4e00\u5b9a\u9002\u7528\u4e8e\u771f\u5b9e\u6570\u636e\u3002", "conclusion": "Trexplorer Super\u663e\u8457\u63d0\u5347\u4e86\u4e2d\u5fc3\u7ebf\u8ffd\u8e2a\u6027\u80fd\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.10957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10957", "abs": "https://arxiv.org/abs/2507.10957", "authors": ["Kalit Inani", "Keshav Kabra", "Vijay Marupudi", "Sashank Varma"], "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models", "comment": "To appear at CogSci 2025", "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7c7b\u6bd4\u63a8\u7406\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5bf9\u6bd4\uff0c\u63a2\u8ba8\u4e86\u8bed\u4e49\u8868\u793a\u548c\u663e\u5f0f\u63d0\u793a\u7684\u6548\u679c\uff0c\u5e76\u8003\u5bdf\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u7c7b\u6bd4\u63a8\u7406\u4efb\u52a1\u4e2d\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6545\u4e8b\u7c7b\u6bd4\u4efb\u52a1\uff0c\u8bc4\u4f30LLMs\u7684\u8bed\u4e49\u8868\u793a\u548c\u663e\u5f0f\u63d0\u793a\u6548\u679c\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u7c7b\u6bd4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u5dee\u8ddd\uff1b\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLMs\u5728\u7c7b\u6bd4\u63a8\u7406\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u66f4\u63a5\u8fd1\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.10831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10831", "abs": "https://arxiv.org/abs/2507.10831", "authors": ["Yilin Xia", "Heng Zheng", "Shawn Bowers", "Bertram Lud\u00e4scher"], "title": "AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks", "comment": "International Conference on Artificial Intelligence and Law (ICAIL),\n  June 16-20, 2025. Chicago, IL, USA", "summary": "Argumentation frameworks (AFs) provide formal approaches for legal reasoning,\nbut identifying sources of ambiguity and explaining argument acceptance remains\nchallenging for non-experts. We present AF-XRAY, an open-source toolkit for\nexploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY\nintroduces: (i) layered visualizations based on game-theoretic argument length\nrevealing well-founded derivation structures; (ii) classification of attack\nedges by semantic roles (primary, secondary, blunders); (iii) overlay\nvisualizations of alternative 2-valued solutions on ambiguous 3-valued grounded\nsemantics; and (iv) identification of critical attack sets whose suspension\nresolves undecided arguments. Through systematic generation of critical attack\nsets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling\nusers to pinpoint specific causes of ambiguity and explore alternative\nresolutions. We use real-world legal cases (e.g., Wild Animals as modeled by\nBench-Capon) to show that our tool supports teleological legal reasoning by\nrevealing how different assumptions lead to different justified conclusions.", "AI": {"tldr": "AF-XRAY\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u63a2\u7d22\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\uff0c\u5e2e\u52a9\u975e\u4e13\u5bb6\u8bc6\u522b\u6b67\u4e49\u6765\u6e90\u5e76\u89e3\u91ca\u8bba\u8bc1\u63a5\u53d7\u6027\u3002", "motivation": "\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u8bba\u8bc1\u6846\u67b6\uff08AFs\uff09\u5b58\u5728\u6b67\u4e49\u6027\u548c\u89e3\u91ca\u6027\u6311\u6218\uff0c\u975e\u4e13\u5bb6\u96be\u4ee5\u7406\u89e3\u3002AF-XRAY\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "AF-XRAY\u63d0\u4f9b\u5206\u5c42\u53ef\u89c6\u5316\u3001\u653b\u51fb\u8fb9\u5206\u7c7b\u3001\u66ff\u4ee3\u89e3\u51b3\u65b9\u6848\u53e0\u52a0\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5173\u952e\u653b\u51fb\u96c6\u89e3\u51b3\u6b67\u4e49\u3002", "result": "\u5de5\u5177\u80fd\u5c06\u6b67\u4e49\u573a\u666f\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u6b67\u4e49\u539f\u56e0\u5e76\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "AF-XRAY\u901a\u8fc7\u5b9e\u9645\u6cd5\u5f8b\u6848\u4f8b\u9a8c\u8bc1\uff0c\u652f\u6301\u76ee\u7684\u6027\u6cd5\u5f8b\u63a8\u7406\uff0c\u5c55\u793a\u4e0d\u540c\u5047\u8bbe\u5982\u4f55\u5bfc\u81f4\u4e0d\u540c\u7ed3\u8bba\u3002"}}
{"id": "2507.10906", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10906", "abs": "https://arxiv.org/abs/2507.10906", "authors": ["Qunhong Zeng", "Yuxia Zhang", "Zexiong Ma", "Bo Jiang", "Ningyuan Sun", "Klaas-Jan Stol", "Xingyu Mou", "Hui Liu"], "title": "Evaluating Generated Commit Messages with Large Language Models", "comment": null, "summary": "Commit messages are essential in software development as they serve to\ndocument and explain code changes. Yet, their quality often falls short in\npractice, with studies showing significant proportions of empty or inadequate\nmessages. While automated commit message generation has advanced significantly,\nparticularly with Large Language Models (LLMs), the evaluation of generated\nmessages remains challenging. Traditional reference-based automatic metrics\nlike BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit\nmessage quality, as they assume a one-to-one mapping between code changes and\ncommit messages, leading researchers to rely on resource-intensive human\nevaluation. This study investigates the potential of LLMs as automated\nevaluators for commit message quality. Through systematic experimentation with\nvarious prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs\ncombining Chain-of-Thought reasoning with few-shot demonstrations achieve near\nhuman-level evaluation proficiency. Our LLM-based evaluator significantly\noutperforms traditional metrics while maintaining acceptable reproducibility,\nrobustness, and fairness levels despite some inherent variability. This work\nconducts a comprehensive preliminary study on using LLMs for commit message\nevaluation, offering a scalable alternative to human assessment while\nmaintaining high-quality evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u6765\u8bc4\u4f30\u63d0\u4ea4\u6d88\u606f\u8d28\u91cf\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u3002", "motivation": "\u63d0\u4ea4\u6d88\u606f\u8d28\u91cf\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u3001ROUGE-L\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u7ed3\u5408Chain-of-Thought\u63a8\u7406\u548c\u5c11\u91cf\u793a\u4f8b\uff0c\u6d4b\u8bd5\u4e0d\u540cLLMs\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "result": "LLMs\u5728\u8bc4\u4f30\u63d0\u4ea4\u6d88\u606f\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff0c\u4e14\u5177\u6709\u53ef\u63a5\u53d7\u7684\u91cd\u590d\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "LLMs\u4e3a\u63d0\u4ea4\u6d88\u606f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u66ff\u4ee3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u8bc4\u4f30\u7684\u9700\u6c42\u3002"}}
{"id": "2507.10893", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.10893", "abs": "https://arxiv.org/abs/2507.10893", "authors": ["Minjong Cheon", "Eunhan Goo", "Su-Hyeon Shin", "Muhammad Ahmed", "Hyungjun Kim"], "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency", "comment": "26pages, 9 Figures", "summary": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u8f7b\u91cf\u7ea7\u5168\u7403\u5929\u6c14\u9884\u62a5\u6a21\u578bKAI-a\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u5728\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684AI\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5176\u8bad\u7ec3\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u9700\u6c42\u5927\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u73b0\u4ee3CNN\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5929\u6c14\u9884\u62a5\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684CNN\u67b6\u6784\uff0c\u7ed3\u5408\u5c3a\u5ea6\u4e0d\u53d8\u8bbe\u8ba1\u548cInceptionNeXt\u6a21\u5757\uff0c\u9488\u5bf9\u5730\u7403\u7cfb\u7edf\u6570\u636e\u4f18\u5316\u3002\u6a21\u578b\u5728ERA5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u53c2\u6570\u4ec5700\u4e07\u3002", "result": "KAI-a\u5728\u4e2d\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\uff0c\u8ba1\u7b97\u9700\u6c42\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u5728\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\uff08\u59822018\u5e74\u6b27\u6d32\u70ed\u6d6a\uff09\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "KAI-a\u5c55\u793a\u4e86CNN\u67b6\u6784\u5728\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10958", "abs": "https://arxiv.org/abs/2507.10958", "authors": ["Anthony Miyaguchi", "David Guecha", "Yuwen Chiu", "Sidharth Gaur"], "title": "DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models", "comment": null, "summary": "This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.", "AI": {"tldr": "DS@GT\u56e2\u961f\u5728eRisk 2025\u6311\u6218\u4e2d\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0c\u5229\u7528LLMs\u8fdb\u884c\u6291\u90c1\u68c0\u6d4b\uff0c\u53d6\u5f97\u7b2c\u4e8c\u540d\u6210\u7ee9\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5bf9\u8bdd\u5f0f\u6291\u90c1\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7BDI-II\u8bc4\u4f30\u6807\u51c6\u63d0\u5347\u6a21\u578b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0c\u8bbe\u8ba1\u591a\u6837\u5316\u7684LLMs\u63d0\u793a\uff0c\u751f\u6210\u7ed3\u6784\u5316JSON\u8f93\u51fa\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u548c\u5185\u90e8\u4e00\u81f4\u6027\u3002", "result": "\u6700\u4f73\u63d0\u4ea4\u5728\u5b98\u65b9\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e8c\uff0c\u6307\u6807\u4e3aDCHR=0.50\u3001ADODL=0.89\u548cASHR=0.27\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u65b9\u6cd5\u6709\u6548\u5bf9\u9f50\u6a21\u578b\u8f93\u51fa\u4e0eBDI-II\u6807\u51c6\uff0c\u4e3a\u5206\u6790\u5bf9\u8bdd\u7ebf\u7d22\u5bf9\u75c7\u72b6\u9884\u6d4b\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.10894", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10894", "abs": "https://arxiv.org/abs/2507.10894", "authors": ["Zongtao He", "Liuyi Wang", "Lu Chen", "Chengju Liu", "Qijun Chen"], "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization", "comment": null, "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method.", "AI": {"tldr": "NavComposer\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5bfc\u822a\u6307\u4ee4\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u91cd\u7ec4\u8bed\u4e49\u5b9e\u4f53\uff08\u5982\u52a8\u4f5c\u3001\u573a\u666f\u548c\u5bf9\u8c61\uff09\u6765\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002NavInstrCritic\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u6307\u4ee4\u8d28\u91cf\u3002", "motivation": "\u4e13\u5bb6\u63d0\u4f9b\u7684\u5bfc\u822a\u6307\u4ee4\u6570\u91cf\u6709\u9650\uff0c\u5408\u6210\u7684\u6307\u4ee4\u8d28\u91cf\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "NavComposer\u901a\u8fc7\u5206\u89e3\u8bed\u4e49\u5b9e\u4f53\u5e76\u91cd\u7ec4\u4e3a\u6307\u4ee4\uff0cNavInstrCritic\u8bc4\u4f30\u6307\u4ee4\u7684\u5bf9\u6bd4\u5339\u914d\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8bed\u8a00\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u652f\u6301\u5927\u89c4\u6a21\u548c\u901a\u7528\u5316\u7814\u7a76\u3002", "conclusion": "NavComposer\u548cNavInstrCritic\u4e3a\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11059", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11059", "abs": "https://arxiv.org/abs/2507.11059", "authors": ["Pavel Adamenko", "Mikhail Ivanov", "Aidar Valeev", "Rodion Levichev", "Pavel Zadorozhny", "Ivan Lopatin", "Dmitry Babayev", "Alena Fenogenova", "Valentin Malykh"], "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08\\% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.", "AI": {"tldr": "SWE-MERA\u662f\u4e00\u4e2a\u52a8\u6001\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3SWE-bench\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6536\u96c6GitHub\u95ee\u9898\u5e76\u9a8c\u8bc1\u8d28\u91cf\uff0c\u8bc4\u4f30\u4e86\u591a\u4e2aLLM\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SWE-bench\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff08\u5982\u89e3\u51b3\u65b9\u6848\u6cc4\u6f0f\u548c\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u8db3\uff09\uff0c\u9650\u5236\u4e86LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86SWE-MERA\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6536\u96c6\u771f\u5b9eGitHub\u95ee\u9898\u5e76\u5b9e\u65bd\u4e25\u683c\u8d28\u91cf\u9a8c\u8bc1\uff0c\u6784\u5efa\u4e86\u7ea610,000\u4e2a\u6f5c\u5728\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u663e\u793aSWE-MERA\u5bf9\u6700\u65b0LLM\u5177\u6709\u5f3a\u533a\u5206\u80fd\u529b\uff0c\u5e76\u62a5\u544a\u4e862024\u5e749\u6708\u81f32025\u5e746\u6708\u6536\u96c6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "SWE-MERA\u89e3\u51b3\u4e86SWE-bench\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.10895", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10895", "abs": "https://arxiv.org/abs/2507.10895", "authors": ["Xiaocong Zeng", "Craig Michoski", "Yan Pang", "Dongyang Kuang"], "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition", "comment": null, "summary": "In this work, we address the often-overlooked issue of Timescale Dependent\nLabel Inconsistency (TsDLI) in training neural network models for EEG-based\nhuman emotion recognition. To mitigate TsDLI and enhance model generalization\nand explainability, we propose two novel regularization strategies: Local\nVariation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods\nincorporate classical mathematical principles--specifically, functions of\nbounded variation and commute-time distances--within a graph theoretic\nframework. Complementing our regularizers, we introduce a suite of new\nevaluation metrics that better capture the alignment between temporally local\npredictions and their associated global emotion labels. We validate our\napproach through comprehensive experiments on two widely used EEG emotion\ndatasets, DREAMER and DEAP, across a range of neural architectures including\nLSTM and transformer-based models. Performance is assessed using five distinct\nmetrics encompassing both quantitative accuracy and qualitative consistency.\nResults consistently show that our proposed methods outperform state-of-the-art\nbaselines, delivering superior aggregate performance and offering a principled\ntrade-off between interpretability and predictive power under label\ninconsistency. Notably, LVL achieves the best aggregate rank across all\nbenchmarked backbones and metrics, while LGCL frequently ranks the second,\nhighlighting the effectiveness of our framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565\uff08LVL\u548cLGCL\uff09\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u65f6\u95f4\u5c3a\u5ea6\u4f9d\u8d56\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u65f6\u95f4\u5c3a\u5ea6\u4f9d\u8d56\u6807\u7b7e\u4e0d\u4e00\u81f4\uff08TsDLI\uff09\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u53d8\u5f02\u635f\u5931\uff08LVL\uff09\u548c\u5c40\u90e8-\u5168\u5c40\u4e00\u81f4\u6027\u635f\u5931\uff08LGCL\uff09\uff0c\u7ed3\u5408\u6709\u754c\u53d8\u5f02\u51fd\u6570\u548c\u4ea4\u6362\u65f6\u95f4\u8ddd\u79bb\u7684\u6570\u5b66\u539f\u7406\u3002", "result": "\u5728DREAMER\u548cDEAP\u6570\u636e\u96c6\u4e0a\uff0cLVL\u548cLGCL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cLVL\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6807\u7b7e\u4e0d\u4e00\u81f4\u60c5\u51b5\u4e0b\u5e73\u8861\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0cLVL\u548cLGCL\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.10972", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10972", "abs": "https://arxiv.org/abs/2507.10972", "authors": ["Zhaoyi An", "Rei Kawakami"], "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production", "comment": "Accepted by IEEE ICIP 2025", "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.", "AI": {"tldr": "TEAM-Sign\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5c06\u5176\u89c6\u4e3a\u53e6\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u6765\u5904\u7406\u624b\u8bed\u751f\u6210\uff0c\u5229\u7528\u9010\u6b65\u63d0\u793a\u7b56\u7565\u89e3\u51b3\u624b\u8bed\u4e0e\u53e3\u8bed\u7684\u5dee\u5f02\u3002", "motivation": "\u624b\u8bed\u751f\u6210\u56e0\u590d\u6742\u6027\u548c\u72ec\u7279\u89c4\u5219\u800c\u53d7\u9650\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5176\u5f71\u54cd\u6709\u9650\u3002", "method": "\u5fae\u8c03LLM\uff0c\u91c7\u7528\u9010\u6b65\u63d0\u793a\u7b56\u7565\u63d0\u53d6\u5176\u5185\u5728\u624b\u8bed\u77e5\u8bc6\uff0c\u652f\u6301\u5b66\u4e60\u548c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728How2Sign\u548cPhoenix14T\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408LLM\u7684\u624b\u8bed\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u9f50\u624b\u8bed\u4e0e\u53e3\u8bed\u7684\u5206\u5e03\u548c\u8bed\u6cd5\u89c4\u5219\u3002", "conclusion": "TEAM-Sign\u6210\u529f\u5229\u7528LLM\u80fd\u529b\uff0c\u4e3a\u624b\u8bed\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10911", "abs": "https://arxiv.org/abs/2507.10911", "authors": ["Yicong Wu", "Ting Chen", "Irit Hochberg", "Zhoujian Sun", "Ruth Edry", "Zhengxing Huang", "Mor Peleg"], "title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation", "comment": null, "summary": "Therapy recommendation for chronic patients with multimorbidity is\nchallenging due to risks of treatment conflicts. Existing decision support\nsystems face scalability limitations. Inspired by the way in which general\npractitioners (GP) manage multimorbidity patients, occasionally convening\nmultidisciplinary team (MDT) collaboration, this study investigated the\nfeasibility and value of using a Large Language Model (LLM)-based multi-agent\nsystem (MAS) for safer therapy recommendations. We designed a single agent and\na MAS framework simulating MDT decision-making by enabling discussion among LLM\nagents to resolve medical conflicts. The systems were evaluated on therapy\nplanning tasks for multimorbidity patients using benchmark cases. We compared\nMAS performance with single-agent approaches and real-world benchmarks. An\nimportant contribution of our study is the definition of evaluation metrics\nthat go beyond the technical precision and recall and allow the inspection of\nclinical goals met and medication burden of the proposed advices to a gold\nstandard benchmark. Our results show that with current LLMs, a single agent GP\nperforms as well as MDTs. The best-scoring models provide correct\nrecommendations that address all clinical goals, yet the advices are\nincomplete. Some models also present unnecessary medications, resulting in\nunnecessary conflicts between medication and conditions or drug-drug\ninteractions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u5728\u6162\u6027\u591a\u75c5\u5171\u5b58\u60a3\u8005\u6cbb\u7597\u63a8\u8350\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4ef7\u503c\uff0c\u6a21\u62df\u591a\u5b66\u79d1\u56e2\u961f\uff08MDT\uff09\u51b3\u7b56\uff0c\u7ed3\u679c\u663e\u793a\u5355\u4ee3\u7406\u7cfb\u7edf\u8868\u73b0\u4e0eMDT\u76f8\u5f53\uff0c\u4f46\u5efa\u8bae\u5b58\u5728\u4e0d\u5b8c\u6574\u548c\u4e0d\u5fc5\u8981\u7684\u836f\u7269\u95ee\u9898\u3002", "motivation": "\u6162\u6027\u591a\u75c5\u5171\u5b58\u60a3\u8005\u7684\u6cbb\u7597\u63a8\u8350\u56e0\u6cbb\u7597\u51b2\u7a81\u98ce\u9669\u800c\u590d\u6742\uff0c\u73b0\u6709\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6269\u5c55\u6027\u6709\u9650\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM-MAS\u6a21\u62dfMDT\u51b3\u7b56\u7684\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u6a21\u62dfMDT\u8ba8\u8bba\u89e3\u51b3\u533b\u7597\u51b2\u7a81\uff0c\u901a\u8fc7\u57fa\u51c6\u6848\u4f8b\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u4e0e\u5355\u4ee3\u7406\u65b9\u6cd5\u548c\u771f\u5b9e\u57fa\u51c6\u6bd4\u8f83\u3002", "result": "\u5f53\u524dLLM\u4e0b\uff0c\u5355\u4ee3\u7406\u7cfb\u7edf\u8868\u73b0\u4e0eMDT\u76f8\u5f53\uff0c\u6700\u4f73\u6a21\u578b\u80fd\u63d0\u4f9b\u6b63\u786e\u5efa\u8bae\u4f46\u5b58\u5728\u4e0d\u5b8c\u6574\u548c\u4e0d\u5fc5\u8981\u7684\u836f\u7269\u95ee\u9898\u3002", "conclusion": "LLM-MAS\u5728\u6cbb\u7597\u63a8\u8350\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6539\u8fdb\u5efa\u8bae\u5b8c\u6574\u6027\u548c\u51cf\u5c11\u4e0d\u5fc5\u8981\u836f\u7269\uff0c\u672a\u6765\u53ef\u4f18\u5316\u6a21\u578b\u4ee5\u63d0\u5347\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11092", "abs": "https://arxiv.org/abs/2507.11092", "authors": ["Gong Chen", "Wenjie Liu", "Xiaoyuan Xie", "Xunzhu Tang", "Tegawend\u00e9 F. Bissyand\u00e9", "Songqiang Chen"], "title": "MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing", "comment": "27 pages", "summary": "Recently, several studies have indicated that data poisoning attacks pose a\nsevere security threat to deep learning-based (DL-based) code search models.\nAttackers inject carefully crafted malicious patterns into the training data,\nmisleading the code search model to learn these patterns during training.\nDuring the usage of the poisoned code search model for inference, once the\nmalicious pattern is triggered, the model tends to rank the vulnerability code\nhigher. However, existing detection methods for data poisoning attacks on\nDL-based code search models remain insufficiently effective. To address this\ncritical security issue, we propose MT4DP, a Data Poisoning Attack Detection\nFramework for DL-based Code Search Models via Metamorphic Testing. MT4DP\nintroduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)\ndesigned to detect data poisoning attacks on DL-based code search models.\nSpecifically, MT4DP first identifies the high-frequency words from search\nqueries as potential poisoning targets and takes their corresponding queries as\nthe source queries. For each source query, MT4DP generates two semantically\nequivalent follow-up queries and retrieves its source ranking list. Then, each\nsource ranking list is re-ranked based on the semantic similarities between its\ncode snippets and the follow-up queries. Finally, variances between the source\nand re-ranked lists are calculated to reveal violations of the SE-MR and warn\nthe data poisoning attack. Experimental results demonstrate that MT4DP\nsignificantly enhances the detection of data poisoning attacks on DL-based code\nsearch models, outperforming the best baseline by 191% on average F1 score and\n265% on average precision. Our work aims to promote further research into\neffective techniques for mitigating data poisoning threats on DL-based code\nsearch models.", "AI": {"tldr": "MT4DP\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u5f62\u6d4b\u8bd5\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u641c\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u7b49\u6548\u53d8\u5f62\u5173\u7cfb\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u6570\u636e\u6295\u6bd2\u653b\u51fb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u641c\u7d22\u6a21\u578b\u6784\u6210\u4e25\u91cd\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u6548\u679c\u4e0d\u8db3\u3002", "method": "MT4DP\u901a\u8fc7\u8bc6\u522b\u9ad8\u9891\u8bcd\u4f5c\u4e3a\u6f5c\u5728\u6295\u6bd2\u76ee\u6807\uff0c\u751f\u6210\u8bed\u4e49\u7b49\u6548\u7684\u540e\u7eed\u67e5\u8be2\uff0c\u5e76\u91cd\u65b0\u6392\u5e8f\u4ee3\u7801\u7247\u6bb5\u4ee5\u68c0\u6d4b\u6295\u6bd2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMT4DP\u5728F1\u5206\u6570\u548c\u7cbe\u786e\u7387\u4e0a\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u5347191%\u548c265%\u3002", "conclusion": "MT4DP\u6709\u6548\u63d0\u5347\u6570\u636e\u6295\u6bd2\u653b\u51fb\u68c0\u6d4b\u80fd\u529b\uff0c\u63a8\u52a8\u76f8\u5173\u5b89\u5168\u7814\u7a76\u3002"}}
{"id": "2507.10935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10935", "abs": "https://arxiv.org/abs/2507.10935", "authors": ["Shaowen Tong", "Zimin Xia", "Alexandre Alahi", "Xuming He", "Yujiao Shi"], "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization", "comment": "accepted by ICCV2025", "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.", "AI": {"tldr": "GeoDistill\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u5f15\u5bfc\u7684\u5f31\u76d1\u7763\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u5b66\u4e60\u548c\u57fa\u4e8e\u89c6\u573a\u7684\u63a9\u7801\uff0c\u63d0\u5347\u8de8\u89c6\u56fe\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u89c6\u56fe\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u5168\u76d1\u7763\u5b66\u4e60\uff0c\u9700\u8981\u7cbe\u786e\u7684\u59ff\u6001\u6807\u6ce8\u3002GeoDistill\u65e8\u5728\u901a\u8fc7\u5f31\u76d1\u7763\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u5b66\u4e60\u6846\u67b6\uff0c\u6559\u5e08\u6a21\u578b\u5b9a\u4f4d\u5168\u666f\u56fe\u50cf\uff0c\u5b66\u751f\u6a21\u578b\u5904\u7406\u6709\u9650\u89c6\u573a\u56fe\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u5bf9\u9f50\u548c\u63a9\u7801\u673a\u5236\u4f18\u5316\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGeoDistill\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65e0\u9700\u7cbe\u786e\u5e73\u9762\u4f4d\u7f6e\u6807\u6ce8\u7684\u65b9\u5411\u4f30\u8ba1\u7f51\u7edc\u3002", "conclusion": "GeoDistill\u4e3a\u8de8\u89c6\u56fe\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.10996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10996", "abs": "https://arxiv.org/abs/2507.10996", "authors": ["Lin Tian", "Johanne R. Trippas", "Marian-Andrei Rizoiu"], "title": "Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection", "comment": "12 pages, 5 tables, CLEF 2025", "summary": "This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u82f1\u8bed\u548c\u897f\u73ed\u7259\u8bed\u63a8\u6587\u4e2d\u7684\u6027\u522b\u6b67\u89c6\uff0c\u901a\u8fc7\u6761\u4ef6\u9002\u914d\u5668\u8def\u7531\u663e\u5f0f\u5efa\u6a21\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u6027\u522b\u6b67\u89c6\u68c0\u6d4b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5c42LoRA\u9002\u914d\u5668\uff0c\u663e\u5f0f\u5efa\u6a21\u4e09\u4e2a\u5b50\u4efb\u52a1\u7684\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5bf9\u6240\u6709\u7ebf\u6027\u53d8\u6362\u8fdb\u884c\u9002\u5e94\uff0c\u800c\u975e\u4ec5\u6ce8\u610f\u529b\u5c42\u3002", "result": "\u5728\u591a\u8bed\u8a00\u8bad\u7ec3\u4e2d\uff0c\u6027\u80fd\u63d0\u53471.7-2.4% F1\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1175%\uff0c\u6a21\u578b\u5b58\u50a8\u51cf\u5c1198%\uff0c\u4e14\u5728\u6240\u6709\u5b50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u591a\u8bed\u8a00\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2507.10923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10923", "abs": "https://arxiv.org/abs/2507.10923", "authors": ["Yuhao Wang", "Keyan Ding", "Kehua Feng", "Zeyuan Wang", "Ming Qin", "Xiaotong Li", "Qiang Zhang", "Huajun Chen"], "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization", "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Protein language models have emerged as powerful tools for sequence\ngeneration, offering substantial advantages in functional optimization and\ndenovo design. However, these models also present significant risks of\ngenerating harmful protein sequences, such as those that enhance viral\ntransmissibility or evade immune responses. These concerns underscore critical\nbiosafety and ethical challenges. To address these issues, we propose a\nKnowledge-guided Preference Optimization (KPO) framework that integrates prior\nknowledge via a Protein Safety Knowledge Graph. This framework utilizes an\nefficient graph pruning strategy to identify preferred sequences and employs\nreinforcement learning to minimize the risk of generating harmful proteins.\nExperimental results demonstrate that KPO effectively reduces the likelihood of\nproducing hazardous sequences while maintaining high functionality, offering a\nrobust safety assurance framework for applying generative models in\nbiotechnology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u5f15\u5bfc\u7684\u504f\u597d\u4f18\u5316\uff08KPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u86cb\u767d\u8d28\u5b89\u5168\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u51cf\u5c11\u751f\u6210\u6709\u5bb3\u86cb\u767d\u8d28\u5e8f\u5217\u7684\u98ce\u9669\u3002", "motivation": "\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u751f\u6210\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4e5f\u53ef\u80fd\u751f\u6210\u6709\u5bb3\u5e8f\u5217\uff0c\u5982\u589e\u5f3a\u75c5\u6bd2\u4f20\u64ad\u6027\u6216\u9003\u907f\u514d\u75ab\u53cd\u5e94\u7684\u86cb\u767d\u8d28\uff0c\u8fd9\u5e26\u6765\u4e86\u751f\u7269\u5b89\u5168\u548c\u4f26\u7406\u6311\u6218\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u56fe\u526a\u679d\u7b56\u7565\uff0c\u8bc6\u522b\u504f\u597d\u5e8f\u5217\u5e76\u6700\u5c0f\u5316\u6709\u5bb3\u86cb\u767d\u8d28\u751f\u6210\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKPO\u80fd\u6709\u6548\u51cf\u5c11\u6709\u5bb3\u5e8f\u5217\u7684\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u529f\u80fd\u6027\u3002", "conclusion": "KPO\u4e3a\u751f\u7269\u6280\u672f\u4e2d\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\u6846\u67b6\u3002"}}
{"id": "2507.11146", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11146", "abs": "https://arxiv.org/abs/2507.11146", "authors": ["Tom Yaacov", "Gera Weiss", "Gal Amram", "Avi Hayoun"], "title": "Automata Models for Effective Bug Description", "comment": "Accepted to the ACM/IEEE 28th International Conference on Model\n  Driven Engineering Languages and Systems (MODELS 2025)", "summary": "Debugging complex systems is a crucial yet time-consuming task. This paper\npresents the use of automata learning and testing techniques to obtain concise\nand informative bug descriptions. We introduce the concepts of Failure\nExplanations (FE), Eventual Failure Explanations (EFE), and Early Detection\n(ED) to provide meaningful summaries of failing behavior patterns. By factoring\nout irrelevant information and focusing on essential test patterns, our\napproach aims to enhance bug detection and understanding. We evaluate our\nmethods using various test patterns and real-world benchmarks, demonstrating\ntheir effectiveness in producing compact and informative bug descriptions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u673a\u5b66\u4e60\u548c\u6d4b\u8bd5\u6280\u672f\u7684\u8c03\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5931\u8d25\u89e3\u91ca\uff08FE\uff09\u3001\u6700\u7ec8\u5931\u8d25\u89e3\u91ca\uff08EFE\uff09\u548c\u65e9\u671f\u68c0\u6d4b\uff08ED\uff09\u6765\u751f\u6210\u7b80\u6d01\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u9519\u8bef\u63cf\u8ff0\u3002", "motivation": "\u8c03\u8bd5\u590d\u6742\u7cfb\u7edf\u8017\u65f6\u4e14\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u6709\u610f\u4e49\u7684\u9519\u8bef\u603b\u7ed3\u3002", "method": "\u5229\u7528\u81ea\u52a8\u673a\u5b66\u4e60\u548c\u6d4b\u8bd5\u6280\u672f\uff0c\u63d0\u53d6\u5173\u952e\u6d4b\u8bd5\u6a21\u5f0f\u5e76\u8fc7\u6ee4\u65e0\u5173\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u6d4b\u8bd5\u6a21\u5f0f\u548c\u5b9e\u9645\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u751f\u6210\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u9519\u8bef\u63cf\u8ff0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u68c0\u6d4b\u548c\u7406\u89e3\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u8c03\u8bd5\u3002"}}
{"id": "2507.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10938", "abs": "https://arxiv.org/abs/2507.10938", "authors": ["Zhengyi Xu", "Haoran Wu", "Wen Jiang", "Jie Geng"], "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing", "comment": null, "summary": "Semantic change detection (SCD) extends the binary change detection task to\nprovide not only the change locations but also the detailed \"from-to\"\ncategories in multi-temporal remote sensing data. Such detailed semantic\ninsights into changes offer considerable advantages for a wide array of\napplications. However, since SCD involves the simultaneous optimization of\nmultiple tasks, the model is prone to negative transfer due to task-specific\nlearning difficulties and conflicting gradient flows. To address this issue, we\npropose Graph Aggregation Prototype Learning for Semantic Change Detection in\nremote sensing(GAPL-SCD). In this framework, a multi-task joint optimization\nmethod is designed to optimize the primary task of semantic segmentation and\nchange detection, along with the auxiliary task of graph aggregation prototype\nlearning. Adaptive weight allocation and gradient rotation methods are used to\nalleviate the conflict between training tasks and improve multi-task learning\ncapabilities. Specifically, the graph aggregation prototype learning module\nconstructs an interaction graph using high-level features. Prototypes serve as\nclass proxies, enabling category-level domain alignment across time points and\nreducing interference from irrelevant changes. Additionally, the proposed\nself-query multi-level feature interaction and bi-temporal feature fusion\nmodules further enhance multi-scale feature representation, improving\nperformance in complex scenes. Experimental results on the SECOND and\nLandsat-SCD datasets demonstrate that our method achieves state-of-the-art\nperformance, with significant improvements in accuracy and robustness for SCD\ntask.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAPL-SCD\u7684\u56fe\u805a\u5408\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u591a\u4efb\u52a1\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u548c\u68af\u5ea6\u65cb\u8f6c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\uff08SCD\uff09\u9700\u8981\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u4efb\u52a1\uff0c\u5bb9\u6613\u56e0\u4efb\u52a1\u95f4\u51b2\u7a81\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u4efb\u52a1\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5272\u3001\u53d8\u5316\u68c0\u6d4b\u548c\u56fe\u805a\u5408\u539f\u578b\u5b66\u4e60\u3002\u91c7\u7528\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u548c\u68af\u5ea6\u65cb\u8f6c\u65b9\u6cd5\u7f13\u89e3\u4efb\u52a1\u51b2\u7a81\u3002", "result": "\u5728SECOND\u548cLandsat-SCD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86SCD\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GAPL-SCD\u65b9\u6cd5\u901a\u8fc7\u591a\u4efb\u52a1\u4f18\u5316\u548c\u7279\u5f81\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11004", "abs": "https://arxiv.org/abs/2507.11004", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification", "comment": "ACL 2025 Workshop (FEVER)", "summary": "This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2.", "AI": {"tldr": "HerO 2\u662fTeam HUMANE\u5728FEVER-25\u7814\u8ba8\u4f1a\u4e0a\u63d0\u51fa\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6587\u6863\u6458\u8981\u548c\u7b54\u6848\u91cd\u6784\u63d0\u5347\u8bc1\u636e\u8d28\u91cf\uff0c\u4f18\u5316\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5e76\u96c6\u6210\u66f4\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e8b\u5b9e\u9a8c\u8bc1\u3002", "motivation": "\u6539\u8fdb\u53bb\u5e74\u7684\u6700\u4f73\u5f00\u6e90\u6a21\u578bHerO\uff0c\u63d0\u5347\u8bc1\u636e\u8d28\u91cf\u548c\u7cfb\u7edf\u6027\u80fd\uff0c\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u4e8b\u5b9e\u9a8c\u8bc1\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6587\u6863\u6458\u8981\u548c\u7b54\u6848\u91cd\u6784\u4f18\u5316\u8bc1\u636e\u8d28\u91cf\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u91cf\u5316\u4f18\u5316\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5e76\u96c6\u6210\u66f4\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e8c\uff0c\u8fd0\u884c\u65f6\u95f4\u6700\u77ed\uff0c\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "HerO 2\u5728\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.10993", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10993", "abs": "https://arxiv.org/abs/2507.10993", "authors": ["Emir Durakovic", "Min-Hong Shih"], "title": "Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction", "comment": "This paper uses a lightly modified version of the AAAI 2025 LaTeX\n  style for formatting consistency. It is not a submission to AAAI and does not\n  include any AAAI-specific headers, footers, or metadata", "summary": "Due to climate-induced changes, many habitats are experiencing range shifts\naway from their traditional geographic locations (Piguet, 2011). We propose a\nsolution to accurately model whether bird species are present in a specific\nhabitat through the combination of Convolutional Neural Networks (CNNs)\n(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery\nand environmental features (e.g., temperature, precipitation, elevation) to\npredict bird presence across various climates. The CNN model captures spatial\ncharacteristics of landscapes such as forestation, water bodies, and\nurbanization, whereas the tabular method uses ecological and geographic data.\nBoth systems predict the distribution of birds with an average accuracy of 85%,\noffering a scalable but reliable method to understand bird migration.", "AI": {"tldr": "\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u8868\u683c\u6570\u636e\uff0c\u901a\u8fc7\u536b\u661f\u56fe\u50cf\u548c\u73af\u5883\u7279\u5f81\u9884\u6d4b\u9e1f\u7c7b\u5728\u4e0d\u540c\u6c14\u5019\u4e2d\u7684\u5206\u5e03\uff0c\u51c6\u786e\u7387\u8fbe85%\u3002", "motivation": "\u7531\u4e8e\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u6816\u606f\u5730\u8303\u56f4\u53d8\u5316\uff0c\u9700\u8981\u51c6\u786e\u9884\u6d4b\u9e1f\u7c7b\u5728\u7279\u5b9a\u6816\u606f\u5730\u7684\u5b58\u5728\u3002", "method": "\u4f7f\u7528CNN\u5206\u6790\u536b\u661f\u56fe\u50cf\u7684\u7a7a\u95f4\u7279\u5f81\uff08\u5982\u68ee\u6797\u3001\u6c34\u4f53\u3001\u57ce\u5e02\u5316\uff09\uff0c\u5e76\u7ed3\u5408\u8868\u683c\u6570\u636e\uff08\u5982\u6e29\u5ea6\u3001\u964d\u6c34\u3001\u6d77\u62d4\uff09\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u9e1f\u7c7b\u5206\u5e03\u7684\u51c6\u786e\u7387\u4e3a85%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u65b9\u5f0f\uff0c\u7528\u4e8e\u7406\u89e3\u9e1f\u7c7b\u8fc1\u5f99\u3002"}}
{"id": "2507.11199", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11199", "abs": "https://arxiv.org/abs/2507.11199", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "title": "New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report", "comment": null, "summary": "Mutation testing has emerged as a powerful technique for evaluating the\neffectiveness of test suites for Deep Neural Networks. Among existing\napproaches, the statistical mutant killing criterion of DeepCrime has leveraged\nstatistical testing to determine whether a mutant significantly differs from\nthe original model. However, it suffers from a critical limitation: it violates\nthe monotonicity property, meaning that expanding a test set may result in\npreviously killed mutants no longer being classified as killed. In this\ntechnical report, we propose a new formulation of statistical mutant killing\nbased on Fisher exact test that preserves the statistical rigour of it while\nensuring monotonicity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFisher\u7cbe\u786e\u68c0\u9a8c\u7684\u7edf\u8ba1\u7a81\u53d8\u6740\u6b7b\u6807\u51c6\uff0c\u89e3\u51b3\u4e86DeepCrime\u8fdd\u53cd\u5355\u8c03\u6027\u7684\u95ee\u9898\u3002", "motivation": "DeepCrime\u7684\u7edf\u8ba1\u7a81\u53d8\u6740\u6b7b\u6807\u51c6\u5b58\u5728\u8fdd\u53cd\u5355\u8c03\u6027\u7684\u95ee\u9898\uff0c\u5373\u6269\u5927\u6d4b\u8bd5\u96c6\u53ef\u80fd\u5bfc\u81f4\u5148\u524d\u6740\u6b7b\u7684\u7a81\u53d8\u4e0d\u518d\u88ab\u5206\u7c7b\u4e3a\u6740\u6b7b\u3002", "method": "\u91c7\u7528Fisher\u7cbe\u786e\u68c0\u9a8c\u91cd\u65b0\u5b9a\u4e49\u7edf\u8ba1\u7a81\u53d8\u6740\u6b7b\u6807\u51c6\uff0c\u4fdd\u6301\u7edf\u8ba1\u4e25\u8c28\u6027\u7684\u540c\u65f6\u786e\u4fdd\u5355\u8c03\u6027\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u7edf\u8ba1\u4e25\u8c28\u6027\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u5355\u8c03\u6027\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8eFisher\u7cbe\u786e\u68c0\u9a8c\u7684\u65b0\u65b9\u6cd5\u662f\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u7edf\u8ba1\u7a81\u53d8\u6740\u6b7b\u6807\u51c6\u3002"}}
{"id": "2507.10943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10943", "abs": "https://arxiv.org/abs/2507.10943", "authors": ["Yushun Fang", "Lu Liu", "Xiang Gao", "Qiang Hu", "Ning Cao", "Jianghe Cui", "Gang Chen", "Xiaoyun Zhang"], "title": "Robust ID-Specific Face Restoration via Alignment Learning", "comment": "17 pages, 8 figures", "summary": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness.", "AI": {"tldr": "RIDFR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684ID\u7279\u5b9a\u4eba\u8138\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5bb9\u6ce8\u5165\u548c\u8eab\u4efd\u6ce8\u5165\u6a21\u5757\uff0c\u7ed3\u5408\u5bf9\u9f50\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u4eba\u8138\u4fee\u590d\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u8eab\u4efd\u6a21\u7cca\u8f93\u5165\u548c\u968f\u673a\u751f\u6210\u8fc7\u7a0b\u65f6\u3002", "method": "RIDFR\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5185\u5bb9\u6ce8\u5165\u6a21\u5757\u548c\u8eab\u4efd\u6ce8\u5165\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u5b66\u4e60\u6291\u5236ID\u65e0\u5173\u8bed\u4e49\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRIDFR\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "RIDFR\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u8eab\u4efd\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u4eba\u8138\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11049", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11049", "abs": "https://arxiv.org/abs/2507.11049", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "comment": "Preprint. 24 pages", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u97e9\u8bed\u65b0\u95fb\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6K-News-Stance\u548c\u6846\u67b6JoA-ICL\uff0c\u7528\u4e8e\u957f\u6587\u672c\u7acb\u573a\u68c0\u6d4b\uff0c\u63d0\u5347\u65b0\u95fb\u63a8\u8350\u591a\u6837\u6027\u548c\u5a92\u4f53\u504f\u89c1\u5206\u6790\u3002", "motivation": "\u5728\u7ebf\u65b0\u95fb\u6d88\u8d39\u589e\u957f\u5bfc\u81f4\u63a8\u8350\u7cfb\u7edf\u53ef\u80fd\u52a0\u5267\u4fe1\u606f\u8327\u623f\u548c\u653f\u6cbb\u6781\u5316\uff0c\u9700\u8981\u7acb\u573a\u68c0\u6d4b\u6280\u672f\u6765\u5f15\u5165\u591a\u5143\u89c6\u89d2\u3002", "method": "\u63d0\u51faK-News-Stance\u6570\u636e\u96c6\u548cJoA-ICL\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5206\u6790\u5173\u952e\u6bb5\u843d\u7acb\u573a\u5e76\u805a\u5408\u63a8\u65ad\u5168\u6587\u7acb\u573a\u3002", "result": "JoA-ICL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6355\u6349\u957f\u65b0\u95fb\u7acb\u573a\uff0c\u6848\u4f8b\u7814\u7a76\u663e\u793a\u5176\u5728\u63a8\u8350\u591a\u6837\u6027\u548c\u5a92\u4f53\u504f\u89c1\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u97e9\u8bed\u957f\u6587\u672c\u7acb\u573a\u68c0\u6d4b\u7684\u7a7a\u767d\uff0cJoA-ICL\u4e3a\u65b0\u95fb\u63a8\u8350\u548c\u5a92\u4f53\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.11060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11060", "abs": "https://arxiv.org/abs/2507.11060", "authors": ["Yilmazcan Ozyurt", "Tunaberk Almaci", "Stefan Feuerriegel", "Mrinmaya Sachan"], "title": "Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing", "comment": null, "summary": "We introduce ExRec, a general framework for personalized exercise\nrecommendation with semantically-grounded knowledge tracing. Our method builds\non the observation that existing exercise recommendation approaches simulate\nstudent performance via knowledge tracing (KT) but they often overlook two key\naspects: (a) the semantic content of questions and (b) the sequential,\nstructured progression of student learning. To address this, our ExRec presents\nan end-to-end pipeline, from annotating the KCs of questions and learning their\nsemantic representations to training KT models and optimizing several\nreinforcement learning (RL) methods. Moreover, we improve standard\nQ-learning-based continuous RL methods via a tailored model-based value\nestimation (MVE) approach that directly leverages the components of KT model in\nestimating cumulative knowledge improvement. We validate the effectiveness of\nour ExRec using various RL methods across four real-world tasks with different\neducational goals in online math learning. We further show that ExRec\ngeneralizes robustly to new, unseen questions and that it produces\ninterpretable student learning trajectories. Together, our findings highlight\nthe promise of KT-guided RL for effective personalization in education.", "AI": {"tldr": "ExRec\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u77e5\u8bc6\u8ffd\u8e2a\u7684\u4e2a\u6027\u5316\u7ec3\u4e60\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u95ee\u9898\u8bed\u4e49\u548c\u5b66\u751f\u5b66\u4e60\u7684\u7ed3\u6784\u5316\u8fdb\u5c55\uff0c\u4f18\u5316\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7ec3\u4e60\u63a8\u8350\u65b9\u6cd5\u5ffd\u7565\u95ee\u9898\u8bed\u4e49\u548c\u5b66\u751f\u5b66\u4e60\u7684\u7ed3\u6784\u5316\u8fdb\u5c55\uff0cExRec\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ExRec\u91c7\u7528\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5305\u62ec\u95ee\u9898\u77e5\u8bc6\u7ec4\u4ef6\u6807\u6ce8\u3001\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u3001\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5e76\u6539\u8fdb\u4e86\u57fa\u4e8eQ\u5b66\u4e60\u7684\u8fde\u7eed\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u5b66\u5b66\u4e60\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86ExRec\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5bf9\u672a\u89c1\u95ee\u9898\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u8f68\u8ff9\u3002", "conclusion": "ExRec\u8bc1\u660e\u4e86\u77e5\u8bc6\u8ffd\u8e2a\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u6559\u80b2\u4e2a\u6027\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11272", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.11272", "abs": "https://arxiv.org/abs/2507.11272", "authors": ["Anh Nguyen-Duc", "Chien Vu Manh", "Bao Anh Tran", "Viet Phuong Ngo", "Luan Le Chi", "Anh Quang Nguyen"], "title": "An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling", "comment": null, "summary": "This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University\nAdmission System), a real-world deployment of a conversational AI platform for\nhigher education admissions counseling in Vietnam. While large language models\n(LLMs) offer potential for automating advisory tasks, most existing solutions\nremain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap\nby combining hybrid retrieval, multi-agent orchestration, and LLM-based\ngeneration into a system tailored for real-world university admissions. In\ncollaboration with the University of Transport Technology (UTT) in Hanoi, we\nconducted a two-phase study involving technical development and real-world\nevaluation. MARAUS processed over 6,000 actual user interactions, spanning six\ncategories of queries. Results show substantial improvements over LLM-only\nbaselines: on average 92 percent accuracy, hallucination rates reduced from 15\nprecent to 1.45 percent, and average response times below 4 seconds. The system\noperated cost-effectively, with a two-week deployment cost of 11.58 USD using\nGPT-4o mini. This work provides actionable insights for the deployment of\nagentic RAG systems in low-resource educational settings.", "AI": {"tldr": "MARAUS\u662f\u4e00\u4e2a\u7ed3\u5408\u591a\u4ee3\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u7684\u5927\u5b66\u62db\u751f\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u5728\u8d8a\u5357\u9ad8\u7b49\u6559\u80b2\u62db\u751f\u54a8\u8be2\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u5316\u54a8\u8be2\u4efb\u52a1\u4e2d\u591a\u4e3a\u539f\u578b\u6216\u5408\u6210\u57fa\u51c6\uff0cMARAUS\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u62db\u751f\u573a\u666f\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u3001\u591a\u4ee3\u7406\u534f\u8c03\u548c\u57fa\u4e8eLLM\u7684\u751f\u6210\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u5b9e\u9645\u62db\u751f\u9700\u6c42\u7684\u7cfb\u7edf\uff0c\u5e76\u4e0e\u8d8a\u5357\u8fd0\u8f93\u6280\u672f\u5927\u5b66\u5408\u4f5c\u8fdb\u884c\u6280\u672f\u5f00\u53d1\u548c\u5b9e\u9645\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u5904\u7406\u4e866000\u591a\u6b21\u7528\u6237\u4ea4\u4e92\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe92%\uff0c\u5e7b\u89c9\u7387\u4ece15%\u964d\u81f31.45%\uff0c\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e4\u79d2\uff0c\u6210\u672c\u6548\u76ca\u9ad8\u3002", "conclusion": "MARAUS\u4e3a\u4f4e\u8d44\u6e90\u6559\u80b2\u73af\u5883\u4e2d\u90e8\u7f72\u4ee3\u7406\u5f0fRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2507.10969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10969", "abs": "https://arxiv.org/abs/2507.10969", "authors": ["Palash Ray", "Mahuya Sasmal", "Asish Bera"], "title": "Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data", "comment": null, "summary": "Sports action classification representing complex body postures and\nplayer-object interactions is an emerging area in image-based sports analysis.\nSome works have contributed to automated sports action recognition using\nmachine learning techniques over the past decades. However, sufficient image\ndatasets representing women sports actions with enough intra- and inter-class\nvariations are not available to the researchers. To overcome this limitation,\nthis work presents a new dataset named WomenSports for women sports\nclassification using small-scale training data. This dataset includes a variety\nof sports activities, covering wide variations in movements, environments, and\ninteractions among players. In addition, this study proposes a convolutional\nneural network (CNN) for deep feature extraction. A channel attention scheme\nupon local contextual regions is applied to refine and enhance feature\nrepresentation. The experiments are carried out on three different sports\ndatasets and one dance dataset for generalizing the proposed algorithm, and the\nperformances on these datasets are noteworthy. The deep learning method\nachieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed\nWomenSports dataset, which is publicly available for research at Mendeley Data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aWomenSports\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5973\u6027\u8fd0\u52a8\u5206\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5973\u6027\u8fd0\u52a8\u52a8\u4f5c\u6570\u636e\u96c6\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u7c7b\u522b\u5185\u548c\u7c7b\u522b\u95f4\u53d8\u5316\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aCNN\u6a21\u578b\uff0c\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728WomenSports\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528ResNet-50\u8fbe\u523089.15%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u5973\u6027\u8fd0\u52a8\u5206\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.11052", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11052", "abs": "https://arxiv.org/abs/2507.11052", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u589e\u5f3a\u7684\u4e34\u5e8aNLP\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6\u5fc3\u8840\u7ba1\u75be\u75c5\u65e9\u671f\u6307\u6807\uff0c\u63d0\u5347\u98ce\u9669\u5206\u5c42\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u53ca\u65f6\u8bc6\u522b\u548c\u51c6\u786e\u98ce\u9669\u5206\u5c42\u5bf9\u964d\u4f4e\u5168\u7403\u6b7b\u4ea1\u7387\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\uff0c\u800c\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u9886\u57df\u9002\u5e94\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u5fc3\u8840\u7ba1\u7279\u5f02\u6027\u5fae\u8c03\u3001\u63d0\u793a\u63a8\u7406\u548c\u5b9e\u4f53\u611f\u77e5\u63a8\u7406\uff0c\u4ece\u81ea\u7531\u6587\u672c\u4e2d\u63d0\u53d6\u75c7\u72b6\u5e76\u8fdb\u884c\u4e0a\u4e0b\u6587\u5206\u6790\u3002", "result": "\u5728MIMIC-III\u548cCARDIO-NLP\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUROC\u5747\u63d0\u5347\uff0c\u4e34\u5e8a\u76f8\u5173\u6027\u9ad8\uff08kappa=0.82\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86LLM\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u63a8\u8fdb\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u5e76\u5c06\u60a3\u8005\u53d9\u8ff0\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2507.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11079", "abs": "https://arxiv.org/abs/2507.11079", "authors": ["Li Wang", "Qizhen Wu", "Lei Chen"], "title": "Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander", "comment": null, "summary": "In multiple unmanned ground vehicle confrontations, autonomously evolving\nmulti-agent tactical decisions from situational awareness remain a significant\nchallenge. Traditional handcraft rule-based methods become vulnerable in the\ncomplicated and transient battlefield environment, and current reinforcement\nlearning methods mainly focus on action manipulation instead of strategic\ndecisions due to lack of interpretability. Here, we propose a vision-language\nmodel-based commander to address the issue of intelligent\nperception-to-decision reasoning in autonomous confrontations. Our method\nintegrates a vision language model for scene understanding and a lightweight\nlarge language model for strategic reasoning, achieving unified perception and\ndecision within a shared semantic space, with strong adaptability and\ninterpretability. Unlike rule-based search and reinforcement learning methods,\nthe combination of the two modules establishes a full-chain process, reflecting\nthe cognitive process of human commanders. Simulation and ablation experiments\nvalidate that the proposed approach achieves a win rate of over 80% compared\nwith baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6307\u6325\u5b98\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5bf9\u6297\u4e2d\u7684\u667a\u80fd\u611f\u77e5\u5230\u51b3\u7b56\u63a8\u7406\u95ee\u9898\uff0c\u7ed3\u5408\u4e86\u573a\u666f\u7406\u89e3\u548c\u6218\u7565\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u9ad8\u80dc\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u5de5\u89c4\u5219\u65b9\u6cd5\u5728\u590d\u6742\u6218\u573a\u73af\u5883\u4e2d\u8106\u5f31\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u4e3b\u8981\u5173\u6ce8\u52a8\u4f5c\u64cd\u4f5c\u800c\u975e\u6218\u7565\u51b3\u7b56\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\u548c\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6218\u7565\u63a8\u7406\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u611f\u77e5\u4e0e\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u548c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u80dc\u7387\u8d85\u8fc780%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u6307\u6325\u5b98\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u667a\u80fd\u51b3\u7b56\u7cfb\u7edf\u3002"}}
{"id": "2507.11346", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11346", "abs": "https://arxiv.org/abs/2507.11346", "authors": ["Pedro Sim\u00f5es", "Rohit Gheyi", "Rian Melo", "Jonhnanthan Oliveira", "M\u00e1rcio Ribeiro", "Wesley K. G. Assun\u00e7\u00e3o"], "title": "RefModel: Detecting Refactorings using Foundation Models", "comment": "Accepted at Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Refactoring is a common software engineering practice that improves code\nquality without altering program behavior. Although tools like ReExtractor+,\nRefactoringMiner, and RefDiff have been developed to detect refactorings\nautomatically, they rely on complex rule definitions and static analysis,\nmaking them difficult to extend and generalize to other programming languages.\nIn this paper, we investigate the viability of using foundation models for\nrefactoring detection, implemented in a tool named RefModel. We evaluate\nPhi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation\ntransformations applied to artificially generated Java programs, covering\nwidely-used refactoring types. We also extend our evaluation by including\nGemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world\nrefactorings extracted from four open-source projects. These models are\ncompared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is\ncompetitive with, and in some cases outperform, traditional tools. In\nreal-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified\n97% of all refactorings, surpassing the best-performing static-analysis-based\ntools. The models showed encouraging generalization to Python and Golang. They\nprovide natural language explanations and require only a single sentence to\ndefine each refactoring type.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u57fa\u7840\u6a21\u578b\uff08\u5982Phi4-14B\u3001Claude 3.5 Sonnet\u7b49\uff09\u68c0\u6d4b\u4ee3\u7801\u91cd\u6784\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u4e86\u5de5\u5177RefModel\uff0c\u5e76\u5728\u6027\u80fd\u548c\u901a\u7528\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u91cd\u6784\u68c0\u6d4b\u5de5\u5177\u4f9d\u8d56\u590d\u6742\u89c4\u5219\u548c\u9759\u6001\u5206\u6790\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u901a\u7528\u5316\uff0c\u56e0\u6b64\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u7840\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u57fa\u7840\u6a21\u578b\uff08\u5982Phi4-14B\u3001Claude 3.5 Sonnet\u7b49\uff09\u5728\u4eba\u5de5\u751f\u6210\u7684Java\u7a0b\u5e8f\u548c\u771f\u5b9e\u5f00\u6e90\u9879\u76ee\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4f20\u7edf\u5de5\u5177\uff08\u5982RefactoringMiner\u3001RefDiff\u7b49\uff09\u5bf9\u6bd4\u3002", "result": "RefModel\u5728\u6027\u80fd\u4e0a\u4e0e\u4f20\u7edf\u5de5\u5177\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0cClaude 3.5 Sonnet\u548cGemini 2.5 Pro\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8bc6\u522b\u4e8697%\u7684\u91cd\u6784\uff0c\u4e14\u80fd\u6cdb\u5316\u5230Python\u548cGolang\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u91cd\u6784\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u6613\u4e8e\u6269\u5c55\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10977", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10977", "abs": "https://arxiv.org/abs/2507.10977", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "Human-object interaction (HOI) detection is essential for accurately\nlocalizing and characterizing interactions between humans and objects,\nproviding a comprehensive understanding of complex visual scenes across various\ndomains. However, existing HOI detectors often struggle to deliver reliable\npredictions efficiently, relying on resource-intensive training methods and\ninefficient architectures. To address these challenges, we conceptualize a\nwavelet attention-like backbone and a novel ray-based encoder architecture\ntailored for HOI detection. Our wavelet backbone addresses the limitations of\nexpressing middle-order interactions by aggregating discriminative features\nfrom the low- and high-order interactions extracted from diverse convolutional\nfilters. Concurrently, the ray-based encoder facilitates multi-scale attention\nby optimizing the focus of the decoder on relevant regions of interest and\nmitigating computational overhead. As a result of harnessing the attenuated\nintensity of learnable ray origins, our decoder aligns query embeddings with\nemphasized regions of interest for accurate predictions. Experimental results\non benchmark datasets, including ImageNet and HICO-DET, showcase the potential\nof our proposed architecture. The code is publicly available at\n[https://github.com/henry-pay/RayEncoder].", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u6ce8\u610f\u529b\u673a\u5236\u548c\u5c04\u7ebf\u7f16\u7801\u5668\u7684\u65b0\u578bHOI\u68c0\u6d4b\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u548c\u8d44\u6e90\u6d88\u8017\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709HOI\u68c0\u6d4b\u5668\u5728\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u548c\u4f4e\u6548\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u5c0f\u6ce2\u6ce8\u610f\u529b\u673a\u5236\u4e3b\u5e72\u7f51\u7edc\u548c\u5c04\u7ebf\u7f16\u7801\u5668\uff0c\u5206\u522b\u7528\u4e8e\u63d0\u53d6\u591a\u9636\u4ea4\u4e92\u7279\u5f81\u548c\u4f18\u5316\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u3002", "result": "\u5728ImageNet\u548cHICO-DET\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86HOI\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11084", "abs": "https://arxiv.org/abs/2507.11084", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha"], "title": "Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach", "comment": "This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library", "summary": "The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408Transformer\u7684\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u7801\u5b5f\u52a0\u62c9\u56fd\u4e03\u6708\u9769\u547d\u671f\u95f4\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u4e2d\u7684\u516c\u4f17\u60c5\u7eea\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u4e2d\u53d6\u5f97\u4e8683.7%\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\uff0c\u63ed\u793a\u5b5f\u52a0\u62c9\u56fd\u4e03\u6708\u9769\u547d\u671f\u95f4\u516c\u4f17\u7684\u60c5\u7eea\u548c\u89c2\u70b9\uff0c\u4ee5\u652f\u6301\u793e\u4f1a\u6b63\u4e49\u548c\u7cfb\u7edf\u6027\u6539\u9769\u7684\u8bc9\u6c42\u3002", "method": "\u91c7\u7528\u6df7\u5408Transformer\u6846\u67b6\uff08\u5305\u62ecBanglaBERT\u3001mBERT\u3001XLM-RoBERTa\u548c\u63d0\u51fa\u7684XMB-BERT\uff09\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408PCA\u964d\u7ef4\u548c11\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u8fdb\u884c\u60c5\u611f\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684XMB-BERT\u4e0e\u6295\u7968\u5206\u7c7b\u5668\u7ec4\u5408\u53d6\u5f97\u4e8683.7%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u7ec4\u5408\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u7684\u793e\u4f1a\u60c5\u611f\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.11083", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11083", "abs": "https://arxiv.org/abs/2507.11083", "authors": ["Longhui Zhang", "Bin Wang", "Jiahao Wang", "Xiaofeng Zhao", "Min Zhang", "Hao Yang", "Meishan Zhang", "Yu Li", "Jing Li", "Jun Yu", "Min Zhang"], "title": "Function-to-Style Guidance of LLMs for Code Translation", "comment": "This paper has been accepted by ICML 2025. Models and benchmarks can\n  be found at https://www.modelscope.cn/collections/F2STrans-42526ff95dd843", "summary": "Large language models (LLMs) have made significant strides in code\ntranslation tasks. However, ensuring both the correctness and readability of\ntranslated code remains a challenge, limiting their effective adoption in\nreal-world software development. In this work, we propose F2STrans, a\nfunction-to-style guiding paradigm designed to progressively improve the\nperformance of LLMs in code translation. Our approach comprises two key stages:\n(1) Functional learning, which optimizes translation correctness using\nhigh-quality source-target code pairs mined from online programming platforms,\nand (2) Style learning, which improves translation readability by incorporating\nboth positive and negative style examples. Additionally, we introduce a novel\ncode translation benchmark that includes up-to-date source code, extensive test\ncases, and manually annotated ground-truth translations, enabling comprehensive\nfunctional and stylistic evaluations. Experiments on both our new benchmark and\nexisting datasets demonstrate that our approach significantly improves code\ntranslation performance. Notably, our approach enables Qwen-1.5B to outperform\nprompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code\ntranslation scenarios.", "AI": {"tldr": "F2STrans\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u529f\u80fd\u5b66\u4e60\u548c\u98ce\u683c\u5b66\u4e60\u63d0\u5347LLM\u7684\u7ffb\u8bd1\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u3002", "method": "F2STrans\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u529f\u80fd\u5b66\u4e60\uff08\u4f18\u5316\u6b63\u786e\u6027\uff09\u548c\u98ce\u683c\u5b66\u4e60\uff08\u63d0\u5347\u53ef\u8bfb\u6027\uff09\uff0c\u5e76\u7ed3\u5408\u65b0\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cF2STrans\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f7f\u8f83\u5c0f\u6a21\u578b\uff08Qwen-1.5B\uff09\u572820\u79cd\u573a\u666f\u4e2d\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\uff08Qwen-32B\u548cGPT-4\uff09\u3002", "conclusion": "F2STrans\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u6b63\u786e\u6027\u548c\u53ef\u8bfb\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11362", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11362", "abs": "https://arxiv.org/abs/2507.11362", "authors": ["Chaima Boufaied", "Taher Ghaleb", "Zainab Masood"], "title": "Security Debt in Practice: Nuanced Insights from Practitioners", "comment": null, "summary": "With the increasing reliance on software and automation nowadays, tight\ndeadlines, limited resources, and prioritization of functionality over security\ncan lead to insecure coding practices. When not handled properly, these\nconstraints cause unaddressed security vulnerabilities to accumulate over time,\nforming Security Debts (SDs). Despite their critical importance, there is\nlimited empirical evidence on how software practitioners perceive, manage, and\ncommunicate SDs in real-world settings. In this paper, we present a qualitative\nempirical study based on semi-structured interviews with 22 software\npractitioners across various roles, organizations, and countries. We address\nfour research questions: i) we assess software practitioners' knowledge of SDs\nand awareness of associated security risks, ii) we investigate their behavior\ntowards SDs, iii) we explore common tools and strategies used to mitigate SDs,\nand iv) we analyze how security risks are communicated within teams and to\ndecision makers. We observe variations in how practitioners perceive and manage\nSDs, with some prioritizing delivery speed over security, while others\nconsistently maintain security as a priority. Our findings emphasize the need\nfor stronger integration of security practices across the Software Development\nLife Cycle (SDLC), more consistent use of mitigation strategies, better\nbalancing of deadlines, resources, and security-related tasks, with attention\nto the Confidentiality, Integrity, and Availability (CIA) triad.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9a\u6027\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u4ece\u4e1a\u8005\u5bf9\u5b89\u5168\u503a\u52a1\uff08SDs\uff09\u7684\u8ba4\u77e5\u3001\u7ba1\u7406\u548c\u6c9f\u901a\u65b9\u5f0f\uff0c\u5f3a\u8c03\u4e86\u5728\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u4e2d\u52a0\u5f3a\u5b89\u5168\u5b9e\u8df5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u548c\u81ea\u52a8\u5316\u7684\u666e\u53ca\uff0c\u65f6\u95f4\u3001\u8d44\u6e90\u9650\u5236\u4ee5\u53ca\u5bf9\u529f\u80fd\u7684\u4f18\u5148\u8003\u8651\u5bfc\u81f4\u4e0d\u5b89\u5168\u7f16\u7801\u5b9e\u8df5\uff0c\u5f62\u6210\u5b89\u5168\u503a\u52a1\uff08SDs\uff09\u3002\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\u4e86\u89e3\u4ece\u4e1a\u8005\u5982\u4f55\u5e94\u5bf9SDs\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c0822\u540d\u6765\u81ea\u4e0d\u540c\u89d2\u8272\u3001\u7ec4\u7ec7\u548c\u56fd\u5bb6\u7684\u8f6f\u4ef6\u4ece\u4e1a\u8005\uff0c\u7814\u7a76\u5176\u5bf9SDs\u7684\u8ba4\u77e5\u3001\u884c\u4e3a\u3001\u5de5\u5177\u7b56\u7565\u53ca\u98ce\u9669\u6c9f\u901a\u3002", "result": "\u53d1\u73b0\u4ece\u4e1a\u8005\u5bf9SDs\u7684\u8ba4\u77e5\u548c\u7ba1\u7406\u5b58\u5728\u5dee\u5f02\uff0c\u90e8\u5206\u4f18\u5148\u4ea4\u4ed8\u901f\u5ea6\u800c\u975e\u5b89\u5168\u3002\u9700\u52a0\u5f3a\u5b89\u5168\u5b9e\u8df5\u3001\u5e73\u8861\u8d44\u6e90\u4e0e\u5b89\u5168\u4efb\u52a1\u3002", "conclusion": "\u5f3a\u8c03\u5728SDLC\u4e2d\u6574\u5408\u5b89\u5168\u5b9e\u8df5\uff0c\u66f4\u4e00\u81f4\u5730\u4f7f\u7528\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u5173\u6ce8CIA\u4e09\u8981\u7d20\u3002"}}
{"id": "2507.10978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10978", "abs": "https://arxiv.org/abs/2507.10978", "authors": ["Ayush Gupta", "Siyuan Huang", "Rama Chellappa"], "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction", "comment": "Accepted at IJCB 2025", "summary": "Gait is becoming popular as a method of person re-identification because of\nits ability to identify people at a distance. However, most current works in\ngait recognition do not address the practical problem of occlusions. Among\nthose which do, some require paired tuples of occluded and holistic sequences,\nwhich are impractical to collect in the real world. Further, these approaches\nwork on occlusions but fail to retain performance on holistic inputs. To\naddress these challenges, we propose RG-Gait, a method for residual correction\nfor occluded gait recognition with holistic retention. We model the problem as\na residual learning task, conceptualizing the occluded gait signature as a\nresidual deviation from the holistic gait representation. Our proposed network\nadaptively integrates the learned residual, significantly improving performance\non occluded gait sequences without compromising the holistic recognition\naccuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR\ndatasets and show that learning the residual can be an effective technique to\ntackle occluded gait recognition with holistic retention.", "AI": {"tldr": "RG-Gait\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u89e3\u51b3\u6b65\u6001\u8bc6\u522b\u4e2d\u906e\u6321\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5b8c\u6574\u6b65\u6001\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u672a\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u6570\u636e\u6536\u96c6\u6216\u727a\u7272\u5b8c\u6574\u6b65\u6001\u6027\u80fd\u3002", "method": "\u5c06\u906e\u6321\u6b65\u6001\u5efa\u6a21\u4e3a\u5b8c\u6574\u6b65\u6001\u7684\u6b8b\u5dee\u504f\u5dee\uff0c\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u7f51\u7edc\u81ea\u9002\u5e94\u6574\u5408\u6b8b\u5dee\u3002", "result": "\u5728Gait3D\u3001GREW\u548cBRIAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u906e\u6321\u6b65\u6001\u8bc6\u522b\u6027\u80fd\u4e14\u4e0d\u5f71\u54cd\u5b8c\u6574\u6b65\u6001\u51c6\u786e\u6027\u3002", "conclusion": "\u6b8b\u5dee\u5b66\u4e60\u662f\u89e3\u51b3\u906e\u6321\u6b65\u6001\u8bc6\u522b\u5e76\u4fdd\u7559\u5b8c\u6574\u6b65\u6001\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.11086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11086", "abs": "https://arxiv.org/abs/2507.11086", "authors": ["Andres Azqueta-Gavald\u00f3n", "Joaquin Ramos Cosgrove"], "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8de8\u5883\u91d1\u878d\u6d3b\u52a8\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u8fdb\u5b9e\u4f53\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0cLLMs\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8de8\u5883\u91d1\u878d\u6d3b\u52a8\u589e\u52a0\uff0c\u9700\u8981\u51c6\u786e\u8bc6\u522b\u548c\u5206\u7c7b\u5916\u56fd\u5b9e\u4f53\u4ee5\u652f\u6301\u98ce\u9669\u7ba1\u7406\u3001\u5408\u89c4\u548c\u9632\u6b3a\u8bc8\u3002\u4f20\u7edf\u5339\u914d\u65b9\u6cd5\u56e0\u8bed\u8a00\u548c\u8bed\u4e49\u95ee\u9898\u6548\u679c\u6709\u9650\u3002", "method": "\u6bd4\u8f83\u4f20\u7edf\u65b9\u6cd5\uff08\u5982Jaccard\u3001\u4f59\u5f26\u3001Levenshtein\u8ddd\u79bb\uff09\u3001Hugging Face\u7684LLMs\u548c\u63a5\u53e3\u578bLLMs\uff08\u5982Microsoft Copilot\u3001Qwen 2.5\uff09\uff0c\u4f7f\u752865\u4e2a\u8461\u8404\u7259\u516c\u53f8\u6848\u4f8b\u6570\u636e\u96c6\u3002", "result": "\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u8d8592%\uff0c\u4f46\u5047\u9633\u6027\u7387\u9ad8\uff0820-40%\uff09\uff1b\u63a5\u53e3\u578bLLMs\u51c6\u786e\u7387\u8d8593%\uff0cF1\u5206\u6570\u8d8596%\uff0c\u5047\u9633\u6027\u7387\u66f4\u4f4e\uff0840-80%\uff09\u3002", "conclusion": "LLMs\u5728\u5b9e\u4f53\u5339\u914d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u8bed\u8a00\u548c\u8bed\u4e49\u590d\u6742\u6027\u65f6\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2507.11117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11117", "abs": "https://arxiv.org/abs/2507.11117", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "AI Agent Architecture for Decentralized Trading of Alternative Assets", "comment": "8 Pages, 1 figure", "summary": "Decentralized trading of real-world alternative assets (e.g., gold) requires\nbridging physical asset custody with blockchain systems while meeting strict\nrequirements for compliance, liquidity, and risk management. We present\nGoldMine OS, a research oriented architecture that employs multiple specialized\nAI agents to automate and secure the tokenization and exchange of physical gold\ninto a blockchain based stablecoin (\"OZ\"). Our approach combines on chain smart\ncontracts for critical risk controls with off chain AI agents for decision\nmaking, blending the transparency and reliability of blockchains with the\nflexibility of AI driven automation. We describe four cooperative agents\n(Compliance, Token Issuance, Market Making, and Risk Control) and a\ncoordinating core, and evaluate the system through simulation and a controlled\npilot deployment. In experiments the prototype delivers on demand token\nissuance in under 1.2 s, more than 100 times faster than manual workflows. The\nMarket Making agent maintains tight liquidity with spreads often below 0.5\npercent even under volatile conditions. Fault injection tests show resilience:\nan oracle price spoofing attack is detected and mitigated within 10 s, and a\nsimulated vault mis reporting halts issuance immediately with minimal user\nimpact. The architecture scales to 5000 transactions per second with 10000\nconcurrent users in benchmarks. These results indicate that an AI agent based\ndecentralized exchange for alternative assets can satisfy rigorous performance\nand safety requirements. We discuss broader implications for democratizing\naccess to traditionally illiquid assets and explain how our governance model --\nmulti signature agent updates and on chain community voting on risk parameters\n-- provides ongoing transparency, adaptability, and formal assurance of system\nintegrity.", "AI": {"tldr": "GoldMine OS\u662f\u4e00\u4e2a\u7814\u7a76\u5bfc\u5411\u7684\u67b6\u6784\uff0c\u5229\u7528\u591a\u4e2a\u4e13\u7528AI\u4ee3\u7406\u5b9e\u73b0\u7269\u7406\u9ec4\u91d1\u7684\u81ea\u52a8\u5316\u3001\u5b89\u5168\u4ee3\u5e01\u5316\u4e0e\u4ea4\u6613\uff0c\u7ed3\u5408\u533a\u5757\u94fe\u667a\u80fd\u5408\u7ea6\u548cAI\u51b3\u7b56\uff0c\u6ee1\u8db3\u5408\u89c4\u6027\u3001\u6d41\u52a8\u6027\u548c\u98ce\u9669\u7ba1\u7406\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u66ff\u4ee3\u8d44\u4ea7\uff08\u5982\u9ec4\u91d1\uff09\u5728\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u4e2d\u5982\u4f55\u6ee1\u8db3\u5408\u89c4\u6027\u3001\u6d41\u52a8\u6027\u548c\u98ce\u9669\u7ba1\u7406\u7684\u4e25\u683c\u9700\u6c42\u3002", "method": "\u91c7\u7528\u533a\u5757\u94fe\u667a\u80fd\u5408\u7ea6\u4e0e\u94fe\u4e0bAI\u4ee3\u7406\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u8bbe\u8ba1\u56db\u4e2a\u534f\u4f5c\u4ee3\u7406\uff08\u5408\u89c4\u3001\u4ee3\u5e01\u53d1\u884c\u3001\u505a\u5e02\u3001\u98ce\u9669\u63a7\u5236\uff09\u548c\u4e00\u4e2a\u534f\u8c03\u6838\u5fc3\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5b9e\u73b01.2\u79d2\u5185\u6309\u9700\u4ee3\u5e01\u53d1\u884c\uff0c\u505a\u5e02\u4ee3\u7406\u5728\u6ce2\u52a8\u6761\u4ef6\u4e0b\u4fdd\u63010.5%\u4ee5\u5185\u7684\u70b9\u5dee\uff0c\u7cfb\u7edf\u53ef\u6269\u5c55\u81f3\u6bcf\u79d25000\u7b14\u4ea4\u6613\u3002", "conclusion": "AI\u4ee3\u7406\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u6240\u53ef\u6ee1\u8db3\u9ad8\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u9700\u6c42\uff0c\u4e3a\u4f20\u7edf\u975e\u6d41\u52a8\u6027\u8d44\u4ea7\u63d0\u4f9b\u6c11\u4e3b\u5316\u8bbf\u95ee\u9014\u5f84\u3002"}}
{"id": "2507.10999", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10999", "abs": "https://arxiv.org/abs/2507.10999", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "The resurgence of convolutional neural networks (CNNs) in visual recognition\ntasks, exemplified by ConvNeXt, has demonstrated their capability to rival\ntransformer-based architectures through advanced training methodologies and\nViT-inspired design principles. However, both CNNs and transformers exhibit a\nsimplicity bias, favoring straightforward features over complex structural\nrepresentations. Furthermore, modern CNNs often integrate MLP-like blocks akin\nto those in transformers, but these blocks suffer from significant information\nredundancies, necessitating high expansion ratios to sustain competitive\nperformance. To address these limitations, we propose SpaRTAN, a lightweight\narchitectural design that enhances spatial and channel-wise information\nprocessing. SpaRTAN employs kernels with varying receptive fields, controlled\nby kernel size and dilation factor, to capture discriminative multi-order\nspatial features effectively. A wave-based channel aggregation module further\nmodulates and reinforces pixel interactions, mitigating channel-wise\nredundancies. Combining the two modules, the proposed network can efficiently\ngather and dynamically contextualize discriminative features. Experimental\nresults in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable\nparameter efficiency while maintaining competitive performance. In particular,\non the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M\nparameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver\nstrong performance through an efficient design. On the COCO benchmark, it\nachieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M\nparameters. The code is publicly available at\n[https://github.com/henry-pay/SpaRTAN].", "AI": {"tldr": "SpaRTAN\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u67b6\u6784\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u9636\u7a7a\u95f4\u7279\u5f81\u548c\u901a\u9053\u805a\u5408\u6a21\u5757\u63d0\u5347\u6027\u80fd\uff0c\u53c2\u6570\u6548\u7387\u9ad8\u3002", "motivation": "\u89e3\u51b3CNN\u548cTransformer\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u7b80\u5355\u6027\u504f\u5dee\u548c\u4fe1\u606f\u5197\u4f59\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53ef\u53d8\u611f\u53d7\u91ce\u7684\u6838\u548c\u6ce2\u57fa\u901a\u9053\u805a\u5408\u6a21\u5757\uff0c\u4f18\u5316\u7a7a\u95f4\u548c\u901a\u9053\u4fe1\u606f\u5904\u7406\u3002", "result": "\u5728ImageNet-1k\u4e0a\u8fbe\u523077.7%\u51c6\u786e\u7387\uff083.8M\u53c2\u6570\uff09\uff0cCOCO\u4e0a50.0% AP\uff0821.5M\u53c2\u6570\uff09\u3002", "conclusion": "SpaRTAN\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u53c2\u6570\u6548\u7387\u663e\u8457\u3002"}}
{"id": "2507.11097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11097", "abs": "https://arxiv.org/abs/2507.11097", "authors": ["Zichen Wen", "Jiashu Qu", "Dongrui Liu", "Zhiyuan Liu", "Ruixi Wu", "Yicun Yang", "Xiangqi Jin", "Haoyun Xu", "Xuyang Liu", "Weijia Li", "Chaochao Lu", "Jing Shao", "Conghui He", "Linfeng Zhang"], "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "comment": "21 pages, 9 figures, work in progress", "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.", "AI": {"tldr": "DIJA\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u5176\u53cc\u5411\u5efa\u6a21\u548c\u5e76\u884c\u89e3\u7801\u673a\u5236\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5bf9\u9f50\u673a\u5236\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1dLLMs\u5728\u4ee3\u7801\u751f\u6210\u548c\u6587\u672c\u586b\u5145\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b89\u5168\u6027\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bf9\u6297\u6027\u63d0\u793a\u7684\u9632\u62a4\u4e0d\u8db3\u3002", "method": "DIJA\u901a\u8fc7\u6784\u9020\u4ea4\u9519\u63a9\u7801-\u6587\u672c\u63d0\u793a\uff0c\u5229\u7528dLLMs\u7684\u53cc\u5411\u5efa\u6a21\u548c\u5e76\u884c\u89e3\u7801\u673a\u5236\uff0c\u7ed5\u8fc7\u6807\u51c6\u5bf9\u9f50\u673a\u5236\uff0c\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002", "result": "DIJA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8d8a\u72f1\u65b9\u6cd5\uff0c\u6700\u9ad8\u53ef\u8fbe100%\u7684\u5173\u952e\u8bcd\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u5927\u5e45\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86dLLMs\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u547c\u5401\u91cd\u65b0\u601d\u8003\u6b64\u7c7b\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u3002"}}
{"id": "2507.11127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11127", "abs": "https://arxiv.org/abs/2507.11127", "authors": ["Lennert De Smet", "Luc De Raedt"], "title": "Defining neurosymbolic AI", "comment": null, "summary": "Neurosymbolic AI focuses on integrating learning and reasoning, in\nparticular, on unifying logical and neural representations. Despite the\nexistence of an alphabet soup of neurosymbolic AI systems, the field is lacking\na generally accepted formal definition of what neurosymbolic models and\ninference really are. We introduce a formal definition for neurosymbolic AI\nthat makes abstraction of its key ingredients. More specifically, we define\nneurosymbolic inference as the computation of an integral over a product of a\nlogical and a belief function. We show that our neurosymbolic AI definition\nmakes abstraction of key representative neurosymbolic AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u7528\u4e8e\u7edf\u4e00\u795e\u7ecf\u7b26\u53f7AI\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u5c06\u5176\u63a8\u7406\u5b9a\u4e49\u4e3a\u903b\u8f91\u51fd\u6570\u4e0e\u7f6e\u4fe1\u51fd\u6570\u7684\u79ef\u5206\u8ba1\u7b97\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7AI\u9886\u57df\u7f3a\u4e4f\u516c\u8ba4\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u95f4\u7684\u6bd4\u8f83\u4e0e\u53d1\u5c55\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u62bd\u8c61\u51fa\u795e\u7ecf\u7b26\u53f7AI\u7684\u5173\u952e\u6210\u5206\uff0c\u5b9a\u4e49\u5176\u63a8\u7406\u4e3a\u903b\u8f91\u51fd\u6570\u4e0e\u7f6e\u4fe1\u51fd\u6570\u7684\u79ef\u5206\u3002", "result": "\u8be5\u5b9a\u4e49\u80fd\u591f\u62bd\u8c61\u51fa\u4ee3\u8868\u6027\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u7684\u6838\u5fc3\u7279\u5f81\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u6709\u52a9\u4e8e\u7edf\u4e00\u795e\u7ecf\u7b26\u53f7AI\u9886\u57df\uff0c\u4fc3\u8fdb\u7cfb\u7edf\u95f4\u7684\u6bd4\u8f83\u4e0e\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.11467", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11467", "abs": "https://arxiv.org/abs/2507.11467", "authors": ["Daniel Nichols", "Konstantinos Parasyris", "Harshitha Menon", "Brian R. Bartoldson", "Giorgis Georgakoudis", "Tal Ben-Nun", "Abhinav Bhatele"], "title": "Modeling Code: Is Text All You Need?", "comment": null, "summary": "Code LLMs have become extremely popular recently for modeling source code\nacross a variety of tasks, such as generation, translation, and summarization.\nHowever, transformer-based models are limited in their capabilities to reason\nthrough structured, analytical properties of code, such as control and data\nflow. Previous work has explored the modeling of these properties with\nstructured data and graph neural networks. However, these approaches lack the\ngenerative capabilities and scale of modern LLMs. In this work, we introduce a\nnovel approach to combine the strengths of modeling both code as text and more\nstructured forms.", "AI": {"tldr": "\u7ed3\u5408\u4ee3\u7801\u6587\u672c\u548c\u7ed3\u6784\u5316\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u5f25\u8865\u4e86\u73b0\u6709LLMs\u5728\u4ee3\u7801\u5206\u6790\u548c\u751f\u6210\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u4ee3\u7801LLMs\u5728\u5206\u6790\u4ee3\u7801\u7ed3\u6784\u5316\u5c5e\u6027\uff08\u5982\u63a7\u5236\u6d41\u548c\u6570\u636e\u6d41\uff09\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u800c\u4f20\u7edf\u7ed3\u6784\u5316\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\u548c\u89c4\u6a21\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ee3\u7801\u6587\u672c\u5efa\u6a21\u548c\u7ed3\u6784\u5316\u5efa\u6a21\u7684\u4f18\u52bf\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u65e8\u5728\u63d0\u5347\u4ee3\u7801\u5206\u6790\u548c\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u671b\u5f25\u8865\u73b0\u6709\u6280\u672f\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4ee3\u7801LLMs\u7684\u7efc\u5408\u80fd\u529b\u3002"}}
{"id": "2507.11003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11003", "abs": "https://arxiv.org/abs/2507.11003", "authors": ["Yuhu Bai", "Jiangning Zhang", "Yunkang Cao", "Guangyuan Lu", "Qingdong He", "Xiangtai Li", "Guanzhong Tian"], "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection", "comment": null, "summary": "With the advent of vision-language models (e.g., CLIP) in zero- and few-shot\nsettings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in\nrecent research, where the rare classes are essential and expected in many\napplications. This study introduces \\textbf{FiSeCLIP} for ZSAD with\ntraining-free \\textbf{CLIP}, combining the feature matching with the\ncross-modal alignment. Testing with the entire dataset is impractical, while\nbatch-based testing better aligns with real industrial needs, and images within\na batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes\nother images in the same batch as reference information for the current image.\nHowever, the lack of labels for these references can introduce ambiguity, we\napply text information to \\textbf{fi}lter out noisy features. In addition, we\nfurther explore CLIP's inherent potential to restore its local\n\\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection\ntasks to enable a more accurate filtering process. Our approach exhibits\nsuperior performance for both anomaly classification and segmentation on\nanomaly detection benchmarks, building a stronger baseline for the direction,\ne.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by\n+4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.", "AI": {"tldr": "FiSeCLIP\u5229\u7528CLIP\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u7279\u5f81\u5339\u914d\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u6279\u6b21\u6d4b\u8bd5\u4e2d\u5229\u7528\u5176\u4ed6\u56fe\u50cf\u4f5c\u4e3a\u53c2\u8003\uff0c\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u8fc7\u6ee4\u566a\u58f0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7f55\u89c1\u7c7b\u522b\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u9002\u5e94\u5de5\u4e1a\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u7279\u5f81\u5339\u914d\u4e0e\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5229\u7528\u6279\u6b21\u5185\u56fe\u50cf\u4f5c\u4e3a\u53c2\u8003\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u4fe1\u606f\u8fc7\u6ee4\u566a\u58f0\uff0c\u540c\u65f6\u6062\u590dCLIP\u7684\u5c40\u90e8\u8bed\u4e49\u76f8\u5173\u6027\u3002", "result": "\u5728MVTec-AD\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFiSeCLIP\u5728\u5f02\u5e38\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FiSeCLIP\u4e3a\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11112", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11112", "abs": "https://arxiv.org/abs/2507.11112", "authors": ["Sanhanat Sivapiromrat", "Caiqi Zhang", "Marco Basaldella", "Nigel Collier"], "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs", "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u6bd2\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u591a\u4e2a\u540e\u95e8\u89e6\u53d1\u5668\u53ef\u4ee5\u5171\u5b58\u4e14\u4e92\u4e0d\u5e72\u6270\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6743\u91cd\u5dee\u5f02\u5206\u6790\u7684\u9009\u62e9\u6027\u91cd\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLM\u4e2d\u6bd2\u653b\u51fb\u7684\u89e6\u53d1\u673a\u5236\u548c\u591a\u89e6\u53d1\u5668\u4ea4\u4e92\u7406\u89e3\u6709\u9650\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u6846\u67b6\u7814\u7a76LLM\u4e2d\u6bd2\uff0c\u5c55\u793a\u591a\u89e6\u53d1\u5668\u7684\u5171\u5b58\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u6743\u91cd\u5dee\u5f02\u5206\u6790\u63d0\u51fa\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u591a\u89e6\u53d1\u5668\u53ef\u5171\u5b58\u4e14\u9c81\u68d2\u6fc0\u6d3b\uff0c\u63ed\u793a\u4e86LLM\u66f4\u5e7f\u6cdb\u7684\u6f0f\u6d1e\u3002\u9632\u5fa1\u65b9\u6cd5\u80fd\u9ad8\u6548\u79fb\u9664\u89e6\u53d1\u5668\u884c\u4e3a\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86LLM\u7684\u591a\u89e6\u53d1\u5668\u4e2d\u6bd2\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2507.11135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11135", "abs": "https://arxiv.org/abs/2507.11135", "authors": ["Selma Saidi", "Omar Laimona", "Christoph Schmickler", "Dirk Ziegenbein"], "title": "Collaborative Trustworthiness for Good Decision Making in Autonomous Systems", "comment": null, "summary": "Autonomous systems are becoming an integral part of many application domains,\nlike in the mobility sector. However, ensuring their safe and correct behaviour\nin dynamic and complex environments remains a significant challenge, where\nsystems should autonomously make decisions e.g., about manoeuvring. We propose\nin this paper a general collaborative approach for increasing the level of\ntrustworthiness in the environment of operation and improve reliability and\ngood decision making in autonomous system. In the presence of conflicting\ninformation, aggregation becomes a major issue for trustworthy decision making\nbased on collaborative data sharing. Unlike classical approaches in the\nliterature that rely on consensus or majority as aggregation rule, we exploit\nthe fact that autonomous systems have different quality attributes like\nperception quality. We use this criteria to determine which autonomous systems\nare trustworthy and borrow concepts from social epistemology to define\naggregation and propagation rules, used for automated decision making. We use\nBinary Decision Diagrams (BDDs) as formal models for beliefs aggregation and\npropagation, and formulate reduction rules to reduce the size of the BDDs and\nallow efficient computation structures for collaborative automated reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u7684\u4fe1\u4efb\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u4e3b\u7cfb\u7edf\u7684\u51b3\u7b56\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u786e\u4fdd\u5b89\u5168\u548c\u6b63\u786e\u884c\u4e3a\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u51b2\u7a81\u4fe1\u606f\u65f6\uff0c\u5982\u4f55\u805a\u5408\u6570\u636e\u4ee5\u652f\u6301\u53ef\u4fe1\u51b3\u7b56\u662f\u5173\u952e\u95ee\u9898\u3002", "method": "\u5229\u7528\u81ea\u4e3b\u7cfb\u7edf\u7684\u4e0d\u540c\u8d28\u91cf\u5c5e\u6027\uff08\u5982\u611f\u77e5\u8d28\u91cf\uff09\u786e\u5b9a\u53ef\u4fe1\u5ea6\uff0c\u5e76\u501f\u9274\u793e\u4f1a\u8ba4\u8bc6\u8bba\u6982\u5ff5\u5b9a\u4e49\u805a\u5408\u548c\u4f20\u64ad\u89c4\u5219\uff0c\u4f7f\u7528BDD\u8fdb\u884c\u4fe1\u5ff5\u805a\u5408\u548c\u4f20\u64ad\u3002", "result": "\u901a\u8fc7BDD\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u548c\u7b80\u5316\u89c4\u5219\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u534f\u4f5c\u81ea\u52a8\u63a8\u7406\u8ba1\u7b97\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u534f\u4f5c\u548c\u4fe1\u4efb\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86\u81ea\u4e3b\u7cfb\u7edf\u7684\u51b3\u7b56\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.11015", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11015", "abs": "https://arxiv.org/abs/2507.11015", "authors": ["Zeyi Hou", "Zeqiang Wei", "Ruixin Yan", "Ning Lang", "Xiuzhuang Zhou"], "title": "Semantically Informed Salient Regions Guided Radiology Report Generation", "comment": null, "summary": "Recent advances in automated radiology report generation from chest X-rays\nusing deep learning algorithms have the potential to significantly reduce the\narduous workload of radiologists. However, due to the inherent massive data\nbias in radiology images, where abnormalities are typically subtle and sparsely\ndistributed, existing methods often produce fluent yet medically inaccurate\nreports, limiting their applicability in clinical practice. To address this\nissue effectively, we propose a Semantically Informed Salient Regions-guided\n(SISRNet) report generation method. Specifically, our approach explicitly\nidentifies salient regions with medically critical characteristics using\nfine-grained cross-modal semantics. Then, SISRNet systematically focuses on\nthese high-information regions during both image modeling and report\ngeneration, effectively capturing subtle abnormal findings, mitigating the\nnegative impact of data bias, and ultimately generating clinically accurate\nreports. Compared to its peers, SISRNet demonstrates superior performance on\nwidely used IU-Xray and MIMIC-CXR datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u663e\u8457\u533a\u57df\u7684\u653e\u5c04\u62a5\u544a\u751f\u6210\u65b9\u6cd5\uff08SISRNet\uff09\uff0c\u901a\u8fc7\u805a\u7126\u533b\u5b66\u5173\u952e\u533a\u57df\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u62a5\u544a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u504f\u5dee\u751f\u6210\u4e0d\u51c6\u786e\u7684\u62a5\u544a\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u5229\u7528\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u8bed\u4e49\u8bc6\u522b\u5173\u952e\u533a\u57df\uff0c\u5e76\u5728\u56fe\u50cf\u5efa\u6a21\u548c\u62a5\u544a\u751f\u6210\u4e2d\u7cfb\u7edf\u5173\u6ce8\u8fd9\u4e9b\u533a\u57df\u3002", "result": "\u5728IU-Xray\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SISRNet\u80fd\u6709\u6548\u6355\u6349\u7ec6\u5fae\u5f02\u5e38\uff0c\u51cf\u5c11\u6570\u636e\u504f\u5dee\u5f71\u54cd\uff0c\u751f\u6210\u4e34\u5e8a\u51c6\u786e\u7684\u62a5\u544a\u3002"}}
{"id": "2507.11114", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11114", "abs": "https://arxiv.org/abs/2507.11114", "authors": ["Seif Ahmed", "Mohamed T. Younes", "Abdelrahman Moustafa", "Abdelrahman Allam", "Hamza Moustafa"], "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models", "comment": null, "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u7cfb\u7edf\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\uff0c\u5728ImageCLEF 2025 EXAMS V\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u6559\u80b2\u573a\u666f\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u63a8\u7406\u9700\u6c42\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7OCR-VLM\u96c6\u6210\u548c\u7cbe\u786e\u63d0\u793a\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "method": "\u96c6\u6210Gemini 2.5 Flash\u3001Gemini 1.5 Pro\u548cGemini 2.5 Pro\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u63d0\u793a\u534f\u8c03\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u3002", "result": "\u5728\u5b98\u65b9\u6392\u884c\u699c\u4e0a\uff0c\u7cfb\u7edf\u4ee581.4%\u7684\u51c6\u786e\u7387\u5728\u591a\u8bed\u8a00\u8d5b\u9053\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u572813\u4e2a\u8bed\u8a00\u8d5b\u9053\u4e2d\u9886\u514811\u4e2a\u3002", "conclusion": "\u8f7b\u91cf\u7ea7OCR-VLM\u96c6\u6210\u7ed3\u5408\u7cbe\u786e\u63d0\u793a\u7b56\u7565\u548c\u591a\u8bed\u8a00\u589e\u5f3a\uff0c\u5728\u9ad8\u98ce\u9669\u591a\u8bed\u8a00\u6559\u80b2\u573a\u666f\u4e2d\u4f18\u4e8e\u7aef\u5230\u7aef\u6a21\u578b\u3002"}}
{"id": "2507.11150", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.11150", "abs": "https://arxiv.org/abs/2507.11150", "authors": ["Alessandro Bertagnon", "Marcello Dalpasso", "Michele Favalli", "Marco Gavanelli"], "title": "Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming", "comment": "Accepted for publication in the issues of Theory and Practice of\n  Logic Programming (TPLP) dedicated to ICLP 2025, 16 pages, 9 figures", "summary": "In the design of integrated circuits, one critical metric is the maximum\ndelay introduced by combinational modules within the circuit. This delay is\ncrucial because it represents the time required to perform a computation: in an\nArithmetic-Logic Unit it represents the maximum time taken by the circuit to\nperform an arithmetic operation. When such a circuit is part of a larger,\nsynchronous system, like a CPU, the maximum delay directly impacts the maximum\nclock frequency of the entire system. Typically, hardware designers use Static\nTiming Analysis to compute an upper bound of the maximum delay because it can\nbe determined in polynomial time. However, relying on this upper bound can lead\nto suboptimal processor speeds, thereby missing performance opportunities. In\nthis work, we tackle the challenging task of computing the actual maximum\ndelay, rather than an approximate value. Since the problem is computationally\nhard, we model it in Answer Set Programming (ASP), a logic language featuring\nextremely efficient solvers. We propose non-trivial encodings of the problem\ninto ASP. Experimental results show that ASP is a viable solution to address\ncomplex problems in hardware design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u8ba1\u7b97\u7ec4\u5408\u7535\u8def\u5b9e\u9645\u6700\u5927\u5ef6\u8fdf\u7684\u65b9\u6cd5\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u9759\u6001\u65f6\u5e8f\u5206\u6790\uff0c\u4ece\u800c\u63d0\u9ad8\u5904\u7406\u5668\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u65f6\u5e8f\u5206\u6790\u867d\u7136\u8ba1\u7b97\u901f\u5ea6\u5feb\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b\u5ef6\u8fdf\u7684\u4e0a\u754c\uff0c\u53ef\u80fd\u5bfc\u81f4\u5904\u7406\u5668\u6027\u80fd\u672a\u8fbe\u6700\u4f18\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cbe\u786e\u8ba1\u7b97\u5b9e\u9645\u6700\u5927\u5ef6\u8fdf\u6765\u4f18\u5316\u6027\u80fd\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\uff0c\u5e76\u63d0\u51fa\u975e\u5e73\u51e1\u7684\u7f16\u7801\u65b9\u6cd5\uff0c\u5229\u7528ASP\u7684\u9ad8\u6548\u6c42\u89e3\u5668\u89e3\u51b3\u8fd9\u4e00\u8ba1\u7b97\u96be\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cASP\u80fd\u591f\u6709\u6548\u89e3\u51b3\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "ASP\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u5ef6\u8fdf\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5904\u7406\u5668\u6027\u80fd\u3002"}}
{"id": "2507.11025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11025", "abs": "https://arxiv.org/abs/2507.11025", "authors": ["Sung Ho Kang", "Hyun-Cheol Park"], "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schr\u00f6dinger Bridge with Conditional Diffusion", "comment": null, "summary": "We present a novel framework for CBCT-to-MDCT translation, grounded in the\nSchrodinger Bridge (SB) formulation, which integrates GAN-derived priors with\nhuman-guided conditional diffusion. Unlike conventional GANs or diffusion\nmodels, our approach explicitly enforces boundary consistency between CBCT\ninputs and pseudo targets, ensuring both anatomical fidelity and perceptual\ncontrollability. Binary human feedback is incorporated via classifier-free\nguidance (CFG), effectively steering the generative process toward clinically\npreferred outcomes. Through iterative refinement and tournament-based\npreference selection, the model internalizes human preferences without relying\non a reward model. Subtraction image visualizations reveal that the proposed\nmethod selectively attenuates shade artifacts in key anatomical regions while\npreserving fine structural detail. Quantitative evaluations further demonstrate\nsuperior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical\ndatasets -- outperforming prior GAN- and fine-tuning-based feedback methods --\nwhile requiring only 10 sampling steps. These findings underscore the\neffectiveness and efficiency of our framework for real-time, preference-aligned\nmedical image translation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchrodinger Bridge\u7684CBCT-to-MDCT\u7ffb\u8bd1\u6846\u67b6\uff0c\u7ed3\u5408GAN\u5148\u9a8c\u548c\u4eba\u7c7b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u786e\u4fdd\u89e3\u5256\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfGAN\u6216\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u8fb9\u754c\u4e00\u81f4\u6027\u548c\u4e34\u5e8a\u504f\u597d\u5bf9\u9f50\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Schrodinger Bridge\u6846\u67b6\uff0c\u7ed3\u5408GAN\u5148\u9a8c\u548c\u4eba\u7c7b\u53cd\u9988\uff08\u901a\u8fc7CFG\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u9526\u6807\u8d5b\u9009\u62e9\u5185\u5316\u4eba\u7c7b\u504f\u597d\u3002", "result": "\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728RMSE\u3001SSIM\u3001LPIPS\u548cDice\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u970010\u6b65\u91c7\u6837\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u3001\u504f\u597d\u5bf9\u9f50\u7684\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u3002"}}
{"id": "2507.11128", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2507.11128", "abs": "https://arxiv.org/abs/2507.11128", "authors": ["Dimitri Staufer"], "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests", "comment": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal", "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5WikiMem\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u4e2a\u4eba\u4e8b\u5b9e\u5173\u8054\u7684\u8bb0\u5fc6\uff0c\u5e76\u652f\u6301\u52a8\u6001\u6784\u5efa\u9057\u5fd8\u96c6\u4ee5\u6ee1\u8db3GDPR\u7684\u201c\u88ab\u9057\u5fd8\u6743\u201d\u8981\u6c42\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3LLM\u53ef\u80fd\u8bb0\u5fc6\u5e76\u6cc4\u9732\u4e2a\u4eba\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728GDPR\u7684RTBF\u8981\u6c42\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u4e2a\u4f53\u7ea7\u6570\u636e\u5173\u8054\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165WikiMem\u6570\u636e\u96c6\uff08\u5305\u542b5000\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u6d4b\u8bd5\u7528\u4f8b\uff09\u548c\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u901a\u8fc7\u6821\u51c6\u8d1f\u5bf9\u6570\u4f3c\u7136\u5bf9\u771f\u5b9e\u503c\u4e0e\u53cd\u4e8b\u5b9e\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u8bc4\u4f30\u4e8615\u4e2aLLM\uff08\u53c2\u6570\u4ece410M\u523070B\uff09\uff0c\u53d1\u73b0\u8bb0\u5fc6\u4e0e\u4e3b\u9898\u7f51\u7edc\u5b58\u5728\u548c\u6a21\u578b\u89c4\u6a21\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc6\u522bLLM\u4e2d\u4e2a\u4f53\u7ea7\u8bb0\u5fc6\u6570\u636e\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u52a8\u6001\u6784\u5efa\u9057\u5fd8\u96c6\u4ee5\u6ee1\u8db3RTBF\u8bf7\u6c42\u3002"}}
{"id": "2507.11229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11229", "abs": "https://arxiv.org/abs/2507.11229", "authors": ["Jin Li", "Zezhong Ding", "Xike Xie"], "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion", "comment": null, "summary": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across\nvarious domains. Recent KG reasoning methods that integrate both global and\nlocal information have achieved promising results. However, existing methods\noften suffer from score over-smoothing, which blurs the distinction between\ncorrect and incorrect answers and hinders reasoning effectiveness. To address\nthis, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with\ndual-pathway global-local fusion. DuetGraph tackles over-smoothing by\nsegregating -- rather than stacking -- the processing of local (via message\npassing) and global (via attention) information into two distinct pathways,\npreventing mutual interference and preserving representational discrimination.\nIn addition, DuetGraph introduces a coarse-to-fine optimization, which\npartitions entities into high- and low-score subsets. This strategy narrows the\ncandidate space and sharpens the score gap between the two subsets, which\nalleviates over-smoothing and enhances inference quality. Extensive experiments\non various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)\nperformance, with up to an 8.7% improvement in reasoning quality and a\n1.8$\\times$ acceleration in training efficiency.", "AI": {"tldr": "DuetGraph\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u5168\u5c40-\u5c40\u90e8\u878d\u5408\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u5904\u7406\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u5206\u6570\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u65f6\u5bb9\u6613\u5bfc\u81f4\u5206\u6570\u8fc7\u5e73\u6ed1\uff0c\u6a21\u7cca\u6b63\u786e\u7b54\u6848\u4e0e\u9519\u8bef\u7b54\u6848\u7684\u533a\u5206\uff0c\u5f71\u54cd\u63a8\u7406\u6548\u679c\u3002", "method": "DuetGraph\u91c7\u7528\u53cc\u8def\u5f84\u673a\u5236\uff0c\u5206\u522b\u5904\u7406\u5c40\u90e8\u4fe1\u606f\uff08\u901a\u8fc7\u6d88\u606f\u4f20\u9012\uff09\u548c\u5168\u5c40\u4fe1\u606f\uff08\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u907f\u514d\u76f8\u4e92\u5e72\u6270\uff1b\u5e76\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5c06\u5b9e\u4f53\u5206\u4e3a\u9ad8\u3001\u4f4e\u5206\u4e24\u4e2a\u5b50\u96c6\uff0c\u7f29\u5c0f\u5019\u9009\u7a7a\u95f4\u5e76\u589e\u5f3a\u5206\u6570\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDuetGraph\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe8.7%\uff0c\u8bad\u7ec3\u6548\u7387\u52a0\u901f1.8\u500d\u3002", "conclusion": "DuetGraph\u901a\u8fc7\u53cc\u8def\u5f84\u878d\u5408\u548c\u7c97\u5230\u7ec6\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2507.11030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11030", "abs": "https://arxiv.org/abs/2507.11030", "authors": ["Sunghyun Park", "Jungsoo Lee", "Shubhankar Borse", "Munawar Hayat", "Sungha Choi", "Kyuwoong Hwang", "Fatih Porikli"], "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025; 15 pages", "summary": "While open-vocabulary semantic segmentation (OVSS) can segment an image into\nsemantic regions based on arbitrarily given text descriptions even for classes\nunseen during training, it fails to understand personal texts (e.g., `my mug\ncup') for segmenting regions of specific interest to users. This paper\naddresses challenges like recognizing `my mug cup' among `multiple mug cups'.\nTo overcome this challenge, we introduce a novel task termed\n\\textit{personalized open-vocabulary semantic segmentation} and propose a text\nprompt tuning-based plug-in method designed to recognize personal visual\nconcepts using a few pairs of images and masks, while maintaining the\nperformance of the original OVSS. Based on the observation that reducing false\npredictions is essential when applying text prompt tuning to this task, our\nproposed method employs `negative mask proposal' that captures visual concepts\nother than the personalized concept. We further improve the performance by\nenriching the representation of text prompts by injecting visual embeddings of\nthe personal concept into them. This approach enhances personalized OVSS\nwithout compromising the original OVSS performance. We demonstrate the\nsuperiority of our method on our newly established benchmarks for this task,\nincluding FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8c03\u4f18\u548c\u8d1f\u63a9\u7801\u63d0\u6848\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8bc6\u522b\u7528\u6237\u7279\u5b9a\u6587\u672c\uff08\u5982\u2018\u6211\u7684\u676f\u5b50\u2019\uff09\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff08OVSS\uff09\u65e0\u6cd5\u5904\u7406\u7528\u6237\u4e2a\u6027\u5316\u6587\u672c\uff08\u5982\u2018\u6211\u7684\u676f\u5b50\u2019\uff09\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5206\u5272\u7528\u6237\u611f\u5174\u8da3\u7684\u7279\u5b9a\u533a\u57df\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u8c03\u4f18\u7684\u63d2\u4ef6\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d1f\u63a9\u7801\u63d0\u6848\u51cf\u5c11\u9519\u8bef\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u5d4c\u5165\u589e\u5f3a\u6587\u672c\u63d0\u793a\u8868\u793a\u3002", "result": "\u5728\u65b0\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08FSS$^\\text{per}$, CUB$^\\text{per}$, ADE$^\\text{per}$\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u539f\u59cbOVSS\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u5206\u5272\u80fd\u529b\u3002"}}
{"id": "2507.11198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11198", "abs": "https://arxiv.org/abs/2507.11198", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "comment": "Manuscript submitted for review", "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6e29\u5ea6\u548c\u667a\u80fd\u4f53\u89d2\u8272\u5bf9\u5171\u8bc6\u8fbe\u6210\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5e76\u672a\u663e\u8457\u63d0\u5347\u7f16\u7801\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5176\u5728\u7f16\u7801\u548c\u6570\u636e\u6807\u6ce8\u4efb\u52a1\u4e2d\u662f\u5426\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\uff0c\u4f7f\u75286\u79cd\u5f00\u6e90LLM\u548c18\u79cd\u914d\u7f6e\uff0c\u5206\u6790\u4e8677,000\u591a\u4e2a\u7f16\u7801\u51b3\u7b56\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6e29\u5ea6\u548c\u89d2\u8272\u5bf9\u5171\u8bc6\u548c\u7f16\u7801\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u6e29\u5ea6\u663e\u8457\u5f71\u54cd\u5171\u8bc6\u8fbe\u6210\uff0c\u591a\u89d2\u8272\u5ef6\u8fdf\u5171\u8bc6\uff0c\u4f46\u672a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002\u4ec5\u7279\u5b9a\u6a21\u578b\u548c\u914d\u7f6e\u4e0bMAS\u8868\u73b0\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u3002", "conclusion": "\u6311\u6218\u4e86\u591a\u89d2\u8272MAS\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u7684\u89c2\u70b9\uff0c\u6307\u51fa\u5176\u5c40\u9650\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u5b9e\u9a8c\u4ee3\u7801\u3002"}}
{"id": "2507.11277", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11277", "abs": "https://arxiv.org/abs/2507.11277", "authors": ["Dany Moshkovich", "Sergey Zeltyn"], "title": "Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed within agentic\nsystems-collections of interacting, LLM-powered agents that execute complex,\nadaptive workflows using memory, tools, and dynamic planning. While enabling\npowerful new capabilities, these systems also introduce unique forms of\nuncertainty stemming from probabilistic reasoning, evolving memory states, and\nfluid execution paths. Traditional software observability and operations\npractices fall short in addressing these challenges.\n  This paper introduces AgentOps: a comprehensive framework for observing,\nanalyzing, optimizing, and automating operation of agentic AI systems. We\nidentify distinct needs across four key roles-developers, testers, site\nreliability engineers (SREs), and business users-each of whom engages with the\nsystem at different points in its lifecycle. We present the AgentOps Automation\nPipeline, a six-stage process encompassing behavior observation, metric\ncollection, issue detection, root cause analysis, optimized recommendations,\nand runtime automation. Throughout, we emphasize the critical role of\nautomation in managing uncertainty and enabling self-improving AI systems-not\nby eliminating uncertainty, but by taming it to ensure safe, adaptive, and\neffective operation.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AgentOps\u6846\u67b6\uff0c\u7528\u4e8e\u89c2\u5bdf\u3001\u5206\u6790\u548c\u4f18\u5316\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u5f3a\u8c03\u81ea\u52a8\u5316\u5728\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u6982\u7387\u63a8\u7406\u3001\u52a8\u6001\u6267\u884c\u8def\u5f84\uff09\u5e26\u6765\u65b0\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51faAgentOps\u6846\u67b6\uff0c\u5305\u542b\u516d\u9636\u6bb5\u81ea\u52a8\u5316\u6d41\u7a0b\uff1a\u884c\u4e3a\u89c2\u5bdf\u3001\u6307\u6807\u6536\u96c6\u3001\u95ee\u9898\u68c0\u6d4b\u3001\u6839\u56e0\u5206\u6790\u3001\u4f18\u5316\u5efa\u8bae\u548c\u8fd0\u884c\u65f6\u81ea\u52a8\u5316\u3002", "result": "\u6846\u67b6\u652f\u6301\u5f00\u53d1\u8005\u3001\u6d4b\u8bd5\u8005\u3001SRE\u548c\u4e1a\u52a1\u7528\u6237\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "AgentOps\u901a\u8fc7\u81ea\u52a8\u5316\u201c\u9a6f\u670d\u201d\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.11035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11035", "abs": "https://arxiv.org/abs/2507.11035", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception", "comment": "12 pages", "summary": "Transformer-based models exhibit strong global modeling capabilities in\nsingle-image dehazing, but their high computational cost limits real-time\napplicability. Existing methods predominantly rely on spatial-domain features\nto capture long-range dependencies, which are computationally expensive and\noften inadequate under complex haze conditions. While some approaches introduce\nfrequency-domain cues, the weak coupling between spatial and frequency branches\nlimits the overall performance. To overcome these limitations, we propose the\nDark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel\ndual-domain framework that performs physically guided degradation alignment\nacross spatial and frequency domains. At its core, the DGFDBlock comprises two\nkey modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a\npixel-level haze confidence map from dark channel priors to adaptively enhance\nhaze-relevant frequency components, thereby achieving global degradation-aware\nspectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which\nfuses multi-scale features through diverse convolutional kernels and hybrid\ngating mechanisms to recover fine structural details. Additionally, a Prior\nCorrection Guidance Branch (PCGB) incorporates a closed-loop feedback\nmechanism, enabling iterative refinement of the prior by intermediate dehazed\nfeatures and significantly improving haze localization accuracy, especially in\nchallenging outdoor scenes. Extensive experiments on four benchmark haze\ndatasets demonstrate that DGFDNet achieves state-of-the-art performance with\nsuperior robustness and real-time efficiency. Code is available at:\nhttps://github.com/Dilizlr/DGFDNet.", "AI": {"tldr": "DGFDNet\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u57df\u53bb\u96fe\u7f51\u7edc\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u9000\u5316\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5355\u5e45\u56fe\u50cf\u53bb\u96fe\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u57df\u7279\u5f81\u6216\u9891\u57df\u7ebf\u7d22\u4f46\u8026\u5408\u4e0d\u8db3\u3002", "method": "DGFDNet\u5305\u542bHAFM\u6a21\u5757\uff08\u81ea\u9002\u5e94\u589e\u5f3a\u96fe\u76f8\u5173\u9891\u57df\u6210\u5206\uff09\u548cMGAM\u6a21\u5757\uff08\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff09\uff0c\u5e76\u5f15\u5165PCGB\u5206\u652f\u8fed\u4ee3\u4f18\u5316\u5148\u9a8c\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDGFDNet\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u517c\u5177\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "DGFDNet\u901a\u8fc7\u53cc\u57df\u534f\u540c\u548c\u7269\u7406\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u96fe\u51b5\u4e0b\u7684\u53bb\u96fe\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2507.11216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11216", "abs": "https://arxiv.org/abs/2507.11216", "authors": ["Valle Ruiz-Fern\u00e1ndez", "Mario Mina", "J\u00falia Falc\u00e3o", "Luis Vasquez-Reina", "Anna Sall\u00e9s", "Aitor Gonzalez-Agirre", "Olatz Perez-de-Vi\u00f1aspre"], "title": "EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering", "comment": null, "summary": "Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u897f\u73ed\u7259\u8bed\u548c\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u7684\u793e\u4f1a\u504f\u89c1\u8bc4\u4f30\u6570\u636e\u96c6EsBBQ\u548cCaBBQ\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u897f\u73ed\u7259\u793e\u4f1a\u80cc\u666f\u4e0b\u7684\u504f\u89c1\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u975e\u82f1\u8bed\u8bed\u8a00\u548c\u7f8e\u56fd\u4ee5\u5916\u793e\u4f1a\u80cc\u666f\u7684\u793e\u4f1a\u504f\u89c1\u8bc4\u4f30\u8d44\u6e90\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u539f\u59cbBBQ\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u897f\u73ed\u7259\u8bed\u548c\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u7684\u5e73\u884c\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u9009\u9898QA\u8bbe\u7f6e\u8bc4\u4f3010\u7c7b\u793e\u4f1a\u504f\u89c1\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u6613\u9009\u9519\u7b54\u6848\uff0c\u4e14\u9ad8QA\u51c6\u786e\u7387\u5e38\u4e0e\u4f9d\u8d56\u793e\u4f1a\u504f\u89c1\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u8bed\u8a00\u548c\u793e\u4f1a\u80cc\u666f\u4e0b\u8bc4\u4f30\u548c\u51cf\u5c11\u6a21\u578b\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11288", "abs": "https://arxiv.org/abs/2507.11288", "authors": ["Th\u00e9o Fagnoni", "Mahsun Altin", "Chia En Chung", "Phillip Kingston", "Alan Tuning", "Dana O. Mohamed", "In\u00e8s Adnani"], "title": "Opus: A Prompt Intention Framework for Complex Workflow Generation", "comment": "39 pages, 24 figures", "summary": "This paper introduces the Opus Prompt Intention Framework, designed to\nimprove complex Workflow Generation with instruction-tuned Large Language\nModels (LLMs). We propose an intermediate Intention Capture layer between user\nqueries and Workflow Generation, implementing the Opus Workflow Intention\nFramework, which consists of extracting Workflow Signals from user queries,\ninterpreting them into structured Workflow Intention objects, and generating\nWorkflows based on these Intentions. Our results show that this layer enables\nLLMs to produce logical and meaningful outputs that scale reliably as query\ncomplexity increases. On a synthetic benchmark of 1,000 multi-intent\nquery-Workflow(s) pairs, applying the Opus Prompt Intention Framework to\nWorkflow Generation yields consistent improvements in semantic Workflow\nsimilarity metrics. In this paper, we introduce the Opus Prompt Intention\nFramework by applying the concepts of Workflow Signal and Workflow Intention to\nLLM-driven Workflow Generation. We present a reproducible, customizable\nLLM-based Intention Capture system to extract Workflow Signals and Workflow\nIntentions from user queries. Finally, we provide empirical evidence that the\nproposed system significantly improves Workflow Generation quality compared to\ndirect generation from user queries, particularly in cases of Mixed Intention\nElicitation.", "AI": {"tldr": "Opus Prompt Intention Framework\u901a\u8fc7\u5f15\u5165\u4e2d\u95f4\u610f\u56fe\u6355\u6349\u5c42\uff0c\u63d0\u5347\u57fa\u4e8eLLM\u7684\u590d\u6742\u5de5\u4f5c\u6d41\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u6839\u636e\u7528\u6237\u67e5\u8be2\u751f\u6210\u5de5\u4f5c\u6d41\u65f6\u903b\u8f91\u6027\u548c\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faOpus Workflow Intention Framework\uff0c\u5305\u62ec\u4ece\u67e5\u8be2\u4e2d\u63d0\u53d6\u4fe1\u53f7\u3001\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u610f\u56fe\u5bf9\u8c61\uff0c\u5e76\u57fa\u4e8e\u610f\u56fe\u751f\u6210\u5de5\u4f5c\u6d41\u3002", "result": "\u57281000\u5bf9\u591a\u610f\u56fe\u67e5\u8be2-\u5de5\u4f5c\u6d41\u6d4b\u8bd5\u4e2d\uff0c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6d41\u751f\u6210\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u6df7\u5408\u610f\u56fe\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.11037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11037", "abs": "https://arxiv.org/abs/2507.11037", "authors": ["Jie-Wen Li", "Zi-Han Ye", "Qingyuan Zhou", "Jiayi Song", "Ying He", "Ben Fei", "Wen-Ming Chen"], "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion", "comment": "15 pages, 10 figures, 2 tables", "summary": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D.", "AI": {"tldr": "FootGait3D\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8db3\u8e1d\u533a\u57df\u7684\u9ad8\u5206\u8fa8\u7387\u70b9\u4e91\u6570\u636e\u96c6\uff0c\u7528\u4e8e3D\u70b9\u4e91\u8865\u5168\u4efb\u52a1\uff0c\u652f\u6301\u751f\u7269\u529b\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u8db3\u8e1d\u5728\u6b65\u6001\u4e2d\u7684\u8fd0\u52a8\u5206\u6790\u5bf9\u751f\u7269\u529b\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5173\u6ce8\u5168\u8eab\u6216\u4e0b\u80a2\u8fd0\u52a8\uff0c\u7f3a\u4e4f\u8db3\u8e1d\u533a\u57df\u7684\u8be6\u7ec6\u6570\u636e\u3002", "method": "FootGait3D\u5305\u542b8,403\u5e27\u70b9\u4e91\u6570\u636e\uff0c\u6765\u81ea46\u540d\u53d7\u8bd5\u8005\uff0c\u4f7f\u7528\u4e94\u6444\u50cf\u5934\u6df1\u5ea6\u4f20\u611f\u7cfb\u7edf\u91c7\u96c6\uff0c\u63d0\u4f9b\u5b8c\u6574\u548c\u90e8\u5206\u906e\u6321\u7684\u70b9\u4e91\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u70b9\u4e91\u8865\u5168\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u4e3a\u8db3\u8e1d\u51e0\u4f55\u6062\u590d\u63d0\u4f9b\u57fa\u51c6\u3002", "conclusion": "FootGait3D\u6709\u671b\u63a8\u52a8\u751f\u7269\u529b\u5b66\u548c\u591a\u6bb5\u8db3\u5efa\u6a21\u7814\u7a76\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u6b65\u6001\u5206\u6790\u3001\u5047\u80a2\u8bbe\u8ba1\u548c\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2507.11323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11323", "abs": "https://arxiv.org/abs/2507.11323", "authors": ["Xiang Yin", "Nico Potyka", "Antonio Rago", "Timotheus Kampik", "Francesca Toni"], "title": "Contestability in Quantitative Argumentation", "comment": null, "summary": "Contestable AI requires that AI-driven decisions align with human\npreferences. While various forms of argumentation have been shown to support\ncontestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks\n(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs\ncan be deployed for this purpose. Specifically, we introduce the contestability\nproblem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)\nto achieve a desired strength for a specific argument of interest (i.e., a\ntopic argument). To address this problem, we propose gradient-based relation\nattribution explanations (G-RAEs), which quantify the sensitivity of the topic\nargument's strength to changes in individual edge weights, thus providing\ninterpretable guidance for weight adjustments towards contestability. Building\non G-RAEs, we develop an iterative algorithm that progressively adjusts the\nedge weights to attain the desired strength. We evaluate our approach\nexperimentally on synthetic EW-QBAFs that simulate the structural\ncharacteristics of personalised recommender systems and multi-layer\nperceptrons, and demonstrate that it can solve the problem effectively.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u8fb9\u52a0\u6743\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08EW-QBAFs\uff09\u5b9e\u73b0\u53ef\u4e89\u8baeAI\u51b3\u7b56\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u5173\u7cfb\u5f52\u56e0\u89e3\u91ca\uff08G-RAEs\uff09\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u4ee5\u8c03\u6574\u8fb9\u6743\u91cd\u8fbe\u5230\u76ee\u6807\u8bba\u8bc1\u5f3a\u5ea6\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4f7fAI\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\uff0c\u652f\u6301\u53ef\u4e89\u8bae\u6027\uff0c\u4f46EW-QBAFs\u5728\u6b64\u9886\u57df\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faG-RAEs\u91cf\u5316\u8fb9\u6743\u91cd\u53d8\u5316\u5bf9\u76ee\u6807\u8bba\u8bc1\u5f3a\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u8fed\u4ee3\u7b97\u6cd5\u8c03\u6574\u6743\u91cd\u3002", "result": "\u5728\u6a21\u62df\u63a8\u8350\u7cfb\u7edf\u548c\u591a\u5c42\u611f\u77e5\u5668\u7684\u5408\u6210EW-QBAFs\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "G-RAEs\u548c\u8fed\u4ee3\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3EW-QBAFs\u4e2d\u7684\u53ef\u4e89\u8bae\u6027\u95ee\u9898\u3002"}}
{"id": "2507.11040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11040", "abs": "https://arxiv.org/abs/2507.11040", "authors": ["Nicolas Drapier", "Aladine Chetouani", "Aur\u00e9lien Chateigner"], "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery", "comment": "11 pages, 9 figures", "summary": "We present GLOD, a transformer-first architecture for object detection in\nhigh-resolution satellite imagery. GLOD replaces CNN backbones with a Swin\nTransformer for end-to-end feature extraction, combined with novel UpConvMixer\nblocks for robust upsampling and Fusion Blocks for multi-scale feature\nintegration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods\nby 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a\nmulti-path head design capturing objects across scales. The architecture is\noptimized for satellite imagery challenges, leveraging spatial priors while\nmaintaining computational efficiency.", "AI": {"tldr": "GLOD\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7Swin Transformer\u548c\u65b0\u578b\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u548c\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5229\u7528Transformer\u67b6\u6784\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528Swin Transformer\u66ff\u4ee3CNN\u4e3b\u5e72\uff0c\u7ed3\u5408UpConvMixer\u5757\u548cFusion Blocks\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u3002", "result": "\u5728xView\u6570\u636e\u96c6\u4e0a\u8fbe\u523032.95%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534711.46%\u3002", "conclusion": "GLOD\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.11230", "categories": ["cs.CL", "68T50"], "pdf": "https://arxiv.org/pdf/2507.11230", "abs": "https://arxiv.org/abs/2507.11230", "authors": ["Lyzander Marciano Andrylie", "Inaya Rahmanisa", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages", "comment": null, "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u65b9\u6cd5SAE-LAPE\uff0c\u7528\u4e8e\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u8bed\u8a00\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u53d1\u73b0\u8fd9\u4e9b\u7279\u5f81\u4e3b\u8981\u5206\u5e03\u5728\u6a21\u578b\u7684\u4e2d\u95f4\u5230\u6700\u7ec8\u5c42\uff0c\u5bf9\u591a\u8bed\u8a00\u6027\u80fd\u548c\u8f93\u51fa\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8bed\u8a00\u7279\u5b9a\u7279\u5f81\u7684\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u8de8\u8bed\u8a00\u8868\u793a\u4e2d\u5206\u79bb\u8bed\u8a00\u7279\u5b9a\u5355\u5143\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5b66\u4e60\u5355\u4e49\u7279\u5f81\uff0c\u5e76\u5f15\u5165SAE-LAPE\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7279\u5f81\u6fc0\u6d3b\u6982\u7387\u8bc6\u522b\u8bed\u8a00\u7279\u5b9a\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u8bed\u8a00\u7279\u5b9a\u7279\u5f81\u4e3b\u8981\u5206\u5e03\u5728\u6a21\u578b\u7684\u4e2d\u95f4\u5230\u6700\u7ec8\u5c42\uff0c\u8fd9\u4e9b\u7279\u5f81\u53ef\u89e3\u91ca\u4e14\u5f71\u54cd\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\u548c\u8f93\u51fa\uff0c\u540c\u65f6\u53ef\u7528\u4e8e\u8bed\u8a00\u8bc6\u522b\u3002", "conclusion": "SAE-LAPE\u65b9\u6cd5\u6709\u6548\u8bc6\u522b\u4e86\u8bed\u8a00\u7279\u5b9a\u7279\u5f81\uff0c\u4e3a\u7406\u89e3LLM\u7684\u591a\u8bed\u8a00\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11334", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11334", "abs": "https://arxiv.org/abs/2507.11334", "authors": ["Yuehao Huang", "Liang Liu", "Shuangming Lei", "Yukai Ma", "Hao Su", "Jianbiao Mei", "Pengxiang Zhao", "Yaqing Gu", "Yong Liu", "Jiajun Lv"], "title": "CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking", "comment": "Accepted by ACM MM 2025", "summary": "Mobile robots are increasingly required to navigate and interact within\nunknown and unstructured environments to meet human demands. Demand-driven\nnavigation (DDN) enables robots to identify and locate objects based on\nimplicit human intent, even when object locations are unknown. However,\ntraditional data-driven DDN methods rely on pre-collected data for model\ntraining and decision-making, limiting their generalization capability in\nunseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that\nemulates the human cognitive and learning mechanisms by integrating fast and\nslow thinking systems and selectively identifying key objects essential to\nfulfilling user demands. CogDDN identifies appropriate target objects by\nsemantically aligning detected objects with the given instructions.\nFurthermore, it incorporates a dual-process decision-making module, comprising\na Heuristic Process for rapid, efficient decisions and an Analytic Process that\nanalyzes past errors, accumulates them in a knowledge base, and continuously\nimproves performance. Chain of Thought (CoT) reasoning strengthens the\ndecision-making process. Extensive closed-loop evaluations on the AI2Thor\nsimulator with the ProcThor dataset show that CogDDN outperforms single-view\ncamera-only methods by 15%, demonstrating significant improvements in\nnavigation accuracy and adaptability. The project page is available at\nhttps://yuehaohuang.github.io/CogDDN/.", "AI": {"tldr": "CogDDN\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u548c\u5b66\u4e60\u673a\u5236\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u7684\u9700\u6c42\u5bfc\u822a\uff08DDN\uff09\u65b9\u6cd5\u4f9d\u8d56\u9884\u6536\u96c6\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u9002\u5e94\u672a\u77e5\u573a\u666f\u3002", "method": "CogDDN\u6574\u5408\u5feb\u901f\u548c\u6162\u901f\u601d\u7ef4\u7cfb\u7edf\uff0c\u8bed\u4e49\u5bf9\u9f50\u68c0\u6d4b\u5bf9\u8c61\u4e0e\u6307\u4ee4\uff0c\u5e76\u91c7\u7528\u53cc\u8fc7\u7a0b\u51b3\u7b56\u6a21\u5757\uff08\u542f\u53d1\u5f0f\u548c\u5206\u6790\u5f0f\uff09\u53ca\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "result": "\u5728AI2Thor\u6a21\u62df\u5668\u548cProcThor\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCogDDN\u6bd4\u5355\u89c6\u89d2\u76f8\u673a\u65b9\u6cd5\u6027\u80fd\u63d0\u534715%\u3002", "conclusion": "CogDDN\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u77e5\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.11055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11055", "abs": "https://arxiv.org/abs/2507.11055", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Jinman Kim"], "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation", "comment": "Accepted to ICCV 2025", "summary": "Medical language-guided segmentation, integrating textual clinical reports as\nauxiliary guidance to enhance image segmentation, has demonstrated significant\nimprovements over unimodal approaches. However, its inherent reliance on paired\nimage-text input, which we refer to as ``textual reliance\", presents two\nfundamental limitations: 1) many medical segmentation datasets lack paired\nreports, leaving a substantial portion of image-only data underutilized for\ntraining; and 2) inference is limited to retrospective analysis of cases with\npaired reports, limiting its applicability in most clinical scenarios where\nsegmentation typically precedes reporting. To address these limitations, we\npropose ProLearn, the first Prototype-driven Learning framework for\nlanguage-guided segmentation that fundamentally alleviates textual reliance. At\nits core, in ProLearn, we introduce a novel Prototype-driven Semantic\nApproximation (PSA) module to enable approximation of semantic guidance from\ntextual input. PSA initializes a discrete and compact prototype space by\ndistilling segmentation-relevant semantics from textual reports. Once\ninitialized, it supports a query-and-respond mechanism which approximates\nsemantic guidance for images without textual input, thereby alleviating textual\nreliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG\ndemonstrate that ProLearn outperforms state-of-the-art language-guided methods\nwhen limited text is available.", "AI": {"tldr": "ProLearn\u6846\u67b6\u901a\u8fc7\u539f\u578b\u9a71\u52a8\u7684\u8bed\u4e49\u8fd1\u4f3c\u6a21\u5757\uff08PSA\uff09\u51cf\u5c11\u5bf9\u6587\u672c\u8f93\u5165\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u6210\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u8f93\u5165\uff0c\u9650\u5236\u4e86\u6570\u636e\u5229\u7528\u548c\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u5f15\u5165PSA\u6a21\u5757\uff0c\u901a\u8fc7\u539f\u578b\u7a7a\u95f4\u548c\u67e5\u8be2-\u54cd\u5e94\u673a\u5236\u8fd1\u4f3c\u8bed\u4e49\u5f15\u5bfc\uff0c\u51cf\u5c11\u5bf9\u6587\u672c\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cProLearn\u5728\u6587\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProLearn\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11273", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11273", "abs": "https://arxiv.org/abs/2507.11273", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Guoming Liu", "Baoyuan Qi", "Hai Zhao"], "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding", "comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.", "AI": {"tldr": "KV-Latent\u901a\u8fc7\u964d\u91c7\u6837Key-Value\u5411\u91cf\u7ef4\u5ea6\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u51cf\u5c11KV Cache\u5360\u7528\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "Decoder\u67b6\u6784\u7684Key-Value (KV)\u7f13\u5b58\u9010\u6e10\u589e\u52a0\u6210\u4e3a\u63a8\u7406\u6548\u7387\u74f6\u9888\uff0c\u5f71\u54cd\u5185\u5b58\u6d88\u8017\u548c\u6570\u636e\u4f20\u8f93\u5e26\u5bbd\u3002", "method": "\u63d0\u51faKV-Latent\u8303\u5f0f\uff0c\u964d\u91c7\u6837KV\u5411\u91cf\u7ef4\u5ea6\u81f3\u6f5c\u5728\u7a7a\u95f4\uff0c\u6539\u8fdbRotary Positional Embedding\u7684\u9891\u7387\u91c7\u6837\u673a\u5236\u4ee5\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aKV-Latent\u5728\u51cf\u5c11KV Cache\u5360\u7528\u548c\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4e14\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u5c0f\u3002", "conclusion": "KV-Latent\u4e3a\u6784\u5efa\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5728KV Cache\u8282\u7701\u548cLLM\u6548\u7387\u65b9\u9762\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2507.11352", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2507.11352", "abs": "https://arxiv.org/abs/2507.11352", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Christian Ellis", "Alvaro Velasquez", "Zhangyang Wang", "Ufuk Topcu"], "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces", "comment": null, "summary": "Logistics operators, from battlefield coordinators rerouting airlifts ahead\nof a storm to warehouse managers juggling late trucks, often face life-critical\ndecisions that demand both domain expertise and rapid and continuous\nreplanning. While popular methods like integer programming yield logistics\nplans that satisfy user-defined logical constraints, they are slow and assume\nan idealized mathematical model of the environment that does not account for\nuncertainty. On the other hand, large language models (LLMs) can handle\nuncertainty and promise to accelerate replanning while lowering the barrier to\nentry by translating free-form utterances into executable plans, yet they\nremain prone to misinterpretations and hallucinations that jeopardize safety\nand cost. We introduce a neurosymbolic framework that pairs the accessibility\nof natural-language dialogue with verifiable guarantees on goal interpretation.\nIt converts user requests into structured planning specifications, quantifies\nits own uncertainty at the field and token level, and invokes an interactive\nclarification loop whenever confidence falls below an adaptive threshold. A\nlightweight model, fine-tuned on just 100 uncertainty-filtered examples,\nsurpasses the zero-shot performance of GPT-4.1 while cutting inference latency\nby nearly 50%. These preliminary results highlight a practical path toward\ncertifiable, real-time, and user-aligned decision-making for complex logistics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u4e0e\u53ef\u9a8c\u8bc1\u4fdd\u8bc1\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u7269\u6d41\u51b3\u7b56\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7269\u6d41\u51b3\u7b56\u4e2d\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6574\u6570\u89c4\u5212\uff09\u901f\u5ea6\u6162\u4e14\u65e0\u6cd5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6613\u8bef\u89e3\u548c\u5e7b\u89c9\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u7528\u6237\u8bf7\u6c42\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u89c4\u5212\u89c4\u8303\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u89e6\u53d1\u4ea4\u4e92\u5f0f\u6f84\u6e05\u5faa\u73af\u3002", "result": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728100\u4e2a\u4e0d\u786e\u5b9a\u6027\u8fc7\u6ee4\u793a\u4f8b\u4e0a\u5fae\u8c03\u540e\uff0c\u6027\u80fd\u8d85\u8d8aGPT-4.1\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u8fd150%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u7269\u6d41\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u3001\u5b9e\u65f6\u4e14\u7528\u6237\u5bf9\u9f50\u7684\u51b3\u7b56\u8def\u5f84\u3002"}}
{"id": "2507.11061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing.", "AI": {"tldr": "RoMaP\u662f\u4e00\u4e2a\u65b0\u7684\u5c40\u90e83D\u9ad8\u65af\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc73D-GALP\u751f\u6210\u51c6\u786e\u76843D\u63a9\u7801\uff0c\u5e76\u7ed3\u5408\u6b63\u5219\u5316SDS\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u5927\u5e45\u5ea6\u7684\u5c40\u90e8\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d3D\u795e\u7ecf\u8868\u793a\u548c\u5b9e\u4f8b\u7ea7\u7f16\u8f91\u6a21\u578b\u5728\u5b9e\u73b0\u7cbe\u786e\u5c40\u90e83D\u7f16\u8f91\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u56e0\u591a\u89c6\u89d22D\u5206\u5272\u4e0d\u4e00\u81f4\u548cSDS\u635f\u5931\u7684\u6a21\u7cca\u6027\u800c\u53d7\u9650\u3002", "method": "RoMaP\u63d0\u51fa3D-GALP\u6a21\u5757\u751f\u6210\u9c81\u68d2\u76843D\u63a9\u7801\uff0c\u5e76\u7ed3\u5408\u6b63\u5219\u5316SDS\u635f\u5931\uff08\u5305\u62ecL1\u951a\u5b9a\u635f\u5931\u548c\u989d\u5916\u6b63\u5219\u5316\u5668\uff09\u5b9e\u73b0\u7cbe\u786e\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoMaP\u5728\u91cd\u5efa\u548c\u751f\u6210\u7684\u9ad8\u65af\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5c40\u90e83D\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "RoMaP\u4e3a3D\u9ad8\u65af\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2507.11275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11275", "abs": "https://arxiv.org/abs/2507.11275", "authors": ["Jiaxuan Xie", "Chengwu Liu", "Ye Yuan", "Siqi Li", "Zhiping Xiao", "Ming Zhang"], "title": "FMC: Formalization of Natural Language Mathematical Competition Problems", "comment": "Accepted in ICML 2025 AI4MATH Workshop", "summary": "Efficient and accurate autoformalization methods, which leverage large-scale\ndatasets of extensive natural language mathematical problems to construct\nformal language datasets, are key to advancing formal mathematical reasoning.\nIn this paper, we propose an autoformalization pipeline based on large language\nmodels with error feedback, achieving a fully automatic and training-free\nformalization approach. Using this pipeline, we curate an Olympiad-level\ndataset aligning natural language problems with Lean formalizations. The\ndataset comprises $3,922$ mathematical problems in natural language and $9,787$\nin Lean, of which $64.46\\%$ were assessed as at least above-average quality,\nmaking it suitable as a benchmark for automated theorem provers. Additionally,\nwe investigate the formalization and reasoning capabilities of various LLMs and\nempirically demonstrate that few-shot learning, error feedback, and increasing\nsampling numbers enhance the autoformalization process. Experiments of three\nautomated theorem provers on the \\dataset\\ dataset also highlight its\nchallenging nature and its value as a benchmark for formal reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9519\u8bef\u53cd\u9988\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u5f62\u5f0f\u5316\u6d41\u7a0b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u3002", "motivation": "\u63a8\u52a8\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u7684\u53d1\u5c55\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u6784\u5efa\u5f62\u5f0f\u8bed\u8a00\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u9519\u8bef\u53cd\u9988\u673a\u5236\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5f62\u5f0f\u5316\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5965\u6797\u5339\u514b\u7ea7\u522b\u7684\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b3,922\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u548c9,787\u4e2aLean\u5f62\u5f0f\u5316\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c64.46%\u88ab\u8bc4\u4e3a\u9ad8\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u3001\u9519\u8bef\u53cd\u9988\u548c\u589e\u52a0\u91c7\u6837\u6570\u91cf\u80fd\u63d0\u5347\u5f62\u5f0f\u5316\u6548\u679c\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u9002\u5408\u4f5c\u4e3a\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u57fa\u51c6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6311\u6218\u6027\u548c\u5728\u5f62\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2507.11075", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2507.11075", "abs": "https://arxiv.org/abs/2507.11075", "authors": ["Chang Peng", "Yifei Zhou", "Huifeng Xi", "Shiqing Huang", "Chuangye Chen", "Jianming Yang", "Bao Yang", "Zhenyu Jiang"], "title": "Joint angle model based learning to refine kinematic human pose estimation", "comment": null, "summary": "Marker-free human pose estimation (HPE) has found increasing applications in\nvarious fields. Current HPE suffers from occasional errors in keypoint\nrecognition and random fluctuation in keypoint trajectories when analyzing\nkinematic human poses. The performance of existing deep learning-based models\nfor HPE refinement is considerably limited by inaccurate training datasets in\nwhich the keypoints are manually annotated. This paper proposed a novel method\nto overcome the difficulty through joint angle-based modeling. The key\ntechniques include: (i) A joint angle-based model of human pose, which is\nrobust to describe kinematic human poses; (ii) Approximating temporal variation\nof joint angles through high order Fourier series to get reliable \"ground\ntruth\"; (iii) A bidirectional recurrent network is designed as a\npost-processing module to refine the estimation of well-established HRNet.\nTrained with the high-quality dataset constructed using our method, the network\ndemonstrates outstanding performance to correct wrongly recognized joints and\nsmooth their spatiotemporal trajectories. Tests show that joint angle-based\nrefinement (JAR) outperforms the state-of-the-art HPE refinement network in\nchallenging cases like figure skating and breaking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u8282\u89d2\u5ea6\u7684\u65e0\u6807\u8bb0\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08HPE\uff09\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u7ea7\u6570\u548c\u9ad8\u9636\u53cc\u5411\u5faa\u73af\u7f51\u7edc\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709HPE\u65b9\u6cd5\u5728\u5173\u952e\u70b9\u8bc6\u522b\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u4e0a\u5b58\u5728\u8bef\u5dee\uff0c\u4e14\u53d7\u9650\u4e8e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4e0d\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5173\u8282\u89d2\u5ea6\u5efa\u6a21\uff0c\u7ed3\u5408\u9ad8\u9636\u5085\u91cc\u53f6\u7ea7\u6570\u8fd1\u4f3c\u548c\u53cc\u5411\u5faa\u73af\u7f51\u7edc\u4f18\u5316HRNet\u7684\u8f93\u51fa\u3002", "result": "\u5728\u82b1\u6837\u6ed1\u51b0\u548c\u8857\u821e\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0cJAR\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709HPE\u4f18\u5316\u7f51\u7edc\u3002", "conclusion": "\u57fa\u4e8e\u5173\u8282\u89d2\u5ea6\u7684\u5efa\u6a21\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86HPE\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.11292", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11292", "abs": "https://arxiv.org/abs/2507.11292", "authors": ["Zewen Bai", "Liang Yang", "Shengdi Yin", "Yuanyuan Sun", "Hongfei Lin"], "title": "Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks", "comment": null, "summary": "The proliferation of hate speech has inflicted significant societal harm,\nwith its intensity and directionality closely tied to specific targets and\narguments. In recent years, numerous machine learning-based methods have been\ndeveloped to detect hateful comments on online platforms automatically.\nHowever, research on Chinese hate speech detection lags behind, and\ninterpretability studies face two major challenges: first, the scarcity of\nspan-level fine-grained annotated datasets limits models' deep semantic\nunderstanding of hate speech; second, insufficient research on identifying and\ninterpreting coded hate speech restricts model explainability in complex\nreal-world scenarios. To address these, we make the following contributions:\n(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE\nToxiCN), the first span-level Chinese hate speech dataset, and evaluate the\nhate semantic understanding of existing models using it. (2) We conduct the\nfirst comprehensive study on Chinese coded hate terms, LLMs' ability to\ninterpret hate semantics. (3) We propose a method to integrate an annotated\nlexicon into models, significantly enhancing hate speech detection performance.\nOur work provides valuable resources and insights to advance the\ninterpretability of Chinese hate speech detection research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e2d\u6587\u7ec6\u7c92\u5ea6\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\uff08STATE ToxiCN\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u7f16\u7801\u4ec7\u6068\u672f\u8bed\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u6807\u6ce8\u8bcd\u5178\u7684\u65b9\u6cd5\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7814\u7a76\u6ede\u540e\uff0c\u4e14\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u96c6\u548c\u7f16\u7801\u4ec7\u6068\u672f\u8bed\u7684\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u89e3\u91ca\u80fd\u529b\u3002", "method": "1. \u5f15\u5165\u9996\u4e2a\u4e2d\u6587\u7ec6\u7c92\u5ea6\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6STATE ToxiCN\uff1b2. \u7814\u7a76\u7f16\u7801\u4ec7\u6068\u672f\u8bed\u53caLLM\u7684\u89e3\u91ca\u80fd\u529b\uff1b3. \u63d0\u51fa\u6574\u5408\u6807\u6ce8\u8bcd\u5178\u7684\u65b9\u6cd5\u3002", "result": "1. \u8bc4\u4f30\u4e86\u73b0\u6709\u6a21\u578b\u5bf9\u4ec7\u6068\u8bed\u4e49\u7684\u7406\u89e3\uff1b2. \u663e\u8457\u63d0\u5347\u4e86\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u4e3a\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u548c\u89c1\u89e3\u3002"}}
{"id": "2507.11473", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11473", "abs": "https://arxiv.org/abs/2507.11473", "authors": ["Tomek Korbak", "Mikita Balesni", "Elizabeth Barnes", "Yoshua Bengio", "Joe Benton", "Joseph Bloom", "Mark Chen", "Alan Cooney", "Allan Dafoe", "Anca Dragan", "Scott Emmons", "Owain Evans", "David Farhi", "Ryan Greenblatt", "Dan Hendrycks", "Marius Hobbhahn", "Evan Hubinger", "Geoffrey Irving", "Erik Jenner", "Daniel Kokotajlo", "Victoria Krakovna", "Shane Legg", "David Lindner", "David Luan", "Aleksander M\u0105dry", "Julian Michael", "Neel Nanda", "Dave Orr", "Jakub Pachocki", "Ethan Perez", "Mary Phuong", "Fabien Roger", "Joshua Saxe", "Buck Shlegeris", "Mart\u00edn Soto", "Eric Steinberger", "Jasmine Wang", "Wojciech Zaremba", "Bowen Baker", "Rohin Shah", "Vlad Mikulik"], "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety", "comment": null, "summary": "AI systems that \"think\" in human language offer a unique opportunity for AI\nsafety: we can monitor their chains of thought (CoT) for the intent to\nmisbehave. Like all other known AI oversight methods, CoT monitoring is\nimperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows\npromise and we recommend further research into CoT monitorability and\ninvestment in CoT monitoring alongside existing safety methods. Because CoT\nmonitorability may be fragile, we recommend that frontier model developers\nconsider the impact of development decisions on CoT monitorability.", "AI": {"tldr": "AI\u7cfb\u7edf\u901a\u8fc7\u4eba\u7c7b\u8bed\u8a00\u201c\u601d\u8003\u201d\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u53ef\u901a\u8fc7\u76d1\u63a7\u5176\u601d\u7ef4\u94fe\uff08CoT\uff09\u68c0\u6d4b\u4e0d\u826f\u610f\u56fe\u3002\u5c3d\u7ba1\u4e0d\u5b8c\u7f8e\uff0c\u4f46CoT\u76d1\u63a7\u6709\u6f5c\u529b\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6295\u8d44\u3002", "motivation": "\u63a2\u7d22\u901a\u8fc7\u76d1\u63a7AI\u7684\u601d\u7ef4\u94fe\u6765\u589e\u5f3aAI\u5b89\u5168\u6027\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u76d1\u63a7AI\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u4ee5\u68c0\u6d4b\u6f5c\u5728\u7684\u4e0d\u826f\u610f\u56fe\u3002", "result": "CoT\u76d1\u63a7\u867d\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5f00\u53d1\u3002", "conclusion": "\u5efa\u8bae\u524d\u6cbf\u6a21\u578b\u5f00\u53d1\u8005\u8003\u8651\u5f00\u53d1\u51b3\u7b56\u5bf9CoT\u53ef\u76d1\u63a7\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u6295\u8d44\u4e8e\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2507.11077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11077", "abs": "https://arxiv.org/abs/2507.11077", "authors": ["Weizhao Ma", "Dong Zhou", "Yuhui Hu", "Zipeng He"], "title": "GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft", "comment": null, "summary": "Monocular pose estimation of non-cooperative spacecraft is significant for\non-orbit service (OOS) tasks, such as satellite maintenance, space debris\nremoval, and station assembly. Considering the high demands on pose estimation\naccuracy, mainstream monocular pose estimation methods typically consist of\nkeypoint detectors and PnP solver. However, current keypoint detectors remain\nvulnerable to structural symmetry and partial occlusion of non-cooperative\nspacecraft. To this end, we propose a graph-based keypoints network for the\nmonocular pose estimation of non-cooperative spacecraft, GKNet, which leverages\nthe geometric constraint of keypoints graph. In order to better validate\nkeypoint detectors, we present a moderate-scale dataset for the spacecraft\nkeypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000\nsimulated images, and corresponding high-precise keypoint annotations.\nExtensive experiments and an ablation study have demonstrated the high accuracy\nand effectiveness of our GKNet, compared to the state-of-the-art spacecraft\nkeypoint detectors. The code for GKNet and the SKD dataset is available at\nhttps://github.com/Dongzhou-1996/GKNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5173\u952e\u70b9\u7f51\u7edc\uff08GKNet\uff09\uff0c\u7528\u4e8e\u975e\u5408\u4f5c\u822a\u5929\u5668\u7684\u5355\u76ee\u59ff\u6001\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u5bf9\u79f0\u6027\u548c\u90e8\u5206\u906e\u6321\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86SKD\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u975e\u5408\u4f5c\u822a\u5929\u5668\u7684\u5355\u76ee\u59ff\u6001\u4f30\u8ba1\u5bf9\u5728\u8f68\u670d\u52a1\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u5bf9\u7ed3\u6784\u5bf9\u79f0\u6027\u548c\u90e8\u5206\u906e\u6321\u654f\u611f\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u56fe\u7684\u5173\u952e\u70b9\u7f51\u7edc\uff08GKNet\uff09\uff0c\u5229\u7528\u5173\u952e\u70b9\u56fe\u7684\u51e0\u4f55\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGKNet\u5728\u7cbe\u5ea6\u548c\u6709\u6548\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GKNet\u548cSKD\u6570\u636e\u96c6\u4e3a\u975e\u5408\u4f5c\u822a\u5929\u5668\u7684\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11299", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11299", "abs": "https://arxiv.org/abs/2507.11299", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian R\u01cedoi"], "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "comment": "10 figures, 2 tables, 2 listings", "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr.Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr.Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings.", "AI": {"tldr": "Dr.Copilot\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u533b\u751f\u5728\u6587\u672c\u8fdc\u7a0b\u533b\u7597\u4e2d\u7684\u6c9f\u901a\u8d28\u91cf\uff0c\u800c\u975e\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u533b\u751f\u5728\u8fdc\u7a0b\u533b\u7597\u4e2d\u4e66\u9762\u56de\u590d\u7684\u5448\u73b0\u8d28\u91cf\u95ee\u9898\uff0c\u800c\u975e\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u4e2aLLM\u4ee3\u7406\uff0c\u901a\u8fc7DSPy\u81ea\u52a8\u4f18\u5316\u63d0\u793a\uff0c\u57fa\u4e8e\u4f4e\u8d44\u6e90\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u6570\u636e\u8bbe\u8ba1\uff0c\u90e8\u7f72\u5f00\u653e\u6743\u91cd\u6a21\u578b\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u548c41\u540d\u533b\u751f\u7684\u5b9e\u9645\u90e8\u7f72\u663e\u793a\u7528\u6237\u8bc4\u4ef7\u548c\u56de\u590d\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Dr.Copilot\u662f\u7f57\u9a6c\u5c3c\u4e9a\u533b\u7597\u73af\u5883\u4e2d\u9996\u6279\u5b9e\u9645\u90e8\u7f72\u7684LLM\u7cfb\u7edf\u4e4b\u4e00\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2507.11479", "categories": ["cs.AI", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11479", "abs": "https://arxiv.org/abs/2507.11479", "authors": ["Daniel Platnick", "Matti Gruener", "Marjan Alirezaie", "Kent Larson", "Dava J. Newman", "Hossein Rahnama"], "title": "Perspective-Aware AI in Extended Reality", "comment": "Accepted to the International Conference on eXtended Reality (2025),\n  12 pages, 3 figures", "summary": "AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive\nexperiences-yet current systems fall short due to shallow user modeling and\nlimited cognitive context. We introduce Perspective-Aware AI in Extended\nReality (PAiR), a foundational framework for integrating Perspective-Aware AI\n(PAi) with XR to enable interpretable, context-aware experiences grounded in\nuser identity. PAi is built on Chronicles: reasoning-ready identity models\nlearned from multimodal digital footprints that capture users' cognitive and\nexperiential evolution. PAiR employs these models in a closed-loop system\nlinking dynamic user states with immersive environments. We present PAiR's\narchitecture, detailing its modules and system flow, and demonstrate its\nutility through two proof-of-concept scenarios implemented in the Unity-based\nOpenDome engine. PAiR opens a new direction for human-AI interaction by\nembedding perspective-based identity models into immersive systems.", "AI": {"tldr": "PAiR\u6846\u67b6\u901a\u8fc7\u6574\u5408Perspective-Aware AI\uff08PAi\uff09\u4e0eXR\uff0c\u63d0\u4f9b\u57fa\u4e8e\u7528\u6237\u8eab\u4efd\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4f53\u9a8c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7528\u6237\u5efa\u6a21\u6d45\u5c42\u548c\u8ba4\u77e5\u4e0a\u4e0b\u6587\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u589e\u5f3a\u7684XR\u7cfb\u7edf\u56e0\u7528\u6237\u5efa\u6a21\u6d45\u5c42\u548c\u8ba4\u77e5\u4e0a\u4e0b\u6587\u6709\u9650\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5145\u5206\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002", "method": "PAiR\u57fa\u4e8eChronicles\uff08\u591a\u6a21\u6001\u6570\u5b57\u8db3\u8ff9\u5b66\u4e60\u5230\u7684\u8eab\u4efd\u6a21\u578b\uff09\u6784\u5efa\uff0c\u91c7\u7528\u95ed\u73af\u7cfb\u7edf\u52a8\u6001\u94fe\u63a5\u7528\u6237\u72b6\u6001\u4e0e\u6c89\u6d78\u5f0f\u73af\u5883\u3002", "result": "\u901a\u8fc7Unity-based OpenDome\u5f15\u64ce\u5b9e\u73b0\u7684\u4e24\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u573a\u666f\u5c55\u793a\u4e86PAiR\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "PAiR\u901a\u8fc7\u5c06\u57fa\u4e8e\u89c6\u89d2\u7684\u8eab\u4efd\u6a21\u578b\u5d4c\u5165\u6c89\u6d78\u5f0f\u7cfb\u7edf\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.11081", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.11081", "abs": "https://arxiv.org/abs/2507.11081", "authors": ["Chang Peng", "Bao Yang", "Meiqi Li", "Ge Zhang", "Hui Sun", "Zhenyu Jiang"], "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification", "comment": null, "summary": "Ground penetrating radar (GPR) has become a rapid and non-destructive\nsolution for road subsurface distress (RSD) detection. However, RSD recognition\nfrom GPR images is labor-intensive and heavily relies on inspectors' expertise.\nDeep learning offers the possibility for automatic RSD recognition, but its\ncurrent performance is limited by two factors: Scarcity of high-quality dataset\nfor network training and insufficient capability of network to distinguish RSD.\nIn this study, a rigorously validated 3D GPR dataset containing 2134 samples of\ndiverse types was constructed through field scanning. Based on the finding that\nthe YOLO model trained with one of the three scans of GPR images exhibits\nvarying sensitivity to specific type of RSD, we proposed a novel\ncross-verification strategy with outstanding accuracy in RSD recognition,\nachieving recall over 98.6% in field tests. The approach, integrated into an\nonline RSD detection system, can reduce the labor of inspection by around 90%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u4eceGPR\u56fe\u50cf\u4e2d\u81ea\u52a8\u8bc6\u522b\u9053\u8def\u5730\u4e0b\u75c5\u5bb3\uff08RSD\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "motivation": "GPR\u56fe\u50cf\u4e2d\u7684RSD\u8bc6\u522b\u4f9d\u8d56\u4eba\u5de5\u4e14\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u7f51\u7edc\u533a\u5206\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u76843D GPR\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eYOLO\u6a21\u578b\u7684\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0c\u53ec\u56de\u7387\u8d85\u8fc798.6%\uff0c\u68c0\u6d4b\u7cfb\u7edf\u53ef\u51cf\u5c11\u7ea690%\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aRSD\u81ea\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11316", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11316", "abs": "https://arxiv.org/abs/2507.11316", "authors": ["Haoran Jin", "Meng Li", "Xiting Wang", "Zhihao Xu", "Minlie Huang", "Yantao Jia", "Defu Lian"], "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation", "comment": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)", "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConVA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5185\u90e8\u503c\u5411\u91cf\uff0c\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u548c\u6d41\u7545\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u4f7f\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u5176\u900f\u660e\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u63a7\u5236\u7684\u503c\u5411\u91cf\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u95e8\u63a7\u503c\u5411\u91cf\u6fc0\u6d3b\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9LLM\u5185\u90e8\u503c\u7684\u7cbe\u786e\u548c\u65e0\u504f\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u572810\u79cd\u57fa\u672c\u4ef7\u503c\u89c2\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u63a7\u5236\u6210\u529f\u7387\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u6d41\u7545\u6027\u3002", "conclusion": "ConVA\u65b9\u6cd5\u6709\u6548\u4e14\u6700\u5c0f\u5316\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u786e\u4fddLLM\u5728\u8f93\u5165\u63d0\u793a\u5bf9\u7acb\u6216\u6076\u610f\u65f6\u4ecd\u80fd\u4fdd\u6301\u76ee\u6807\u4ef7\u503c\u89c2\u3002"}}
{"id": "2507.11482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11482", "abs": "https://arxiv.org/abs/2507.11482", "authors": ["Mani Hamidi", "Terrence W. Deacon"], "title": "Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light", "comment": null, "summary": "Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f00\u653e\u8fdb\u5316\u7406\u8bba\u7684\u6846\u67b6\uff0c\u91cd\u65b0\u5ba1\u89c6\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u4e2a\u6838\u5fc3\u5047\u8bbe\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u7406\u8bba\u548c\u5e94\u7528\u4e2d\u7684\u610f\u4e49\u3002", "motivation": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u4e2d\u5173\u4e8e\u4ee3\u7406\u5b9a\u4e49\u3001\u5b66\u4e60\u76ee\u6807\u548c\u5956\u52b1\u5047\u8bbe\u7684\u4e09\u4e2a\u6838\u5fc3\u5047\u8bbe\u8fdb\u884c\u6982\u5ff5\u6027\u4fee\u8ba2\uff0c\u4ee5\u63a8\u52a8\u7406\u8bba\u548c\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u501f\u9274\u5f00\u653e\u8fdb\u5316\u7406\u8bba\uff0c\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e09\u4e2a\u5047\u8bbe\uff0c\u5e76\u7ed3\u5408\u8fdb\u5316\u52a8\u529b\u5b66\u548c\u8d77\u6e90\u751f\u547d\u7406\u8bba\u8fdb\u884c\u5206\u6790\u3002", "result": "\u63d0\u51fa\u8fdb\u5316\u52a8\u529b\u5b66\u53ef\u4ee5\u5728\u4e2a\u4f53\u751f\u547d\u5468\u671f\u5185\u8fd0\u4f5c\uff0c\u4e30\u5bcc\u4e86\u5b66\u4e60\u89c6\u89d2\uff0c\u5e76\u63a2\u8ba8\u4e86\u5956\u52b1\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8fdb\u5316\u8303\u5f0f\u867d\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u4ee3\u7406\u95ee\u9898\uff0c\u4f46\u4e3a\u7406\u89e3\u751f\u7269\u7cfb\u7edf\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5efa\u8bae\u7ed3\u5408\u8d77\u6e90\u751f\u547d\u7406\u8bba\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.11085", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11085", "abs": "https://arxiv.org/abs/2507.11085", "authors": ["Tianchi Xu"], "title": "Atmos-Bench: 3D Atmospheric Structures for Climate Insight", "comment": null, "summary": "Atmospheric structure, represented by backscatter coefficients (BC) recovered\nfrom satellite LiDAR attenuated backscatter (ATB), provides a volumetric view\nof clouds, aerosols, and molecules, playing a critical role in human\nactivities, climate understanding, and extreme weather forecasting. Existing\nmethods often rely on auxiliary inputs and simplified physics-based\napproximations, and lack a standardized 3D benchmark for fair evaluation.\nHowever, such approaches may introduce additional uncertainties and\ninsufficiently capture realistic radiative transfer and atmospheric\nscattering-absorption effects. To bridge these gaps, we present Atmos-Bench:\nthe first 3D atmospheric benchmark, along with a novel FourCastX:\nFrequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)\ngenerates 921,600 image slices from 3D scattering volumes simulated at 532 nm\nand 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean\ntime steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC\nphysical constraints into the model architecture, promoting energy consistency\nduring restoration; (c) achieves consistent improvements on the Atmos-Bench\ndataset across both 355 nm and 532 nm bands, outperforming state-of-the-art\nbaseline models without relying on auxiliary inputs. Atmos-Bench establishes a\nnew standard for satellite-based 3D atmospheric structure recovery and paves\nthe way for deeper climate insight.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a3D\u5927\u6c14\u57fa\u51c6Atmos-Bench\u548c\u65b0\u578b\u7f51\u7edcFourCastX\uff0c\u7528\u4e8e\u6062\u590d\u5927\u6c14\u7ed3\u6784\uff0c\u65e0\u9700\u8f85\u52a9\u8f93\u5165\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8f85\u52a9\u8f93\u5165\u548c\u7b80\u5316\u7269\u7406\u8fd1\u4f3c\uff0c\u7f3a\u4e4f\u6807\u51c6\u53163D\u57fa\u51c6\uff0c\u53ef\u80fd\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u4e14\u672a\u80fd\u5145\u5206\u6355\u6349\u771f\u5b9e\u8f90\u5c04\u4f20\u8f93\u548c\u5927\u6c14\u6563\u5c04-\u5438\u6536\u6548\u5e94\u3002", "method": "\u901a\u8fc7\u8026\u5408WRF\u548c\u589e\u5f3a\u7684COSP\u6a21\u62df\u5668\u751f\u6210921,600\u5f20\u56fe\u50cf\u5207\u7247\uff0c\u6784\u5efa3D\u6563\u5c04\u4f53\u79ef\uff1b\u5728\u6a21\u578b\u4e2d\u5d4c\u5165ATB-BC\u7269\u7406\u7ea6\u675f\uff0c\u786e\u4fdd\u80fd\u91cf\u4e00\u81f4\u6027\u3002", "result": "\u5728Atmos-Bench\u6570\u636e\u96c6\u4e0a\uff0c355 nm\u548c532 nm\u6ce2\u6bb5\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Atmos-Bench\u4e3a\u536b\u661f3D\u5927\u6c14\u7ed3\u6784\u6062\u590d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7684\u6c14\u5019\u7814\u7a76\u3002"}}
{"id": "2507.11330", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11330", "abs": "https://arxiv.org/abs/2507.11330", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5b66\u672f\u8bba\u6587\u7684\u65b9\u6cd5\u65b0\u9896\u6027\uff0c\u901a\u8fc7\u63d0\u53d6\u540c\u884c\u8bc4\u5ba1\u62a5\u544a\u4e2d\u7684\u65b0\u9896\u6027\u76f8\u5173\u53e5\u5b50\u548cLLM\u603b\u7ed3\u7684\u65b9\u6cd5\u90e8\u5206\u6765\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6587\u672c\u5f15\u5bfc\u878d\u5408\u6a21\u5757\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b0\u9896\u6027\u8bc4\u4f30\u65b9\u6cd5\uff08\u4e13\u5bb6\u5224\u65ad\u6216\u5f15\u7528\u7ec4\u5408\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4e13\u5bb6\u77e5\u8bc6\u6709\u9650\u6216\u5f15\u7528\u7ec4\u5408\u6709\u6548\u6027\u4e0d\u786e\u5b9a\u3002\u7ed3\u5408LLM\u7684\u77e5\u8bc6\u548c\u4eba\u7c7b\u4e13\u5bb6\u7684\u5224\u65ad\u80fd\u529b\u53ef\u4ee5\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u4ece\u540c\u884c\u8bc4\u5ba1\u62a5\u544a\u4e2d\u63d0\u53d6\u65b0\u9896\u6027\u76f8\u5173\u53e5\u5b50\uff0c\u7528LLM\u603b\u7ed3\u8bba\u6587\u65b9\u6cd5\u90e8\u5206\uff0c\u7528\u4e8e\u5fae\u8c03PLM\uff1b\u8bbe\u8ba1\u6587\u672c\u5f15\u5bfc\u878d\u5408\u6a21\u5757\uff08Sparse-Attention\uff09\u6574\u5408\u4eba\u7c7b\u548cLLM\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u65b9\u6cd5\u65b0\u9896\u6027\u65b9\u9762\u4f18\u4e8e\u5927\u91cf\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408\u4eba\u7c7b\u548cLLM\u77e5\u8bc6\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u5b66\u672f\u8bba\u6587\u7684\u65b9\u6cd5\u65b0\u9896\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.11527", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.11527", "abs": "https://arxiv.org/abs/2507.11527", "authors": ["Yinsheng Li", "Zhen Dong", "Yi Shao"], "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering", "comment": "Project page: https://github.com/Eason-Li-AIS/DrafterBench", "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.", "AI": {"tldr": "DrafterBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u571f\u6728\u5de5\u7a0b\u56fe\u7eb8\u4fee\u8ba2\u4efb\u52a1\u4e2d\u7684\u5f00\u6e90\u57fa\u51c6\uff0c\u5305\u542b12\u7c7b\u4efb\u52a1\u300146\u4e2a\u5b9a\u5236\u529f\u80fd\u548c1920\u4e2a\u4efb\u52a1\uff0c\u65e8\u5728\u5168\u9762\u6d4b\u8bd5\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4ece\u5de5\u4e1a\u89d2\u5ea6\uff08\u5982\u571f\u6728\u5de5\u7a0b\uff09\u7cfb\u7edf\u8bc4\u4f30LLM\u4ee3\u7406\u81ea\u52a8\u5316\u80fd\u529b\u7684\u57fa\u51c6\uff0cDrafterBench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u603b\u7ed3\u5b9e\u9645\u56fe\u7eb8\u6587\u4ef6\u4e2d\u7684\u4efb\u52a1\uff0c\u8bbe\u8ba1\u5305\u542b\u591a\u79cd\u529f\u80fd\u548c\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4ee3\u7406\u5728\u590d\u6742\u6307\u4ee4\u7406\u89e3\u3001\u77e5\u8bc6\u5229\u7528\u548c\u52a8\u6001\u9002\u5e94\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "DrafterBench\u63d0\u4f9b\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u548c\u9519\u8bef\u7edf\u8ba1\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u5e2e\u52a9\u6df1\u5165\u4e86\u89e3\u4ee3\u7406\u80fd\u529b\u5e76\u786e\u5b9a\u6539\u8fdb\u76ee\u6807\u3002", "conclusion": "DrafterBench\u4e3aLLM\u4ee3\u7406\u5728\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4e86\u6d4b\u8bd5\u96c6\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.11099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11099", "abs": "https://arxiv.org/abs/2507.11099", "authors": ["Qiyang Wan", "Chengzhi Gao", "Ruiping Wang", "Xilin Chen"], "title": "A Survey on Interpretability in Visual Recognition", "comment": "20 pages, 7 figures, 2 tables. Under review", "summary": "In recent years, visual recognition methods have advanced significantly,\nfinding applications across diverse fields. While researchers seek to\nunderstand the mechanisms behind the success of these models, there is also a\ngrowing impetus to deploy them in critical areas like autonomous driving and\nmedical diagnostics to better diagnose failures, which promotes the development\nof interpretability research. This paper systematically reviews existing\nresearch on the interpretability of visual recognition models and proposes a\ntaxonomy of methods from a human-centered perspective. The proposed taxonomy\ncategorizes interpretable recognition methods based on Intent, Object,\nPresentation, and Methodology, thereby establishing a systematic and coherent\nset of grouping criteria for these XAI methods. Additionally, we summarize the\nrequirements for evaluation metrics and explore new opportunities enabled by\nrecent technologies, such as large multimodal models. We aim to organize\nexisting research in this domain and inspire future investigations into the\ninterpretability of visual recognition models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u6307\u6807\u548c\u65b0\u6280\u672f\u7684\u673a\u9047\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7406\u89e3\u5176\u673a\u5236\u548c\u5931\u8d25\u539f\u56e0\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u3001\u5bf9\u8c61\u3001\u5448\u73b0\u548c\u65b9\u6cd5\u7684\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5316\u6574\u7406\u4e86\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u6807\u51c6\uff0c\u5e76\u603b\u7ed3\u4e86\u8bc4\u4f30\u6307\u6807\u7684\u9700\u6c42\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u7ec4\u7ec7\u73b0\u6709\u7814\u7a76\u5e76\u542f\u53d1\u672a\u6765\u5bf9\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u63a2\u7d22\u3002"}}
{"id": "2507.11356", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11356", "abs": "https://arxiv.org/abs/2507.11356", "authors": ["Alexis Brissard", "Fr\u00e9d\u00e9ric Cuppens", "Amal Zouaq"], "title": "What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models", "comment": "12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings", "summary": "Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u8fc7\u7a0b\u6a21\u578b\u8868\u793a\uff08PMR\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51faPMo\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540cPMR\u5728\u8fc7\u7a0b\u5efa\u6a21\uff08PMo\uff09\u548c\u8fc7\u7a0b\u6a21\u578b\u751f\u6210\uff08PMG\uff09\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709PMR\u5728\u7ed3\u6784\u3001\u590d\u6742\u6027\u548c\u53ef\u7528\u6027\u4e0a\u5dee\u5f02\u5927\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\uff0c\u5bfc\u81f4LLM\u5728PMo\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u96be\u4ee5\u8bc4\u4f30\u3002", "method": "\u5f15\u5165PMo\u6570\u636e\u96c6\uff08\u542b55\u4e2a\u8fc7\u7a0b\u63cf\u8ff0\u548c9\u79cdPMR\u6a21\u578b\uff09\uff0c\u4eceLLM\u9002\u7528\u6027\u548cPMG\u6027\u80fd\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30PMR\u3002", "result": "Mermaid\u5728\u516d\u9879PMo\u6807\u51c6\u4e2d\u5f97\u5206\u6700\u9ad8\uff0cBPMN text\u5728\u8fc7\u7a0b\u5143\u7d20\u76f8\u4f3c\u6027\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5728PMo\u4efb\u52a1\u4e2d\u7684PMR\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0cMermaid\u548cBPMN text\u5404\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.11538", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11538", "abs": "https://arxiv.org/abs/2507.11538", "authors": ["Daniel Jaroslawicz", "Brendan Whiting", "Parth Shah", "Karime Maamari"], "title": "How Many Instructions Can LLMs Follow at Once?", "comment": null, "summary": "Production-grade LLM systems require robust adherence to dozens or even\nhundreds of instructions simultaneously. However, the instruction-following\ncapabilities of LLMs at high instruction densities have not yet been\ncharacterized, as existing benchmarks only evaluate models on tasks with a\nsingle or few instructions. We introduce IFScale, a simple benchmark of 500\nkeyword-inclusion instructions for a business report writing task to measure\nhow instruction-following performance degrades as instruction density\nincreases. We evaluate 20 state-of-the-art models across seven major providers\nand find that even the best frontier models only achieve 68% accuracy at the\nmax density of 500 instructions. Our analysis reveals model size and reasoning\ncapability to correlate with 3 distinct performance degradation patterns, bias\ntowards earlier instructions, and distinct categories of instruction-following\nerrors. Our insights can help inform design of instruction-dense prompts in\nreal-world applications and highlight important performance-latency tradeoffs.\nWe open-source the benchmark and all results for further analysis at\nhttps://distylai.github.io/IFScale.", "AI": {"tldr": "IFScale\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u9ad8\u5bc6\u5ea6\u6307\u4ee4\u4e0b\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728500\u6761\u6307\u4ee4\u65f6\u51c6\u786e\u7387\u4ec5\u4e3a68%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8bc4\u4f30\u5355\u6761\u6216\u5c11\u91cf\u6307\u4ee4\u7684\u4efb\u52a1\uff0c\u65e0\u6cd5\u53cd\u6620\u751f\u4ea7\u7ea7LLM\u7cfb\u7edf\u9700\u540c\u65f6\u9075\u5faa\u5927\u91cf\u6307\u4ee4\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165IFScale\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u6761\u5173\u952e\u8bcd\u5305\u542b\u6307\u4ee4\uff0c\u7528\u4e8e\u5546\u4e1a\u62a5\u544a\u5199\u4f5c\u4efb\u52a1\uff0c\u8bc4\u4f3020\u4e2a\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728500\u6761\u6307\u4ee4\u65f6\u51c6\u786e\u7387\u4e3a68%\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u4e0e\u6027\u80fd\u4e0b\u964d\u6a21\u5f0f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u9ad8\u5bc6\u5ea6\u6307\u4ee4\u63d0\u793a\uff0c\u5e76\u63ed\u793a\u4e86\u6027\u80fd\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\uff0c\u57fa\u51c6\u6d4b\u8bd5\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11102", "abs": "https://arxiv.org/abs/2507.11102", "authors": ["Jie Yang", "Wang Zeng", "Sheng Jin", "Lumin Xu", "Wentao Liu", "Chen Qian", "Zhen Li", "Ruimao Zhang"], "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model", "comment": "Extended Version of KptLLM. arXiv admin note: text overlap with\n  arXiv:2411.01846", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has revolutionized\nimage understanding by bridging textual and visual modalities. However, these\nmodels often struggle with capturing fine-grained semantic information, such as\nthe precise identification and analysis of object keypoints. Keypoints, as\nstructure-aware, pixel-level, and compact representations of objects,\nparticularly articulated ones, play a crucial role in applications such as\nfine-grained image analysis, object retrieval, and behavior recognition. In\nthis paper, we propose KptLLM++, a novel multimodal large language model that\nspecifically designed for generic keypoint comprehension through the\nintegration of diverse input modalities guided by user-defined instructions. By\nunifying keypoint detection across varied contexts, KptLLM++ establishes itself\nas an advanced interface, fostering more effective human-AI collaboration. The\nmodel is built upon a novel identify-then-detect paradigm, which first\ninterprets keypoint semantics and subsequently localizes their precise\npositions through a structured chain-of-thought reasoning mechanism. To push\nthe boundaries of performance, we have scaled up the training dataset to over\n500K samples, encompassing diverse objects, keypoint categories, image styles,\nand scenarios with complex occlusions. This extensive scaling enables KptLLM++\nto unlock its potential, achieving remarkable accuracy and generalization.\nComprehensive experiments on multiple keypoint detection benchmarks demonstrate\nits state-of-the-art performance, underscoring its potential as a unified\nsolution for fine-grained image understanding and its transformative\nimplications for human-AI interaction.", "AI": {"tldr": "KptLLM++\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u901a\u7528\u5173\u952e\u70b9\u7406\u89e3\uff0c\u901a\u8fc7\u7528\u6237\u6307\u4ee4\u6574\u5408\u591a\u79cd\u8f93\u5165\u6a21\u6001\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u5173\u952e\u70b9\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u800c\u5173\u952e\u70b9\u5bf9\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u6790\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faKptLLM++\uff0c\u91c7\u7528\u2018\u8bc6\u522b-\u68c0\u6d4b\u2019\u8303\u5f0f\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\uff0c\u5e76\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u96c6\u81f350\u4e07\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u5173\u952e\u70b9\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "conclusion": "KptLLM++\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u548c\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u5177\u6709\u53d8\u9769\u6027\u610f\u4e49\u3002"}}
{"id": "2507.11384", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11384", "abs": "https://arxiv.org/abs/2507.11384", "authors": ["Xia Cui"], "title": "Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss", "comment": "10 pages, 1 figure, SemEval 2025", "summary": "This paper explores the application of a simple weighted loss function to\nTransformer-based models for multi-label emotion detection in SemEval-2025\nShared Task 11. Our approach addresses data imbalance by dynamically adjusting\nclass weights, thereby enhancing performance on minority emotion classes\nwithout the computational burden of traditional resampling methods. We evaluate\nBERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such\nas Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.\nThe results demonstrate that the weighted loss function improves performance on\nhigh-frequency emotion classes but shows limited impact on minority classes.\nThese findings underscore both the effectiveness and the challenges of applying\nthis approach to imbalanced multi-label emotion detection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728Transformer\u6a21\u578b\u4e2d\u5e94\u7528\u52a0\u6743\u635f\u5931\u51fd\u6570\u8fdb\u884c\u591a\u6807\u7b7e\u60c5\u611f\u68c0\u6d4b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u6743\u91cd\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728BRIGHTER\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86BERT\u3001RoBERTa\u548cBART\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u591a\u6807\u7b7e\u60c5\u611f\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u91cd\u91c7\u6837\u65b9\u6cd5\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u7684\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u8bc4\u4f30BERT\u3001RoBERTa\u548cBART\u6a21\u578b\u5728BRIGHTER\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u52a0\u6743\u635f\u5931\u51fd\u6570\u63d0\u9ad8\u4e86\u9ad8\u9891\u60c5\u611f\u7c7b\u522b\u7684\u6027\u80fd\uff0c\u4f46\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6807\u7b7e\u60c5\u611f\u68c0\u6d4b\u4e2d\u6709\u6548\uff0c\u4f46\u4ecd\u9762\u4e34\u5904\u7406\u5c11\u6570\u7c7b\u522b\u7684\u6311\u6218\u3002"}}
{"id": "2507.11116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11116", "abs": "https://arxiv.org/abs/2507.11116", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha", "Mostofa Kamal Nasir"], "title": "Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach", "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library", "summary": "Jellyfish, a diverse group of gelatinous marine organisms, play a crucial\nrole in maintaining marine ecosystems but pose significant challenges for\nbiodiversity and conservation due to their rapid proliferation and ecological\nimpact. Accurate identification of jellyfish species is essential for\necological monitoring and management. In this study, we proposed a deep\nlearning framework for jellyfish species detection and classification using an\nunderwater image dataset. The framework integrates advanced feature extraction\ntechniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,\ncombined with seven traditional machine learning classifiers and three\nFeedforward Neural Network classifiers for precise species identification.\nAdditionally, we activated the softmax function to directly classify jellyfish\nspecies using the convolutional neural network models. The combination of the\nArtificial Neural Network with MobileNetV3 is our best-performing model,\nachieving an exceptional accuracy of 98%, significantly outperforming other\nfeature extractor-classifier combinations. This study demonstrates the efficacy\nof deep learning and hybrid frameworks in addressing biodiversity challenges\nand advancing species detection in marine environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u6bcd\u7269\u79cd\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u7ed3\u5408\u591a\u79cd\u7279\u5f81\u63d0\u53d6\u6280\u672f\u548c\u5206\u7c7b\u5668\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe98%\u3002", "motivation": "\u6c34\u6bcd\u5728\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5176\u5feb\u901f\u7e41\u6b96\u548c\u751f\u6001\u5f71\u54cd\u5bf9\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u6784\u6210\u6311\u6218\uff0c\u51c6\u786e\u8bc6\u522b\u7269\u79cd\u5bf9\u751f\u6001\u76d1\u6d4b\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6574\u5408MobileNetV3\u3001ResNet50\u7b49\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528softmax\u51fd\u6570\u76f4\u63a5\u5206\u7c7b\u3002", "result": "MobileNetV3\u4e0e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u5408\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe98%\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6df7\u5408\u6846\u67b6\u5728\u89e3\u51b3\u751f\u7269\u591a\u6837\u6027\u6311\u6218\u548c\u6d77\u6d0b\u7269\u79cd\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2507.11405", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11405", "abs": "https://arxiv.org/abs/2507.11405", "authors": ["Cheng Xu", "Nan Yan", "Shuhao Guan", "Changhong Jin", "Yuke Mei", "Yibing Guo", "M-Tahar Kechadi"], "title": "DCR: Quantifying Data Contamination in LLMs Evaluation", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data, inflating performance metrics and undermining genuine\ngeneralization assessment. This paper introduces the Data Contamination Risk\n(DCR) framework, a lightweight, interpretable pipeline designed to detect and\nquantify BDC across four granular levels: semantic, informational, data, and\nlabel. By synthesizing contamination scores via a fuzzy inference system, DCR\nproduces a unified DCR Factor that adjusts raw accuracy to reflect\ncontamination-aware performance. Validated on 9 LLMs (0.5B-72B) across\nsentiment analysis, fake news detection, and arithmetic reasoning tasks, the\nDCR framework reliably diagnoses contamination severity and with accuracy\nadjusted using the DCR Factor to within 4% average error across the three\nbenchmarks compared to the uncontaminated baseline. Emphasizing computational\nefficiency and transparency, DCR provides a practical tool for integrating\ncontamination assessment into routine evaluations, fostering fairer comparisons\nand enhancing the credibility of LLM benchmarking practices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u6570\u636e\u6c61\u67d3\u98ce\u9669\uff08DCR\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc4\u4f30\u6570\u636e\u4e2d\u7684\u6c61\u67d3\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u51c6\u786e\u7387\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8bc4\u4f30\u6570\u636e\u6c61\u67d3\uff08BDC\uff09\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5bfc\u81f4\u6027\u80fd\u6307\u6807\u865a\u9ad8\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u771f\u5b9e\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u3002", "method": "DCR\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u7c92\u5ea6\u7ea7\u522b\uff08\u8bed\u4e49\u3001\u4fe1\u606f\u3001\u6570\u636e\u548c\u6807\u7b7e\uff09\u68c0\u6d4b\u6c61\u67d3\uff0c\u5e76\u5229\u7528\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u5408\u6210\u6c61\u67d3\u5206\u6570\uff0c\u751f\u6210\u7edf\u4e00\u7684DCR\u56e0\u5b50\u6765\u8c03\u6574\u51c6\u786e\u7387\u3002", "result": "\u57289\u4e2aLLM\uff080.5B-72B\uff09\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cDCR\u80fd\u53ef\u9760\u8bca\u65ad\u6c61\u67d3\u4e25\u91cd\u7a0b\u5ea6\uff0c\u8c03\u6574\u540e\u7684\u51c6\u786e\u7387\u4e0e\u672a\u6c61\u67d3\u57fa\u51c6\u76f8\u6bd4\u5e73\u5747\u8bef\u5dee\u57284%\u4ee5\u5185\u3002", "conclusion": "DCR\u6846\u67b6\u8ba1\u7b97\u9ad8\u6548\u4e14\u900f\u660e\uff0c\u4e3a\u65e5\u5e38\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6c61\u67d3\u68c0\u6d4b\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u516c\u5e73\u7684\u6bd4\u8f83\u548c\u63d0\u5347LLM\u57fa\u51c6\u6d4b\u8bd5\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.11119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11119", "abs": "https://arxiv.org/abs/2507.11119", "authors": ["Hankun Liu", "Yujian Zhao", "Guanglin Niu"], "title": "Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID", "comment": null, "summary": "Hard samples pose a significant challenge in person re-identification (ReID)\ntasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent\nambiguity or similarity, coupled with the lack of explicit definitions, makes\nthem a fundamental bottleneck. These issues not only limit the design of\ntargeted learning strategies but also diminish the model's robustness under\nclothing or viewpoint changes. In this paper, we propose a novel\nmultimodal-guided Hard Sample Generation and Learning (HSGL) framework, which\nis the first effort to unify textual and visual modalities to explicitly\ndefine, generate, and optimize hard samples within a unified paradigm. HSGL\ncomprises two core components: (1) Dual-Granularity Hard Sample Generation\n(DGHSG), which leverages multimodal cues to synthesize semantically consistent\nsamples, including both coarse- and fine-grained hard positives and negatives\nfor effectively increasing the hardness and diversity of the training data. (2)\nHard Sample Adaptive Learning (HSAL), which introduces a hardness-aware\noptimization strategy that adjusts feature distances based on textual semantic\nlabels, encouraging the separation of hard positives and drawing hard negatives\ncloser in the embedding space to enhance the model's discriminative capability\nand robustness to hard samples. Extensive experiments on multiple CC-ReID\nbenchmarks demonstrate the effectiveness of our approach and highlight the\npotential of multimodal-guided hard sample generation and learning for robust\nCC-ReID. Notably, HSAL significantly accelerates the convergence of the\ntargeted learning procedure and achieves state-of-the-art performance on both\nPRCC and LTCC datasets. The code is available at\nhttps://github.com/undooo/TryHarder-ACMMM25.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u786c\u6837\u672c\u751f\u6210\u4e0e\u5b66\u4e60\u6846\u67b6\uff08HSGL\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u6765\u5b9a\u4e49\u3001\u751f\u6210\u548c\u4f18\u5316\u786c\u6837\u672c\uff0c\u63d0\u5347\u4e86\u670d\u88c5\u53d8\u5316\u884c\u4eba\u91cd\u8bc6\u522b\uff08CC-ReID\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u786c\u6837\u672c\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u670d\u88c5\u53d8\u5316\u573a\u666f\u4e0b\u3002\u5176\u6a21\u7cca\u6027\u548c\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u9650\u5236\u4e86\u5b66\u4e60\u7b56\u7565\u7684\u8bbe\u8ba1\u548c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "HSGL\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53cc\u7c92\u5ea6\u786c\u6837\u672c\u751f\u6210\uff08DGHSG\uff09\u548c\u786c\u6837\u672c\u81ea\u9002\u5e94\u5b66\u4e60\uff08HSAL\uff09\uff0c\u5206\u522b\u901a\u8fc7\u591a\u6a21\u6001\u751f\u6210\u6837\u672c\u548c\u786c\u5ea6\u611f\u77e5\u4f18\u5316\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2aCC-ReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHSGL\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u5728PRCC\u548cLTCC\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u786c\u6837\u672c\u751f\u6210\u4e0e\u5b66\u4e60\u4e3aCC-ReID\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5224\u522b\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11407", "abs": "https://arxiv.org/abs/2507.11407", "authors": ["LG AI Research", ":", "Kyunghoon Bae", "Eunbi Choi", "Kibong Choi", "Stanley Jungkyu Choi", "Yemuk Choi", "Kyubeen Han", "Seokhee Hong", "Junwon Hwang", "Taewan Hwang", "Joonwon Jang", "Hyojin Jeon", "Kijeong Jeon", "Gerrard Jeongwon Jo", "Hyunjik Jo", "Jiyeon Jung", "Euisoon Kim", "Hyosang Kim", "Jihoon Kim", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Youchul Kim", "Edward Hwayoung Lee", "Gwangho Lee", "Haeju Lee", "Honglak Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Young Min Paik", "Yongmin Park", "Youngyong Park", "Sanghyun Seo", "Sihoon Yang", "Heuiyeen Yeen", "Sihyuk Yi", "Hyeongu Yun"], "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes", "comment": "Technical Report, 30 Pages", "summary": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.", "AI": {"tldr": "EXAONE 4.0\u6574\u5408\u4e86\u975e\u63a8\u7406\u548c\u63a8\u7406\u6a21\u5f0f\uff0c\u63d0\u5347\u4e86\u53ef\u7528\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u591a\u8bed\u8a00\uff0c\u5e76\u63a8\u51fa\u4e86\u4e24\u79cd\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u4e3a\u8fce\u63a5\u667a\u80fd\u4ee3\u7406\u65f6\u4ee3\uff0cEXAONE 4.0\u589e\u5f3a\u4e86\u5de5\u5177\u4f7f\u7528\u548c\u591a\u8bed\u8a00\u652f\u6301\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "EXAONE 4.0\u7ed3\u5408\u4e86\u975e\u63a8\u7406\u6a21\u5f0f\uff08\u7c7b\u4f3cEXAONE 3.5\uff09\u548c\u63a8\u7406\u6a21\u5f0f\uff08\u7c7b\u4f3cEXAONE Deep\uff09\uff0c\u5e76\u6269\u5c55\u4e86\u591a\u8bed\u8a00\u652f\u6301\uff08\u5305\u62ec\u897f\u73ed\u7259\u8bed\uff09\u3002\u63d0\u4f9b\u4e8632B\u548c1.2B\u4e24\u79cd\u89c4\u6a21\u7684\u6a21\u578b\u3002", "result": "EXAONE 4.0\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\uff0c\u751a\u81f3\u80fd\u4e0e\u524d\u6cbf\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "EXAONE 4.0\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4e14\u6613\u4e8e\u83b7\u53d6\u7684\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u4ee3\u7406\u548c\u591a\u8bed\u8a00AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11129", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11129", "abs": "https://arxiv.org/abs/2507.11129", "authors": ["Zhifeng Gu", "Bing Wang"], "title": "MMOne: Representing Multiple Modalities in One Scene", "comment": "Accepted to ICCV 2025", "summary": "Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMOne\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u573a\u666f\u8868\u793a\u4e2d\u7684\u6a21\u6001\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u6001\u5efa\u6a21\u6a21\u5757\u548c\u591a\u6a21\u6001\u5206\u89e3\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7d27\u51d1\u548c\u9ad8\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u591a\u6a21\u6001\u7ebf\u7d22\u611f\u77e5\u4e16\u754c\uff0c\u4f46\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u56fa\u6709\u5dee\u5f02\uff08\u5982\u5c5e\u6027\u5dee\u5f02\u548c\u7c92\u5ea6\u5dee\u5f02\uff09\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u8868\u793a\u591a\u6a21\u6001\u573a\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMMOne\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u6001\u5efa\u6a21\u6a21\u5757\uff08\u4f7f\u7528\u6a21\u6001\u6307\u793a\u5668\u6355\u6349\u6a21\u6001\u7279\u6027\uff09\u548c\u591a\u6a21\u6001\u5206\u89e3\u673a\u5236\uff08\u5c06\u591a\u6a21\u6001\u9ad8\u65af\u5206\u5e03\u5206\u89e3\u4e3a\u5355\u6a21\u6001\u9ad8\u65af\u5206\u5e03\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5404\u6a21\u6001\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u652f\u6301\u6269\u5c55\u5230\u66f4\u591a\u6a21\u6001\u3002", "conclusion": "MMOne\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u573a\u666f\u8868\u793a\u4e2d\u7684\u6a21\u6001\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11408", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.11408", "abs": "https://arxiv.org/abs/2507.11408", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Saptarshi Saha", "Utpal Garain", "Nicholas Asher"], "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?", "comment": "15 pages, 9 figures", "summary": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u94fe\u5f0f\u601d\u7ef4\u56fe\uff08CCGs\uff09\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u53d1\u73b0\u63a8\u7406\u8282\u70b9\u662f\u6700\u7ec8\u7b54\u6848\u7684\u4e2d\u4ecb\uff0c\u4e14\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u4e0eCCGs\u76f8\u4f3c\u3002", "motivation": "\u63a2\u7d22\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5982\u4f55\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u5171\u8bc6\u673a\u5236\u3002", "method": "\u63d0\u51fa\u56e0\u679c\u94fe\u5f0f\u601d\u7ef4\u56fe\uff08CCGs\uff09\uff0c\u4ece\u63a8\u7406\u8f68\u8ff9\u4e2d\u81ea\u52a8\u63d0\u53d6\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5efa\u6a21\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u7ec6\u7c92\u5ea6\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u63a8\u7406\u8282\u70b9\u662f\u7b54\u6848\u7684\u4e2d\u4ecb\uff0c\u4e14\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u4e0eCCGs\u76f8\u4f3c\u3002", "conclusion": "KisMATH\u6570\u636e\u96c6\u652f\u6301\u53ef\u63a7\u7684\u56fe\u5bf9\u9f50\u5e72\u9884\uff0c\u4e3a\u7814\u7a76CoT\u5728LLM\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.11143", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11143", "abs": "https://arxiv.org/abs/2507.11143", "authors": ["Lam Pham", "Cam Le", "Hieu Tang", "Khang Truong", "Truong Nguyen", "Jasmin Lampert", "Alexander Schindler", "Martin Boyer", "Son Phan"], "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images", "comment": null, "summary": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5229\u7528\u9065\u611f\u56fe\u50cf\u81ea\u52a8\u89c2\u6d4b\u6ed1\u5761\u4e8b\u4ef6\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u6781\u7aef\u5929\u6c14\u548c\u4eba\u7c7b\u6d3b\u52a8\u5bfc\u81f4\u6ed1\u5761\u707e\u5bb3\u9891\u53d1\uff0c\u4f46\u4f20\u7edf\u89c2\u6d4b\u65b9\u6cd5\u5728\u5927\u8303\u56f4\u548c\u590d\u6742\u5730\u5f62\u4e0b\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u3002", "method": "\u91c7\u7528\u9065\u611f\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u6570\u636e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u6ed1\u5761\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u68c0\u6d4b\u4efb\u52a1\u7684F1\u5206\u6570\u5206\u522b\u4e3a98.23\u548c93.83\uff0c\u5206\u5272\u4efb\u52a1\u7684mIoU\u5206\u6570\u4e3a63.74\u548c76.88\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u6ed1\u5761\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11412", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11412", "abs": "https://arxiv.org/abs/2507.11412", "authors": ["Orion Weller", "Kathryn Ricci", "Marc Marone", "Antoine Chaffin", "Dawn Lawrie", "Benjamin Van Durme"], "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders", "comment": null, "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Ettin\u6a21\u578b\u5957\u4ef6\uff0c\u6bd4\u8f83\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5728\u5206\u7c7b\u3001\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5404\u81ea\u64c5\u957f\u7684\u4efb\u52a1\u4e0d\u540c\uff0c\u4e14\u4e92\u76f8\u8f6c\u6362\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u7814\u7a76\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u907f\u514d\u4ee5\u5f80\u7814\u7a76\u4e2d\u53c2\u6570\u3001\u8bad\u7ec3\u6280\u672f\u548c\u6570\u636e\u96c6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u76f8\u540c\u8bad\u7ec3\u65b9\u6cd5\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\uff081700\u4e07\u523010\u4ebf\u53c2\u6570\uff09\uff0c\u5e76\u5728\u5206\u7c7b\u3001\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u6bd4\u8f83\u5176\u6027\u80fd\u3002", "result": "\u7f16\u7801\u5668\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u89e3\u7801\u5668\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u66f4\u4f18\uff1b\u4e92\u76f8\u8f6c\u6362\u6548\u679c\u4e0d\u5982\u76f4\u63a5\u4f7f\u7528\u5bf9\u5e94\u67b6\u6784\u3002", "conclusion": "\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5404\u6709\u4f18\u52bf\uff0c\u76f4\u63a5\u4f7f\u7528\u5bf9\u5e94\u67b6\u6784\u4f18\u4e8e\u4e92\u76f8\u8f6c\u6362\uff0c\u5f00\u6e90\u6240\u6709\u7814\u7a76\u8d44\u6599\u4ee5\u4f9b\u8fdb\u4e00\u6b65\u5206\u6790\u3002"}}
{"id": "2507.11152", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11152", "abs": "https://arxiv.org/abs/2507.11152", "authors": ["Duoyou Chen", "Yunqing Chen", "Can Zhang", "Zhou Wang", "Cheng Chen", "Ruoxiu Xiao"], "title": "Latent Space Consistency for Sparse-View CT Reconstruction", "comment": "ACMMM2025 Accepted", "summary": "Computed Tomography (CT) is a widely utilized imaging modality in clinical\nsettings. Using densely acquired rotational X-ray arrays, CT can capture 3D\nspatial features. However, it is confronted with challenged such as significant\ntime consumption and high radiation exposure. CT reconstruction methods based\non sparse-view X-ray images have garnered substantial attention from\nresearchers as they present a means to mitigate costs and risks. In recent\nyears, diffusion models, particularly the Latent Diffusion Model (LDM), have\ndemonstrated promising potential in the domain of 3D CT reconstruction.\nNonetheless, due to the substantial differences between the 2D latent\nrepresentation of X-ray modalities and the 3D latent representation of CT\nmodalities, the vanilla LDM is incapable of achieving effective alignment\nwithin the latent space. To address this issue, we propose the Consistent\nLatent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature\ncontrastive learning to efficiently extract latent 3D information from 2D X-ray\nimages and achieve latent space alignment between modalities. Experimental\nresults indicate that CLS-DM outperforms classical and state-of-the-art\ngenerative models in terms of standard voxel-level metrics (PSNR, SSIM) on the\nLIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing\nthe effectiveness and economic viability of sparse X-ray reconstructed CT but\ncan also be generalized to other cross-modal transformation tasks, such as\ntext-to-image synthesis. We have made our code publicly available at\nhttps://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research\nand applications in other domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLS-DM\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e862D X\u5c04\u7ebf\u56fe\u50cf\u4e0e3D CT\u56fe\u50cf\u5728\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edfCT\u6210\u50cf\u5b58\u5728\u65f6\u95f4\u6d88\u8017\u5927\u548c\u8f90\u5c04\u66b4\u9732\u9ad8\u7684\u95ee\u9898\uff0c\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u65b9\u6cd5\u53ef\u4ee5\u964d\u4f4e\u6210\u672c\u4e0e\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86CLS-DM\u6a21\u578b\uff0c\u5229\u7528\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ece2D X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u63d0\u53d63D\u6f5c\u5728\u4fe1\u606f\uff0c\u5e76\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5728LIDC-IDRI\u548cCTSpine1K\u6570\u636e\u96c6\u4e0a\uff0cCLS-DM\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u4f18\u4e8e\u7ecf\u5178\u548c\u6700\u65b0\u751f\u6210\u6a21\u578b\u3002", "conclusion": "CLS-DM\u4e0d\u4ec5\u63d0\u5347\u4e86\u7a00\u758fX\u5c04\u7ebf\u91cd\u5efaCT\u7684\u6548\u679c\u548c\u7ecf\u6d4e\u6027\uff0c\u8fd8\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u8de8\u6a21\u6001\u8f6c\u6362\u4efb\u52a1\uff0c\u5982\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3002"}}
{"id": "2507.11423", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11423", "abs": "https://arxiv.org/abs/2507.11423", "authors": ["Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Thierry Charnois"], "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "comment": null, "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u903b\u8f91\u95ee\u9898\u89e3\u51b3\u7684\u5f71\u54cd\u3002", "motivation": "LLMs\u901a\u5e38\u503e\u5411\u4e8e\u5355\u4e00\u63a8\u7406\u7b56\u7565\uff0c\u53ef\u80fd\u9650\u5236\u5176\u5728\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u5bf9LLMs\u63a8\u7406\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u65b9\u6cd5\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5355\u4e00\u7b56\u7565\u65e0\u6cd5\u6301\u7eed\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u9002\u5e94\u6027\u9009\u62e9\u7b56\u7565\u53ef\u6539\u5584\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4f18\u5316LLMs\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u5f3a\u8c03\u7b56\u7565\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11153", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11153", "abs": "https://arxiv.org/abs/2507.11153", "authors": ["Hongfei Ye", "Bin Chen", "Wenxi Liu", "Yu Zhang", "Zhao Li", "Dandan Ni", "Hongyang Chen"], "title": "Assessing Color Vision Test in Large Vision-language Models", "comment": null, "summary": "With the widespread adoption of large vision-language models, the capacity\nfor color vision in these models is crucial. However, the color vision\nabilities of large visual-language models have not yet been thoroughly\nexplored. To address this gap, we define a color vision testing task for large\nvision-language models and construct a dataset \\footnote{Anonymous Github\nShowing some of the data\nhttps://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers\nmultiple categories of test questions and tasks of varying difficulty levels.\nFurthermore, we analyze the types of errors made by large vision-language\nmodels and propose fine-tuning strategies to enhance their performance in color\nvision tests.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8272\u5f69\u89c6\u89c9\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u6d4b\u8bd5\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u9519\u8bef\u7c7b\u578b\u53ca\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8272\u5f69\u89c6\u89c9\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u4e86\u8272\u5f69\u89c6\u89c9\u6d4b\u8bd5\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u591a\u7c7b\u522b\u3001\u591a\u96be\u5ea6\u7ea7\u522b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u7684\u9519\u8bef\u7c7b\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u5fae\u8c03\u7b56\u7565\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u8272\u5f69\u89c6\u89c9\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u6d4b\u8bd5\u548c\u4f18\u5316\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8272\u5f69\u89c6\u89c9\u80fd\u529b\u3002"}}
{"id": "2507.11502", "categories": ["cs.CL", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11502", "abs": "https://arxiv.org/abs/2507.11502", "authors": ["Sirui Han", "Junqi Zhu", "Ruiyuan Zhang", "Yike Guo"], "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong", "comment": null, "summary": "This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities.", "AI": {"tldr": "HKGAI-V1\u662f\u4e00\u4e2a\u4e3a\u9999\u6e2f\u5b9a\u5236\u7684\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u8c03\u6574\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u672c\u5730\u6587\u5316\u3001\u6cd5\u5f8b\u548c\u4ef7\u503c\u89c2\u9700\u6c42\uff0c\u5e76\u5728\u654f\u611f\u67e5\u8be2\u548c\u6570\u5b57\u4e3b\u6743\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u9999\u6e2f\u5efa\u7acb\u7b26\u5408\u672c\u5730\u591a\u8bed\u8a00\u73af\u5883\u3001\u6cd5\u5f8b\u6846\u67b6\u548c\u6587\u5316\u4ef7\u503c\u89c2\u7684AI\u57fa\u7840\u8bbe\u65bd\uff0c\u786e\u4fddAI\u5e94\u7528\u5728\u5173\u952e\u9886\u57df\u7684\u53ef\u63a7\u6027\u3002", "method": "\u57fa\u4e8eDeepSeek\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u53c2\u6570\u5fae\u8c03\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u5f00\u53d1\u4e86HKGAI-V1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5bf9\u6297\u6027\u9999\u6e2f\u4ef7\u503c\u89c2\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "HKGAI-V1\u5728\u672c\u5730\u654f\u611f\u67e5\u8be2\u5904\u7406\u4e0a\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u6570\u5b57\u4e3b\u6743\uff1b\u5f00\u53d1\u4e86\u8bc4\u4f30\u5de5\u5177Adversarial HK Value Benchmark\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u6280\u672f\u6210\u679c\u548c\u53ef\u590d\u5236\u7684\u84dd\u56fe\uff0c\u652f\u6301\u5f00\u53d1\u7b26\u5408\u533a\u57df\u7279\u8272\u7684\u9ad8\u7ea7AI\u7cfb\u7edf\u3002"}}
{"id": "2507.11171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11171", "abs": "https://arxiv.org/abs/2507.11171", "authors": ["Jun Chen", "Yonghua Yu", "Weifu Li", "Yaohui Chen", "Hong Chen"], "title": "Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification", "comment": "11 pages, 5 figures", "summary": "Citrus, as one of the most economically important fruit crops globally,\nsuffers severe yield depressions due to various diseases. Accurate disease\ndetection and classification serve as critical prerequisites for implementing\ntargeted control measures. Recent advancements in artificial intelligence,\nparticularly deep learning-based computer vision algorithms, have substantially\ndecreased time and labor requirements while maintaining the accuracy of\ndetection and classification. Nevertheless, these methods predominantly rely on\nmassive, high-quality annotated training examples to attain promising\nperformance. By introducing two key designs: contrasting with cluster centroids\nand a multi-layer contrastive training (MCT) paradigm, this paper proposes a\nnovel clustering-guided self-supervised multi-layer contrastive representation\nlearning (CMCRL) algorithm. The proposed method demonstrates several advantages\nover existing counterparts: (1) optimizing with massive unannotated samples;\n(2) effective adaptation to the symptom similarity across distinct citrus\ndiseases; (3) hierarchical feature representation learning. The proposed method\nachieves state-of-the-art performance on the public citrus image set CDD,\noutperforming existing methods by 4.5\\%-30.1\\% accuracy. Remarkably, our method\nnarrows the performance gap with fully supervised counterparts (all samples are\nlabeled). Beyond classification accuracy, our method shows great performance on\nother evaluation metrics (F1 score, precision, and recall), highlighting the\nrobustness against the class imbalance challenge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMCRL\u7684\u81ea\u76d1\u7763\u591a\u5c42\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u5f15\u5bfc\u548c\u5bf9\u6bd4\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67d1\u6a58\u75c5\u5bb3\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u67d1\u6a58\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u4ea7\u91cf\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800cCMCRL\u65e8\u5728\u5229\u7528\u65e0\u6807\u6ce8\u6570\u636e\u4f18\u5316\u6027\u80fd\u3002", "method": "\u5f15\u5165\u805a\u7c7b\u4e2d\u5fc3\u5bf9\u6bd4\u548c\u591a\u5c42\u5bf9\u6bd4\u8bad\u7ec3\uff08MCT\uff09\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u9002\u5e94\u75c7\u72b6\u76f8\u4f3c\u6027\u548c\u5206\u5c42\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6CDD\u4e0a\uff0cCMCRL\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53474.5%-30.1%\uff0c\u5e76\u5728F1\u5206\u6570\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CMCRL\u4e0d\u4ec5\u7f29\u5c0f\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u8fd8\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11508", "abs": "https://arxiv.org/abs/2507.11508", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Ond\u0159ej Du\u0161ek", "Saad Mahamood"], "title": "Real-World Summarization: When Evaluation Reaches Its Limits", "comment": null, "summary": "We examine evaluation of faithfulness to input data in the context of hotel\nhighlights: brief LLM-generated summaries that capture unique features of\naccommodations. Through human evaluation campaigns involving categorical error\nassessment and span-level annotation, we compare traditional metrics, trainable\nmethods, and LLM-as-a-judge approaches. Our findings reveal that simpler\nmetrics like word overlap correlate surprisingly well with human judgments\n(Spearman correlation rank of 0.63), often outperforming more complex methods\nwhen applied to out-of-domain data. We further demonstrate that while LLMs can\ngenerate high-quality highlights, they prove unreliable for evaluation as they\ntend to severely under- or over-annotate. Our analysis of real-world business\nimpacts shows incorrect and non-checkable information pose the greatest risks.\nWe also highlight challenges in crowdsourced evaluations.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u751f\u6210\u7684\u9152\u5e97\u6458\u8981\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u7b80\u5355\u6307\u6807\uff08\u5982\u8bcd\u91cd\u53e0\uff09\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u9ad8\uff0c\u800cLLM\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u4e0d\u53ef\u9760\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u6709\u6548\u8bc4\u4f30LLM\u751f\u6210\u7684\u9152\u5e97\u6458\u8981\u5bf9\u8f93\u5165\u6570\u636e\u7684\u5fe0\u5b9e\u6027\uff0c\u4ee5\u89e3\u51b3\u5b9e\u9645\u4e1a\u52a1\u4e2d\u7684\u98ce\u9669\u548c\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u6d3b\u52a8\uff08\u5206\u7c7b\u9519\u8bef\u8bc4\u4f30\u548c\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff09\uff0c\u6bd4\u8f83\u4f20\u7edf\u6307\u6807\u3001\u53ef\u8bad\u7ec3\u65b9\u6cd5\u548cLLM\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u7684\u6548\u679c\u3002", "result": "\u7b80\u5355\u6307\u6807\uff08\u5982\u8bcd\u91cd\u53e0\uff09\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u9ad8\uff08Spearman\u79e9\u76f8\u5173\u7cfb\u65700.63\uff09\uff0c\u800cLLM\u8bc4\u4f30\u5de5\u5177\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6458\u8981\u8d28\u91cf\u9ad8\uff0c\u4f46\u8bc4\u4f30\u4e0d\u53ef\u9760\uff1b\u7b80\u5355\u6307\u6807\u5728\u8de8\u9886\u57df\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9700\u6ce8\u610f\u4e1a\u52a1\u98ce\u9669\u548c\u4f17\u5305\u8bc4\u4f30\u7684\u6311\u6218\u3002"}}
{"id": "2507.11200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11200", "abs": "https://arxiv.org/abs/2507.11200", "authors": ["Che Liu", "Jiazhen Pan", "Weixiang Shen", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study", "comment": "Accepted by the International Conference on AI in Healthcare 2025", "summary": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5f00\u6e90\u901a\u7528\u548c\u533b\u5b66\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u7528\u5927\u6a21\u578b\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u5df2\u4f18\u4e8e\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4ecd\u662f\u74f6\u9888\uff0c\u4e14\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e863B\u81f372B\u53c2\u6570\u7684VLMs\u5728\u516b\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u4e3a\u7406\u89e3\u548c\u63a8\u7406\u4e24\u90e8\u5206\u3002", "result": "\u901a\u7528\u5927\u6a21\u578b\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u6027\u80fd\u56e0\u4efb\u52a1\u8bbe\u8ba1\u3001\u6807\u6ce8\u8d28\u91cf\u548c\u77e5\u8bc6\u9700\u6c42\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5c1a\u672a\u8fbe\u5230\u4e34\u5e8a\u90e8\u7f72\u7684\u53ef\u9760\u6027\u6807\u51c6\uff0c\u9700\u52a0\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2507.11202", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11202", "abs": "https://arxiv.org/abs/2507.11202", "authors": ["Xinkui Zhao", "Jinsong Shu", "Yangyang Wu", "Guanjie Cheng", "Zihe Liu", "Naibo Wang", "Shuiguang Deng", "Zhongle Xie", "Jianwei Yin"], "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition", "comment": null, "summary": "Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCULoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u7ec4\u5408\u7684\u5171\u4eab\u4fe1\u606f\u548c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6bd4\u4f8b\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u6a21\u6001\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u6216\u9690\u79c1\u95ee\u9898\u4e0d\u5b8c\u6574\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u6a21\u6001\u7ec4\u5408\u8bad\u7ec3\u68af\u5ea6\u51b2\u7a81\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "MCULoRA\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aMCLA\uff08\u89e3\u8026\u6a21\u6001\u7ec4\u5408\u4fe1\u606f\uff09\u548cDPFT\uff08\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6bd4\u4f8b\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMCULoRA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MCULoRA\u4e3a\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53c2\u6570\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2507.11245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11245", "abs": "https://arxiv.org/abs/2507.11245", "authors": ["X. Feng", "H. Yu", "M. Wu", "S. Hu", "J. Chen", "C. Zhu", "J. Wu", "X. Chu", "K. Huang"], "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models", "comment": "Project Page: https://amap-ml.github.io/NarrLV-Website/", "summary": "With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86NarrLV\uff0c\u9996\u4e2a\u5168\u9762\u8bc4\u4f30\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7Temporal Narrative Atom\uff08TNA\uff09\u548c\u81ea\u52a8\u63d0\u793a\u751f\u6210\u7ba1\u9053\u91cf\u5316\u53d9\u4e8b\u4e30\u5bcc\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8eMLLM\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u73b0\u6709\u57fa\u51c6\uff08\u5982VBench\uff09\u4ec5\u652f\u6301\u7b80\u5355\u53d9\u4e8b\u63d0\u793a\u3002", "method": "1. \u5f15\u5165TNA\u4f5c\u4e3a\u57fa\u672c\u53d9\u4e8b\u5355\u5143\uff1b2. \u6784\u5efa\u81ea\u52a8\u63d0\u793a\u751f\u6210\u7ba1\u9053\uff1b3. \u8bbe\u8ba1\u57fa\u4e8eMLLM\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u53d9\u4e8b\u8868\u8fbe\u4e0a\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "NarrLV\u4e3a\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u53d9\u4e8b\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.11247", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11247", "abs": "https://arxiv.org/abs/2507.11247", "authors": ["Veronika Shilova", "Emmanuel Malherbe", "Giovanni Palma", "Laurent Risser", "Jean-Michel Loubes"], "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone", "comment": null, "summary": "Within a legal framework, fairness in datasets and models is typically\nassessed by dividing observations into predefined groups and then computing\nfairness measures (e.g., Disparate Impact or Equality of Odds with respect to\ngender). However, when sensitive attributes such as skin color are continuous,\ndividing into default groups may overlook or obscure the discrimination\nexperienced by certain minority subpopulations. To address this limitation, we\npropose a fairness-based grouping approach for continuous (possibly\nmultidimensional) sensitive attributes. By grouping data according to observed\nlevels of discrimination, our method identifies the partition that maximizes a\nnovel criterion based on inter-group variance in discrimination, thereby\nisolating the most critical subgroups.\n  We validate the proposed approach using multiple synthetic datasets and\ndemonstrate its robustness under changing population distributions - revealing\nhow discrimination is manifested within the space of sensitive attributes.\nFurthermore, we examine a specialized setting of monotonic fairness for the\ncase of skin color. Our empirical results on both CelebA and FFHQ, leveraging\nthe skin tone as predicted by an industrial proprietary algorithm, show that\nthe proposed segmentation uncovers more nuanced patterns of discrimination than\npreviously reported, and that these findings remain stable across datasets for\na given model. Finally, we leverage our grouping model for debiasing purpose,\naiming at predicting fair scores with group-by-group post-processing. The\nresults demonstrate that our approach improves fairness while having minimal\nimpact on accuracy, thus confirming our partition method and opening the door\nfor industrial deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516c\u5e73\u6027\u7684\u5206\u7ec4\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u654f\u611f\u5c5e\u6027\uff0c\u4ee5\u8bc6\u522b\u5173\u952e\u5b50\u7fa4\u4f53\u5e76\u4f18\u5316\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8fde\u7eed\u654f\u611f\u5c5e\u6027\uff08\u5982\u80a4\u8272\uff09\u5212\u5206\u4e3a\u9ed8\u8ba4\u7ec4\u53ef\u80fd\u5ffd\u7565\u5c11\u6570\u7fa4\u4f53\u7684\u6b67\u89c6\u95ee\u9898\uff0c\u9700\u66f4\u7cbe\u7ec6\u7684\u5206\u7ec4\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u6b67\u89c6\u6c34\u5e73\u5206\u7ec4\u6570\u636e\uff0c\u63d0\u51fa\u6700\u5927\u5316\u7ec4\u95f4\u6b67\u89c6\u65b9\u5dee\u7684\u65b0\u6807\u51c6\uff0c\u8bc6\u522b\u5173\u952e\u5b50\u7fa4\u4f53\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08CelebA\u3001FFHQ\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u66f4\u7ec6\u5fae\u7684\u6b67\u89c6\u6a21\u5f0f\uff0c\u5e76\u7528\u4e8e\u53bb\u504f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u516c\u5e73\u6027\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2507.11252", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11252", "abs": "https://arxiv.org/abs/2507.11252", "authors": ["Guanghao Wu", "Chen Xu", "Hai Song", "Chong Wang", "Qixing Zhang"], "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection", "comment": "18 pages, 11 figures", "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of\ndeep learning, image-based smoke detection has become a crucial method for\ndetecting and preventing forest fires. However, the scarcity of smoke image\ndata from forest fires is one of the significant factors hindering the\ndetection of forest fire smoke. Image generation models offer a promising\nsolution for synthesizing realistic smoke images. However, current inpainting\nmodels exhibit limitations in generating high-quality smoke representations,\nparticularly manifesting as inconsistencies between synthesized smoke and\nbackground contexts. To solve these problems, we proposed a comprehensive\nframework for generating forest fire smoke images. Firstly, we employed the\npre-trained segmentation model and the multimodal model to obtain smoke masks\nand image captions.Then, to address the insufficient utilization of masks and\nmasked images by inpainting models, we introduced a network architecture guided\nby mask and masked image features. We also proposed a new loss function, the\nmask random difference loss, which enhances the consistency of the generated\neffects around the mask by randomly expanding and eroding the mask\nedges.Finally, to generate a smoke image dataset using random masks for\nsubsequent detection tasks, we incorporated smoke characteristics and use a\nmultimodal large language model as a filtering tool to select diverse and\nreasonable smoke images, thereby improving the quality of the synthetic\ndataset. Experiments showed that our generated smoke images are realistic and\ndiverse, and effectively enhance the performance of forest fire smoke detection\nmodels. Code is available at https://github.com/wghr123/MFGDiffusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u4fee\u590d\u6a21\u578b\u548c\u5f15\u5165\u65b0\u635f\u5931\u51fd\u6570\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u70df\u96fe\u56fe\u50cf\uff0c\u63d0\u5347\u70df\u96fe\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u4fee\u590d\u6a21\u578b\u751f\u6210\u70df\u96fe\u56fe\u50cf\u8d28\u91cf\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u83b7\u53d6\u70df\u96fe\u63a9\u7801\u548c\u56fe\u50cf\u63cf\u8ff0\uff0c\u8bbe\u8ba1\u63a9\u7801\u548c\u63a9\u7801\u56fe\u50cf\u7279\u5f81\u5f15\u5bfc\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u51fa\u63a9\u7801\u968f\u673a\u5dee\u5f02\u635f\u5931\u51fd\u6570\u3002", "result": "\u751f\u6210\u7684\u70df\u96fe\u56fe\u50cf\u771f\u5b9e\u591a\u6837\uff0c\u6709\u6548\u63d0\u5347\u70df\u96fe\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u89e3\u51b3\u4e86\u70df\u96fe\u56fe\u50cf\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u70df\u96fe\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002"}}
{"id": "2507.11261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11261", "abs": "https://arxiv.org/abs/2507.11261", "authors": ["Ronggang Huang", "Haoxin Yang", "Yan Cai", "Xuemiao Xu", "Huaidong Zhang", "Shengfeng He"], "title": "ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition", "comment": "Accepted by ICCV 2025", "summary": "3D visual grounding aims to identify and localize objects in a 3D space based\non textual descriptions. However, existing methods struggle with disentangling\ntargets from anchors in complex multi-anchor queries and resolving\ninconsistencies in spatial descriptions caused by perspective variations. To\ntackle these challenges, we propose ViewSRD, a framework that formulates 3D\nvisual grounding as a structured multi-view decomposition process. First, the\nSimple Relation Decoupling (SRD) module restructures complex multi-anchor\nqueries into a set of targeted single-anchor statements, generating a\nstructured set of perspective-aware descriptions that clarify positional\nrelationships. These decomposed representations serve as the foundation for the\nMulti-view Textual-Scene Interaction (Multi-TSI) module, which integrates\ntextual and scene features across multiple viewpoints using shared, Cross-modal\nConsistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a\nTextual-Scene Reasoning module synthesizes multi-view predictions into a\nunified and robust 3D visual grounding. Experiments on 3D visual grounding\ndatasets show that ViewSRD significantly outperforms state-of-the-art methods,\nparticularly in complex queries requiring precise spatial differentiation.", "AI": {"tldr": "ViewSRD\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u89c6\u89d2\u5206\u89e3\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u89c6\u56fe\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u5728\u590d\u6742\u67e5\u8be2\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u951a\u70b9\u67e5\u8be2\u4e2d\u7684\u76ee\u6807\u89e3\u8026\u548c\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u7a7a\u95f4\u63cf\u8ff0\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u63d0\u51faViewSRD\u6846\u67b6\uff0c\u5305\u542b\u7b80\u5355\u5173\u7cfb\u89e3\u8026\u6a21\u5757\uff08SRD\uff09\u3001\u591a\u89c6\u89d2\u6587\u672c-\u573a\u666f\u4ea4\u4e92\u6a21\u5757\uff08Multi-TSI\uff09\u548c\u6587\u672c-\u573a\u666f\u63a8\u7406\u6a21\u5757\u3002", "result": "\u57283D\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\uff0cViewSRD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u533a\u5206\u7684\u590d\u6742\u67e5\u8be2\u4e2d\u3002", "conclusion": "ViewSRD\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u89c6\u89d2\u5206\u89e3\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u590d\u6742\u67e5\u8be2\u95ee\u9898\u3002"}}
{"id": "2507.11267", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11267", "abs": "https://arxiv.org/abs/2507.11267", "authors": ["Aon Safdar", "Usman Akram", "Waseem Anwar", "Basit Malik", "Mian Ibad Ali"], "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery", "comment": "Published in 25th Irish Machine Vision and Image Processing Conf.,\n  Galway, Ireland, Aug 30-Sep 1 2023 Also available at\n  https://doi.org/10.5281/zenodo.8264062", "summary": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared\n(TI) imagery in the defense and surveillance domain is a challenging computer\nvision (CV) task in comparison to the commercial autonomous vehicle perception\ndomain. Limited datasets, peculiar domain-specific and TI modality-specific\nchallenges, i.e., limited hardware, scale invariance issues due to greater\ndistances, deliberate occlusion by tactical vehicles, lower sensor resolution\nand resultant lack of structural information in targets, effects of weather,\ntemperature, and time of day variations, and varying target to clutter ratios\nall result in increased intra-class variability and higher inter-class\nsimilarity, making accurate real-time ATR a challenging CV task. Resultantly,\ncontemporary state-of-the-art (SOTA) deep learning architectures underperform\nin the ATR domain. We propose a modified anchor-based single-stage detector,\ncalled YOLOatr, based on a modified YOLOv5s, with optimal modifications to the\ndetection heads, feature fusion in the neck, and a custom augmentation profile.\nWe evaluate the performance of our proposed model on a comprehensive DSIAC MWIR\ndataset for real-time ATR over both correlated and decorrelated testing\nprotocols. The results demonstrate that our proposed model achieves\nstate-of-the-art ATR performance of up to 99.6%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668YOLOatr\uff0c\u7528\u4e8e\u70ed\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u591a\u9879\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e8699.6%\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u70ed\u7ea2\u5916\u56fe\u50cf\u5728\u56fd\u9632\u548c\u76d1\u63a7\u9886\u57df\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u6570\u636e\u96c6\u6709\u9650\u3001\u786c\u4ef6\u9650\u5236\u3001\u5929\u6c14\u5f71\u54cd\u7b49\uff0c\u5bfc\u81f4\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u6539\u8fdb\u7684YOLOv5s\uff0c\u4f18\u5316\u4e86\u68c0\u6d4b\u5934\u3001\u7279\u5f81\u878d\u5408\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u51fa\u4e86YOLOatr\u6a21\u578b\u3002", "result": "\u5728DSIAC MWIR\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cYOLOatr\u5728\u5b9e\u65f6ATR\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8699.6%\u7684SOTA\u6027\u80fd\u3002", "conclusion": "YOLOatr\u5728\u70ed\u7ea2\u5916\u56fe\u50cf\u76ee\u6807\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u7684\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2507.11279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11279", "abs": "https://arxiv.org/abs/2507.11279", "authors": ["Yujie Zhang", "Sabine Struckmeyer", "Andreas Kolb", "Sven Reichardt"], "title": "Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping", "comment": null, "summary": "Observer bias and inconsistencies in traditional plant phenotyping methods\nlimit the accuracy and reproducibility of fine-grained plant analysis. To\novercome these challenges, we developed TomatoMAP, a comprehensive dataset for\nSolanum lycopersicum using an Internet of Things (IoT) based imaging system\nwith standardized data acquisition protocols. Our dataset contains 64,464 RGB\nimages that capture 12 different plant poses from four camera elevation angles.\nEach image includes manually annotated bounding boxes for seven regions of\ninterest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,\naxillary shoot, shoot and whole plant area, along with 50 fine-grained growth\nstage classifications based on the BBCH scale. Additionally, we provide 3,616\nhigh-resolution image subset with pixel-wise semantic and instance segmentation\nannotations for fine-grained phenotyping. We validated our dataset using a\ncascading model deep learning framework combining MobileNetv3 for\nclassification, YOLOv11 for object detection, and MaskRCNN for segmentation.\nThrough AI vs. Human analysis involving five domain experts, we demonstrate\nthat the models trained on our dataset achieve accuracy and speed comparable to\nthe experts. Cohen's Kappa and inter-rater agreement heatmap confirm the\nreliability of automated fine-grained phenotyping using our approach.", "AI": {"tldr": "TomatoMAP\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u8054\u7f51\u7684\u756a\u8304\u690d\u7269\u8868\u578b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u91c7\u96c6\u534f\u8bae\u63d0\u4f9b64,464\u5f20RGB\u56fe\u50cf\uff0c\u5305\u542b7\u4e2a\u533a\u57df\u7684\u6807\u6ce8\u548c50\u4e2a\u751f\u957f\u9636\u6bb5\u5206\u7c7b\u3002\u9a8c\u8bc1\u8868\u660e\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e0a\u4e0e\u4e13\u5bb6\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u690d\u7269\u8868\u578b\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u89c2\u5bdf\u8005\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u7cbe\u7ec6\u690d\u7269\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u5f00\u53d1\u4e86TomatoMAP\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u7269\u8054\u7f51\u6210\u50cf\u7cfb\u7edf\u91c7\u96c6\u6807\u51c6\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7MobileNetv3\u3001YOLOv11\u548cMaskRCNN\u7ec4\u6210\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e0a\u4e0e\u4e13\u5bb6\u76f8\u5f53\uff0cCohen's Kappa\u548c\u8bc4\u5206\u8005\u4e00\u81f4\u6027\u70ed\u56fe\u9a8c\u8bc1\u4e86\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "conclusion": "TomatoMAP\u4e3a\u7cbe\u7ec6\u690d\u7269\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u96c6\u548c\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11287", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11287", "abs": "https://arxiv.org/abs/2507.11287", "authors": ["An-Lun Liu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers", "comment": "Accepted by ICCV 2025", "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u4eba\u4f53\u6293\u53d6\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u63a5\u89e6\u56fe\u7ed3\u5408\u573a\u666f\u548c\u4efb\u52a1\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6293\u53d6\u5408\u6210\u65b9\u6cd5\u4ec5\u5173\u6ce8\u7269\u4f53\u4e0e\u624b\u7684\u5173\u7cfb\uff0c\u7f3a\u4e4f\u5bf9\u573a\u666f\u548c\u4efb\u52a1\u7684\u8003\u8651\uff0c\u5bfc\u81f4\u6293\u53d6\u59ff\u52bf\u4e0e\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u751f\u6210\u4efb\u52a1\u611f\u77e5\u63a5\u89e6\u56fe\uff0c\u968f\u540e\u5229\u7528\u8be5\u56fe\u5408\u6210\u4efb\u52a1\u5bfc\u5411\u7684\u6293\u53d6\u59ff\u52bf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u5408\u573a\u666f\u548c\u4efb\u52a1\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u65b0\u65b9\u6cd5\u5728\u6293\u53d6\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4efb\u52a1\u611f\u77e5\u63a5\u89e6\u56fe\u662f\u5b9e\u73b0\u9ad8\u6548\u4efb\u52a1\u5bfc\u5411\u6293\u53d6\u7684\u5173\u952e\uff0c\u65b0\u65b9\u6cd5\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11293", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11293", "abs": "https://arxiv.org/abs/2507.11293", "authors": ["J. Senthilnath", "Chen Hao", "F. C. Wellstood"], "title": "3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images", "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In semiconductor packaging, accurately recovering 3D information is crucial\nfor non-destructive testing (NDT) to localize circuit defects. This paper\npresents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),\nwhich leverages Magnetic Field Images (MFI) to retrieve the parameters for the\n3D current flow of a single-segment. The 3D MIR integrates a deep learning\n(DL)-based Convolutional Neural Network (CNN), spatial-physics-based\nconstraints, and optimization techniques. The method operates in three stages:\ni) The CNN model processes the MFI data to predict ($\\ell/z_o$), where $\\ell$\nis the wire length and $z_o$ is the wire's vertical depth beneath the magnetic\nsensors and classify segment type ($c$). ii) By leveraging\nspatial-physics-based constraints, the routine provides initial estimates for\nthe position ($x_o$, $y_o$, $z_o$), length ($\\ell$), current ($I$), and current\nflow direction (positive or negative) of the current segment. iii) An optimizer\nthen adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\\ell$, $I$) to\nminimize the difference between the reconstructed MFI and the actual MFI. The\nresults demonstrate that the 3D MIR method accurately recovers 3D information\nwith high precision, setting a new benchmark for magnetic image reconstruction\nin semiconductor packaging. This method highlights the potential of combining\nDL and physics-driven optimization in practical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D MIR\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7269\u7406\u7ea6\u675f\uff0c\u7528\u4e8e\u4ece\u78c1\u573a\u56fe\u50cf\u4e2d\u6062\u590d\u534a\u5bfc\u4f53\u5c01\u88c5\u4e2d\u76843D\u7535\u6d41\u4fe1\u606f\u3002", "motivation": "\u5728\u534a\u5bfc\u4f53\u5c01\u88c5\u4e2d\uff0c\u51c6\u786e\u6062\u590d3D\u4fe1\u606f\u5bf9\u4e8e\u65e0\u635f\u68c0\u6d4b\u548c\u7535\u8def\u7f3a\u9677\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\u3002", "method": "3D MIR\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1) CNN\u5904\u7406\u78c1\u573a\u56fe\u50cf\u9884\u6d4b\u53c2\u6570\uff1b2) \u5229\u7528\u7a7a\u95f4\u7269\u7406\u7ea6\u675f\u63d0\u4f9b\u521d\u59cb\u4f30\u8ba1\uff1b3) \u4f18\u5316\u5668\u8c03\u6574\u53c2\u6570\u4ee5\u6700\u5c0f\u5316\u91cd\u5efa\u4e0e\u5b9e\u9645\u78c1\u573a\u7684\u5dee\u5f02\u3002", "result": "3D MIR\u65b9\u6cd5\u80fd\u591f\u9ad8\u7cbe\u5ea6\u6062\u590d3D\u4fe1\u606f\uff0c\u4e3a\u534a\u5bfc\u4f53\u5c01\u88c5\u4e2d\u7684\u78c1\u573a\u56fe\u50cf\u91cd\u5efa\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u7269\u7406\u9a71\u52a8\u4f18\u5316\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11301", "abs": "https://arxiv.org/abs/2507.11301", "authors": ["Pa\u00fal Maji", "Marlon T\u00faquerres", "Stalin Valencia", "Marcela Valenzuela", "Christian Mejia-Escobar"], "title": "Detecci\u00f3n y Cuantificaci\u00f3n de Erosi\u00f3n Fluvial con Visi\u00f3n Artificial", "comment": "18 pages, in Spanish language, 13 figures, 4 tables", "summary": "Fluvial erosion is a natural process that can generate significant impacts on\nsoil stability and strategic infrastructures. The detection and monitoring of\nthis phenomenon is traditionally addressed by photogrammetric methods and\nanalysis in geographic information systems. These tasks require specific\nknowledge and intensive manual processing. This study proposes an artificial\nintelligence-based approach for automatic identification of eroded zones and\nestimation of their area. The state-of-the-art computer vision model YOLOv11,\nadjusted by fine-tuning and trained with photographs and LiDAR images, is used.\nThis combined dataset was segmented and labeled using the Roboflow platform.\nExperimental results indicate efficient detection of erosion patterns with an\naccuracy of 70%, precise identification of eroded areas and reliable\ncalculation of their extent in pixels and square meters. As a final product,\nthe EROSCAN system has been developed, an interactive web application that\nallows users to upload images and obtain automatic segmentations of fluvial\nerosion, together with the estimated area. This tool optimizes the detection\nand quantification of the phenomenon, facilitating decision making in risk\nmanagement and territorial planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\uff0c\u5229\u7528YOLOv11\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u4fb5\u8680\u533a\u57df\u5e76\u4f30\u7b97\u9762\u79ef\uff0c\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u7f51\u9875\u5e94\u7528EROSCAN\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\u548c\u624b\u52a8\u5904\u7406\uff0c\u6548\u7387\u4f4e\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528YOLOv11\u6a21\u578b\uff0c\u7ed3\u5408\u7167\u7247\u548cLiDAR\u56fe\u50cf\uff0c\u901a\u8fc7Roboflow\u5e73\u53f0\u8fdb\u884c\u6570\u636e\u6807\u6ce8\u548c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u51c6\u786e\u738770%\uff0c\u80fd\u7cbe\u786e\u8bc6\u522b\u4fb5\u8680\u533a\u57df\u5e76\u8ba1\u7b97\u9762\u79ef\uff0c\u5f00\u53d1\u4e86EROSCAN\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86\u4fb5\u8680\u68c0\u6d4b\u548c\u91cf\u5316\uff0c\u652f\u6301\u98ce\u9669\u7ba1\u7406\u548c\u571f\u5730\u89c4\u5212\u51b3\u7b56\u3002"}}
{"id": "2507.11321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11321", "abs": "https://arxiv.org/abs/2507.11321", "authors": ["Haoxuan Qu", "Yujun Cai", "Hossein Rahmani", "Ajay Kumar", "Junsong Yuan", "Jun Liu"], "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction", "comment": null, "summary": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface\nreconstruction. However, while 3D objects can be of complex and diverse shapes\nin the real world, existing GS-based methods only limitedly use a single type\nof splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent\nobject surfaces during their reconstruction. In this paper, we highlight that\nthis can be insufficient for object surfaces to be represented in high quality.\nThus, we propose a novel framework that, for the first time, enables Gaussian\nSplatting to incorporate multiple types of (geometrical) primitives during its\nsurface reconstruction process. Specifically, in our framework, we first\npropose a compositional splatting strategy, enabling the splatting and\nrendering of different types of primitives in the Gaussian Splatting pipeline.\nIn addition, we also design our framework with a mixed-primitive-based\ninitialization strategy and a vertex pruning mechanism to further promote its\nsurface representation learning process to be well executed leveraging\ndifferent types of primitives. Extensive experiments show the efficacy of our\nframework and its accurate surface reconstruction performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u9996\u6b21\u5f15\u5165\u591a\u79cd\u51e0\u4f55\u57fa\u5143\u4ee5\u63d0\u9ad8\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u4e00\u57fa\u5143\uff08\u692d\u5706\u6216\u692d\u7403\uff09\u8868\u793a\u590d\u6742\u591a\u6837\u7684\u7269\u4f53\u8868\u9762\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u7ec4\u5408\u6cfc\u6e85\u7b56\u7565\u3001\u6df7\u5408\u57fa\u5143\u521d\u59cb\u5316\u7b56\u7565\u548c\u9876\u70b9\u4fee\u526a\u673a\u5236\uff0c\u652f\u6301\u591a\u79cd\u57fa\u5143\u7684\u9ad8\u65af\u6cfc\u6e85\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u8868\u9762\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u591a\u79cd\u57fa\u5143\uff0c\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u65af\u6cfc\u6e85\u7684\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2507.11325", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11325", "abs": "https://arxiv.org/abs/2507.11325", "authors": ["Arefin Ittesafun Abian", "Ripon Kumar Debnath", "Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Asif Karim", "Reem E. Mohamed", "Sami Azam"], "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging", "comment": "10 figures. Will be submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences", "summary": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the\n3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of\n1.525 mm, and VOE of 19.71%, indicating strong generalization across different\ndatasets. These results confirm the effectiveness and robustness of HANS-Net in\nproviding anatomically consistent, accurate, and confident liver and tumor\nsegmentation.", "AI": {"tldr": "HANS-Net\u662f\u4e00\u79cd\u65b0\u578b\u809d\u810f\u548c\u80bf\u7624\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u66f2\u5377\u79ef\u3001\u591a\u5c3a\u5ea6\u7eb9\u7406\u5b66\u4e60\u548c\u751f\u7269\u542f\u53d1\u7684\u53ef\u5851\u6027\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8179\u90e8CT\u56fe\u50cf\u4e2d\u809d\u810f\u548c\u80bf\u7624\u5206\u5272\u7684\u6311\u6218\uff0c\u5982\u590d\u6742\u89e3\u5256\u7ed3\u6784\u3001\u80bf\u7624\u5916\u89c2\u591a\u53d8\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u3002", "method": "\u7ed3\u5408\u53cc\u66f2\u5377\u79ef\u3001\u5c0f\u6ce2\u5206\u89e3\u6a21\u5757\u3001\u7a81\u89e6\u53ef\u5851\u6027\u673a\u5236\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6ce8\u610f\u529b\u3002", "result": "\u5728LiTS\u6570\u636e\u96c6\u4e0aDice\u5206\u657093.26%\uff0cIoU 88.09%\uff1b\u57283D-IRCADb-01\u6570\u636e\u96c6\u4e0aDice 87.45%\uff0cIoU 80.30%\u3002", "conclusion": "HANS-Net\u5728\u809d\u810f\u548c\u80bf\u7624\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u9c81\u68d2\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.11333", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11333", "abs": "https://arxiv.org/abs/2507.11333", "authors": ["Jianfei Jiang", "Qiankun Liu", "Haochen Yu", "Hongyuan Liu", "Liyong Wang", "Jiansheng Chen", "Huimin Ma"], "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network", "comment": "Accepted by ICCV 2025", "summary": "Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for\na sequence of calibrated images to recover dense point clouds. However,\nexisting MVS methods often struggle with challenging regions, such as\ntextureless regions and reflective surfaces, where feature matching fails. In\ncontrast, monocular depth estimation inherently does not require feature\nmatching, allowing it to achieve robust relative depth estimation in these\nregions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature\nand depth guided MVS network that integrates powerful priors from a monocular\nfoundation model into multi-view geometry. Firstly, the monocular feature of\nthe reference view is integrated into source view features by the attention\nmechanism with a newly designed cross-view position encoding. Then, the\nmonocular depth of the reference view is aligned to dynamically update the\ndepth candidates for edge regions during the sampling procedure. Finally, a\nrelative consistency loss is further designed based on the monocular depth to\nsupervise the depth prediction. Extensive experiments demonstrate that\nMonoMVSNet achieves state-of-the-art performance on the DTU and\nTanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate\nand Advanced benchmarks. The source code is available at\nhttps://github.com/JianfeiJ/MonoMVSNet.", "AI": {"tldr": "MonoMVSNet\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\uff0c\u901a\u8fc7\u5355\u76ee\u7279\u5f81\u548c\u6df1\u5ea6\u5f15\u5bfc\u6539\u8fdbMVS\u5728\u7eb9\u7406\u7f3a\u5931\u548c\u53cd\u5c04\u533a\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MVS\u65b9\u6cd5\u5728\u7eb9\u7406\u7f3a\u5931\u548c\u53cd\u5c04\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65e0\u9700\u7279\u5f81\u5339\u914d\uff0c\u80fd\u63d0\u4f9b\u9c81\u68d2\u7684\u76f8\u5bf9\u6df1\u5ea6\u3002", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5355\u76ee\u7279\u5f81\uff0c\u52a8\u6001\u66f4\u65b0\u6df1\u5ea6\u5019\u9009\uff0c\u5e76\u8bbe\u8ba1\u76f8\u5bf9\u4e00\u81f4\u6027\u635f\u5931\u76d1\u7763\u6df1\u5ea6\u9884\u6d4b\u3002", "result": "\u5728DTU\u548cTanks-and-Temples\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "MonoMVSNet\u901a\u8fc7\u5355\u76ee\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u4e86MVS\u5728\u6311\u6218\u6027\u533a\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11336", "abs": "https://arxiv.org/abs/2507.11336", "authors": ["Peiran Wu", "Yunze Liu", "Zhengdong Zhu", "Enmin Zhou", "Shawn Shen"], "title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks", "comment": null, "summary": "Real-world user-generated videos, especially on platforms like TikTok, often\nfeature rich and intertwined audio visual content. However, existing video\ncaptioning benchmarks and models remain predominantly visual centric,\noverlooking the crucial role of audio in conveying scene dynamics, speaker\nintent, and narrative context. This lack of omni datasets and lightweight,\ncapable models hampers progress in fine grained, multimodal video\nunderstanding. To address these challenges, we introduce UGC-VideoCap, a new\nbenchmark and model framework specifically designed for detailed omnimodal\ncaptioning of short form user-generated videos. Unlike prior datasets,\nUGC-VideoCap emphasizes balanced integration of audio and visual modalities,\nfeaturing 1000 TikTok videos annotated through a structured three stage\nhuman-in-the-loop pipeline covering audio only, visual only, and joint audio\nvisual semantics. The benchmark also includes 4000 carefully crafted QA pairs\nprobing both unimodal and cross modal understanding. Alongside the dataset, we\npropose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from\nGemini 2.5 Flash. Using a novel two-stage training strategy supervised fine\ntuning followed by Group Relative Policy Optimization (GRPO), our approach\nenables efficient adaptation from limited data while maintaining competitive\nperformance. Together, our benchmark and model offer a high-quality foundation\nand a data-efficient solution for advancing omnimodal video captioning in\nunconstrained real-world UGC settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UGC-VideoCap\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u97f3\u9891\u548c\u89c6\u89c9\u5e73\u8861\u6574\u5408\u7684\u77ed\u89c6\u9891\u5b57\u5e55\u751f\u6210\u57fa\u51c6\u548c\u6a21\u578b\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u97f3\u9891\u91cd\u8981\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\uff0c\u5ffd\u89c6\u4e86\u97f3\u9891\u5728\u4f20\u8fbe\u573a\u666f\u52a8\u6001\u548c\u53d9\u4e8b\u80cc\u666f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u9ad8\u6548\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86UGC-VideoCap\u57fa\u51c6\uff0c\u5305\u542b1000\u4e2aTikTok\u89c6\u9891\u548c4000\u4e2aQA\u5bf9\uff0c\u5e76\u5f00\u53d1\u4e86UGC-VideoCaptioner(3B)\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03+GRPO\uff09\u3002", "result": "UGC-VideoCap\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u8868\u73b0\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u5f02\u3002", "conclusion": "UGC-VideoCap\u4e3a\u65e0\u7ea6\u675f\u7684\u771f\u5b9e\u7528\u6237\u751f\u6210\u89c6\u9891\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u57fa\u51c6\u548c\u6570\u636e\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u89c6\u9891\u5b57\u5e55\u751f\u6210\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11372", "abs": "https://arxiv.org/abs/2507.11372", "authors": ["Pierrick Leroy", "Antonio Mastropietro", "Marco Nurisso", "Francesco Vaccarino"], "title": "Attributes Shape the Embedding Space of Face Recognition Models", "comment": null, "summary": "Face Recognition (FR) tasks have made significant progress with the advent of\nDeep Neural Networks, particularly through margin-based triplet losses that\nembed facial images into high-dimensional feature spaces. During training,\nthese contrastive losses focus exclusively on identity information as labels.\nHowever, we observe a multiscale geometric structure emerging in the embedding\nspace, influenced by interpretable facial (e.g., hair color) and image\nattributes (e.g., contrast). We propose a geometric approach to describe the\ndependence or invariance of FR models to these attributes and introduce a\nphysics-inspired alignment metric. We evaluate the proposed metric on\ncontrolled, simplified models and widely used FR models fine-tuned with\nsynthetic data for targeted attribute augmentation. Our findings reveal that\nthe models exhibit varying degrees of invariance across different attributes,\nproviding insight into their strengths and weaknesses and enabling deeper\ninterpretability. Code available here:\nhttps://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\u6765\u63cf\u8ff0\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u5bf9\u53ef\u89e3\u91ca\u9762\u90e8\u548c\u56fe\u50cf\u5c5e\u6027\u7684\u4f9d\u8d56\u6027\u6216\u4e0d\u53d8\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u7269\u7406\u542f\u53d1\u7684\u5bf9\u9f50\u5ea6\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8eab\u4efd\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u5d4c\u5165\u7a7a\u95f4\u4e2d\u591a\u5c3a\u5ea6\u51e0\u4f55\u7ed3\u6784\u7684\u5b58\u5728\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u7ed3\u6784\u53ca\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u542f\u53d1\u7684\u5bf9\u9f50\u5ea6\u91cf\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u4e0d\u540c\u5c5e\u6027\u7684\u4f9d\u8d56\u6027\u6216\u4e0d\u53d8\u6027\u3002\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u548c\u5408\u6210\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5bf9\u4e0d\u540c\u5c5e\u6027\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u4f9d\u8d56\u6027\u6216\u4e0d\u53d8\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6df1\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u6027\u80fd\u548c\u884c\u4e3a\u3002"}}
{"id": "2507.11441", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.11441", "abs": "https://arxiv.org/abs/2507.11441", "authors": ["Kaif Shaikh", "Antoni Kowalczuk", "Franziska Boenisch", "Adam Dziedzic"], "title": "Implementing Adaptations for Vision AutoRegressive Model", "comment": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025", "summary": "Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.", "AI": {"tldr": "VAR\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u672c\u6587\u7814\u7a76\u5176\u9002\u5e94\u6027\u548c\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u9002\u5e94\u6027\u95ee\u9898\uff0c\u53d1\u73b0VAR\u5728\u975eDP\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4f46DP\u6027\u80fd\u8f83\u5dee\u3002", "motivation": "\u63a2\u7d22VAR\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u5dee\u5206\u9690\u79c1\u9002\u5e94\u6027\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2dVAR\u9002\u5e94\u6027\u548cDP\u89e3\u51b3\u65b9\u6848\u7684\u7a7a\u767d\u3002", "method": "\u5b9e\u73b0\u5e76\u6bd4\u8f83\u591a\u79cdVAR\u9002\u5e94\u7b56\u7565\uff0c\u5e76\u4e0e\u6269\u6563\u6a21\u578b\u7684\u6700\u5148\u8fdb\u9002\u5e94\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "VAR\u5728\u975eDP\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4f46\u5728DP\u9002\u5e94\u4e2d\u6027\u80fd\u8f83\u5dee\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76VAR\u7684\u9690\u79c1\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u5176DP\u6027\u80fd\u3002"}}
{"id": "2507.11443", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11443", "abs": "https://arxiv.org/abs/2507.11443", "authors": ["Haoran Wang", "Hanyu Pei", "Yang Lyu", "Kai Zhang", "Li Li", "Feng-Lei Fan"], "title": "COLI: A Hierarchical Efficient Compressor for Large Images", "comment": null, "summary": "The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.", "AI": {"tldr": "COLI\u6846\u67b6\u5229\u7528\u795e\u7ecf\u8868\u793a\u89c6\u9891\uff08NeRV\uff09\u6280\u672f\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3-\u5fae\u8c03\u3001\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5e76\u884c\u5316\u76ee\u6807\u52a0\u901fINR\u538b\u7f29\uff0c\u540c\u65f6\u5f15\u5165\u8d85\u538b\u7f29\u6280\u672f\u63d0\u5347\u538b\u7f29\u6bd4\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u5927\u89c6\u573a\u56fe\u50cf\u7684\u538b\u7f29\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u7ec6\u8282\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0cINR\u867d\u5177\u6f5c\u529b\u4f46\u9762\u4e34\u538b\u7f29\u901f\u5ea6\u6162\u548c\u538b\u7f29\u6bd4\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCOLI\u6846\u67b6\uff0c\u7ed3\u5408NeRV\u6280\u672f\uff0c\u91c7\u7528\u9884\u8bad\u7ec3-\u5fae\u8c03\u3001\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5e76\u884c\u5316\u76ee\u6807\u52a0\u901fINR\u538b\u7f29\uff0c\u5e76\u5f15\u5165\u8d85\u538b\u7f29\u6280\u672f\u63d0\u5347\u538b\u7f29\u6bd4\u3002", "result": "\u5728\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0cCOLI\u663e\u8457\u964d\u4f4e\u6bd4\u7279\u7387\uff08bpp\uff09\uff0cPSNR\u548cSSIM\u6307\u6807\u4f18\u4e8e\u6216\u63a5\u8fd1\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u8bad\u7ec3\u901f\u5ea6\u63d0\u53474\u500d\u3002", "conclusion": "COLI\u901a\u8fc7\u521b\u65b0\u6280\u672f\u89e3\u51b3\u4e86INR\u538b\u7f29\u7684\u6548\u7387\u548c\u538b\u7f29\u6bd4\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11474", "abs": "https://arxiv.org/abs/2507.11474", "authors": ["Pan Du", "Mingqi Xu", "Xiaozhi Zhu", "Jian-xun Wang"], "title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing", "comment": "59 pages, 9 figures", "summary": "Accurate characterization of vascular geometry is essential for\ncardiovascular diagnosis and treatment planning. Traditional statistical shape\nmodeling (SSM) methods rely on linear assumptions, limiting their expressivity\nand scalability to complex topologies such as multi-branch vascular structures.\nWe introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular\ngeometry Synthesis, which integrates NURBS surface parameterization with\ndiffusion-based generative modeling to synthesize realistic, fine-grained\naortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates\nanatomically faithful aortas with supra-aortic branches, yielding biomarker\ndistributions that closely match those of the original dataset. HUG-VAS adopts\na hierarchical architecture comprising a denoising diffusion model that\ngenerates centerlines and a guided diffusion model that synthesizes radial\nprofiles conditioned on those centerlines, thereby capturing two layers of\nanatomical variability. Critically, the framework supports zero-shot\nconditional generation from image-derived priors, enabling practical\napplications such as interactive semi-automatic segmentation, robust\nreconstruction under degraded imaging conditions, and implantable device\noptimization. To our knowledge, HUG-VAS is the first SSM framework to bridge\nimage-derived priors with generative shape modeling via a unified integration\nof NURBS parameterization and hierarchical diffusion processes.", "AI": {"tldr": "HUG-VAS\u662f\u4e00\u79cd\u57fa\u4e8eNURBS\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u8840\u7ba1\u51e0\u4f55\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e3b\u52a8\u8109\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u5f62\u72b6\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u6027\u5047\u8bbe\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u8840\u7ba1\u62d3\u6251\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "HUG-VAS\u7ed3\u5408NURBS\u53c2\u6570\u5316\u548c\u5206\u5c42\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u4e2d\u5fc3\u7ebf\u548c\u5f84\u5411\u8f6e\u5ed3\uff0c\u652f\u6301\u96f6\u6837\u672c\u6761\u4ef6\u751f\u6210\u3002", "result": "\u751f\u6210\u7684\u4e3b\u52a8\u8109\u51e0\u4f55\u7ed3\u6784\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u751f\u7269\u6807\u5fd7\u7269\u5206\u5e03\u9ad8\u5ea6\u5339\u914d\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "HUG-VAS\u9996\u6b21\u5c06\u56fe\u50cf\u5148\u9a8c\u4e0e\u751f\u6210\u5f62\u72b6\u5efa\u6a21\u7edf\u4e00\u7ed3\u5408\uff0c\u4e3a\u5fc3\u8840\u7ba1\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.11476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11476", "abs": "https://arxiv.org/abs/2507.11476", "authors": ["Esteban Rom\u00e1n Catafau", "Torbj\u00f6rn E. M. Nordling"], "title": "C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images", "comment": "22 pages, 16 figures", "summary": "This paper addresses the fundamental computer vision challenge of robust\ncircle detection and fitting in degraded imaging conditions. We present\nCombinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an\nalgorithm that bridges the gap between circle detection and precise parametric\nfitting by combining (1) efficient combinatorial edge pixel (edgel) sampling\nand (2) convolution-based density estimation in parameter space.\n  We evaluate 3C-FBI across three experimental frameworks: (1) real-world\nmedical data from Parkinson's disease assessments (144 frames from 36 videos),\n(2) controlled synthetic data following established circle-fitting benchmarks,\nand (3) systematic analysis across varying spatial resolutions and outlier\ncontamination levels. Results show that 3C-FBI achieves state-of-the-art\naccuracy (Jaccard index 0.896) while maintaining real-time performance (40.3\nfps), significantly outperforming classical methods like RCD (6.8 fps) on a\nstandard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost\n1.0) at high resolutions (480x480) and reliable performance (Jaccard higher\nthan 0.95) down to 160x160 with up to 20% outliers.\n  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989\nacross contamination levels, comparable to modern methods like Qi et al. (2024,\n0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and\nrobustness makes 3C-FBI ideal for medical imaging, robotics, and industrial\ninspection under challenging conditions.", "AI": {"tldr": "3C-FBI\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u7ec4\u5408\u8fb9\u7f18\u50cf\u7d20\u91c7\u6837\u548c\u5377\u79ef\u5bc6\u5ea6\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5728\u6a21\u7cca\u56fe\u50cf\u4e2d\u9ad8\u7cbe\u5ea6\u4e14\u5b9e\u65f6\u7684\u5706\u68c0\u6d4b\u4e0e\u62df\u5408\u3002", "motivation": "\u89e3\u51b3\u5728\u9000\u5316\u6210\u50cf\u6761\u4ef6\u4e0b\u9c81\u68d2\u7684\u5706\u68c0\u6d4b\u4e0e\u62df\u5408\u8fd9\u4e00\u57fa\u7840\u8ba1\u7b97\u673a\u89c6\u89c9\u6311\u6218\u3002", "method": "\u7ed3\u5408\u9ad8\u6548\u7684\u7ec4\u5408\u8fb9\u7f18\u50cf\u7d20\u91c7\u6837\u548c\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u5377\u79ef\u5bc6\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u79cd\u5b9e\u9a8c\u6846\u67b6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u771f\u5b9e\u533b\u7597\u6570\u636e\u3001\u5408\u6210\u6570\u636e\u53ca\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5f02\u5e38\u503c\u6761\u4ef6\u4e0b\u7684\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff08Jaccard\u6307\u65700.896\uff09\u548c\u5b9e\u65f6\u6027\u80fd\uff0840.3 fps\uff09\u3002", "conclusion": "3C-FBI\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u4f7f\u5176\u9002\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u3001\u673a\u5668\u4eba\u548c\u5de5\u4e1a\u68c0\u6d4b\u7b49\u6311\u6218\u6027\u573a\u666f\u3002"}}
{"id": "2507.11488", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11488", "abs": "https://arxiv.org/abs/2507.11488", "authors": ["Pakizar Shamoi", "Nuray Toganas", "Muragul Muratbekova", "Elnara Kadyrgali", "Adilet Yerkin", "Ayan Igali", "Malika Ziyada", "Ayana Adilova", "Aron Karatayev", "Yerdauit Torekhan"], "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation", "comment": "submitted to IEEE for consideration", "summary": "Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u611f\u77e5\u7684\u6a21\u7cca\u989c\u8272\u6a21\u578bCOLIBRI\uff0c\u65e8\u5728\u5f25\u5408\u8ba1\u7b97\u673a\u989c\u8272\u8868\u793a\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u901a\u8fc7\u5b9e\u9a8c\u548c\u8c03\u67e5\uff0c\u6a21\u578b\u751f\u6210\u6a21\u7cca\u5206\u533a\u548c\u96b6\u5c5e\u51fd\u6570\uff0c\u5e76\u5c55\u793a\u51fa\u4f18\u4e8e\u4f20\u7edf\u989c\u8272\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u5bf9\u989c\u8272\u7684\u611f\u77e5\u4e0e\u8ba1\u7b97\u673a\u7684\u989c\u8272\u8868\u793a\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u8d34\u8fd1\u4eba\u7c7b\u611f\u77e5\u7684\u989c\u8272\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5b9e\u9a8c\u65b9\u6cd5\uff1a\u521d\u6b65\u5b9e\u9a8c\u786e\u5b9a\u53ef\u533a\u5206\u7684\u989c\u8272\u523a\u6fc0\uff0c\u5927\u89c4\u6a21\u4eba\u7c7b\u5206\u7c7b\u8c03\u67e5\uff081000+\u53d7\u8bd5\u8005\uff09\uff0c\u63d0\u53d6\u6a21\u7cca\u5206\u533a\u548c\u751f\u6210\u96b6\u5c5e\u51fd\u6570\u3002", "result": "\u6a21\u578b\u5728\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8eRGB\u3001HSV\u548cLAB\u7b49\u4f20\u7edf\u989c\u8272\u6a21\u578b\u3002", "conclusion": "COLIBRI\u6a21\u578b\u5728\u8bbe\u8ba1\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u989c\u8272\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u4eba\u7c7b\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11522", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11522", "abs": "https://arxiv.org/abs/2507.11522", "authors": ["Tariq Mehmood", "Hamza Ahmad", "Muhammad Haroon Shakeel", "Murtaza Taj"], "title": "CATVis: Context-Aware Thought Visualization", "comment": "Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)", "summary": "EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76845\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4eceEEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u8868\u793a\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u91cd\u65b0\u6392\u5e8f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684EEG\u5230\u56fe\u50cf\u751f\u6210\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "EEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u8868\u793a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u5176\u590d\u6742\u548c\u566a\u58f0\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "5\u9636\u6bb5\u6846\u67b6\uff1aEEG\u7f16\u7801\u5668\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u6807\u9898\u91cd\u65b0\u6392\u5e8f\u3001\u52a0\u6743\u63d2\u503c\u548c\u56fe\u50cf\u751f\u6210\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u534713.43%\uff0c\u751f\u6210\u51c6\u786e\u7387\u63d0\u534715.21%\uff0cFID\u964d\u4f4e36.61%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3aEEG\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11533", "abs": "https://arxiv.org/abs/2507.11533", "authors": ["Mengyu Wang", "Henghui Ding", "Jianing Peng", "Yao Zhao", "Yunpeng Chen", "Yunchao Wei"], "title": "CharaConsist: Fine-Grained Consistent Character Generation", "comment": "ICCV 2025 accepted paper, project page:\n  https://murray-wang.github.io/CharaConsist/", "summary": "In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist", "AI": {"tldr": "CharaConsist\u901a\u8fc7\u70b9\u8ddf\u8e2a\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8eab\u4efd\u548c\u80cc\u666f\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u80cc\u666f\u7ec6\u8282\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u70b9\u8ddf\u8e2a\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\uff0c\u7ed3\u5408\u524d\u666f\u548c\u80cc\u666f\u7684\u89e3\u8026\u63a7\u5236\u3002", "result": "CharaConsist\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u9002\u7528\u4e8e\u8fde\u7eed\u6216\u79bb\u6563\u573a\u666f\u3002", "conclusion": "CharaConsist\u662f\u9996\u4e2a\u9488\u5bf9DiT\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u4e00\u81f4\u6027\u751f\u6210\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.11539", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11539", "abs": "https://arxiv.org/abs/2507.11539", "authors": ["Dong Zhuo", "Wenzhao Zheng", "Jiahe Guo", "Yuqi Wu", "Jie Zhou", "Jiwen Lu"], "title": "Streaming 4D Visual Geometry Transformer", "comment": "Code is available at: https://github.com/wzzheng/StreamVGGT", "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f4D\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u5b9e\u65f6\u611f\u77e5\u548c\u91cd\u5efa\u89c6\u9891\u4e2d\u76844D\u65f6\u7a7a\u51e0\u4f55\uff0c\u91c7\u7528\u56e0\u679c\u53d8\u6362\u5668\u67b6\u6784\u548c\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u63a8\u7406\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u4ece\u89c6\u9891\u4e2d\u5b9e\u65f6\u611f\u77e5\u548c\u91cd\u5efa4D\u65f6\u7a7a\u51e0\u4f55\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u57fa\u7840\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u652f\u6301\u4ea4\u4e92\u5f0f\u5e94\u7528\u3002", "method": "\u91c7\u7528\u56e0\u679c\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5229\u7528\u65f6\u95f4\u56e0\u679c\u6ce8\u610f\u529b\u548c\u5386\u53f2\u952e\u503c\u7f13\u5b58\u4f5c\u4e3a\u9690\u5f0f\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u6d41\u5f0f\u957f\u671f4D\u91cd\u5efa\u3002\u8bad\u7ec3\u65f6\u4ece\u53cc\u5411\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff08VGGT\uff09\u4e2d\u84b8\u998f\u77e5\u8bc6\uff0c\u63a8\u7406\u65f6\u652f\u6301\u9ad8\u6548\u6ce8\u610f\u529b\u64cd\u4f5c\uff08\u5982FlashAttention\uff09\u3002", "result": "\u5728\u591a\u4e2a4D\u51e0\u4f55\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u5728\u7ebf\u573a\u666f\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u53ef\u6269\u5c55\u548c\u4ea4\u4e92\u5f0f\u76844D\u89c6\u89c9\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11540", "abs": "https://arxiv.org/abs/2507.11540", "authors": ["Zhen Xu", "Hongyu Zhou", "Sida Peng", "Haotong Lin", "Haoyu Guo", "Jiahao Shao", "Peishan Yang", "Qinglin Yang", "Sheng Miao", "Xingyi He", "Yifan Wang", "Yue Wang", "Ruizhen Hu", "Yiyi Liao", "Xiaowei Zhou", "Hujun Bao"], "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation", "comment": null, "summary": "Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u6df1\u5ea6\u4f30\u8ba1\u57283D\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63a2\u8ba8\u4e86\u4f20\u7edf\u786c\u4ef6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u57fa\u4e8e\u89c6\u89c9\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u73af\u5883\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u65b9\u6cd5\u7684\u6cdb\u5316\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u8c03\u67e5\u4e86\u5355\u76ee\u3001\u7acb\u4f53\u3001\u591a\u89c6\u56fe\u548c\u5355\u76ee\u89c6\u9891\u8bbe\u7f6e\u4e0b\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u8303\u5f0f\uff0c\u63a2\u8ba8\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u4f5c\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u6982\u5ff5\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
