<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 162]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.NI](#cs.NI) [Total: 14]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 41]
- [econ.EM](#econ.EM) [Total: 2]
- [econ.GN](#econ.GN) [Total: 7]
- [econ.TH](#econ.TH) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 论文提出了一种通过去除推理过程中的冗余注意力来提高大型语言模型性能的方法，特别是在数学竞赛等需要复杂推理的任务中表现显著。


<details>
  <summary>Details</summary>
Motivation: 观察到大型语言模型在长推理过程中存在冗余注意力，尤其是错误答案中注意力分散更严重，希望通过去除冗余提升性能。

Method: 通过测量特殊‘思考结束’标记的注意力分数识别冗余，采用结构感知剪枝去除低贡献推理块，并恢复生成过程。

Result: 方法显著提高了推理密集型任务的准确性，尤其在数学竞赛基准测试（如AIME和AMC）中表现优异。

Conclusion: 去除推理冗余能有效提升模型性能，无需额外训练，适用于复杂推理任务。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出了一种结合两种虚拟差距分析（VGA）模型的新型多准则评估（MCA）方法，以提高评估的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多准则评估方法（如DEA、SFA、MCDM）在假设和主观判断上的局限性，以及如何处理定量和定性数据的挑战。

Method: 结合两种VGA模型，基于线性规划的框架，提出新的MCA方法。

Result: 通过两个数值示例验证了方法的准确性和透明度。

Conclusion: 该方法为自动化决策系统和决策支持系统提供了强大且灵活的解决方案，推动了该领域的进步。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种基于桌游角色扮演游戏（TTRPG）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足生成式AI在模拟、叙事和评估等多样化应用场景中的需求，需要一个灵活的场景定义框架。

Method: 采用实体-组件架构模式，将游戏管理员（GM）设计为可配置的实体，由组件构成，实现工程师与设计师的分工协作。

Result: 通过Concordia库的实践，展示了该框架如何有效支持用户根据特定目标配置场景。

Conclusion: 该框架通过分离关注点，实现了快速迭代、模块化和可扩展性，适用于生成式AI的多样化应用。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是一个基于Transformer架构的AI基础模型，专为生物多样性分析和保护规划设计，通过多模态数据预训练，适用于多种下游任务，并在数据稀缺场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物多样性快速丧失对生态研究和保护策略提出严峻挑战，需要综合监测和预测能力。AI基础模型在科学领域的成功应用为生物多样性保护提供了新思路。

Method: BioAnalyst采用Transformer架构，预训练于物种记录、遥感数据、气候和环境变量等多模态数据集，支持下游任务微调，如物种分布建模和栖息地评估。

Result: BioAnalyst在两种下游任务中表现出优于现有方法的泛化能力，特别是在数据稀缺场景中，为生态预测设立了新的准确性基准。

Conclusion: 通过开源BioAnalyst及其微调流程，旨在推动生物多样性建模的协作，并推进AI驱动的生态挑战解决方案。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，尽管开发者预期AI工具能减少任务完成时间，但实际使用AI工具反而增加了19%的时间，与经济学和机器学习专家的预测相反。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对实际软件开发中开发者生产力的影响，尤其是在经验丰富的开源开发者中的效果。

Method: 采用随机对照试验（RCT），16名有中等AI经验的开发者在成熟项目中完成246项任务，随机分配是否允许使用2025年的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 允许使用AI工具时，任务完成时间增加了19%，与开发者预期的减少20%和专家预测的减少38-39%形成鲜明对比。

Conclusion: AI工具在实际应用中可能并未如预期提升生产力，甚至可能因某些因素导致效率下降，这一现象值得进一步研究。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体强化学习（MARL）的框架，用于检测去中心化金融（DeFi）中的市场操纵行为，通过动态对抗游戏建模操纵者与检测器的交互。


<details>
  <summary>Details</summary>
Motivation: 去中心化金融缺乏中心化监管，导致市场操纵行为频发，亟需一种去中心化的检测方法。

Method: 采用MARL框架，结合GRPO优化学习稳定性，理论驱动的奖励函数，以及多模态智能体管道整合语义、社交图和链上数据。

Result: 在真实数据和对抗模拟中验证，Hide-and-Shill系统在检测准确性和因果归因方面表现优异。

Conclusion: 该研究将多智能体系统与金融监控结合，为去中心化市场情报提供了新范式。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 论文首次系统评估了基于LLM的编码代理的安全性，发现21%的操作存在安全隐患，并提出了检测系统和缓解策略。


<details>
  <summary>Details</summary>
Motivation: LLM编码代理在软件开发中广泛应用，但其安全性尚未被充分研究，可能引入不安全实践。

Method: 分析了12,000多个操作，覆盖5种先进模型在93个真实任务中的表现，开发了高精度检测系统。

Result: 21%的操作不安全，信息暴露最常见；GPT-4.1的缓解成功率高达96.8%。

Conclusion: 研究为评估编码代理安全性提供了框架，并强调下一代LLM编码代理需注重安全设计。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 报告提出了一种由AI引发的潜在灭绝性事件的分类及示例，旨在通过公开讨论支持预防措施。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可能导致的人类灭绝风险，以促进公众支持预防措施。

Method: 提出分类法并列举潜在灭绝性事件的示例。

Result: 明确了AI可能带来的灾难性风险，并呼吁采取预防行动。

Conclusion: 通过公开讨论这些可能性，可以推动预防措施的实施，减少AI带来的灭绝风险。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个端到端框架，用于提升多模态大语言模型在科学任务中的推理能力，通过EduPRM和EduMCTS实现多步推理和自校正。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在科学任务中表现不佳，尤其是在需要多步和可解释推理的场景中，缺乏全局一致性和自校正能力。

Method: 提出EduFlow框架，包括EduPRM（过程感知奖励模型）和EduMCTS（领域适应的搜索框架），通过课程学习和自反射机制优化推理轨迹。

Result: 实验表明EduFlow显著提升了推理的一致性和连贯性，并构建了EduMCTS-160K数据集。

Conclusion: EduFlow为科学推理任务提供了一种有效的解决方案，未来将发布代码、数据和模型。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 论文探讨了可解释性与适应性在AI系统中的结合，特别是针对可迁移和可解释的神经符号AI系统，研究了知识表示对LLM查询三元组存储的影响。


<details>
  <summary>Details</summary>
Motivation: 可解释性和适应性是下一代AI系统的关键，尤其是在大型语言模型和生成式AI中。研究旨在结合这两点，设计可迁移且可解释的神经符号AI系统。

Method: 聚焦于"Agentic Retrieval-Augmented Generation"系统，评估知识表示的结构和复杂性对LLM查询三元组存储的影响。

Result: 结果显示不同知识表示方法对LLM查询效果有显著影响。

Conclusion: 研究强调了知识表示在可解释性和适应性AI系统中的重要性，并讨论了其影响和意义。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文综述了基于LLM的表格代理，旨在通过整合预处理、推理和领域适应来自动化表格任务，分析了五大核心能力，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 现实中的表格任务常涉及噪声、结构异质性和语义复杂性，而现有研究多针对干净的学术数据集，未充分探索这些问题。

Method: 定义了五大核心能力（C1-C5）来分析比较现有方法，并以Text-to-SQL代理为例详细探讨性能差距。

Result: 发现开源模型在学术基准和实际场景间存在性能差距，并提出了改进建议。

Conclusion: 论文为提升基于LLM的表格代理在实践中的鲁棒性、泛化性和效率提供了实用见解。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [12] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 论文提出了一种LLM-Stackelberg博弈框架，将大语言模型（LLMs）融入领导者与追随者的战略互动中，突破了传统Stackelberg博弈的完全信息和理性假设。


<details>
  <summary>Details</summary>
Motivation: 传统Stackelberg博弈假设完全信息和理性行为，而现实中的决策往往涉及不完全信息和有限理性。LLMs的引入为模拟更真实的战略互动提供了可能。

Method: 通过结构化提示让代理进行推理，生成概率行为，并通过内部认知和信念更新调整策略。定义了两种均衡概念：推理与行为均衡、推测推理均衡。

Result: 通过钓鱼攻击案例研究展示了LLM-Stackelberg博弈的认知丰富性和对抗潜力。

Conclusion: LLM-Stackelberg博弈为网络安全、错误信息和推荐系统等领域的决策建模提供了新范式。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [13] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出从反应式转向生成式AI的多智能体强化学习，以解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法在联合动作空间、非平稳环境和部分可观测性方面存在挑战，且缺乏对新场景的适应能力。

Method: 采用生成式AI的强化学习，使智能体能够预测未来交互、生成协调动作序列并进行战略推理。

Result: 生成式AI方法可实现主动决策、无缝协调和动态适应，推动分布式智能的发展。

Conclusion: 这一范式转变有望解决传统反应式框架无法处理的协作问题，应用于自主系统、机器人等领域。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [14] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP是一种基于一致性轨迹模型的离线强化学习方法，通过单步轨迹生成实现高效优化，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在规划中计算成本高的问题，同时保持策略质量。

Method: 利用一致性轨迹模型（CTM）进行单步轨迹生成，避免迭代采样。

Result: 在D4RL基准测试中表现优于现有扩散方法，计算速度提升120倍。

Conclusion: CTP是一种高效、低延迟的离线规划方法，适用于高性能任务。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [15] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 提出了一种基于Metropolis-Hastings采样的框架，用于训练脉冲神经网络（SNN）在强化学习任务中，无需依赖梯度方法。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNN）在实时控制系统中具有生物启发性和高能效，但其训练面临非可微分性的挑战。

Method: 使用Metropolis-Hastings采样技术，基于累积奖励信号迭代更新网络参数，避免反向传播的限制。

Result: 在AcroBot和CartPole基准测试中，该方法优于传统深度Q学习和现有SNN方法，实现了更高的累积奖励和更低的资源消耗。

Conclusion: 该框架为SNN在强化学习中的应用提供了一种高效且直接的训练方法，适用于神经形态平台。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [16] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个AIaaS平台，专注于企业数据、工作流程和主流LLM的无缝集成，提供数据安全和知识保留，同时通过AI代理提升团队效率和业务成果。


<details>
  <summary>Details</summary>
Motivation: 解决企业在AI应用中面临的数据安全、知识保留和高效工作流程的需求，同时支持多种主流LLM。

Method: 结合结构化文档处理、混合向量检索和无代码编排（通过LangChain），并支持多种LLM。关键组件THOR Agent处理结构化查询并生成可操作的洞察。

Result: 实验显示，512令牌的块大小在检索精度上表现最佳（Top-3准确率：91.3%），生成质量测试中eSapiens在上下文一致性上提升23%。

Conclusion: eSapiens在高风险领域（如法律和金融）中实现了可信、可审计的AI工作流程，验证了其有效性。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [17] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 该论文综述了人工智能快速发展带来的环境与伦理挑战，重点关注能源消耗、电子废物、计算资源不平等和网络安全系统的隐性能源负担。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在性能之外对环境和社会的影响，推动负责任AI的发展。

Method: 通过分析近期研究和机构报告，系统梳理AI在能源、废物、资源分配和网络安全方面的负面影响。

Result: 揭示了AI发展中的系统性环境和社会问题，如高排放、硬件快速淘汰、全球资源不平等和网络安全能源需求。

Conclusion: 呼吁AI发展需结合伦理责任和环境保护，以实现可持续和包容的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [18] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 提出了一种神经符号框架，结合多模态语言模型和知识图谱，以支持机器人应用的互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器人系统依赖专有硬编码方案、难以适应和扩展的问题，同时克服符号系统和多模态模型的局限性。

Method: 结合多模态语言模型（如LLaMA和GPT）与知识图谱，生成符合本体论的KG，评估不同模型的性能和一致性。

Result: GPT-o1和LLaMA 4 Maverick表现最佳，但新模型不一定更好，集成策略是关键。

Conclusion: 神经符号框架有效支持机器人互操作性，但模型选择和集成策略对性能至关重要。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [19] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理系统中AI系统的互连及其重复使用特性，提供公平性和鲁棒性的闭环保证。


<details>
  <summary>Details</summary>
Motivation: 多代理AI系统需要满足公平性和鲁棒性的先验保证，但现有方法在闭环模型中实现这些保证较为复杂。

Method: 使用基于PyTorch的工具包，通过随机控制技术建模AI系统与代理的互连，并提供闭环公平性和鲁棒性保证。

Result: 工具包简化了多代理系统中公平性保证的实现，并提供了先验的闭环保证。

Conclusion: 该工具包为多代理AI系统的公平性和鲁棒性建模提供了一种高效且实用的解决方案。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [20] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务中表现优异，但存在推理链冗长的问题，导致资源浪费。本文综述了简洁和自适应推理的最新进展。


<details>
  <summary>Details</summary>
Motivation: LRMs在复杂推理任务中表现出色，但对简单问题生成冗长推理链，浪费资源且影响实际应用。

Method: 综述了简洁和自适应推理的方法论、基准和未来挑战。

Result: 提供了该领域的全面概述，帮助研究者快速了解现状。

Conclusion: 希望激发新的自适应推理思路，优化LRMs的实际应用。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [21] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局，以优化异常检测。


<details>
  <summary>Details</summary>
Motivation: AI驱动的制造业中，实时监控数据流的需求增长，但资源有限，无法在所有位置部署传感器。现有方法多忽略因果关系，或依赖不切实际的干预手段。

Method: 通过在每个Q网络训练阶段集成因果信息，开发了Causal DQ方法，实现更快的收敛和更紧的理论误差界限。

Result: Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。

Conclusion: 该方法不仅适用于传感器布局，还为因果驱动的机器学习在工程应用中的推广提供了新思路。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [22] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出了一种将大语言模型（LLM）整合到形式语义解释函数中的方法，以解决其逻辑不一致性问题，并验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言理解和生成方面表现出色，但其输出存在逻辑不一致问题，需要一种方法在形式推理中利用其广泛的知识。

Method: 将LLM直接整合到一种矛盾容忍逻辑的形式语义解释函数中，并通过实验验证其可行性。

Result: 实验结果表明，该方法能够利用LLM的知识，同时保持逻辑的健全性和完备性。

Conclusion: 该方法为神经符号推理提供了一个理论框架，既能利用LLM的知识，又能保持逻辑的一致性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [23] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 论文探讨了AI快速发展带来的风险，并提出通过技术干预实现协调暂停危险AI活动的可能性。


<details>
  <summary>Details</summary>
Motivation: AI系统的快速发展带来了失控、滥用、地缘政治不稳定和权力集中等前所未有的风险，需要政府采取行动避免最坏结果。

Method: 提出了关键的技术干预措施，以协调暂停危险的AI开发和部署活动。

Result: 这些干预措施可以限制多种危险的AI活动，并为潜在的AI治理计划提供技术基础。

Conclusion: 通过技术干预实现协调暂停危险AI活动是可行的，并为AI治理提供了技术支撑。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [24] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过少量高质量的长链思维（CoT）示例微调基础模型，可以显著提升其推理能力，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 探索是否仅通过提示或最小化微调，就能在基础模型中诱导出长链思维推理能力。

Method: 使用20个来自推理模型的长链思维示例对基础模型进行轻量微调，并尝试其他来源的CoT数据（如非推理模型和人工标注）。

Result: 微调后的模型表现优于更大规模的模型，但其他来源的CoT数据效果不及推理模型的数据。

Conclusion: 高质量的小规模推理数据可以激活基础模型的推理能力，但专家CoT的某些潜在特性难以复制。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [25] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 论文提出将指令调优的大语言模型重新解释为基于模型的符号AI系统，利用自然语言作为符号层，并通过模型的内部表示空间实现接地。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络和符号AI的互补优势，探索新的学习和推理方法。

Method: 将大语言模型视为符号AI系统，自然语言作为符号层，模型内部表示空间作为接地机制。

Result: 初步评估显示，该方法在提高学习效率和推理可靠性方面有效。

Conclusion: 该框架为神经符号AI提供了新的学习和推理范式，具有潜力。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [26] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了VerifyBench，一个跨领域的综合基准，用于系统评估验证器性能，揭示了专用验证器和通用LLM在准确性和泛化能力上的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器在复杂、多样化的模型生成响应中表现不一致，缺乏系统评估，限制了RLVR的可靠发展。

Method: 构建包含4,000个专家级问题的基准，覆盖数学、物理、化学和生物领域，设计四维实验框架比较验证器性能。

Result: 专用验证器准确性高但召回率低，通用模型包容性强但精度不稳定，验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 研究揭示了当前验证器技术的瓶颈，为未来改进提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [27] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布V3和R1系列模型，因其低成本、高性能和开源优势引发关注。论文回顾了大模型演变，重点介绍了DeepSeek的新算法和工程突破，并分析了其对AI竞争格局的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepSeek模型的技术创新及其对AI领域的潜在影响，为未来大模型发展提供参考。

Method: 回顾大模型演变，分析DeepSeek的新算法（如MLA、MoE等）和工程优化，并与主流LLM对比。

Result: DeepSeek模型在性能、成本和开源方面表现突出，对AI竞争格局产生显著影响。

Conclusion: DeepSeek的创新为未来大模型发展提供了重要启示，尤其是在数据、训练和推理方面。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [28] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法通过引入最优边际Q函数和广义Q批评器，解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 异构多智能体强化学习中，单调改进与部分参数共享之间存在冲突，导致性能下降。

Method: 提出OMDPG算法，包括最优边际Q函数、广义Q批评器和集中式批评器分组执行器架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于现有MARL基线。

Conclusion: OMDPG成功解决了单调改进与部分参数共享的冲突，显著提升了异构多智能体强化学习的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [29] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文提出了一种基于Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性，低成本地评估数据中的潜在意图性。


<details>
  <summary>Details</summary>
Motivation: 探讨意图和意图性在科学与技术中的实际意义，弥补Searle之后相关研究的不足。

Method: 利用Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性分离意图内容与环境背景。

Result: 提供了一种低成本、无需大规模训练或推理的潜在意图性评估方法，适用于基础生物体。

Conclusion: 该方法为意图性提供了一种实用且低计算成本的解释，但概念形成水平受限于代理的记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [30] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本文提出了一种通过利用模型的真实性编码来校准链式思维（CoT）推理准确性的新方法，显著提升了推理任务的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在大型语言模型中表现出强大的深度推理能力，但其可靠性因中间步骤错误的累积而受到限制。

Method: 通过发现特定注意力头激活能可靠反映CoT推理步骤的真实性，训练了一个置信度预测器，动态选择最合理的推理路径。

Result: 实验结果表明，该方法在数学、符号和常识推理任务中显著优于现有基线方法，并在单模态和多模态设置中表现出更高的准确性和可靠性。

Conclusion: 本研究为CoT推理提供了一种新颖的可靠性改进路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [31] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 研究评估了三种大型语言模型（LLM）在SPARQL查询翻译任务中的表现，发现模型性能和提示策略差异显著，且Wikidata到DBpedia的翻译效果优于反向。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱（KG）互操作性研究中SPARQL查询自动翻译的空白，评估LLM在不同KG模式间的翻译能力。

Method: 使用三种不同规模和架构的LLM（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），通过零样本、少样本和思维链变体进行测试，并构建了两个基准数据集（DBpedia-Wikidata和DBLP-OpenAlex）。

Result: 模型性能和提示策略差异显著，Wikidata到DBpedia的翻译效果优于反向。

Conclusion: LLM在SPARQL翻译任务中表现不一，提示策略和模型选择对结果有显著影响。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [32] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新的渐进语义家族，用于为假设（ABA框架中的核心组件）赋予辩证强度，填补了渐进语义在假设基础论证（ABA）中的空白。


<details>
  <summary>Details</summary>
Motivation: 渐进语义在计算论证中是一种细粒度的替代方案，但尚未应用于假设基础论证（ABA），尽管ABA是一种流行的结构化论证形式。本文旨在填补这一空白。

Method: 通过将双极集基论证框架作为ABA框架的抽象，并推广最先进的模块化渐进语义（QBAFs），提出了一种新的渐进ABA语义。

Result: 实验表明，渐进ABA语义满足平衡性和单调性等理想性质，并通过合成ABA框架验证了其性能。

Conclusion: 本文成功地将渐进语义引入ABA框架，并验证了其有效性和适用性。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [33] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 本文介绍了BlueGlass框架，旨在通过统一基础设施整合多样化的AI安全工具，以提升AI系统的安全性，并通过视觉语言模型的三项安全分析验证其效用。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强和普及，确保其安全性至关重要。现有安全工具往往针对不同方面，无法单独提供全面保障，因此需要集成和复合方法。

Method: 提出BlueGlass框架，提供统一基础设施，支持多样化安全工具的集成与组合，覆盖模型内部和输出。通过三项视觉语言模型的安全分析验证框架效用。

Result: 三项分析包括：分布评估揭示性能权衡和潜在失败模式；基于探针的层级动态分析显示共享分层学习；稀疏自编码器识别可解释概念。

Conclusion: BlueGlass为构建更稳健可靠的AI系统提供了基础架构和发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [34] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 论文探讨了边缘-云系统中应用迁移的自动化编排问题，通过MDP模型比较了AI规划和强化学习方法，并提出了基于状态空间定义的新分类。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决边缘-云系统中应用迁移的高效自动化编排问题，以提升服务质量和成本效益。

Method: 从MDP模型出发，分析比较了AI规划和强化学习方法，并引入基于状态空间的新分类。

Result: 研究识别并比较了适用于ToH问题的AI规划和RL方法，为计算连续环境中的迁移编排提供了技术参考。

Conclusion: 论文为边缘-云系统中应用迁移的自动化编排提供了技术分析和分类框架。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [35] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 论文探讨了利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少大型语言模型（LLM）中的偏见，展示了这些提示如何帮助模型识别自身偏见并提供更全面的反思。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs仍在发展中，当前的偏见可能随时间变化，因此需要通用的去偏见策略。借鉴人类决策中的去偏见方法，尤其是元认知提示，可能为LLMs提供长期有效的解决方案。

Method: 研究采用人类决策中的元认知提示（如“你可能是错的吗？”）应用于LLMs，通过提问引导模型揭示潜在偏见、错误和矛盾信息。

Result: 元认知提示成功引导LLMs识别自身偏见，并提供初始回答中未体现的额外信息，如错误、矛盾证据和替代方案。

Conclusion: 人类心理学中的元认知提示为LLMs的提示工程提供了新思路，能够有效改善模型的决策和反思能力。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [36] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，以实时优化飞行控制和数据收集调度，最小化信息年龄（AoI）。


<details>
  <summary>Details</summary>
Motivation: 无人机在野火监测中至关重要，但现有深度强化学习（DRL）方法存在采样效率低、仿真与现实的差距及复杂训练等问题，不适用于时间敏感任务。

Method: FRSICL利用自然语言任务描述和环境反馈，动态生成数据收集计划和飞行速度控制，无需大量重新训练。

Result: 仿真结果表明，FRSICL在最小化平均AoI方面优于PPO和最近邻基线方法。

Conclusion: FRSICL为无人机野火监测提供了一种高效、动态的实时优化方案，解决了DRL的局限性。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [37] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 论文提出“适应性”概念，作为评估多智能体强化学习（MARL）在动态环境中可靠性的统一框架，包括学习适应性、策略适应性和场景驱动适应性三个维度。


<details>
  <summary>Details</summary>
Motivation: 现实世界多智能体系统（MAS）的复杂性和动态性限制了MARL的实际应用，需要算法在变化的环境中保持有效性。

Method: 引入适应性概念，提出包含三个维度的结构化框架：学习适应性、策略适应性和场景驱动适应性。

Result: 通过适应性视角，支持更系统的MARL性能评估，超越传统基准测试。

Conclusion: 该框架有助于开发更适合动态现实世界多智能体系统的MARL算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [38] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 论文介绍了瑞士食品知识图谱（SwissFKG），整合了食谱、食材、替代品、营养数据、饮食限制和过敏信息，并利用LLM增强图谱内容，展示了其在个性化营养查询中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统常忽略非视觉因素（如食材替代对营养的影响）和个性化需求（如过敏、文化习惯）。瑞士缺乏整合相关信息的中心化资源。

Method: 构建SwissFKG，利用LLM增强图谱内容，并开发Graph-RAG应用展示其在营养查询中的潜力。

Result: LLM能有效丰富图谱营养信息，SwissFKG提供食材级信息和营养指南支持，Graph-RAG应用验证了其回答用户查询的能力。

Conclusion: SwissFKG为下一代饮食评估工具奠定了基础，融合了视觉、情境和文化维度。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [39] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 论文比较了Decision Transformer (DT)与Filtered Behavior Cloning (FBC)在稀疏奖励环境中的表现，发现FBC更优。


<details>
  <summary>Details</summary>
Motivation: 探讨DT在离线强化学习中的实际优势，尤其是在稀疏奖励和低质量数据环境下。

Method: 在Robomimic和D4RL任务上实验，对比DT与FBC的性能。FBC通过过滤低质量轨迹后进行行为克隆。

Result: FBC在稀疏奖励环境中表现优于DT，且更高效。

Conclusion: DT在稀疏和密集奖励环境中均不占优，质疑其适用性。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [40] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 论文提出了一种基于“什么、为什么、谁”三个维度分类和比较可解释人工智能（XAI）研究的方法，以解决当前研究中任务描述不足、脱离上下文和用户测试不充分的问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在大量矛盾且缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。

Method: 结合视觉分析、认知科学和仪表板设计等多个领域，提出分类和比较XAI研究的三维框架。

Result: 研究发现主要问题包括任务描述不足、脱离上下文的研究和用户测试不充分，并提出应明确报告用户领域、AI和数据分析专长。

Conclusion: 论文为XAI研究提供了设计和报告指南，帮助研究者更好地识别相关研究、填补研究空白并处理设计矛盾。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [41] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA）方法，结合DIMACS数据集，研究了CVRP问题中实例特征与元启发式算法性能的关系，并提供了投影矩阵以简化新实例的分析。


<details>
  <summary>Details</summary>
Motivation: 解决CVRP研究中实例特征与元启发式算法性能之间复杂关系的问题，为领域提供新的分析视角。

Method: 采用实例空间分析（ISA）方法，结合DIMACS数据集，通过PRELIM、SIFTED和PILOT阶段（包括降维和机器学习方法）生成二维实例空间投影。

Result: 识别了23个相关实例特征，并提供了投影矩阵，便于新实例的纳入分析。

Conclusion: ISA方法为CVRP领域提供了一种新的实例分析工具，有助于理解实例结构对算法行为的影响。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [42] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 论文提出了一种结合BERT情感分析和XGBoost的模型，用于预测远程学习中的学生辍学风险，准确率达84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程学习中的学生辍学问题严重，早期预测对干预和提升学生坚持性至关重要。

Method: 结合BERT对学生的评论进行情感分析，以及XGBoost分析社会人口和行为数据，通过特征重要性技术选择关键特征。

Result: 模型在未见数据上达到84%的准确率，优于基线模型的82%，且在精确率和F1分数上表现更优。

Conclusion: 该方法可成为开发个性化策略以减少辍学率和鼓励学生坚持的重要工具。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [43] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 论文提出了一种在数据稀缺领域（如计算化学、医学成像）中高效获取先验知识的方法，通过神经记忆和超网络设计，结合MAML，实现了在小样本和动态分布下的高效迁移学习。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的领域（如计算化学、计算免疫学和医学成像），训练大型预训练模型或基础模型不可行，因此需要设计高效获取先验知识的架构。

Method: 使用神经记忆适应动态分布的小样本任务，设计超网络结合MAML获取更通用的先验，并应用于3D场景生成和分割，以及分子生成预训练框架。

Result: 超网络在小样本任务中表现出色，实现了高效的3D场景生成和分割，并在分子属性预测中提升了性能。

Conclusion: 提出的方法在数据稀缺领域实现了高效的迁移学习，为小样本任务提供了可行的解决方案。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [44] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一种基于LLM的新型系统，用于自动化科学文献合成，支持递归、深度和广度可控的探索，提升文献检索的多样性和细致度。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成流程缺乏用户可控性和透明性，DeepResearch旨在解决这一问题，提供可配置的参数驱动方法，以高效整合领域证据并保持分析严谨性。

Method: DeepResearch通过递归、深度和广度可控的探索方式，结合透明推理和参数驱动配置，实现科学文献的高效合成。

Result: 在49个生态研究问题中，DeepResearch实现了21倍的源整合提升和14.9倍的每千字源整合增加，高参数设置下达到专家级分析深度和多样性。

Conclusion: DeepResearch$^{\text{Eco}}$通过可控性和透明性显著提升了科学文献合成的效率和质量，适用于高要求的生态研究。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [45] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 提出了一种基于Transformer的模型Spatial ModernBERT，用于从复杂财务文档中提取表格和键值对，通过空间嵌入和多头分类任务实现高精度提取。


<details>
  <summary>Details</summary>
Motivation: 财务文档中的表格和键值对提取对审计、数据分析和自动化发票处理等业务流程至关重要。

Method: 使用Spatial ModernBERT模型，结合空间嵌入，通过三个分类头（标签、列、行）进行标记分类，并在PubTables-1M数据集上预训练后微调。

Result: 模型通过后处理方法合并标记并重构表格布局，实验表明其在真实财务文档中实现了高精度的提取。

Conclusion: Spatial ModernBERT通过结合文本和空间信息，显著提升了表格和键值对提取的准确性。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [46] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard是一种多语言护栏，旨在解决现有护栏在多语言安全对齐上的不足，通过LoRA技术改进多语言不安全提示的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有护栏（如LlamaGuard）在多语言不安全输入（尤其是低资源语言）上表现不佳，导致LLM系统易受攻击。

Method: 采用低秩适应（LoRA）技术将通用多语言模型转化为多语言护栏，并构建SEALSBench数据集（含26万条多语言提示）。

Result: SEALGuard在多语言不安全提示检测上优于现有护栏，DSR提升48%，且在所有指标（DSR、精确率、F1分数）上表现最佳。

Conclusion: SEALGuard通过有效的多语言护栏技术，显著提升了LLM系统的安全对齐能力。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [47] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 论文分析了当前用于评估大型语言模型（LLMs）在医学问答中表现的基准数据集的局限性，并提出需要更严谨、无偏且反映临床复杂性的标准化框架。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在医学问答中的局限性，特别是数据集的质量问题，以推动更有效的评估方法。

Method: 回顾了MedQA、MedMCQA、PubMedQA和MMLU等基准数据集，并分析了医学期刊中的挑战问题作为替代方案。

Result: 现有数据集缺乏临床真实性、透明性和验证过程；挑战问题虽有益但规模小、范围窄且可能被LLM训练暴露。

Conclusion: 需要标准化框架和多方合作来开发更严谨、无偏的数据集和方法论。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [48] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 论文介绍了两个韩语专家级基准测试KMMLU-Redux和KMMLU-Pro，用于评估大语言模型在韩国工业领域的适用性。


<details>
  <summary>Details</summary>
Motivation: 开发能够全面评估大语言模型在现实场景中适用性的基准测试，特别是针对韩国的工业领域。

Method: 重构现有KMMLU为KMMLU-Redux，去除关键错误以提高可靠性；基于韩国国家专业执照考试创建KMMLU-Pro。

Result: 实验表明，这两个基准测试能全面代表韩国的工业知识。

Conclusion: 论文公开了数据集，为评估大语言模型在韩国工业领域的表现提供了可靠工具。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [49] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS是一种无需外部监督的自改进模型引导框架，通过自主生成和优化对比样本，显著提升了大型语言模型（LLM）的引导效果和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统模型引导方法依赖外部标注数据，限制了其适应性和效果。SIMS旨在通过自改进机制解决这一问题。

Method: SIMS通过迭代自改进循环生成和优化对比样本，并采用提示排名和对比采样等新策略增强引导效果。

Result: 在多种LLM和基准测试中，SIMS在引导效果和适应性上显著优于现有方法。

Conclusion: 自改进模型引导是未来LLM推理时对齐研究的一个有前景的方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [50] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 研究发现，电子健康记录（EHR）中存在对特定患者群体的污名化语言，尤其是黑人、低收入患者及某些疾病患者，且不同医疗提供者类型的使用频率不同。


<details>
  <summary>Details</summary>
Motivation: 探讨EHR中污名化语言的普遍性及其对患者群体的影响。

Method: 通过扩展词典匹配和监督学习分类器识别MIMIC-III EHR中的怀疑标记和污名化标签，并使用泊松回归模型分析预测因素。

Result: 黑人、低收入患者及某些疾病患者的污名化标签率更高，护士和社会工作者的使用频率也较高。

Conclusion: 污名化语言在EHR中普遍存在，需采取措施减少其对患者的影响。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [51] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 研究发现，视觉表象能力不同的人在Ganzflicker诱导的幻觉中看到的内容不同，强表象者描述复杂自然内容，弱表象者描述简单几何图案。视觉语言模型能更好捕捉这些差异。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉表象能力（从无表象到典型表象再到生动表象）如何影响Ganzflicker诱导的幻觉内容。

Method: 使用自然语言处理工具分析4000多名参与者对幻觉的自由文本描述，比较不同表象能力者的描述差异。

Result: 强表象者描述复杂自然内容，弱表象者描述简单几何图案；视觉语言模型比纯文本模型更能捕捉这些差异。

Conclusion: 结果可能反映了早期视觉区与高阶区在表象能力个体差异中的协调作用。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [52] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一种线性化框架，将预训练的Transformer LLM转化为子二次复杂度架构，解决长上下文生成中的内存和计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: Transformer LLM在长上下文生成中面临内存和计算瓶颈，Lizard旨在通过子二次注意力机制解决这些问题。

Method: 引入子二次注意力机制，结合门控模块和混合机制（全局压缩与局部滑动窗口），并优化硬件感知算法。

Result: 在标准语言建模任务中几乎无损恢复教师模型性能，MMLU基准提升18分，关联召回任务显著改进。

Conclusion: Lizard是一种高效、灵活的线性化方法，显著优于现有方法。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [53] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN系统通过基于提示的动态个性化方法，将LLM与用户多样化的价值观和偏好对齐，支持结构化输出和多种算法实现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM比较工具主要关注基准测试任务，而ALIGN系统旨在解决LLM在决策辅助中的个性化需求。

Method: ALIGN系统采用提示对齐方法，支持配置管理、结构化输出生成和可替换的LLM算法实现。

Result: 系统在公共意见调查和医疗分诊决策两个领域进行了定量分析，展示了其对齐效果。

Conclusion: ALIGN框架开源，为可靠、负责任和个性化的LLM决策辅助研究提供了新工具。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [54] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: OpenCodeReasoning-II是一个包含2.5M个问题-解决方案-评论三元组的数据集，用于代码生成和评论的联合训练。通过两阶段微调策略，Qwen2.5-Instruct模型在代码生成和评论任务中表现优异，并在竞赛编程中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成和评论领域依赖于大规模高质量数据集，但现有数据集规模有限。OpenCodeReasoning-II的引入填补了这一空白。

Method: 采用两阶段监督微调策略：第一阶段专注于代码生成，第二阶段联合训练代码生成和评论模型。

Result: 微调后的Qwen2.5-Instruct模型在代码生成任务中表现优于或等于现有最佳开源模型，联合训练显著提升了竞赛编程性能。

Conclusion: OpenCodeReasoning-II数据集和两阶段微调策略为代码生成和评论任务提供了新的解决方案，并通过扩展LiveCodeBench支持更全面的LLM评估。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [55] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 论文提出了一种动态参数记忆（DPM）机制，用于解决语音大语言模型（SLLM）在处理长音频序列时的上下文限制问题，显著提升了情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 语音模态的高帧率限制了SLLM的信号处理能力，传统压缩方法忽视了情感在对话中的连续性。

Method: 通过动态参数记忆（DPM）机制，结合上下文语义和句子级情感编码，逐步将信息存储到临时LoRA模块中。

Result: 在IEMOCAP数据集上，DPM显著提升了SLLM处理长音频序列时的情感识别能力，达到最先进水平。

Conclusion: DPM机制有效解决了SLLM的上下文限制问题，为长音频情感识别提供了新思路。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [56] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: CompassJudger-2是一个新型通用评判模型，通过任务驱动的多领域数据策略提升评判能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前评判模型存在专业狭窄和鲁棒性不足的问题，限制了其全面评估能力。

Method: 采用任务驱动的多领域数据策略，结合可验证奖励和监督判断任务，通过拒绝采样和边际策略梯度损失优化学习目标。

Result: CompassJudger-2在多个评判和奖励基准测试中表现优异，7B模型与更大模型竞争激烈。

Conclusion: 该研究推动了鲁棒、可扩展的LLM评判，并建立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [57] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD是一个用于晶体学问答的开卷管道，结合GPT-4.5生成的简洁支持内容，显著提升了小模型在X射线衍射（XRD）任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统扫描教材可能带来的版权问题，同时填补小模型在晶体学领域的知识空白。

Method: 通过GPT-4.5生成紧凑的领域特定参考内容，支持小模型理解XRD关键概念，并在217个专家级问题上评估不同视觉语言模型的表现。

Result: 使用GPT-4.5生成摘要的模型在准确性上有显著提升，尤其是在晶体学训练有限的情况下。

Conclusion: OPENXRD展示了专业开卷系统在材料科学中的潜力，并为科学领域的自然语言处理工具奠定了基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [58] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级模型PU-Lie，结合冻结BERT嵌入、可解释特征和PU学习目标，用于检测战略对话中的欺骗行为，在Diplomacy数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 战略对话中欺骗检测的复杂性和类别不平衡问题（欺骗信息占比不到5%），需要一种高效且针对性的方法。

Method: 结合冻结BERT嵌入、可解释的语言和游戏特定特征，采用PU学习目标，专注于少数标记的欺骗信息。

Result: 模型在宏观F1上达到0.60，训练参数减少650倍以上，验证了PU学习、语言可解释性和说话者感知表示的价值。

Conclusion: 在欺骗检测中，准确识别欺骗信息比识别真实信息更重要，PU学习能有效建模稀有但关键的欺骗类别。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [59] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA是一个检索增强的多代理框架，用于验证多媒体虚假信息，通过精确查询、跨验证证据聚合和多代理架构提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息的快速扩散对自动化事实核查系统提出了挑战，尤其是当信息模糊或缺乏上下文时。

Method: RAMA采用三种创新方法：战略查询制定、跨验证证据聚合和多代理集成架构。

Result: 实验表明，RAMA在基准数据集上表现优异，尤其在处理模糊或不可信信息时。

Conclusion: 结合网络证据和多代理推理对可信的多媒体验证至关重要，RAMA为可靠且可扩展的事实核查提供了新方向。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [60] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 论文提出了一种基于剪枝的微调方法，通过识别并移除与数据集特定机制相关的神经元，提升大语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常依赖数据集特定的机制，导致在新任务或分布上性能下降，因此需要一种方法增强其泛化能力。

Method: 使用Integrated Gradients量化神经元对高置信度预测的影响，识别并剪枝与数据集特定机制相关的神经元。

Result: 在多选题基准测试中，该方法显著提升了性能，优于之前的非剪枝适应方法。

Conclusion: 通过剪枝数据集特定神经元，模型能够依赖更具泛化性的表示，从而提升性能。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [61] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 论文提出了Banzhida，一个针对藏语的多语言大语言模型，通过构建最大的藏语预训练语料库，显著提升了藏语生成AI的性能。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言在现有模型中代表性不足，缺乏高质量训练语料。

Method: 收集多样化的藏语数据，构建专用清洗和处理流程，预训练多语言基础模型为Banzhida。

Result: Banzhida在多种任务中显著优于类似规模的开源模型和针对藏语的模型。

Conclusion: Banzhida填补了藏语生成AI的空白，为低资源语言提供了有效解决方案。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [62] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 研究分析了视觉隐喻（如融化的冰川表现为融化的冰手榴弹）在气候变化传播中的效果，发现其虽增加认知负荷，但能引发更深层次的认知抽象和更积极的体验。


<details>
  <summary>Details</summary>
Motivation: 探索视觉隐喻在气候变化传播中的实际效果，填补相关研究的空白。

Method: 创建MetaClimage数据库，包含隐喻和直白图像，通过人类评分和自然语言处理分析语义和情感变量。

Result: 视觉隐喻更难理解但更美观，未在效果和情感唤起上优于直白图像，但能引发更多标签和更积极的词汇。

Conclusion: 视觉隐喻在气候变化传播中具有认知和美学优势，但需权衡其复杂性。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [63] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: Swa-bhasha Resource Hub提供2020-2025年罗马化僧伽罗语到僧伽罗语转写的资源和算法，推动了僧伽罗语NLP研究。


<details>
  <summary>Details</summary>
Motivation: 为僧伽罗语NLP研究提供数据和工具支持，特别是罗马化僧伽罗语的转写任务。

Method: 收集并公开数据集和工具，进行现有转写应用的比较分析。

Result: 资源中心为研究和应用开发提供了重要支持。

Conclusion: 该资源中心对僧伽罗语NLP领域的研究和应用有显著贡献。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [64] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 提出了一种心理学启发的幽默分解机制（HDM），结合思维链（CoT）和幽默理论，显著提升了幽默翻译的质量。


<details>
  <summary>Details</summary>
Motivation: 幽默翻译在跨文化交流中至关重要，但现有大语言模型（LLMs）在幽默翻译上表现不佳，存在语言干扰和幽默缺失问题。

Method: 采用心理学启发的HDM，利用CoT模仿人类思维过程，并结合幽默理论优化翻译文本的幽默性和可读性。

Result: 在开源幽默数据集上的实验显示，该方法在幽默性、流畅性和连贯性上分别平均提升了7.75%、2.81%和6.13%。

Conclusion: HDM显著提升了幽默翻译的质量，为跨文化交流提供了更有效的工具。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [65] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 本文提出了一种名为ClaritySpeech的新框架，通过结合ASR、文本混淆和零样本TTS技术，改善痴呆症患者的语音可懂度，同时保护说话者身份。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者的语音模式异常，现有语音技术难以处理，导致沟通障碍和隐私问题。

Method: 采用ASR、文本混淆和零样本TTS技术，无需微调即可在低数据环境下工作。

Result: 在ADReSS和ADReSSo数据集上，F1分数分别下降16%和10%，但保持了50%的说话者相似性，同时显著降低了WER并提高了语音质量。

Conclusion: ClaritySpeech在提升痴呆症患者语音隐私和可懂度方面表现出色。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [66] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM是一个用于评估语言模型中数据归因方法的统一基准，通过三个关键任务衡量归因质量，并发现现有方法在不同任务中存在权衡。


<details>
  <summary>Details</summary>
Motivation: 填补语言模型数据归因方法系统性评估的空白，为研究和应用提供统一标准。

Method: 引入DATE-LM基准，通过训练数据选择、毒性/偏见过滤和事实归因三个任务评估数据归因方法。

Result: 现有方法在不同任务中表现不一，无单一方法全面占优，且性能受任务设计影响。

Conclusion: DATE-LM为未来语言模型数据归因研究奠定基础，并公开排行榜以促进社区参与。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [67] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 优化DRAGON Longformer模型用于临床文本分类，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何优化预训练模型以适应临床文本分类任务，提升医疗案例描述的二元分类效果。

Method: 通过超参数调优、领域特定预处理和架构调整（如增加序列长度、调整学习率和训练轮数）优化模型。

Result: 优化后模型性能显著提升：准确率从72.0%升至85.2%，其他指标（如F1分数）也有类似提升。

Conclusion: 优化模型在医疗领域表现优异，具有广泛临床应用潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [68] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文介绍了CoNLL-2013共享任务的定义、数据集、评估指标及参与者采用的方法和结果。


<details>
  <summary>Details</summary>
Motivation: 共享任务旨在推动语法错误纠正领域的研究。

Method: 任务定义、数据集准备、评估指标设计，并汇总参与者采用的不同方法。

Result: 展示了参与团队的评估结果。

Conclusion: 通过共享任务，促进了语法错误纠正技术的进步。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [69] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文综述了检索增强生成（RAG）与推理方法的结合，提出了一种统一的推理-检索视角，展示了如何通过高级推理优化RAG的各阶段，以及如何通过检索知识支持复杂推理。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在多步推理问题上的不足以及纯推理方法在事实基础和幻觉方面的缺陷。

Method: 通过分析推理如何优化RAG的各个阶段（Reasoning-Enhanced RAG），以及检索知识如何支持复杂推理（RAG-Enhanced Reasoning），并探讨了新兴的协同RAG-推理框架。

Result: 提出了一个统一的推理-检索视角，并展示了协同RAG-推理框架在知识密集型任务上的先进性能。

Conclusion: 论文总结了方法、数据集和开放挑战，并展望了更有效、多模态适应、可信和以人为中心的RAG-推理系统的研究方向。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [70] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 论文提出了M2SaG数据集和ViSP框架，用于多模态讽刺生成，通过PPO和对比学习提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺生成研究依赖文本模态，忽视视觉线索，且数据集图像内容与讽刺意图不匹配。

Method: 提出ViSP框架，结合PPO和对比学习，利用DIP奖励分数优化讽刺文本生成。

Result: ViSP在五个指标上超越基线模型，生成文本的讽刺分数（0.898 vs. 0.770）和事实不一致性（0.768 vs. 0.739）更高。

Conclusion: ViSP能生成更高质量的讽刺内容，数据集和代码将公开。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [71] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的ABSA方法，通过数据增强生成平衡的训练数据，并结合强化学习优化数据质量，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法因短文本和小规模不平衡数据（多为正面情感）难以学习上下文信息，数据增强虽可行但质量难以保证。

Method: 利用LLM生成增强数据以构建更大规模且平衡的训练集，并通过强化学习优化数据增强质量。

Result: 在英文ABSA基准数据集上表现优于现有方法。

Conclusion: 该方法通过数据增强和强化学习显著提升了ABSA任务的性能。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [72] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax是一个协议驱动的多智能体协作框架，通过标准化通信和分层记忆系统提升协调性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统单用途AI系统在协调性、记忆重用和任务分解方面存在不足，难以满足现代企业复杂任务的需求。

Method: GoalfyMax采用基于MCP的A2A通信层和XP分层记忆系统，支持异步协调和持续学习。

Result: 实验表明，GoalfyMax在复杂任务协调和适应性上优于基线框架。

Conclusion: GoalfyMax为多智能体系统提供了可扩展的未来解决方案。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [73] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 论文提出了Ref-Long基准，用于评估长上下文语言模型（LCLMs）的长上下文引用能力，发现现有模型（包括GPT-4o）在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 长上下文引用是LCLMs的关键任务，但尚未充分研究，因此需要专门的评估基准。

Method: 设计了Ref-Long基准，包含三个子集（从合成到真实场景），要求模型识别引用特定关键词的文档索引。

Result: 实验显示13种LCLMs在长上下文引用任务中存在显著不足。

Conclusion: 通过分析揭示了LCLMs的局限性，并提供了数据集和代码以促进未来研究。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [74] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 论文研究了提示质量对大型语言模型（LLM）在机器翻译和翻译评估任务中表现的影响，发现错误类型和数量对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器翻译中表现优异，但对提示中的错误和扰动敏感，研究旨在系统评估这种影响。

Method: 通过定量分析和定性研究，评估不同类型和程度的提示错误对LLM性能的影响。

Result: 提示质量显著影响翻译性能，字符级和组合噪声对性能影响最大；LLM在随机噪声下仍能翻译。

Conclusion: 提示质量对LLM的指令遵循能力影响更大，而非直接影响翻译质量；LLM在极端噪声下仍具翻译能力。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [75] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 论文探讨了如何利用现有模型为未支持语言（如白俄罗斯语）生成定义，提出新数据集并验证了模型适应性的可行性。


<details>
  <summary>Details</summary>
Motivation: 支持更多语言的定义建模任务，帮助词典编纂者扩展语言覆盖范围。

Method: 提出一个包含43,150条定义的白俄罗斯语数据集，并测试现有模型的适应性。

Result: 实验表明，模型适应需要少量数据，但自动评估指标存在局限性。

Conclusion: 现有模型可适应新语言，但需改进评估方法。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [76] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: NMIXX是一种针对金融领域的跨语言嵌入模型，通过微调18.8K高质量三元组数据，显著提升了金融语义的捕捉能力，并在低资源语言（如韩语）中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型难以捕捉金融领域的专业语义，尤其是在低资源语言中，存在领域术语、时间语义变化和双语词汇不对齐等问题。

Method: 提出NMIXX模型，通过微调包含领域内同义句、语义偏移的负样本和精确韩英翻译的三元组数据，并发布KorFinSTS基准数据集。

Result: NMIXX在英语FinSTS和韩语KorFinSTS上分别提升Spearman's rho 0.10和0.22，优于其他基线模型。

Conclusion: NMIXX和KorFinSTS的发布为金融领域的跨语言表示学习提供了有效工具，同时强调了分词器设计在低资源语言中的重要性。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [77] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy是一个Python库，用于模拟认知单层和多层网络中的激活扩散，支持结构-功能关系的数值模拟研究。


<details>
  <summary>Details</summary>
Motivation: 研究认知、心理和临床现象中的激活动态如何反映网络结构，并通过模拟验证理论。

Method: 使用SpreadPy进行数值模拟，基于实证或理论网络建模，分析激活扩散的动态过程。

Result: 通过三个案例研究验证了SpreadPy的实用性：数学焦虑学生的知识网络差异、创造力任务中的认知负荷影响、以及失语症患者的词汇网络与错误类型的关联。

Conclusion: SpreadPy为心理学、神经科学和教育研究提供了一个灵活且可重复的工具，支持对个体差异和认知障碍的机制研究。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [78] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 本文首次研究了阿拉伯语的知识编辑（KE），评估了四种方法在阿拉伯语翻译数据集上的表现，发现参数化方法在跨语言泛化上表现较差，而指令调优方法更稳健。


<details>
  <summary>Details</summary>
Motivation: 研究动机是填补阿拉伯语知识编辑的空白，探索其在形态丰富语言中的行为。

Method: 评估了ROME、MEMIT、ICE和LTE四种方法，并在Llama-2-7B-chat模型上进行了实验，扩展了LTE到多语言设置。

Result: 参数化方法在跨语言泛化上表现不佳，而指令调优方法表现更稳健；多语言联合训练提升了编辑和迁移能力。

Conclusion: 研究为阿拉伯语知识编辑提供了基准和多语言训练数据，支持未来研究。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [79] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 本文提出了一种基于GRPO的方法，显著提升了泰国法律问答系统中LLM的法律引用准确性和回答质量，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在泰国法律问答中表现有限，尤其是需要复杂法律推理的问题。

Method: 采用Group-Relative Policy Optimization (GRPO)方法，结合BGE-M3嵌入作为语义相似性奖励，显著降低计算成本。

Result: 在NitiBench基准测试中，GRPO实现了90%的引用F1提升和31%的联合质量指标提升。

Conclusion: 该方法在复杂法律推理任务中表现出更强的鲁棒性，为泰国法律LLM提供了一种高效且资源节约的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [80] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval是一个多语言文化评估框架，通过动态文化问题构建和反事实重述分析，评估大型语言模型的文化意识和偏见，揭示语言与文化对齐对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在文化偏见和跨文化理解能力不足的问题，尤其在服务全球多样化用户时。

Method: 提出MCEval框架，采用动态文化问题构建、反事实重述和混杂重述方法，覆盖13种文化和语言。

Result: 评估生成39,897个文化意识和17,940个文化偏见实例，显示性能差异与语言-文化对齐相关，并揭示公平性问题。

Conclusion: MCEval是首个全面的多语言文化评估框架，为LLMs的文化理解提供深入洞察。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [81] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）的潜在空间几何结构对语义理解至关重要，高维语义信息集中在低维子空间中，且在不同领域形成线性可分的表示。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs内部如何组织与语义理解相关的表示，以改进模型对齐和行为解释。

Method: 对11种基于Transformer的解码器模型进行大规模实证研究，分析6个科学主题和12个层的隐藏状态。

Result: 发现深层语义信息集中在低维子空间中，且在不同领域线性可分；深层和结构化提示下可分性更明显。

Conclusion: 潜在空间的几何结构为开发直接操作表示的几何感知工具（如防御对抗性内容）提供了基础。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [82] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 提出一种自适应的课程学习范式，利用预训练语言模型预测样本难度，优化微调顺序，提升学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖人工定义的难度指标（如文本长度），可能无法准确反映模型视角。

Method: 基于预训练语言模型预测的难度分数，探索从易到难、从难到易及混合采样的训练策略。

Result: 在四个自然语言理解数据集上验证，相比随机采样，方法收敛更快且性能更优。

Conclusion: 自适应课程学习能更高效地指导模型训练，提升性能。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [83] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 论文重新定义了点击诱饵（clickbait），提出好奇心缺口是其核心特征，并创建了首个西班牙语点击诱饵检测开源数据集TA1C。


<details>
  <summary>Details</summary>
Motivation: 当前对点击诱饵的定义缺乏共识，研究旨在明确其与类似现象的区别，并提供更客观的检测方法。

Method: 通过细化概念界限和标注标准，减少主观性，创建并发布了TA1C数据集，包含3,500条手动标注的推文。

Result: 数据集达到0.825的Fleiss' K评分，基线模型F1得分为0.84。

Conclusion: 研究为点击诱饵检测提供了更清晰的定义和高质量数据集，推动了相关领域的发展。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [84] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型通过上下文学习执行未见任务的能力，揭示了其内部机制如何驱动任务级泛化，重点分析了‘加一’任务的泛化机制。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何通过内部计算实现任务级泛化，尤其是对反事实任务（如‘加一’加法）的适应能力。

Method: 使用路径修补等电路式可解释性技术，分析模型在‘加一’任务中的内部计算，并识别关键机制。

Result: 发现了一种函数归纳机制，该机制由多个注意力头并行控制，并能推广到其他任务（如移位选择题和基8加法）。

Conclusion: 研究揭示了语言模型中可重用和可组合的结构如何支持任务级泛化，为模型内部机制提供了新见解。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [85] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种改进RAG系统的方法，通过分层文本分割和聚类生成更有语义意义的块，提高了检索的精确性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 传统分块方法未能充分捕捉语义信息，且未考虑文本结构，导致检索效果不佳。

Method: 提出了一种结合分层文本分割和聚类的新框架，利用段级和簇级向量表示进行检索。

Result: 在NarrativeQA、QuALITY和QASPER数据集上，该方法优于传统分块技术。

Conclusion: 通过改进分块策略，RAG系统的检索效果得到显著提升。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [86] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM是一种小型双向掩码语言模型家族，参数仅4亿，性能媲美大175倍的模型，适用于奖励建模任务。


<details>
  <summary>Details</summary>
Motivation: 解决大型解码器模型在奖励建模中推理成本高的问题。

Method: 结合FLAN风格提示、定向低秩适应（DoRA）和层冻结技术。

Result: 在RewardBench上表现优异，资源消耗显著减少。

Conclusion: 轻量级双向架构有望成为高效、可扩展的偏好建模替代方案。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [87] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics是一个开创性基准，建立了组学表达与分子文本描述的一一对应关系，并提出了生成框架ToDi，用于生成生物相关、化学有效的类命中分子。


<details>
  <summary>Details</summary>
Motivation: 解决靶向药物发现中缺乏异构数据和统一框架的问题。

Method: 提出TextOmics基准和ToDi框架，利用两个编码器（OmicsEn和TextEn）和多条件扩散模型（DiffGen）进行分子生成。

Result: ToDi在实验中表现优于现有方法，并在零样本治疗分子生成中显示出潜力。

Conclusion: TextOmics和ToDi为靶向药物发现提供了有效工具，具有广泛应用前景。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [88] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 论文介绍了CodeJudgeBench，一个专门评估LLM作为裁判在代码生成、修复和单元测试生成任务中性能的基准，发现思维模型表现更优但存在随机性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 填补LLM作为裁判在编码任务中缺乏专门评估基准的空白。

Method: 引入CodeJudgeBench基准，评估26个LLM裁判模型在三个编码任务中的表现，并研究最优提示策略。

Result: 思维模型显著优于非思维模型，但所有模型在判断中存在随机性；配对比较和保留完整响应可提升性能。

Conclusion: LLM作为裁判在编码任务中表现不稳定，需进一步优化以提高可靠性和一致性。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [89] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 该研究提出了一种新框架，通过联合学习风险和保护因素对自杀风险的动态影响，预测后续自杀风险，并引入了一个包含保护因素的数据集和动态因素影响学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注风险因素，忽视了保护因素的作用，且未能捕捉自杀风险的动态变化。

Method: 提出了一个保护因素感知数据集和动态因素影响学习方法，结合Reddit帖子数据，学习风险和保护因素的动态影响。

Result: 实验表明，该模型显著优于现有方法，并提供可解释的权重，帮助临床医生理解自杀模式。

Conclusion: 该框架为自杀风险预测提供了更全面的视角，有助于制定更有针对性的干预策略。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [90] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo是一种基于进化的LLM压缩方法，通过层折叠高效探索压缩解空间，支持单目标和多目标优化，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）因计算需求高面临部署和使用障碍，模型压缩是缓解问题的关键，但现有方法如结构化剪枝成本高且可能忽略更优解。

Method: GeLaCo采用基于种群的搜索和模块相似性适应度函数，支持单目标和多目标进化压缩搜索，首次建立压缩与质量的帕累托前沿。

Result: 在基础模型和指令调优模型上，GeLaCo通过困惑度和生成评估优于现有方法。

Conclusion: GeLaCo为LLM压缩提供了一种高效且性能优越的解决方案。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [91] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）无法代表多样文化道德框架，尽管其语言能力强。模型在19种文化背景下与人类道德直觉存在显著差距，且模型规模增加并未改善文化代表性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否能真正代表人类价值观，还是仅对其进行平均化处理。

Method: 应用道德基础问卷（Moral Foundations Questionnaire）在19种文化背景下比较LLMs与人类基线数据。

Result: LLMs系统性地同质化道德多样性，模型规模增加未显著提升文化代表性。

Conclusion: 当前AI对齐方法存在根本性局限，需更接地气的对齐目标和评估指标以确保AI系统代表多样人类价值观。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [92] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 提出了一种名为CRFT的新方法，通过优化关键表示来提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 直接使用原生ReFT方法在复杂推理任务中表现不佳，因为固定位置的表示对输出的影响不确定。

Method: 通过信息流分析识别关键表示，并在低秩线性子空间中动态优化这些表示，同时冻结基础模型。

Result: 在八个算术和常识推理基准测试中验证了方法的有效性和效率，并在少样本设置中提升了16.4%的单次准确率。

Conclusion: CRFT展示了表示级优化在复杂推理任务中的潜力，为传统PEFT方法提供了轻量且强大的替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [93] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种结合LLM和Transformer的新架构，用于时间序列预测，融合语义和时间信息，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和Transformer在时间序列预测中各有不足，LLM擅长语义但难以处理连续数值数据，而Transformer缺乏高级语义理解。

Method: 设计了一种混合架构，融合LLM的语义表示和Transformer的时间动态信息，生成混合表示。

Result: 实验表明，该方法在基准数据集上表现优于现有模型。

Conclusion: 通过结合LLM和Transformer的优势，新架构显著提升了时间序列预测的性能。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [94] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 提出了一种基于任务的特征蒸馏方法，无需引入新参数即可在不同隐藏层维度的师生模型间传递知识。


<details>
  <summary>Details</summary>
Motivation: 传统特征蒸馏方法要求师生模型隐藏层尺寸相同，限制了学生模型的灵活性，而线性投影方法会引入额外参数并降低性能。

Method: 通过识别教师模型中与任务最相关的隐藏单元，直接将其激活蒸馏到学生模型。

Result: 在分类、指令跟随和摘要等任务上优于基线方法，性能提升达3%。

Conclusion: 该方法灵活且高效，无需额外参数即可实现知识蒸馏，适用于多种任务。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [95] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 该研究探索了大型语言模型（LLMs）在将侮辱性文本转换为非侮辱性版本时的表现，评估了Gemini、GPT-4o、DeepSeek和Groq等模型的能力，发现Groq与其他模型结果差异显著。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现优异，但其在识别和转换侮辱性文本方面的效果仍需探索。

Method: 使用LLMs转换侮辱性文本（推文和评论），保留文本意图，并通过情感和语义分析评估原始与转换后数据集。

Result: Groq与其他模型结果差异显著，GPT-4o与DeepSeek-V3表现相似。

Conclusion: LLMs在侮辱性文本转换方面具有潜力，但不同模型表现差异明显，需进一步研究。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [96] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 论文介绍了Absher基准，用于评估大语言模型在沙特阿拉伯方言中的表现，揭示了模型在文化和上下文理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型对沙特阿拉伯方言和文化细微差别的理解能力，以提升其在阿拉伯语应用中的实际表现。

Method: 设计包含18,000多个多选题的Absher基准，涵盖六类任务，评估多种先进大语言模型。

Result: 模型在需要文化推断或上下文理解的任务中表现不佳，显示出显著的性能差距。

Conclusion: 需要方言感知训练和文化对齐的评估方法，以改进大语言模型在阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [97] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 提出了一种基于进化搜索的自动离散提示优化方法，针对复杂任务和小型模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务中提示优化文本量大、小型模型对提示设计敏感的问题。

Method: 采用两阶段进化搜索：第一阶段通过语法引导的遗传编程合成提示创建程序；第二阶段对最佳程序进行局部搜索。

Result: 在三个小型通用LLM和四个领域特定任务中，优于PromptWizard、OPRO和RL-Prompt。

Conclusion: 该方法在几乎所有任务-模型组合中提升性能，且性能下降极小。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [98] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 论文提出了一种基于Growth Bound Matrices（GBM）的新正则化技术，用于提升NLP模型对对抗攻击（如同义词替换）的鲁棒性，特别针对LSTM、S4和CNN架构。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域有进展，但模型仍易受对抗攻击影响，尤其是循环网络和现代状态空间模型（如S4）的鲁棒性研究不足。

Method: 通过计算GBM来减少输入扰动对模型输出的影响，重点关注LSTM、S4和CNN三种架构。

Result: 实验表明，该方法在对抗鲁棒性上比现有基线提升高达8.8%，并在对抗防御中优于多种先进方法。

Conclusion: GBM方法有效提升了NLP模型的鲁棒性，特别是在对抗攻击和干净文本上的表现均有显著改进。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [99] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在语言学研究中作为可靠分析工具的潜力，特别是在涉及运动动词的时间表达中情感意义的生成。通过四项心理语言学实验，研究发现人类与AI的反应高度一致，表明LLMs可以补充传统人类实验。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证LLMs（如GPT-4）是否能复现人类在语言学任务中的细微判断，以评估其作为研究工具的可靠性。

Method: 通过四项心理语言学实验（涉及情感意义生成、情感变化、情绪语境中的动词选择及句子-表情符号关联），分别对人类参与者和LLM进行任务复现，并进行统计分析。

Result: 结果显示人类与AI反应高度一致（Spearman's rho = .73-.96），仅在少数情况下存在微小差异，但不影响整体解释结果。

Conclusion: 研究支持LLMs作为语言学研究中可信赖的辅助工具，能够扩展研究规模且不损害解释有效性。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [100] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 提出了一种分层隐喻处理模型，将意义分为内容分析、概念融合和语用意三个层次，为计算系统提供更丰富的隐喻解释框架。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义并非简单的概念映射，而是复杂的认知现象，需要多层次整合。

Method: 分层模型包括内容分析、概念融合和语用意三个层次，逐步处理隐喻的各个层面。

Result: 模型为计算系统提供了更深入、更语境敏感的隐喻理解方法。

Conclusion: 该分层框架为计算隐喻处理奠定了基础，支持超越表面关联的深层推理。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [101] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）能通过文本描述理解图结构，并提出了一种新视角（ISF）解释其机制。


<details>
  <summary>Details</summary>
Motivation: 探索解码器-仅Transformer架构如何理解图结构，特别是子结构提取任务。

Method: 通过实证和理论分析提出ISF视角，验证其在LLM中的内部动态，并扩展到多种图类型。

Result: LLM能有效提取子结构，ISF揭示了Transformer的层间一致性动态。

Conclusion: 序列型Transformer能高效处理图数据子结构提取，为理解其机制提供了新视角。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [102] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究探讨了LLMs在任务导向对话中提出澄清问题的能力，发现人类和LLMs在模糊性处理上表现差异显著，且LLMs的提问能力可能与推理能力相关。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在异步任务导向对话中提出澄清问题的能力，并与人类行为进行对比。

Method: 结合现有Minecraft对话语料库的两种标注，构建新语料库，比较LLMs与人类在模糊性情况下的提问行为，并测试不同推理方法对LLMs提问能力的影响。

Result: 人类很少为指代模糊提问，但常为任务不确定性提问；LLMs则相反。推理能力可能提升LLMs的提问频率和相关性。

Conclusion: LLMs的澄清提问能力与推理能力相关，但与人类行为差异显著，需进一步研究其机制。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [103] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 研究比较了经典编码器和新一代LLM在仇恨言论检测上的表现，验证大规模模型是否提升实际效果。


<details>
  <summary>Details</summary>
Motivation: 在线平台难以平衡仇恨言论过滤与言论自由，需验证大规模LLM是否优于传统模型。

Method: 通过基准测试比较经典编码器与新一代LLM在仇恨言论检测任务（Hate or No Hate）上的表现。

Result: 未明确提及具体结果，但研究旨在验证LLM的实际效果。

Conclusion: 研究为验证大规模LLM在仇恨言论检测中的实用性提供了实验基础。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [104] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MLAR是一种基于RPA和LLM的创新ATS系统，通过三层处理优化简历筛选，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统招聘流程在简历筛选和候选人短名单上存在时间和资源瓶颈，MLAR旨在解决这些问题。

Method: MLAR利用LLM分三层处理：提取职位关键特征、解析简历信息、语义匹配候选人。

Result: MLAR处理2400份简历时，平均每份5.4秒，比主流RPA平台快16.9%-17.1%。

Conclusion: MLAR为现代招聘需求提供了高效、准确且可扩展的解决方案。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [105] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 论文比较了扩散生成文本（LLaDA）和自回归生成文本（LLaMA）的检测难度，发现LLaDA更接近人类文本，现有检测方法对其效果不佳，需开发针对扩散模型的检测技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展引发了对AI生成文本可靠检测的担忧，尤其是扩散模型的效果尚不明确。

Method: 使用2000个样本，比较LLaDA和LLaMA在困惑度、爆发性、词汇多样性、可读性及BLEU/ROUGE分数上的表现。

Result: LLaDA在困惑度和爆发性上接近人类文本，导致现有检测方法高假阴性率；LLaMA困惑度低但词汇保真度差。单一指标无法区分扩散输出和人类写作。

Conclusion: 需开发针对扩散模型的检测技术，如混合模型、扩散特定风格特征和鲁棒水印。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [106] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: Mixture-of-Recursions (MoR) 是一种结合参数共享和自适应计算的高效框架，通过递归Transformer实现，显著降低计算和内存需求，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注参数共享或自适应计算，未能同时实现两者，导致训练和部署成本高昂。

Method: MoR 通过共享层堆栈和轻量级路由器实现参数效率和自适应计算，动态分配递归深度，选择性缓存键值对。

Result: 在135M到1.7B参数范围内，MoR在相同训练FLOPs下显著降低验证困惑度，提升少样本准确率，并提高吞吐量。

Conclusion: MoR 是一种高效路径，能够在不增加大模型成本的情况下实现大模型质量。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [107] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST框架通过同时测试多个问题来评估大型推理模型（LRMs），揭示了现有单问题评估的局限性，并展示了更强的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于单问题推理，无法反映真实世界的多上下文压力需求，且易受数据污染影响。

Method: 提出REST框架，同时测试多个问题，评估模型的上下文优先级分配、跨问题干扰抵抗和动态认知负载管理能力。

Result: SOTA模型在REST下表现显著下降，REST显示出更强的区分能力，揭示了单问题评估中未发现的性能差异。

Conclusion: REST是一种高效、面向未来的评估范式，更贴近真实需求，减少对人类标注的依赖。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 论文提出V2-VLNCE（多视角视觉语言导航）和VIL（视角不变学习），通过对比学习和师生框架提升导航策略对视角变化的鲁棒性，并在多个数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有导航策略对视角变化敏感，影响智能体在连续环境中的导航性能。

Method: 提出VIL，采用对比学习框架学习稀疏且视角不变的特征，并引入师生框架优化Waypoint Predictor Module。

Result: 在V2-VLNCE上性能提升8-15%，在标准VLNCE和RxR-CE数据集上也表现优异。

Conclusion: VIL可作为即插即用的后训练方法，提升导航策略的鲁棒性且不影响标准视角性能。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [109] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出了一种利用面部生物特征异常模式检测深度伪造视频的新方法，评估了其可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术易被用于欺诈和政治虚假信息，亟需有效的检测手段。

Method: 利用面部生物特征中的异常模式，开发了一种机器学习技术。

Result: 在大规模数据集上评估了该技术的可靠性，并测试了对未知生成器的泛化能力。

Conclusion: 该方法能有效检测深度伪造视频，具有实际应用潜力。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [110] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM是一种无需数据和任务无关的方法，用于减少视觉语言模型中的隐性偏差，通过生成场景描述和对比式去偏损失实现。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）常继承并放大训练数据中的偏差，导致预测结果不公。PRISM旨在无需预定义偏差类别或额外数据的情况下解决这一问题。

Method: PRISM分两阶段：1）用LLM生成包含虚假相关性的场景描述；2）使用对比式去偏损失学习投影，最小化虚假相关性并保持图像与文本嵌入的对齐。

Result: 在Waterbirds和CelebA数据集上，PRISM表现优于现有去偏方法。

Conclusion: PRISM提供了一种有效且无需外部数据的去偏解决方案，代码已开源。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [111] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: HMR-ViT结合时空与运动学信息，通过Vision Transformer和CRM提升人体网格恢复精度。


<details>
  <summary>Details</summary>
Motivation: 现有HMR方法仅利用时空或运动学信息，未结合两者，限制了性能。

Method: 构建时空-运动学特征图像，使用CRM和Vision Transformer，通过回归网络推断SMPL参数。

Result: 在3DPW和Human3.6M数据集上表现优异。

Conclusion: HMR-ViT通过结合时空与运动学信息，显著提升了HMR任务的性能。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [112] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合NeRF和MPM的新框架，通过视觉观测推断颗粒材料特性，摩擦角估计误差在2度以内。


<details>
  <summary>Details</summary>
Motivation: 在直接测量不可行的情况下，通过视觉观测逆向分析颗粒材料特性。

Method: 生成合成实验数据，用NeRF重建3D几何，MPM模拟材料行为，贝叶斯优化估计摩擦角。

Result: 摩擦角估计误差在2度以内。

Conclusion: 该方法为颗粒材料特性表征提供了有效解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [113] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，旨在提升多模态基础模型生成标签的质量，通过结合多阶段数据验证策略和人类专业知识，解决现有方法在数据质量验证上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注数据量而非质量，且缺乏全面验证大规模无标注数据的手段，导致多模态模型性能受限。

Method: 提出VISTA框架，整合多阶段数据验证策略与人类专家知识，用于识别和修正标签中的隐藏问题。

Result: 在开放词汇图像分割任务中，VISTA通过定量和定性分析验证了其有效性。

Conclusion: VISTA显著提升了多模态模型生成标签的质量，为复杂任务提供了更可靠的数据支持。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [114] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个用于构建模块化脑部病变图像分析管道的Python工具包，旨在简化开发流程并提供灵活的预处理功能。


<details>
  <summary>Details</summary>
Motivation: 为临床和科研实践提供一种简化复杂工作流的方法，减少认知负担，支持多模态图像分析。

Method: 基于Pythonic原则设计，包含灵活的预处理模块（如配准、去颅骨等），利用BraTS挑战赛算法合成缺失模态、修复病变并生成肿瘤分割。

Result: 支持量化分割模型性能，适用于脑部病变（如胶质瘤、转移瘤和多发性硬化）分析，并可扩展到其他生物医学图像应用。

Conclusion: BrainLesion Suite是一个功能强大且灵活的工具包，适用于脑部病变分析及其他生物医学图像处理任务。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [115] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 论文提出两种对比损失函数，用于解决类别不平衡数据中尾部类别图像合成多样性不足的问题，同时保持头部类别的保真度和多样性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡数据导致尾部类别图像合成多样性降低，模式崩溃。目标是提升尾部类别多样性，同时不影响头部类别。

Method: 引入两种对比损失函数：无监督InfoNCE损失和MSE损失，前者增加尾部类别图像间的差异，后者通过对比条件与非条件生成增强尾部类别多样性。

Result: 方法在多个数据集（如CIFAR10/100-LT等）上优于标准DDPM和其他替代方法。

Conclusion: 对比学习框架简单有效，成功解决了类别不平衡扩散模型中的多样性问题。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [116] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文探讨了当前视频理解模型的局限性，提出了“无限视频理解”作为未来研究的前沿目标，并指出了实现这一目标的关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLMs）和多模态扩展（MLLMs）在处理长时间视频时仍面临计算、内存和时序一致性等挑战，因此需要探索更高效的解决方案。

Method: 通过分析现有技术的局限性（如Video-XL-2、HoPE和VideoRoPE++等），提出将“无限视频理解”作为研究方向，并探讨了相关技术需求。

Result: 论文未提供具体实验结果，但提出了实现无限视频理解所需的关键技术方向，如流式架构、持久内存机制和事件中心推理等。

Conclusion: 无限视频理解是一个具有挑战性但重要的研究方向，将为多媒体和AI领域带来创新，并推动相关技术的发展。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [117] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight是一种无需训练的方法，通过利用注意力稀疏性优化视觉语言模型（VLM）的推理，减少计算量（FLOPs降低32%-41%）且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉数据的加入显著增加了提示长度和注意力计算的复杂度，导致预填充时间延长。通过分析注意力模式，发现存在稀疏性，可以优化计算。

Method: 提出BlindSight方法，利用输入模板感知的注意力稀疏掩码，根据数据集样本为每个注意力头分配稀疏类别（如仅接收器、文档掩码等）。

Result: 在Qwen2-VL等模型上测试，平均减少32%-41%的FLOPs，准确性变化在-2%到+2%之间。

Conclusion: BlindSight有效优化了VLM推理，显著减少计算开销且对准确性影响极小。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [118] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 论文综述了定量遥感反演方法的演变，从物理模型到机器学习，再到基础模型，并讨论了其应用场景、局限性和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着遥感系统和人工智能的发展，传统物理模型逐渐被数据驱动和基础模型方法取代，需要系统梳理方法演变及其挑战。

Method: 系统回顾了物理模型（如PROSPECT）、机器学习（如深度学习）和基础模型（如SatMAE）的方法，比较了建模假设、应用场景和局限性。

Result: 总结了各范式的优缺点，强调了基础模型在自监督预训练、多模态整合和跨任务适应方面的进展。

Conclusion: 展望了下一代基础模型的发展方向，强调统一建模能力、跨域泛化和物理可解释性。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [119] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 论文提出了一种无需微调的方法，通过扰动和跟踪传播从视频生成模型中提取光流，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以利用冻结的自监督视频模型（仅用于未来帧预测）直接提取光流，避免因标签稀缺和合成数据差距带来的微调问题。

Method: 基于CWM范式，通过注入微小扰动并跟踪其传播，利用KL散度计算预测分布差异，提出KL-tracing方法。

Result: 在真实数据集TAP-Vid DAVIS和合成数据集TAP-Vid Kubric上分别实现了16.6%和4.7%的相对改进。

Conclusion: 表明通过可控生成视频模型的因果提示是一种可扩展且高效的光流提取方法。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [120] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种名为MI CAM的后验视觉解释方法，通过激活映射和互信息加权生成显著性可视化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在医疗和自动化电厂等关键领域的应用，理解卷积神经网络的内部机制及其推理原因变得至关重要。

Method: MI CAM基于激活映射，通过特征图与输入图像的互信息加权，生成线性组合的显著性可视化，并通过反事实分析验证因果解释。

Result: MI CAM在定性和定量指标上优于部分现有方法，与最先进方法表现相当。

Conclusion: MI CAM提供了无偏的模型推理解释，并在视觉性能和解释能力上表现出色。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [121] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo利用放射科医生的眼动视频序列提升LVLMs在胸部X光分析中的性能，显著提高了报告生成和疾病诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了眼动的时序信息，而RadEyeVideo通过整合眼动视频序列，捕捉时空动态，以增强LVLMs的临床任务表现。

Method: 提出RadEyeVideo方法，将眼动数据作为视频序列输入到支持视频的LVLMs中，用于胸部X光报告生成和疾病诊断。

Result: 模型性能在报告生成任务中提升24.6%，两项任务平均提升15.2%，甚至超越专业医学LVLMs。

Conclusion: RadEyeVideo展示了专家知识（眼动信息）与LVLMs结合能显著提升通用模型在临床任务中的能力，为医疗图像分析提供了可扩展的人本方法。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [122] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD利用Stable Diffusion（SD）模型增强3D点云的自监督学习，通过点云引导去噪图像并提取SD特征，提升3D表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型受限于小规模数据集，而SD模型在大规模数据集上表现优异，因此探索如何利用SD模型提升3D自监督学习。

Method: 提出PointSD框架，将SD模型的文本编码器替换为3D编码器，训练点云到图像的扩散模型，并提取SD特征以对齐3D主干网络特征。

Result: 实验证明PointSD能有效提升点云自监督学习在下游任务中的表现。

Conclusion: SD模型可用于增强3D点云的自监督学习，PointSD框架为3D表示学习提供了新思路。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [123] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），解决了传统方法的错误累积和实时性限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法在推理阶段无法避免错误累积，而扩散模型虽能高质量生成但迭代性质限制了实时性。

Method: 采用混合自回归和扩散模型的方法，设计了多尺度姿态表示模块和置信感知因果注意力机制。

Result: 在PHOENIX14T和How2Sign数据集上验证了方法在生成质量和实时流效率上的有效性。

Conclusion: 混合方法结合了两种模型的优势，显著提升了手语生成的准确性和实时性。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [124] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出了首个针对人-物交互（HOI）检测的鲁棒性基准RoHOI，并提出了语义感知掩码渐进学习（SAMPL）方法，显著提升了模型在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型在真实世界中的性能因环境干扰（如遮挡、噪声）而下降，缺乏鲁棒性评估标准。

Method: 提出了RoHOI基准，包含20种干扰类型和新鲁棒性指标；设计了SAMPL方法，通过动态优化提升模型鲁棒性。

Result: 实验表明SAMPL方法优于现有技术，为鲁棒HOI检测设定了新标准。

Conclusion: RoHOI基准和SAMPL方法为提升HOI检测的鲁棒性提供了有效工具和方向。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [125] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 论文提出MG-CLIP方法，通过分析CLIP模型中的模态间隙变化，改进其在类增量学习中的性能，无需额外回放数据。


<details>
  <summary>Details</summary>
Motivation: 利用CLIP模型在持续学习中的潜力，关注其模态间隙对知识保留的影响。

Method: 提出MG-CLIP方法，通过模态间隙保留和补偿来减少遗忘并增强新数据学习能力。

Result: 在多个基准测试中表现优于现有方法。

Conclusion: 模态间隙是持续学习的关键因素，MG-CLIP方法简单有效。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [126] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen是一个新的文本-动作数据集，包含高质量动作捕捉数据和详细的文本标注，支持长期动作生成研究。MoMask++模型通过多尺度标记序列和生成掩码变换器，在HumanML3D和SnapMoGen基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法受限于短文本或通用提示，数据集约束导致细粒度控制和泛化能力不足。

Method: 提出SnapMoGen数据集（20K动作片段，44小时，122K详细文本描述），改进生成掩码建模方法，开发MoMask++模型，利用多尺度标记序列和单一生成掩码变换器。

Result: MoMask++在HumanML3D和SnapMoGen基准上表现最佳，并能通过LLM处理用户提示。

Conclusion: SnapMoGen数据集和MoMask++模型显著提升了文本到动作生成的细粒度控制和泛化能力。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [127] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM提出了一种基于大型语言模型（LLM）的姿态估计框架，通过非线性MLP视觉语言连接器提升定位精度，优于现有方法LocLLM。


<details>
  <summary>Details</summary>
Motivation: 传统姿态估计方法依赖关键点先验，泛化能力有限；语言引导方法如LocLLM的线性投影器无法捕捉复杂空间-文本交互。

Method: PoseLLM用非线性MLP替换线性投影器，实现分层跨模态特征转换，增强视觉块与文本关键点描述的融合。

Result: 在COCO验证集上达到77.8 AP，优于LocLLM +0.4 AP，同时在Human-Art和MPII上保持强零样本泛化能力。

Conclusion: 简单而强大的非线性连接器显著提升定位精度且不牺牲泛化能力，推动了语言引导姿态估计的先进水平。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [128] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 提出了一种名为$I^{2}$-World的高效4D占用预测框架，通过解耦场景标记化和采用编码器-解码器架构，显著提升了性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂3D场景标记化的挑战，以改进自动驾驶系统中的场景预测和生成。

Method: 采用双标记器设计（场景内和场景间），结合多尺度残差量化和残差聚合，以及编码器-解码器架构。

Result: 在4D占用预测中，mIoU和IoU分别提升25.1%和36.9%，训练内存仅需2.9 GB，推理速度达37.0 FPS。

Conclusion: $I^{2}$-World在性能和效率上均优于现有方法，为自动驾驶系统提供了强大的场景预测工具。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [129] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 论文提出了一种名为稳定分数蒸馏（SSD）的新方法，解决了现有文本引导图像和3D编辑方法在稳定性、空间控制和编辑强度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Delta Denoising Score依赖复杂的辅助结构，导致优化信号冲突和局部编辑不精确，限制了编辑效果。

Method: SSD通过锚定单一分类器到源提示，利用Classifier-Free Guidance方程实现跨提示对齐，并引入常量空文本分支稳定优化过程。

Result: SSD在2D和3D编辑任务中（如NeRF和文本驱动风格编辑）取得了最先进的结果，收敛更快且复杂度更低。

Conclusion: SSD为文本引导编辑提供了高效、稳定的解决方案，保持了原始内容结构并增强了编辑强度。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [130] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的RGB与深度模态融合方法，通过对比无监督学习和课程学习增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度信息对场景外观变化具有鲁棒性且携带3D空间细节，因此融合RGB和深度模态以提升泛化能力。

Method: 使用独立的CNN处理不同模态，结合卷积特征输入可扩展视觉Transformer；设计对比无监督学习方案和课程学习计划。

Result: 通过模态融合和无监督学习提升了样本效率，课程学习实现了sim2real的灵活迁移。

Conclusion: 提出的方法有效融合多模态信息，增强了模型的泛化能力和迁移性能。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [131] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文研究了Few-Shot Class-Incremental Learning (FSCIL)中基于提示池方法的性能下降问题，并提出了一种新的空间提示方法LGSP-Prompt，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: FSCIL在数据稀缺和增量学习的双重挑战下表现不佳，现有提示池方法在FSCIL中的有效性尚未探索。

Method: 提出LGSP-Prompt方法，将提示学习从token维度转移到空间维度，结合局部空间特征和全局频域表示生成空间提示。

Result: 实验表明，LGSP-Prompt在多个FSCIL基准测试中达到最优性能，显著优于现有方法。

Conclusion: LGSP-Prompt通过空间提示解决了token维度饱和问题，有效提升了FSCIL的性能。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [132] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 论文提出MCA-LLaVA模型，通过改进RoPE的长距离衰减问题，缓解LVLMs中的图像对齐偏差，减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）中的幻觉问题主要由多模态特征不对齐引起，尤其是RoPE的长距离衰减导致图像与指令的感知不均。

Method: 提出基于曼哈顿距离的MCA-LLaVA模型，将一维序列顺序与二维空间位置结合，实现多方向空间衰减。

Result: 实验表明MCA-LLaVA在幻觉和通用基准测试中表现优异。

Conclusion: MCA-LLaVA通过优化位置建模，显著改善了多模态对齐，减少了幻觉现象。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [133] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种名为THYME的动态场景图生成方法，通过分层特征聚合和循环时间细化解决现有方法的局限性，并在新数据集AeroEye-v1.0上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态场景理解的需求日益增长，但现有方法在捕捉细粒度空间细节和长程时间依赖性方面表现不足。

Method: 提出THYME方法，结合分层特征聚合和循环时间细化，同时建模多尺度空间上下文并确保时间一致性。

Result: 在ASPIRe和AeroEye-v1.0数据集上的实验表明，THYME优于现有方法，提升了场景理解的准确性。

Conclusion: THYME方法在动态场景图生成中表现出色，为地面和空中场景提供了更准确的场景理解。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [134] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 通过视频分析表面波的传播特性，推断材料厚度和刚度的方法。


<details>
  <summary>Details</summary>
Motivation: 利用表面波传播信息获取材料内部物理特性，为家庭健康监测和人机交互提供新方法。

Method: 从视频中提取色散关系，通过基于物理的优化问题求解最佳厚度和刚度参数。

Result: 在模拟和真实数据中验证，结果与真实测量高度一致。

Conclusion: 该方法为家庭健康监测提供了概念验证，并适用于人机交互等领域。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [135] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Expert-CFG的专家参与框架，无需额外训练即可将医学视觉语言模型与临床专业知识对齐，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型存在概率不确定性，可能产生错误或未经验证的响应，这在医疗应用中具有严重风险。现有方法依赖训练调整，成本高且与临床专业知识对齐不足。

Method: 提出Expert-CFG框架，通过不确定性估计识别不可靠输出，检索相关参考文献辅助专家标记关键术语，并应用无分类器引导优化模型输出。

Result: 在三个医学视觉问答基准测试中，Expert-CFG以4.2B参数和有限专家标注，优于13B参数的现有最佳模型。

Conclusion: Expert-CFG展示了在资源有限环境中部署临床可用系统的可行性。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [136] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于立体视觉的3D异常物体检测算法（S3AD），通过解耦2D和3D训练策略提升泛化能力，并设计了异常评分算法。同时，构建了KITTI-AR数据集验证算法性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D检测模型在开放道路中对罕见异常物体的误检或漏检问题，提升模型对任意形状目标的泛化能力。

Method: 提出S3AD算法，解耦2D和3D训练策略，设计基于前景置信度预测的异常评分算法；构建KITTI-AR数据集，包含新增的97个类别。

Result: 实验验证了算法和数据集的有效性，提升了3D异常检测的泛化能力。

Conclusion: S3AD算法和KITTI-AR数据集为3D异常检测提供了新的解决方案和验证平台。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [137] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出一种球形采样方法，利用现有预训练模型处理全景图像，减少失真并提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模全景图像数据集，现有二维预训练模型无法有效处理全景图像的失真和不连续性。

Method: 采用球形离散采样方法，基于预训练模型权重，直接利用二维预训练模型处理全景图像，并应用于全景图像分割任务。

Result: 在Stanford2D3D室内数据集上取得了良好效果。

Conclusion: 球形采样方法有效解决了全景图像处理中的失真问题，提升了模型性能。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [138] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 论文提出了一种在线点跟踪方法Track-On，利用视觉基础模型和Transformer架构，实现了无需未来帧信息的长时跟踪。


<details>
  <summary>Details</summary>
Motivation: 现实场景需要在线预测，而现有方法多为离线处理，无法满足实时需求。视觉基础模型虽能提供几何表示，但缺乏时间推理能力。

Method: 结合视觉基础模型和Transformer架构，提出Track-On模型，将每个跟踪点作为查询逐帧处理。

Result: Track-On在七个公开基准测试中达到最新技术水平，证明了无需未来帧的长时跟踪可行性。

Conclusion: Track-On通过专用设计和内存机制，成功解决了在线长时点跟踪问题。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [139] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM是一个统一框架，解决了基础模型（如CLIP和SAM）在部署时面临的分布偏移和置信度错位问题，通过Fisher信息惩罚和置信度错位惩罚提升性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在低样本迁移学习中表现出色，但面临分布偏移和置信度错位问题，现有解决方案多为领域特定。

Method: 提出StaRFM框架，包括Fisher信息惩罚（FIP）和置信度错位惩罚（CMP），分别处理分布偏移和置信度校准。

Result: 在19个视觉数据集和医学分割任务中表现优异，如准确率提升3.5%，ECE降低28%，DSC达84.7%，HD95为4.8mm。

Conclusion: StaRFM是一个即插即用的通用框架，显著提升了基础模型的跨域性能和不确定性校准。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [140] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生成先验的方法，从第一人称视角重建可动画化虚拟形象，利用Stable Diffusion减少训练负担并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决从第一人称视角重建人体外观时的遮挡和比例失真问题，并减少对多视角数据集的依赖。

Method: 使用ControlNet和Stable Diffusion生成正面视图，并将其输入图像到动作模型以生成虚拟形象动作。

Result: 实现了从单张第一人称图像生成逼真正面视图，并进一步生成虚拟形象动作。

Conclusion: 该方法为更易用和泛化的远程呈现系统提供了可能。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [141] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的框架PPJudge，用于评估绘画过程，并引入了首个大规模绘画过程评估数据集PPAD。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注静态最终图像，忽略了绘画过程的动态性和多阶段性。

Method: 提出了PPJudge模型，采用时间感知位置编码和异构专家混合架构，结合PPAD数据集进行评估。

Result: 实验表明，该方法在准确性、鲁棒性和与人类判断的一致性上优于现有基线。

Conclusion: 该方法为计算创造力和艺术教育提供了新见解。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [142] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net通过注意力引导的上下文去偏模型，结合因果干预模块，有效减少情感识别中的上下文偏差，在CAER-S数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别方法存在上下文偏差问题，如背景与情感标签的虚假关联，影响识别准确性。

Method: 提出AGCD-Net模型，采用Hybrid ConvNeXt编码器，集成空间变换网络和SE层，结合AG-CIM模块进行因果干预和注意力校正。

Result: 在CAER-S数据集上实现最优性能，验证了因果去偏对复杂场景下情感识别的有效性。

Conclusion: AGCD-Net通过因果干预和注意力机制，显著提升了情感识别的鲁棒性。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [143] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为AAHR的框架，通过动态聚类原型对比学习和多模态关系矩阵，解决了图像-文本匹配中的语义模糊和高阶关联问题，显著提升了匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理相似实例的高阶关联和语义模糊时存在不足，尤其是软正样本和软负样本的区分问题，以及未能充分利用训练批次中的邻域关系。

Method: AAHR通过动态聚类原型对比学习构建统一表示空间，结合全局和局部特征提取、自适应聚合网络、GNN增强语义交互，以及动量对比学习扩展负样本集。

Result: 在Flickr30K、MSCOCO和ECCV Caption数据集上，AAHR优于现有方法，显著提高了匹配准确性和效率。

Conclusion: AAHR框架有效解决了图像-文本匹配中的语义模糊和高阶关联问题，为跨模态任务提供了新的解决方案。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [144] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种无注释的手语翻译方法，通过分段视觉标记化和对比对齐目标，显著减少计算需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有无注释手语翻译方法虽性能提升，但模型复杂度和计算需求高，难以扩展到大尺度数据集。

Method: 采用分段视觉标记化框架将连续视频转换为离散标记，结合对比对齐目标和双重监督策略。

Result: 在PHOENIX14T基准上性能超越现有方法，序列长度减少50%，内存使用降低2.67倍。

Conclusion: 该方法在减少计算需求的同时提升性能，展示了标记化和对齐策略的潜力。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [145] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨知识蒸馏（CKD）方法，通过利用语义相似性和滑动替换解决跨模态问题，并通过间接分阶段知识蒸馏解决跨架构问题，提升了SNN在DVS数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注的事件数据有限和SNN架构不成熟，SNN的性能仍不如ANN。论文旨在通过知识蒸馏提升SNN在DVS数据上的表现。

Method: 提出CKD方法，结合语义相似性和滑动替换解决跨模态问题，采用间接分阶段知识蒸馏解决跨架构问题。

Result: 在N-Caltech101和CEP-DVS等主流神经形态数据集上，CKD方法优于当前最优方法。

Conclusion: CKD方法有效提升了SNN的性能，为跨模态和跨架构知识蒸馏提供了新思路。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [146] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust是一个基于强化学习的框架，旨在通过上下文感知的辅助提示改善多模态大语言模型（MLLMs）在医疗领域的置信度校准，提升其可信度和任务准确性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医疗等安全关键领域的应用受到提示设计敏感性和高置信度错误响应的限制，需要改进其置信度校准以提高可靠性。

Method: 提出Prompt4Trust框架，通过训练轻量级LLM生成辅助提示，指导下游任务MLLM生成置信度更准确的响应。

Result: 在PMC-VQA基准测试中达到最先进的医学视觉问答性能，并展示了零样本泛化能力。

Conclusion: Prompt4Trust展示了自动化提示工程在提升MLLMs可信度方面的潜力，特别是在安全关键领域。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [147] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种基于深度生成模型的盲运动去模糊框架，通过预训练的GAN生成器和初始化器优化模糊核的初始估计，提高了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度先验方法在盲运动去模糊中因优化过程的高度非凸性对初始模糊核极度敏感，导致性能受限。

Method: 预训练基于GAN的模糊核生成器和初始化器，约束解在紧凑的潜在核流形中，降低对初始化的敏感性。

Result: 在挑战性基准数据集上实现了最先进的性能，并可轻松集成到现有方法中。

Conclusion: 提出的框架有效解决了模糊核初始化的敏感性问题，提升了盲运动去模糊的性能，且具有通用性和扩展性。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [148] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出了一种语义感知的定位框架，通过联合估计深度和语义光线，显著提升了楼层平面图的定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有楼层平面图定位技术主要依赖深度结构线索，忽略了平面图中的丰富语义信息。

Method: 采用从粗到细的方式构建结构-语义概率体积，先采样少量光线生成低分辨率概率体积，再在高概率区域密集采样以细化预测。

Result: 在两个标准基准测试中显著优于现有方法，召回率显著提升，并能轻松整合额外元数据（如房间标签）以进一步提高精度和效率。

Conclusion: 该框架通过结合语义信息，显著提升了楼层平面图定位的性能和灵活性。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [149] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet结合RGB图像和深度信息，通过几何感知框架提升手术阶段识别性能，在复杂手术场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别在智能辅助系统中至关重要，但RGB图像缺乏结构线索且视觉相似性高，深度信息能提供几何线索以补充外观特征。

Method: 提出Geo-RepNet框架，基于RepVGG骨干网络，整合深度信息，包含DGPG模块提取几何先验和GEMA模块注入空间引导。

Result: 在真实ESD视频数据集上实验表明，Geo-RepNet达到最先进性能，保持鲁棒性和高效计算。

Conclusion: 深度信息显著提升手术阶段识别性能，Geo-RepNet在复杂手术环境中表现卓越。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [150] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet结合ViT-Small和原型网络，在少样本图像分类中表现优异，优于CNN原型网络和部分Transformer方法。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）在少样本分类中的潜力尚未充分挖掘，因此提出ViT-ProtoNet以提升性能。

Method: 将ViT-Small嵌入原型网络框架，通过平均支持样本的token嵌入构建鲁棒原型。

Result: 在多个基准测试中，ViT-ProtoNet在5-shot设置下表现优于CNN原型网络（提升3.2%），且特征可分性更强。

Conclusion: ViT-ProtoNet是一种强大且灵活的少样本分类方法，为基于Transformer的元学习设定了新基准。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [151] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAA*的新方法，通过引入路径角度自由度（PAF）改进A*算法，以提升路径平滑性和相似性。


<details>
  <summary>Details</summary>
Motivation: 路径平滑性在专家演示的路径模仿学习中常被忽视，因此需要一种自适应平滑性的方法。

Method: 结合PAF的DAA*方法，通过优化路径缩短和平滑性，提升路径相似性。

Result: 在7个数据集上，DAA*显著优于现有方法，路径相似性提升9.0% SPR、6.9% ASIM和3.9% PSIM。

Conclusion: DAA*在路径最优性和搜索效率之间取得平衡，显著提升了模仿学习的性能。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [152] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 论文提出了ALPHA基准和ALPHAVAE模型，用于高效生成RGBA图像，显著提升了透明图像的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型在RGB图像合成上表现优异，但透明或分层内容（RGBA）的生成缺乏大规模基准和高效方法。

Method: 通过ALPHA基准扩展RGB指标至四通道图像，并设计ALPHAVAE模型，结合多种损失函数优化RGBA重建。

Result: ALPHAVAE在仅8K图像训练下，PSNR提升4.9 dB，SSIM提升3.2%，且透明图像生成效果更优。

Conclusion: ALPHAVAE为RGBA图像生成提供了高效解决方案，代码和模型已开源。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [153] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 提出了一种基于自适应多尺度融合的多光谱点云分类方法，解决了稀疏标注、地物尺度差异和长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法主要针对室内数据集，应用于室外数据集时面临稀疏标注、地物尺度差异和长尾分布等问题。

Method: 设计了网格平衡采样策略生成训练样本，提出多尺度特征融合模块和自适应混合损失模块。

Result: 在三个多光谱点云数据集上的实验表明，该方法优于现有方法。

Conclusion: 该方法有效解决了室外多光谱点云分类中的关键问题，提升了分类性能。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [154] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 论文提出了ProactiveBench基准和PAUC指标，用于评估多模态对话系统的主动交互能力，PAUC比传统指标更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统研究的深入，用户期望系统能更主动地交互，例如在视频播放时实时决定多轮响应时机。

Method: 引入ProactiveBench基准和PAUC指标，考虑响应时间动态性，通过基准测试和用户研究验证。

Result: PAUC比传统指标更符合人类偏好，能更准确地评估主动交互场景下的用户体验。

Conclusion: PAUC为主动交互场景提供了更可靠的评估方法，推动了多模态对话系统的发展。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [155] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 研究评估了多种机器学习方法（如XGBoost、LightGBM和CNN）用于高光谱卫星影像的云和云影掩模，其中CNN在准确性和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 云和云影掩模是高光谱卫星影像预处理的关键步骤，影响数据质量。研究旨在找到高效、轻量化的AI模型以支持实时处理。

Method: 比较了梯度提升方法（XGBoost、LightGBM）和卷积神经网络（CNN），评估其准确性、存储需求和推理速度。

Result: 所有模型准确率超过93%，CNN在特征缩减后表现最优，兼顾高准确率、低存储需求和快速推理。

Conclusion: 轻量级AI模型（如CNN）适合实时高光谱影像处理，支持卫星AI系统的开发。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [156] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 论文提出了一种动态调整类间混淆损失的音频-视频预训练编码器（DICCAE），通过细粒度类别对齐和自监督预训练策略，提升了模型在相似活动间的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视频预训练方法仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。

Method: 提出DICCAE编码器，动态调整类间混淆损失，并结合音频-视频模态及其融合的训练框架；采用聚类引导的自监督预训练策略解决数据稀缺问题。

Result: 在VGGSound数据集上达到65.5%的top-1准确率，接近最先进水平；通过消融实验验证了各模块的必要性。

Conclusion: DICCAE通过细粒度类别对齐和动态混淆损失，显著提升了音频-视频模型的区分能力，适用于人类活动识别任务。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [157] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D是一个用于3D多模态大语言模型（MLLMs）的视觉令牌修剪框架，通过全局注意力预测和样本自适应修剪技术，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 3D MLLMs在场景理解方面表现出色，但计算效率低下限制了其实际部署，主要原因是处理过多对象中心视觉令牌的开销。

Method: 提出Fast3D框架，包含全局注意力预测（GAP）和样本自适应修剪（SAP）两项技术，无需修改目标模型参数。

Result: 在五个基准测试中验证了Fast3D的有效性，尤其是在高视觉令牌修剪率下表现突出。

Conclusion: Fast3D为3D MLLMs提供了一种高效且可扩展的视觉令牌修剪解决方案。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [158] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 研究发现，简单的编码器架构结合强预训练在交通异常检测（TAD）中表现优异，甚至超越复杂方法。自监督预训练和领域自适应预训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂架构是否必要，验证简单架构结合预训练能否在TAD中取得更好效果。

Method: 使用简单的Video ViTs编码器架构，研究不同预训练方法（弱监督、全监督、自监督MVM和领域自适应预训练）对TAD性能的影响。

Result: 强预训练使简单模型性能媲美或超越复杂方法；自监督MVM和领域自适应预训练效果最佳。

Conclusion: 预训练是关键，简单架构结合有效预训练可实现高效、可扩展的TAD模型。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [159] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN的图像分类系统，用于自动化检测和分类八种常见作物病害，并通过移动平台为农民提供实时诊断和治疗建议。


<details>
  <summary>Details</summary>
Motivation: 作物病害对农业生产和全球粮食安全构成重大威胁，尤其是在大规模农业中，早期识别往往延迟或不准确。

Method: 研究采用完整的深度学习流程，包括图像采集、预处理、模型训练（使用TensorFlow和Keras），CNN架构包含卷积层、池化层和全连接层。

Result: 系统在训练数据上达到约90%的准确率，验证准确率约60%，表明存在轻微过拟合。模型还集成了治疗建议模块。

Conclusion: 该研究为精准农业提供了可扩展且易用的工具，结合深度学习和实际农艺支持，展示了CNN在作物健康监测中的潜力。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [160] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种低资源处理相机陷阱数据的流程，结合ML/AI功能，适合资源有限的小型研究团队。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱数据量大、标注复杂、环境多变，且现有ML/AI工具难以集成到资源有限的工作流程中。

Method: 开发了一种低资源流程，支持数据本地处理，包括数据传输、推理和评估。

Result: 提供了一种实用的解决方案，帮助研究人员从大量相机陷阱数据中提取有意义的信息。

Conclusion: 该流程为资源有限的研究团队提供了高效处理相机陷阱数据的可行方法。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [161] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级神经网络方法，用于实时检测和描述天体地形特征，解决了传统方法计算量大和数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的光度测量方法依赖大量先验图像和离线处理，计算成本高且泛化能力有限，而现有学习方法又因计算需求和数据稀缺难以实时运行。

Method: 采用轻量级神经网络架构，改进领域适应方法用于特征检测，并提出注意力对齐方法用于特征描述。

Result: 提出的系统在性能上优于现有技术，适用于实时天体地形特征跟踪。

Conclusion: 该方法为航天器自主导航提供了高效、实时的解决方案，克服了传统和现有学习方法的局限性。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [162] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种高效的多人物3D运动预测模型，通过简化时空交互，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多人物运动预测的复杂性源于个体运动与交互依赖，现有方法计算成本高。

Method: 设计轻量级双分支学习局部和全局表示，引入跨层级交互块整合时空表示，并显式嵌入空间人际距离。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW数据集上取得最优性能，同时显著降低计算成本。

Conclusion: 该模型高效且性能优越，适用于多人物3D运动预测。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [163] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D是一个新颖的3D点云实例分割框架，结合注意力机制、嵌入学习和跨模态对齐，支持无监督实例分割和零样本检索。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云实例分割中几何结构建模和多模态理解的统一问题，减少监督需求并提升实用性。

Method: 构建分层特征提取器增强几何建模，通过对比聚类实现无监督实例分割，并在共享语义空间中对齐3D数据与自然语言查询。

Result: 在实例分割和多模态理解方面优于Mask3D和ULIP等方法，具有更少的监督需求和更高的实用性。

Conclusion: SegVec3D成功统一了实例分割和多模态理解，为3D点云处理提供了高效且实用的解决方案。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [164] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA框架通过双级知识对齐和任务置信度引导的适配器混合，提升了持续学习模型对误导任务ID的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调（PEFT）方法中因独立训练子模块导致的特征子空间不对齐问题，避免在误导任务ID下产生模糊决策。

Method: 1. 双级知识对齐（DKA）：对齐不同子空间内的类内特征分布，学习全局分类器。2. 任务置信度引导的适配器混合（TC-MoA）：基于置信度分数自适应聚合任务特定知识。

Result: CKAA在实验中优于现有PEFT-based持续学习方法。

Conclusion: CKAA通过特征对齐和自适应知识聚合，显著提升了模型对误导任务ID的鲁棒性。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [165] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net提出了一种在双曲空间中结合掩码图像建模和知识蒸馏的高效方法，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视觉和语义概念通常以层次结构组织，现有方法MERU在双曲空间中成功捕捉了这种层次结构，但如何更高效地训练模型仍是一个关键问题。

Method: 提出HMID-Net，在双曲空间中整合掩码图像建模和知识蒸馏技术，并设计了一种专门的双曲空间蒸馏损失函数。

Result: 实验表明，该方法在图像分类和检索任务中显著优于MERU和CLIP等现有模型。

Conclusion: HMID-Net证明了在双曲空间中应用MIM和知识蒸馏的高效性，为多模态学习提供了新思路。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [166] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE是一个新的视频基准测试，旨在评估大型视觉语言模型（LVLMs）是否能真正理解视频内容，而不仅仅是静态图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试多基于静态图像问题，无法评估模型是否具备深度时间推理能力。

Method: GLIMPSE包含3,269个视频和4,342个视觉中心问题，覆盖11个类别，要求模型观看完整视频并进行推理。

Result: 人类评估准确率为94.82%，而最佳模型GPT-o3仅达到66.43%，显示LVLMs在深度视频理解上仍有不足。

Conclusion: GLIMPSE揭示了LVLMs在真正理解视频内容上的挑战，为未来研究提供了方向。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [167] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 提出了一种结合张量分解和正则化的自适应性网络SDTN，以及轻量级网络TRN，用于高光谱图像分类，显著提升精度并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高维数据、光谱-空间冗余和标记样本稀缺时性能不佳，需要更高效的解决方案。

Method: SDTN通过动态调整张量秩优化特征表示，TRN整合SDTN提取的特征并捕获多尺度光谱-空间特征。

Result: 在PaviaU数据集上实验显示，相比现有方法，精度显著提升且模型参数减少。

Conclusion: SDTN和TRN框架适合资源受限环境下的实时部署，为高光谱图像分类提供了高效解决方案。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [168] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出了一种可靠的测试时适应方法（ReTA），通过一致性感知熵重加权（CER）和多样性驱动的分布校准（DDC）解决现有缓存方法在分布偏移下的不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在零样本任务中表现优异，但在无标注数据的情况下对分布偏移的适应能力不足，测试时适应（TTA）方法因此被提出以提升性能。

Method: ReTA结合了CER和DDC两种策略：CER通过一致性约束加权熵以优化缓存更新，DDC通过建模类级文本嵌入为高斯分布来调整决策边界。

Result: 实验表明，ReTA在真实世界分布偏移下显著优于现有方法。

Conclusion: ReTA通过增强缓存可靠性和决策边界灵活性，有效提升了VLMs在测试时的适应能力。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [169] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: HFUT-VUT团队提出了一种用于微手势在线识别的新方法，结合手工数据增强和时空注意力机制，显著提升了分类和定位精度，在IJCAI 2025 MiGA挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 微手势在线识别任务极具挑战性，需在未修剪视频中定位并识别多个微手势实例的时间和类别，且微手势多为自发性动作，差异较大。

Method: 采用手工数据增强和时空注意力机制，提升模型对微手势的分类和定位能力。

Result: F1分数达到38.03，比之前最优方法提升了37.9%，在比赛中排名第一。

Conclusion: 该方法在微手势识别任务中表现优异，显著提升了性能。

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [170] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap通过修剪冗余空间激活提升VMamba和MedMamba的效率，无需重新训练，显著提高吞吐量且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 解决VMamba等SSM模型在四向扫描中的空间冗余问题，提升效率而不影响精度。

Method: 提出QuarterMap，一种后训练激活修剪方法，通过移除冗余空间激活并恢复维度来优化扫描过程。

Result: 在ImageNet-1K上实现11%的速度提升，精度下降小于0.9%；在ADE20K和医疗影像任务中表现类似。

Conclusion: QuarterMap是一种即插即用的高效工具，适用于SSM模型，无需合并-解合并操作，保持模型可迁移性。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [171] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: DehazeSB是一种基于Schrödinger Bridge的新型无配对去雾框架，利用最优传输理论直接连接雾图和清晰图的分布，并通过细节保留正则化和提示学习提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的无配对去雾方法因生成器的传输映射能力有限而效果受限，需要更优的解决方案。

Method: 提出DehazeSB框架，结合最优传输理论和细节保留正则化，并引入提示学习利用预训练CLIP模型区分雾图和清晰图。

Result: 在多个真实数据集上的实验表明，DehazeSB优于现有方法，生成高质量去雾结果。

Conclusion: DehazeSB通过优化传输映射和细节保留，显著提升了无配对去雾的性能。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [172] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct是一种多模态大语言模型，通过内容感知的标记化策略和显式布局建模，显著提升了密集文档的信息提取效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在密集文档上表现不佳，且视觉标记化方法存在计算冗余和内存效率低的问题。

Method: 采用内容感知的标记化策略，按文档复杂度生成标记，并结合三阶段训练范式。

Result: 在KIE基准测试中达到SOTA，减少约3.6倍的图像标记，零样本评估中F1分数提升5.5点。

Conclusion: 内容感知标记化与显式布局建模为文档理解提供了有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [173] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 提出了一种动态RPCA网络（DRPCA-Net），通过结合稀疏性先验和动态展开机制，显著提升了红外小目标检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在红外小目标检测中过于复杂且缺乏可解释性，忽视了目标稀疏性的先验信息。

Method: 基于RPCA模型，设计动态展开网络DRPCA-Net，通过轻量级超网络动态生成迭代参数，并引入动态残差组（DRG）模块优化背景建模。

Result: 在多个公开红外数据集上，DRPCA-Net显著优于现有方法。

Conclusion: DRPCA-Net通过动态机制和稀疏性建模，实现了高效且鲁棒的红外小目标检测。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [174] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——顺序CSIST解混，并贡献了一个开源生态系统，包括数据集SeqCSIST和工具包，同时提出了DeRefNet模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 远距离紧密排列红外小目标（CSIST）在红外图像中常表现为混合斑点，缺乏高质量公共数据集限制了研究进展。

Method: 提出了Deformable Refinement Network（DeRefNet），引入Temporal Deformable Feature Alignment（TDFA）模块，实现自适应帧间信息聚合。

Result: 在SeqCSIST数据集上，DeRefNet的mAP指标比现有方法提高了5.3%。

Conclusion: 该研究首次在多帧范式中解决CSIST解混任务，提供了数据集和工具包以促进未来研究。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [175] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 论文提出了一种分段式架构EHPE，用于增强3D手部姿态估计，重点解决了远端指尖（TIP）和手腕关节预测中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D手部姿态估计中忽视了TIP和手腕的重要性，且未解决远端关节误差累积问题，导致整体重建质量下降。

Method: EHPE分为两个阶段：TW阶段（提取TIP和手腕关节）和PG阶段（通过双分支交互网络细化其余关节位置）。

Result: 在两个广泛使用的基准测试中，EHPE实现了最先进的性能。

Conclusion: EHPE通过分段架构有效减少了误差累积，提升了手部姿态估计的准确性。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [176] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文综述了Segment Anything Model（SAM）中提示工程的技术、应用与挑战，填补了相关文献的空白。


<details>
  <summary>Details</summary>
Motivation: 探索提示工程在SAM及其变体中的关键作用，填补该领域的研究空白。

Method: 系统整理和分析提示工程的技术、应用及挑战，涵盖从基础方法到多模态应用的演变。

Result: 揭示了提示工程从简单几何输入到复杂多模态方法的发展，并识别了优化中的独特挑战。

Conclusion: 为分割基础模型中的提示工程提供了结构化框架，并指出了未来研究方向。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [177] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个交互式艺术字体系统，利用扩散模型支持局部编辑、多字符组合和开放式提示解释，显著提升了艺术字体合成的交互性。


<details>
  <summary>Details</summary>
Motivation: 传统艺术字体设计依赖手工，现有生成模型缺乏交互性，无法满足局部编辑、迭代优化和多字符组合的需求。

Method: WordCraft结合扩散模型，采用无训练的区域注意力机制和噪声混合技术，并集成大型语言模型解析用户提示。

Result: 系统能够生成高质量、多样化的艺术字体，支持多语言输入和用户中心化工作流程。

Conclusion: WordCraft提升了艺术字体合成的交互性和创造性，为设计师提供了更多可能性。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [178] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR是一个新型自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，提升生成控制性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型在精确视觉控制、多模态输入平衡和复杂多模态图像生成训练需求方面的不足。

Method: 结合自回归图像生成器和两阶段训练范式：1) 多模态对齐阶段；2) 多模态指令调整阶段。

Result: 在DreamBench++基准测试中表现优异，优于基线模型，且在图像重建、任务适应性和训练效率方面优于基于扩散的方法。

Conclusion: MENTOR通过高效的多模态条件调整，显著提升了图像生成的精确性和控制性。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [179] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2是一种无需训练的视频对象分割策略，针对手术视频中的复杂场景（如快速器械移动和遮挡）进行了优化，性能优于SAM2。


<details>
  <summary>Details</summary>
Motivation: 手术视频分割对提升手术质量和患者结果至关重要，但现有SAM2框架在复杂手术视频中表现不佳。

Method: 提出了MA-SAM2，采用上下文感知和遮挡弹性记忆模型，无需额外训练。

Result: 在EndoVis2017和EndoVis2018数据集上，MA-SAM2性能分别提升4.36%和6.1%。

Conclusion: MA-SAM2在复杂手术视频中表现出色，具有实际应用潜力。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [180] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一种基于扩散的文本到图像生成模型，旨在实现文本与图像的高质量对齐，同时保持图像质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 开发FLUX.1的目的是在文本到图像生成领域实现更高的性能，超越现有模型如Midjourney、DALL-E 3等。

Method: 通过反向工程从源代码中解析FLUX.1的架构，以支持其作为未来研究和开发的基础。

Result: FLUX.1被认为是当前文本到图像生成领域的先进模型，性能优于多个流行模型。

Conclusion: 本报告是对FLUX.1架构的非官方技术总结，旨在促进其进一步研究和应用。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [181] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频推理范式ViTCoT，结合视觉和文本信息，显著提升了视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理方法主要依赖文本信息，忽视了视觉模态的重要性，而人类在推理时会自然结合视觉内容。

Method: 构建了Video-Text Interleaved Benchmark (ViTIB)，并探索了ViTCoT范式在视频理解中的应用。

Result: 实验表明，ViTCoT显著优于传统仅文本的CoT范式，并激活了更多MLLM神经元。

Conclusion: ViTCoT为视频推理提供了更直观和认知对齐的方法，具有广泛应用潜力。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [182] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former通过优化密集令牌处理的计算分配，提出四种关键改进，实现了在CPU设备上的高效高精度交互式分割。


<details>
  <summary>Details</summary>
Motivation: 当前交互式分割方法在密集令牌处理与稀疏提示令牌之间存在效率与质量的权衡，Inter2Former旨在解决这一问题。

Method: 提出动态提示嵌入（DPE）、动态混合注意力（DHA）、混合专家（HMoE）和动态局部上采样（DLU）四种改进方法。

Result: 在高精度交互式分割基准测试中，Inter2Former实现了最优性能，并在CPU设备上保持高效。

Conclusion: Inter2Former通过自适应计算策略，成功平衡了分割质量与效率，为交互式分割领域提供了新的解决方案。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [183] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR方法通过动态对齐图像特征与语言嵌入，改进无监督适应中的伪标签生成，显著提升细粒度分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督适应方法在细粒度分类中表现不佳，因固定对齐分数无法捕捉细微类别差异或计算成本高。

Method: 提出FAIR方法，使用Class Description Anchors动态对齐特征，定义Learned Alignment Score，并结合自训练加权机制优化伪标签。

Result: 在13个细粒度数据集上，FAIR比现有方法平均提升2.78%的性能。

Conclusion: FAIR通过动态跨模态交互和伪标签优化，显著提升了细粒度无监督适应的效果。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [184] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 研究重新评估了两种CLIP变体（ResNet和ViT）在bouba-kiki效应中的表现，发现模型未能一致体现人类认知中的跨模态关联。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型（VLMs）是否像人类一样整合跨模态信息，以bouba-kiki效应为测试案例。

Method: 采用基于提示的概率评估和Grad-CAM视觉注意力分析，对比人类实验数据。

Result: 模型未表现出稳定的bouba-kiki效应，且与人类数据相比表现显著不足。

Conclusion: VLMs在跨模态概念理解上存在局限，与人类认知不一致。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [185] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA是一个基于预训练潜在扩散模型的区域引导框架，用于生成真实且对齐的异常图像-掩码对，解决了现有方法在异常合成中的低真实性和对齐问题。


<details>
  <summary>Details</summary>
Motivation: 工业制造中异常样本稀缺，现有合成方法存在低真实性、掩码对齐不准确和泛化能力差的问题。

Method: GAA通过局部概念分解建模异常特征，自适应多轮异常聚类增强一致性，区域引导掩码生成确保对齐，并引入低质量样本过滤模块。

Result: 在MVTec AD和LOCO数据集上，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。

Conclusion: GAA框架有效提升了异常合成的真实性和对齐性，为工业制造中的异常检测提供了高质量数据支持。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [186] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 提出了一种基于MaxViT的多类别中风分类AI框架，结合数据增强和XAI技术，实现了98%的准确率和F1分数，旨在提高中风早期诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死因之一，早期准确诊断对改善患者预后至关重要，尤其是在紧急情况下。CT扫描因其快速、可及性和成本效益成为关键成像方式。

Method: 采用MaxViT作为主要深度学习模型，结合其他Transformer变体，并应用数据增强技术（如合成图像生成）解决类别不平衡问题。集成Grad-CAM++提供模型决策的可视化解释。

Result: MaxViT模型在增强数据训练下表现最佳，准确率和F1分数达98%，优于其他模型和基线方法。

Conclusion: 该研究为开发可信赖的AI辅助中风诊断工具提供了支持，有助于临床实践中的集成，提高急诊科中风诊断的及时性和准确性。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [187] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM是一个专为面部图像理解设计的多模态大语言模型，通过ChatGPT生成的弱监督数据训练，提升了面部中心任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在通用数据集上训练，缺乏对领域特定视觉线索（如面部图像）的推理能力，限制了其在面部结构、表情、情感等任务中的应用。

Method: 提出弱监督流程，利用ChatGPT生成基于FairFace数据集的高质量问答对（FairFaceGPT），训练FaceLLM模型。

Result: FaceLLM在多种面部中心任务中表现优异，达到SOTA性能。

Conclusion: FaceLLM展示了语言模型合成监督在构建领域专用MLLMs中的潜力，为可信赖、以人为中心的多模态AI系统奠定了基础。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [188] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 论文评估了三种AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，并探讨了解缠技术对减少偏差的影响。结果显示模型性能高，但存在公平性问题，解缠效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致工作年龄成年人视力丧失的主要原因，传统筛查方法成本高且难以普及。AI算法提供了一种可扩展的诊断方案，但公平性和泛化性仍是问题。

Method: 使用mBRSET眼底数据集，训练了ConvNeXt V2、DINOv2和Swin V2三种模型预测DR和敏感属性（如年龄、性别），并评估公平性及解缠技术对偏差的影响。

Result: 所有模型在DR预测中表现优异（AUROC高达94%），但公平性评估显示存在差异（如DINOv2中年龄组间AUROC差距达10%）。解缠技术对模型性能影响不一。

Conclusion: 研究强调了医学影像AI中公平性的重要性，解缠技术在减少偏差方面效果因模型而异，需进一步探索以确保公平可靠的医疗解决方案。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [189] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度卷积神经网络的手写Devanagari字符识别方法，旨在解决Devanagari脚本缺乏数字化工具的问题，并取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: Devanagari脚本是印度最古老的语言之一，但缺乏数字化工具。研究旨在通过自动化方法识别手写Devanagari字符，以节省时间并避免数据过时。

Method: 采用两层深度卷积神经网络，使用Devanagari手写字符数据集（DHCD）进行训练和测试，每类字符包含1700张图像。

Result: 测试准确率达到96.36%，训练准确率为99.55%。

Conclusion: 该方法在手写Devanagari字符识别中表现出色，具有实际应用潜力。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [190] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg是一个新颖的眼部分割框架，通过贝叶斯不确定性学习解决运动模糊、眼睑遮挡和域差距问题，提升AR/VR中的眼部分割和注视估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有眼部分割方法在运动模糊、眼睑遮挡和域差距情况下表现不佳，影响AR/VR交互体验。

Method: 设计了一个基于贝叶斯不确定性学习的框架，显式建模不确定性，并通过后验统计量化分割不确定性。

Result: 在MIoU、E1、F1和ACC指标上超越现有方法，尤其在运动模糊和跨域挑战中表现优异。

Conclusion: EyeSeg通过不确定性感知提升分割鲁棒性，为AR/VR交互提供更准确的注视估计支持。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [191] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 论文提出CrisisLandMark数据集和CLOSP框架，通过文本对齐光学与SAR图像，提升检索性能54%，并整合地理坐标优化特定任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索系统多限于RGB数据，未能充分利用多传感器（如SAR和多光谱）的物理信息。

Method: 引入CrisisLandMark数据集，开发CLOSP框架，通过对比学习对齐光学与SAR图像；GeoCLOSP进一步整合地理坐标。

Result: CLOSP提升检索性能54%，GeoCLOSP在位置相关任务中表现更优。

Conclusion: 多传感器数据与地理背景的整合对遥感档案潜力释放至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [192] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose是一种基于WiFi的深度学习框架，用于准确连续的人体姿态估计，通过双流时空注意力架构和速度建模分支，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: WiFi姿态估计因其穿透性和隐私优势成为非视觉替代方案，但现有方法在准确性和连续性上仍有不足。

Method: 提出ViSTA-Former双流时空注意力架构，分别捕捉时间依赖和结构关系，并集成速度建模分支以增强细微运动敏感性。

Result: 在自建数据集上PCK@50达92.2%，优于现有方法8.3%；在公共MMFi数据集上验证了3D姿态估计的鲁棒性。

Conclusion: VST-Pose为室内连续运动分析提供了可靠且隐私保护的解决方案。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [193] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 提出了一种基于提示的单目深度估计框架，用于生成高分辨率数字高程模型（DEM），显著提升了分辨率（从30米到30厘米），并在多种地形中表现出色。


<details>
  <summary>Details</summary>
Motivation: 高分辨率高程数据对水文、城市形态和生态系统研究至关重要，但现有方法（如超分辨率技术和单目深度估计）存在局限性。

Method: 结合低分辨率SRTM数据和高分辨率NAIP图像，利用视觉变换器编码器和LiDAR数据微调，实现DEM估计、填补和更新。

Result: 分辨率提升100倍（30米到30厘米），在三种地形中误差小于5米，比SRTM提升18%，适用于水文和环境研究。

Conclusion: 该框架具有强泛化能力和可扩展性，为全球高程测绘提供了新范式。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [194] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 论文介绍了EmRACE-3K数据集，用于评估视觉语言模型在交互式环境中的表现，并展示了通过微调提升模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在被动任务中表现优异，但在需要在线交互和主动场景理解的具身环境中表现有限。

Method: 构建EmRACE-3K数据集，包含3000多个语言指导任务，并通过监督学习和强化学习微调Qwen2.5-VL-7B模型。

Result: 在零样本设置下，所有模型成功率低于20%；微调后性能显著提升。

Conclusion: EmRACE-3K为具身推理能力的发展提供了有效工具，并揭示了当前模型的局限性。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [195] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 论文提出了自动生成多学科科学实验评论的任务，构建了首个数据集ExpInstruct，并开发了模型ExpStar，显著优于现有大型多模态模型。


<details>
  <summary>Details</summary>
Motivation: 解决人工教师准备实验评论耗时且依赖专业知识的问题，探索大型多模态模型在生成细粒度实验评论方面的潜力。

Method: 构建ExpInstruct数据集（7K+评论，涵盖21个学科），提出检索增强模型ExpStar。

Result: ExpStar在实验中显著优于14种领先的大型多模态模型。

Conclusion: ExpStar在AI辅助科学实验教学中具有巨大潜力。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [196] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 论文提出了对视觉Transformer（ViT）推理加速的令牌压缩技术的系统分类和比较研究，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 由于ViT的二次计算复杂度，令牌压缩技术旨在移除不重要的令牌以提高推理效率。然而，现有研究缺乏系统分类，且对紧凑型ViT的效果未知。

Method: 提出了一种系统分类法，并在标准和紧凑型ViT架构上评估了代表性令牌压缩技术。

Result: 实验表明，令牌压缩对通用ViT有效，但对紧凑型ViT效果不佳。

Conclusion: 研究为未来在边缘AI和AI代理应用中优化紧凑型Transformer提供了实践指导和研究方向。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [197] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 本文综述了基于多模态大语言模型（MLLMs）的视觉丰富文档理解（VRDU）的最新进展，包括特征编码与融合方法、训练范式及数据集，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉丰富文档理解（VRDU）的需求日益增长，需要自动处理包含复杂视觉、文本和布局信息的文档。多模态大语言模型（MLLMs）在这一领域展现出巨大潜力。

Method: 综述了MLLMs在VRDU中的三大核心组件：文本、视觉和布局特征的编码与融合方法；训练范式（预训练、指令调整等）；以及相关数据集。

Result: 总结了MLLMs在VRDU中的成功应用，并分析了当前方法的局限性。

Conclusion: 提出了未来研究方向，以提高VRDU系统的效率、泛化性和鲁棒性。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [198] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 论文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过调整优化顺序和线性化模型，解决了传统VSD方法收敛慢和不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统变分分数蒸馏（VSD）方法在实践中存在收敛慢和不稳定的问题，论文旨在通过改进优化顺序和模型线性化来提升生成质量。

Method: 论文提出线性化前瞻变分分数蒸馏（$L^2$-VSD），通过调整优化顺序和使用线性化模型，提高分数蒸馏的稳定性和效率。

Result: 实验验证了$L^2$-VSD的优越性，其表现优于现有分数蒸馏方法，并能无缝集成到其他VSD框架中。

Conclusion: $L^2$-VSD通过改进优化策略和模型线性化，显著提升了文本到3D生成的稳定性和质量。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [199] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种高效的混合（几何和图像）方法，用于计算碎片对的最优对齐，无需假设其形状、尺寸或图像内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理真实拼图中碎片的几何特性，且依赖碎片形状的限制。

Method: 提出了一种混合方法，结合几何和图像信息，并引入新的碎片数据集和侵蚀模型。

Result: 在RePAIR 2D数据集上实现了最先进的邻域级精度和召回率。

Conclusion: 该方法显著提升了碎片兼容性计算的性能，适用于考古拼图等实际应用。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [200] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine提出了一种改进的负标签细化框架，用于零样本OOD检测，通过过滤子类别标签和专有名词，并引入多匹配感知评分函数，提高了检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于负标签的方法（如NegLabel和CSP）在区分OOD样本时存在误检问题，尤其是当负标签是分布内标签的子类别或专有名词时。此外，这些方法难以处理同时匹配多个分布内和负标签的图像。

Method: NegRefine通过过滤机制排除负标签集中的子类别标签和专有名词，并采用多匹配感知评分函数动态调整多个标签对图像的贡献。

Result: 在ImageNet-1K等大规模基准测试中，NegRefine表现优于现有方法，实现了更鲁棒的分布内与OOD样本分离。

Conclusion: NegRefine通过负标签细化和多匹配感知评分，显著提升了零样本OOD检测的性能，解决了现有方法的局限性。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [201] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: VRU-Accident是一个用于评估多模态大语言模型（MLLMs）在涉及弱势道路使用者（VRUs）的高风险交通场景中的推理能力的基准。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏标准化基准来定量评估MLLMs在涉及VRUs的复杂安全关键场景中的推理能力。

Method: 提出VRU-Accident基准，包含1K真实事故视频、6K多选题对和1K密集场景描述。

Result: 评估17个先进MLLMs，发现其在视觉属性上表现良好，但在事故原因、类型和可预防性推理上存在挑战。

Conclusion: VRU-Accident填补了评估MLLMs在VRU安全场景中的空白，揭示了其推理能力的不足。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [202] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 论文探讨了人类和深度学习模型在识别稀疏3D形状（如点云）时的表现差异，发现视觉变换器模型更接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型是否形成与人类相似的3D形状表征，以理解模型与人类视觉的异同。

Method: 通过两个人类实验（操纵点密度、对象方向和局部几何结构）比较人类与两种深度学习模型（DGCNN和点变换器）的表现。

Result: 点变换器模型在模拟人类表现上优于卷积模型，因其支持3D形状的层次抽象。

Conclusion: 点变换器模型的机制更接近人类3D形状表征，为模型设计提供了新方向。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [203] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文评估了GPT-4o-mini和Gemini 2.0 Flash在细粒度时尚属性识别任务中的零样本性能，发现Gemini 2.0 Flash表现最佳，为电商产品属性标注提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 时尚零售业务依赖于对产品的理解，而产品属性标注直接影响客户的发现体验。尽管大语言模型（LLMs）在多模态数据理解方面表现出色，但其在细粒度时尚属性识别中的性能尚未充分探索。

Method: 使用DeepFashion-MultiModal数据集，以图像为唯一输入，评估GPT-4o-mini和Gemini 2.0 Flash在18个时尚属性类别中的表现。

Result: Gemini 2.0 Flash的宏F1得分为56.79%，优于GPT-4o-mini的43.28%。

Conclusion: 研究为电商产品属性标注任务提供了实用见解，并强调了领域特定微调的必要性，为时尚AI和多模态属性提取的未来研究奠定了基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [204] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 论文介绍了SpeakerVid-5M数据集，首个大规模、高质量的音频-视觉双人交互虚拟人生成数据集，包含520万视频片段，并提供了基准模型VidChatBench。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，音频-视觉双人交互虚拟人成为新的研究热点，但缺乏相关数据集。

Method: 构建了SpeakerVid-5M数据集，按交互类型和数据质量分类，并提供了基于自回归的视频聊天基准模型。

Result: 数据集包含8743小时视频，覆盖多种交互类型，并提供了预训练和微调子集。

Conclusion: SpeakerVid-5M为音频-视觉双人交互虚拟人研究提供了重要资源，推动了该领域的发展。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [205] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: Hyma提出了一种基于超网络的方法，用于高效选择和训练多模态模型中的单模态模型及其连接模块，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型通过连接预训练的单模态模型实现，但选择和训练连接模块的计算成本高，需要更高效的解决方案。

Method: 利用超网络的参数预测能力，联合训练连接模块，覆盖多种单模态模型组合。

Result: Hyma将最优单模态模型对的搜索成本降低10倍，同时性能与网格搜索相当。

Conclusion: Hyma为多模态模型的高效构建提供了一种新颖且实用的解决方案。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [206] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级机器学习方法，通过分析家禽粪便图像检测疾病，实现了高准确率和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 家禽养殖易受传染病影响，需要高效、低成本的检测方法。

Method: 采用多颜色空间特征提取和多种描述符，结合PCA和XGBoost降维，使用ANN分类器。

Result: 模型准确率达95.85%，无需GPU，执行时间短。

Conclusion: 该方法为低资源农业环境提供了一种高效、可解释的疾病检测替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [207] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出首个4D人体解析框架，通过减少推理时间和引入开放词汇能力，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 动态3D人体表示在虚拟和扩展现实应用中日益重要，但现有方法受限于封闭数据集和长推理时间。

Method: 1) 采用基于掩码的视频对象跟踪；2) 设计掩码验证模块；3) 提出4D掩码融合模块。

Result: 实验显示方法有效且灵活，推理速度提升93.3%。

Conclusion: 该框架显著提升了4D人体解析的效率和适用性。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [208] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS框架通过因果引导的对抗方法生成反事实解释，避免虚假相关性的干扰，提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。

Method: 提出CECAS框架，结合因果视角和对抗方法生成反事实解释。

Result: 在多个基准数据集上优于现有方法，实现有效性、稀疏性、邻近性和真实性的平衡。

Conclusion: CECAS通过因果引导显著提升反事实解释的质量和实用性。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [209] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA提出了一种两阶段方法，通过先学习光谱模式再估计RGB到HSI的映射，解决了现有方法直接从低维到高维转换的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接学习RGB到HSI的映射，忽略了从低维到高维转换的固有挑战。

Method: MCGA采用两阶段方法：第一阶段用多尺度VQ-VAE学习光谱模式，提取混合码本（MoC）；第二阶段通过查询MoC特征优化RGB到HSI的映射。还引入了灰度感知注意力和量化自注意力。

Result: MCGA在实验中实现了最先进的性能。

Conclusion: MCGA通过两阶段方法和注意力机制，高效且轻量地实现了HSI重建。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [210] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 论文研究了现代点跟踪方法在超声心动图中的潜力，通过改进训练策略和提出轻量级网络，显著提升了运动估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂心脏运动估计中表现不佳，而现代点跟踪方法在超声心动图领域尚未充分探索。

Method: 分析心脏运动偏差，改进训练策略并提出轻量级网络，利用多尺度成本体积提升性能。

Result: 改进后的方法在位置准确性和轨迹误差上分别提升了60.7%和61.5%，并在临床评估中表现出更好的可重复性。

Conclusion: 提出的方法在超声心动图中优于现有技术，展示了更好的泛化能力和临床实用性。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [211] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 论文探讨了旋转等变性在航空图像目标检测中的重要性，提出了一种严格旋转等变的网络结构，并通过多分支头网络减少参数并提高精度，最终在多个数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 航空图像中目标的任意方向性使得旋转等变性成为关键特性，但目前研究较少，且现有方法多为近似旋转等变。论文旨在验证严格旋转等变性是否必要，并提升检测性能。

Method: 实现严格旋转等变的骨干和颈部网络，提出多分支头网络以减少参数并提高精度，最终构建MessDet检测器。

Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上取得最优性能，且参数数量极低。

Conclusion: 严格旋转等变性对航空图像目标检测有显著提升，多分支头网络有效平衡参数与精度，MessDet成为高效解决方案。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [212] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为IGD的方法，通过自然语言指令快速生成可编辑的多模态图层，解决了现有方法缺乏创造力和可编辑性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图形设计方法缺乏创造力和智能性，依赖布局生成的两阶段方法劳动密集，扩散模型生成的文件不可编辑且文本渲染效果差。

Method: IGD采用参数化渲染和图像资产生成的新范式，利用MLLM的多模态理解和推理能力预测属性、排序和布局，并结合扩散模型生成图像内容。

Result: 实验结果表明，IGD在复杂图形设计任务中具有可扩展性和扩展性，提供了新的解决方案。

Conclusion: IGD为图形设计提供了一种高效、可编辑且智能的新方法。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [213] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 论文提出Crucial-Diff框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺导致模型过拟合和数据集不平衡，现有生成模型生成的样本重复且简单，无法针对下游模型的弱点提供关键信息。

Method: 提出Crucial-Diff框架，包含场景无关特征提取器（SAFE）和弱点感知样本挖掘器（WASM），生成多样且高质量的样本。

Result: 在MVTec数据集上达到83.63%的像素级AP和78.12%的F1-MAX；在息肉数据集上达到81.64%的mIoU和87.69%的mDice。

Conclusion: Crucial-Diff能有效生成关键样本，提升模型性能，且具有领域无关性。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [214] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: 提出了ECP框架，通过两阶段方法解决MLLM在高分辨率图像上的性能问题，无需训练且任务无关。


<details>
  <summary>Details</summary>
Motivation: MLLM在高分辨率图像上表现不佳，因固定分辨率训练导致细节丢失或泛化能力差。

Method: ECP框架：先通过低分辨率图像预测候选区域，再基于候选区域进行最终预测。

Result: 在4K GUI grounding和4K、8K MLLM感知任务中分别提升21.3%、5.8%、5.2%。

Conclusion: ECP有效保留细节并提升性能，适用于高分辨率图像任务。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [215] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 提出了一种基于多图像超分辨率（MISR）和卷积神经网络（CNN）的方法，用于在超低剂量条件下实现原子级分辨率的电子显微镜成像。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜在光束敏感材料（如蛋白质和2D材料）上的应用受到辐射损伤的限制，需要突破传统电子显微镜的剂量限制。

Method: 结合多图像超分辨率（MISR）和卷积神经网络（CNN），开发了一种双路径、注意力引导的网络，用于4D-STEM，从超低剂量数据中实现原子级超分辨率。

Result: 在超低剂量条件下，该方法的空间分辨率与传统ptychography相当，适用于非晶、半晶和晶态光束敏感样品。

Conclusion: 该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了一种通用且高效的新方法。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [216] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 论文介绍了ProGait数据集，用于支持视觉任务如视频对象分割、2D人体姿态估计和步态分析，并展示了其在假肢特定任务中的改进性能。


<details>
  <summary>Details</summary>
Motivation: 假肢步态分析对康复至关重要，但现有视觉机器学习方法在假肢检测和分析上存在挑战。

Method: 提出ProGait数据集，包含412个视频片段，并提供了基准任务和微调基线模型。

Result: 基线模型在假肢特定任务中表现优于预训练视觉模型。

Conclusion: ProGait数据集为假肢步态分析提供了实用工具，并展示了其潜力。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [217] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net提出了一种基于Hölder散度的多视图分类和聚类方法，通过结合Dempster-Shafer证据理论和变分狄利克雷分布，提升了不确定性和多视图融合的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖KL散度估计不确定性，忽略了模态间的领域差距，KPHD-Net旨在解决这一问题。

Method: 使用变分狄利克雷分布表示类别概率分布，结合Dempster-Shafer证据理论和Kalman滤波器进行多视图融合。

Result: 实验表明KPHD-Net在分类和聚类任务中优于现有方法，具有更高的准确性、鲁棒性和可靠性。

Conclusion: KPHD-Net通过改进的不确定性估计和多视图融合方法，显著提升了多视图学习的性能。

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [218] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 本文分析了潜在扩散模型（LDMs）中自编码器的关键属性，提出了变分掩码自编码器（VMAEs），并将其整合为LDMAEs，显著提升了图像生成质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型在图像生成方面潜力巨大，但自编码器的理想属性和最优设计尚未充分探索。本文旨在填补这一空白。

Method: 通过分析自编码器的三个关键属性（潜在平滑性、感知压缩质量和重建质量），提出VMAEs，并整合为LDMAEs。

Result: 实验表明，LDMAEs显著提升了图像生成质量和计算效率。

Conclusion: VMAEs和LDMAEs为潜在扩散模型中的自编码器设计提供了新的方向，具有实际应用价值。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [219] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出3DGAA框架，通过联合优化几何和外观属性，生成物理可实现的对抗性物体，显著降低检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D物理攻击难以平衡物理真实性和攻击鲁棒性，需改进。

Method: 利用3D高斯抛光的14维参数化，联合优化几何和外观，引入物理过滤和增强模块。

Result: 检测mAP从87.21%降至7.38%，优于现有方法，且具有高迁移性。

Conclusion: 3DGAA是评估自动驾驶感知系统安全性的实用攻击框架。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [220] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 该研究利用多壳层扩散MRI数据和基于视觉Transformer的深度学习框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累检测。


<details>
  <summary>Details</summary>
Motivation: 通过利用多壳层扩散MRI数据的微观结构信息，结合深度学习技术，提高阿尔茨海默病和淀粉样蛋白积累的早期诊断准确性。

Method: 采用Swin Transformer模型对多壳层扩散MRI数据进行分类，提取DTI和NODDI关键指标并投影到2D平面，结合低秩适应技术优化模型。

Result: 在阿尔茨海默病痴呆与认知正常个体的分类中达到95.2%的平衡准确率，淀粉样蛋白检测的准确率为77.2%和67.9%。

Conclusion: 研究表明扩散MRI和Transformer架构在阿尔茨海默病早期检测中具有潜力，支持数据有限的生物医学诊断。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [221] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 论文总结了反无人机跟踪技术的重要性、现状及未来方向，重点分析了基于视觉和视觉融合的算法，并提供了公开数据集链接。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的快速发展和广泛应用，反无人机跟踪在公共安全、边境巡逻等复杂环境中变得至关重要。

Method: 综述了反无人机检测与跟踪技术的特点和挑战，整理了公开数据集，并分析了近年来的视觉和视觉融合算法。

Result: 提供了反无人机跟踪领域的研究现状和算法分析，为研究者提供了资源支持。

Conclusion: 论文指出了未来研究方向，旨在推动反无人机跟踪领域的进一步发展。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [222] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 提出两种方法（P-BSC和I-BSC）解决相位移动轮廓测量（PSP）在动态测量中的运动误差问题，I-BSC通过加权求和条纹图像而非相位帧，显著降低计算复杂度和误差累积。


<details>
  <summary>Details</summary>
Motivation: PSP在动态测量中因物体运动导致误差，需解决运动误差问题。

Method: P-BSC加权求和运动影响的相位帧，I-BSC改为加权求和条纹图像，仅计算一次反正切函数。

Result: BSC方法显著减少运动误差，I-BSC计算复杂度降低一阶，计算帧率提升数倍至数十倍。

Conclusion: I-BSC在减少运动误差和提升计算效率方面优于P-BSC，适用于高分辨率3D重建。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [223] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA是首个用于珊瑚礁分析的大规模视觉问答数据集，包含12,805张真实珊瑚图像和277,653个问题-答案对，旨在解决珊瑚图像分析的领域特定和多维问题。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁是重要但脆弱的生态系统，需要持续监测以支持保护。现有珊瑚图像分析需要领域专业知识，而视觉问答（VQA）技术可提供用户友好的交互方式。

Method: 开发了一个半自动数据构建流程，结合海洋生物学家合作，确保数据质量和可扩展性。

Result: CoralVQA为珊瑚礁图像的视觉语言推理提供了全面基准，并揭示了当前大型视觉语言模型的关键局限和机会。

Conclusion: 该研究为未来大型视觉语言模型的发展奠定了基础，特别强调支持珊瑚保护工作。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [224] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet提出了一种基于内容自适应卷积的新架构，通过RAPConv和PAN-DFF模块提升遥感图像融合的精度和效果。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在遥感图像融合中因卷积核的均匀应用而忽视局部内容变化，限制了性能。

Method: 采用RAPConv生成空间自适应卷积核，结合PAN-DFF模块的动态特征融合和注意力机制。

Result: 在公开数据集上，RAPNet在定量和定性评估中均优于现有方法。

Conclusion: RAPNet通过自适应组件显著提升了遥感图像融合的性能。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [225] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 提出了一种结合半监督联邦学习、室内定位导航和视觉识别的框架，用于高精度、隐私保护的老年人跌倒检测。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化加剧，老年人跌倒问题日益严重，及时检测可减少医疗费用和恢复时间，同时需兼顾隐私保护。

Method: 结合SF2D（半监督联邦学习）、室内定位导航系统和视觉识别系统，通过可穿戴设备、边缘设备和机器人实现多系统协同检测。

Result: SF2D准确率99.19%，视觉识别准确率96.3%，导航系统成功率95%，整体框架准确率达99.99%。

Conclusion: 该框架不仅高效可靠，还能保护隐私，为老年人跌倒检测提供了安全解决方案。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [226] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种选择性优化框架，结合低分辨率反向传播（BP-low）和高分辨率零阶优化（ZO-high），以实现内存高效且高质量的文本到图像扩散模型个性化。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现高效且隐私保护的文本到图像扩散模型个性化，同时解决内存限制问题。

Method: 提出选择性优化框架，动态选择BP-low和ZO-high，并通过时间步感知的概率函数优化策略。

Result: 实验表明，该方法在显著降低内存消耗的同时，保持了高质量的性能，适用于设备端个性化。

Conclusion: 该框架通过结合BP-low和ZO-high的优势，实现了高效、高质量的模型个性化，适用于资源受限的设备。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [227] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 论文提出LifelongPR，一种持续学习框架，用于点云地点识别（PCPR），通过动态样本选择和提示学习解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有PCPR模型在适应新环境或传感器类型时易发生灾难性遗忘，导致性能下降和实用性受限。

Method: 提出动态样本选择方法和基于提示学习的CL框架，结合两阶段训练策略。

Result: 在多个数据集上验证，性能显著优于现有方法，mIR@1提升6.50%，mR@1提升7.96%，F值降低8.95%。

Conclusion: LifelongPR有效解决了PCPR中的持续学习问题，提升了模型的适应性和实用性。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [228] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 论文提出了一个全面的视网膜异常检测基准，解决了现有方法在数据、算法和评估上的局限性，并提出了一种新方法NFM-DRA，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜异常检测领域缺乏公开的全面基准，导致方法评估受限，且现有方法未能充分利用临床数据。

Method: 通过引入一个系统性基准，并提出了NFM-DRA方法，结合解耦异常表示（DRA）和正常特征记忆机制。

Result: NFM-DRA在性能上优于现有方法，但对某些未见异常仍存在性能下降。

Conclusion: 该基准和方法为视网膜异常检测提供了更全面的评估框架，并推动了该领域的发展。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [229] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo是一种新型多模态Transformer，用于漫画书的页面流分割（PSS），在视觉和多模态版本中均优于传统基线和大规模通用视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 漫画书的PSS是自动化内容理解的关键任务，为角色分析、故事索引等下游任务提供基础。

Method: 开发了CoSMo的视觉和多模态变体，并在新的20,800页标注数据集上验证。

Result: CoSMo在F1-Macro、全景质量和流级指标上显著优于基线模型，视觉特征主导宏观结构，多模态有助于解决模糊问题。

Conclusion: CoSMo为漫画书分析设立了新标准，支持可扩展的内容理解。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [230] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 论文比较了多种方法在多视图Transformer中引入相机几何信息，提出了一种新的相对位置编码PRoPE，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多视图计算机视觉任务中，相机几何关系对3D感知至关重要，需要有效方法将视觉标记与3D空间关联。

Method: 比较了三种方法：标记级射线图编码、注意力级相对位姿编码，以及新提出的PRoPE（捕捉完整相机参数）。

Result: PRoPE在多个任务（如新视图合成、立体深度估计）中表现优异，尤其在泛化性和模型规模扩展时效果显著。

Conclusion: PRoPE在多视图Transformer中有效整合相机几何信息，提升了性能，适用于多种任务和场景。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [231] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一种新颖的前馈模型，能够在一秒内从单目视频合成4D动态新视图，统一建模外观、几何和运动。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景中外观、几何和运动的统一建模问题，并支持多任务应用。

Method: 使用像素对齐的高斯基元网格表示动态3D场景，显式监督其时间变化运动。

Result: 在多个任务中验证了MoVieS的有效性和高效性，性能竞争且速度显著提升。

Conclusion: MoVieS通过统一建模和高效训练，支持多种零样本应用，具有广泛潜力。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [232] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 本文通过分析扩散模型中预测噪声图像能量的变化，提出了一种基于小波变换的频率域调节机制，显著改善了生成质量并解决了曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成能力上表现优异，但受到曝光偏差的显著影响。本文旨在通过分析能量变化规律，提出改进方法。

Method: 利用小波变换分别调节低频和高频子带，并基于能量减少现象分析曝光偏差。

Result: 提出的训练免费、即插即用方法显著提升了多种扩散模型的生成质量。

Conclusion: 频率域调节机制为不同架构的扩散模型提供了解决曝光偏差的稳健方案。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [233] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer的两阶段迁移学习策略，显著提升了遥感图像水体分割任务在目标域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像水体分割中域偏移和小样本问题。

Method: 采用两阶段迁移学习策略，先在源域训练基础模型，再在目标域微调。

Result: IoU从25.50%提升至64.84%，显著优于直接迁移。

Conclusion: 该策略有效解决域差异导致的性能下降，为数据稀缺且环境独特的遥感场景提供了高精度信息提取技术范式。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [234] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出一种完全自监督的方法，从无标签的相机陷阱视频中学习黑猩猩面部嵌入，性能优于有监督基线。


<details>
  <summary>Details</summary>
Motivation: 解决野生动物监测中手动识别个体的瓶颈问题。

Method: 利用DINOv2框架，在自动提取的面部图像上训练Vision Transformers，无需身份标签。

Result: 在Bossou等挑战性基准上表现出色，超越有监督基线。

Conclusion: 自监督学习在生物多样性监测中潜力巨大，为可扩展、非侵入性种群研究铺路。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


### [235] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP通过双分支训练、区域提示和分层特征对齐模块，解决了CLIP在长文本任务中的输入限制，并在长文本和短文本检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CLIP在短文本任务中表现良好，但在长文本任务中因输入长度限制表现不佳，需要改进。

Method: 提出FIX-CLIP，包括双分支训练、多区域提示和分层特征对齐模块，并利用合成数据训练。

Result: FIX-CLIP在长文本和短文本检索任务中达到最优性能，并在扩散模型中表现出色。

Conclusion: FIX-CLIP有效解决了CLIP的长文本输入问题，并在多个任务中表现优异。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [236] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 提出了一种多摄像头多目标跟踪框架，通过轨迹和外观特征实现跨视图的全局身份一致性分配。


<details>
  <summary>Details</summary>
Motivation: 解决多摄像头场景下目标跟踪中全局身份分配不一致的问题。

Method: 采用BoT-SORT单摄像头跟踪，通过轨迹特征匹配初始化全局ID，后续帧使用优先全局匹配策略。

Result: 实现了跨视图的全局身份一致性分配，并通过3D位置估计进行空间验证。

Conclusion: 该框架在多摄像头多目标跟踪中有效解决了身份一致性问题。

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [237] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 提出了一种新的半监督全景分割方法DEARLi，通过结合两个专用基础模型，显著提升了识别和定位能力，在标签稀缺的场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决像素级标注成本高的问题，探索基础模型在半监督分割中的应用潜力。

Method: 结合无监督掩码变换一致性和CLIP特征的零样本分类增强识别，通过SAM伪标签进行类无关解码器预热以提升定位。

Result: 在ADE20K数据集上仅用158张标注图像达到29.9 PQ和38.9 mIoU，性能优于现有方法且GPU内存需求降低8倍。

Conclusion: DEARLi在标签稀缺和大分类场景下表现卓越，为半监督分割提供了高效解决方案。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [238] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 论文提出了一种受预测编码启发的反馈机制，通过循环优化内部状态，显著提升了模型在噪声条件下的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统依赖反馈连接优化感知，而人工神经网络多为前馈结构。本文旨在通过反馈机制提升模型的鲁棒性和适应性。

Method: 在标准U-Net架构中引入反馈循环，结合软最大投影和指数衰减操作确保稳定性。

Result: 反馈模型在噪声条件下表现更优，仅需两个训练样本即可超越随机性能，而前馈模型需至少四个样本。

Conclusion: 反馈机制增强了模型的鲁棒性和数据效率，为更适应性和生物启发的神经网络架构提供了方向。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [239] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard是一个基于AI的视频系统，用于实时自动评估混凝土的工作性，解决了传统坍落度测试的不足。


<details>
  <summary>Details</summary>
Motivation: 传统坍落度测试方法耗时、手动且结果不一致，无法满足实时监控需求，因此需要一种更高效、准确的解决方案。

Method: 开发了SlumpGuard系统，通过视频分析混凝土流动情况，实现全批次自动检测，无需人工干预。

Result: 实证结果表明，SlumpGuard在实际应用中有效提升了质量控制的准确性和效率。

Conclusion: SlumpGuard为现代混凝土质量保证提供了一种实用的解决方案。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [240] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的人物检索方法，通过图像和区域级别的域适应技术（DaD和MRA）解决合成数据与真实数据之间的域差距问题，并在多个数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和手动标注成本高，合成数据成为预训练模型的选择，但合成数据与真实数据之间的域差距（如光照、颜色和视角差异）限制了预训练-微调范式的效果。

Method: 提出了一种统一的文本人物检索流程，包含图像级别的域感知扩散（DaD）和区域级别的多粒度关系对齐（MRA）。DaD迁移图像分布，MRA通过视觉区域与描述性句子的对应关系进行精细对齐。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上取得了最先进的结果。

Conclusion: 双级别域适应方法有效解决了合成数据与真实数据之间的域差距问题，显著提升了文本人物检索的性能。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [241] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种非对称表示学习（ARL）策略，通过不平衡优化提升多模态学习性能，证明模态依赖比例与方差比例成反比时性能最优。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因优化不足而表现不如单模态学习，现有方法通过梯度平衡解决，但本文认为平衡学习并非最优，需根据模态方差调整依赖比例。

Method: 提出ARL策略，通过辅助正则器计算模态预测方差，重新加权优化各模态，使依赖比例与方差比例成反比，并联合优化预测偏差与多模态损失。

Result: 在多个数据集上的实验验证了ARL的有效性和通用性，且无需额外参数。

Conclusion: ARL通过不平衡优化显著提升多模态学习性能，且独立于模型结构和融合方法。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [242] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 情绪表达研究通常假设情绪具有跨文化普遍性，但本研究质疑这一假设，探索种族背景对情绪表达的影响。

Method: 构建跨文化微表情数据库，标注种族标签，进行单一种族与混合种族的对比实验，并提出考虑种族背景的框架。

Result: 实验揭示了种族偏见对情绪表达的影响，提出的框架能更好地识别不同种族的微表情。

Conclusion: 情绪普遍性假设存在过度简化，种族背景是情绪表达分析中不可忽视的因素。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [243] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文揭示了多模态学习中模态编码器与模态融合模块之间的优化冲突，并提出解耦梯度学习（DGL）框架以解决此问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因优化不足而表现不如单模态学习，现有方法未能解释主导模态在多模态模型中表现不佳的原因。

Method: 提出DGL框架，通过截断多模态损失对模态编码器的梯度，并用单模态损失梯度替代，同时移除单模态损失对模态融合模块的梯度干扰。

Result: 实验证明DGL在多种模态、任务和框架中有效且通用。

Conclusion: DGL通过解耦优化过程，显著提升了多模态模型的性能。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [244] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Wardrobe Polyptych LoRA的新方法，用于个性化人类图像生成，通过训练LoRA层减少计算负担，并利用空间参考和选择性损失提升生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化人类图像生成中面临计算成本高和实时性差的问题，需要解决精确属性保留的挑战。

Method: 训练LoRA层，利用空间参考和选择性主题区域损失，减少信息丢失并提升生成质量。

Result: 实验表明，该方法在保真度和一致性上显著优于现有技术，支持少样本训练和单模型推理。

Conclusion: Wardrobe Polyptych LoRA提供了一种高效且高质量的个性化人类图像生成解决方案，无需额外推理参数。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [245] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO提出了一种改进Reflow的方法，通过噪声优化和轨迹增强，显著提升了单步和少步图像生成的质量。


<details>
  <summary>Details</summary>
Motivation: Reflow在训练中通过确定性耦合改善图像生成质量，但存在分布差距问题，导致生成速度和质量受限。

Method: VRFNO结合编码器和神经速度场，引入历史速度项和噪声优化，形成优化的耦合。

Result: 实验表明，VRFNO在单步和少步生成任务中表现优异，显著优于Reflow。

Conclusion: VRFNO有效解决了Reflow的局限性，成为图像生成领域的新标杆。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [246] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Spatial Lifting (SL) 是一种新颖的密集预测方法，通过将输入提升到高维空间并处理，显著减少模型参数和推理成本，同时在多个基准任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统密集预测方法通常计算成本高且参数多，SL旨在通过维度提升解决这些问题，同时保持性能。

Method: SL将2D输入（如图像）提升到高维空间（如3D），并使用对应维度的网络（如3D U-Net）处理，生成结构化输出。

Result: 在19个基准数据集（13个语义分割和6个深度估计）上验证，SL在减少98%参数和降低推理成本的同时，性能仍具竞争力。

Conclusion: SL为密集预测任务提供了一种更高效、准确和可靠的视觉建模新范式。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [247] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD利用基础模型生成合成OOD数据，通过迭代修复和噪声调整增强CLIP模型的边界区分能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在处理接近InD数据的OOD样本时的误分类问题。

Method: 利用扩散模型和MLLMs生成边界对齐的合成OOD数据，通过梯度调整噪声，并微调CLIP模型的图像编码器和负标签特征。

Result: 在ImageNet基准测试中，AUROC提升2.80%，FPR95降低11.13%，性能显著优于现有方法。

Conclusion: SynOOD通过合成边界OOD数据有效提升了模型的OOD检测能力，且参数和运行时增加极少。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [248] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了AI生成图像检测（AID）在现实世界中的挑战，提出新数据集ITW-SM并分析影响检测性能的四个关键因素，最终将AUC平均提升26.87%。


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的快速发展，AI生成图像的质量已足以欺骗人类，但现有AID模型在现实场景中表现不佳，亟需改进。

Method: 引入ITW-SM数据集，分析骨干架构、训练数据组成、预处理策略和数据增强组合对AID性能的影响。

Result: 通过系统优化，AID模型在现实条件下的AUC平均提升了26.87%。

Conclusion: 研究揭示了AID模型在现实世界中的关键改进方向，为未来技术发展提供了重要参考。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [249] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 研究探讨了风格迁移在语义分割中减少纹理偏置和增强鲁棒性的效果，通过随机Voronoi细胞区域进行风格迁移，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 探索风格迁移是否能在语义分割中减少纹理偏置并提高鲁棒性，以改进DNN的泛化能力。

Method: 使用随机Voronoi细胞区域进行风格迁移，生成风格化数据训练语义分割DNN，减少纹理依赖。

Result: 风格迁移显著减少纹理偏置，增强对图像损坏和对抗攻击的鲁棒性，适用于多种网络架构和数据集。

Conclusion: 风格迁移在语义分割中有效减少纹理偏置并提升鲁棒性，具有普适性。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [250] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种基于多折叠径向对称的Kaleidoscopic Background Attack（KBA），通过优化背景纹理攻击相机姿态估计模型，显著提高了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 在稀疏输入的对象中心场景中，背景纹理对相机姿态估计的准确性有显著影响，因此需要一种有效的攻击方法。

Method: 使用相同片段形成多折叠径向对称的圆盘，并提出投影方向一致性损失优化这些片段。

Result: 实验表明，优化的对抗性背景能有效攻击多种相机姿态估计模型。

Conclusion: KBA方法通过优化背景纹理，显著提升了攻击相机姿态估计模型的效果。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [251] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于聚类的动态视觉标记生成方法FTCFormer，通过语义而非空间位置分配标记，提升了特征表示能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构将图像嵌入为均匀的网格标记，忽略了语义信息，导致特征表示不理想。

Method: 引入聚类下采样模块，动态生成视觉标记；提出DPC-FKNN、SCS和Cmerge策略优化特征提取。

Result: 在32个数据集上验证，FTCFormer在图像分类任务中表现优于基线，提升幅度为1.43%（细粒度）、1.09%（自然图像）、0.97%（医学）和0.55%（遥感）。

Conclusion: FTCFormer通过语义驱动的标记分配策略，显著提升了视觉任务的性能。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [252] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种利用高质量参考图像作为视觉提示的面部视频恢复方法，通过解耦交叉注意力机制和反馈学习，有效解决身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在严重退化情况下难以保留细粒度的身份特征，导致恢复结果缺乏个体特性。

Method: IP-FVR结合参考图像的语义信息，采用解耦交叉注意力机制和反馈学习方法，并通过指数混合策略和多流负提示增强恢复效果。

Result: 实验表明IP-FVR在质量和身份保留方面优于现有方法。

Conclusion: IP-FVR在面部视频恢复中具有显著的实际应用潜力。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [253] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo是一种新的视觉封装方法，用于视频多模态大语言模型（video MLLMs），通过视觉概念区分器和时间焦点校准器，解决了语义模糊和时间不连贯的问题。


<details>
  <summary>Details</summary>
Motivation: 现有线性投影器在视频封装中导致语义模糊和时间不连贯，而重采样器结构虽有望解决但尚未实现有效方案。

Method: DisCo包含视觉概念区分器（VCD）和时间焦点校准器（TFC），分别确保语义清晰和时间一致性。

Result: 在多个视频MLLM框架上，DisCo显著优于现有方法，并在视频理解基准测试中表现优异，同时提高了令牌效率。

Conclusion: DisCo通过改进视觉封装，显著提升了视频MLLMs的性能和效率。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [254] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于双视觉编码器的无注释手语翻译框架，通过对比视觉-语言预训练实现高效翻译。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵且不完整的注释，无法充分捕捉连续手语的复杂性。

Method: 采用双视觉编码器框架，通过对比目标对齐视觉和文本嵌入，下游任务中融合视觉特征输入编码器-解码器模型。

Result: 在Phoenix-2014T基准测试中，双编码器架构表现优于单流变体，并取得最高BLEU-4分数。

Conclusion: 双视觉编码器框架在无注释手语翻译中具有显著优势。

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [255] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为IMD的新框架，通过预训练扩散模型解决图像特征匹配中的对齐问题，显著提升了多实例场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入基础模型进行特征匹配时忽视了单图像理解与跨图像理解需求之间的不对齐问题，导致多实例特征匹配效果不佳。

Method: IMD框架包含两部分：1) 使用生成式扩散模型捕捉实例级细节；2) 提出跨图像交互提示模块，促进图像对间的双向信息交互。

Result: IMD在常用基准测试中达到新SOTA，在提出的IMIM基准上性能提升12%。

Conclusion: IMD有效解决了基础模型在特征匹配中的不对齐问题，显著提升了多实例场景下的匹配性能。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [256] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP是一种新的量化方法，利用文本提示为扩散模型的每一层和时间步选择比特精度，降低计算复杂度并提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成任务中表现出色，但其高计算复杂度限制了在资源受限环境中的应用。现有量化方法未充分利用输入条件（如文本提示）的信息。

Method: 提出QLIP方法，通过文本提示指导每一层和时间步的比特精度选择，并可无缝集成到现有量化方法中。

Result: 实验表明QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。

Conclusion: QLIP为扩散模型量化提供了高效且灵活的解决方案，显著提升了资源受限环境下的实用性。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [257] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet是一种多头部特征引导的语义分割架构，旨在提升平面图中墙壁分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 改进平面图中墙壁分割的泛化能力。

Method: 采用U-Net分割主干，结合多头部专用特征提取器，提取领域特定特征图并注入U-Net潜在空间以引导分割过程。特征提取器通过训练编码-解码器生成墙壁补丁的压缩潜在表示，并预测墙壁宽度。

Result: 实验表明，注入特征后性能优于普通U-Net，验证了方法的有效性。

Conclusion: FGSSNet通过特征引导显著提升了墙壁分割的泛化能力。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [258] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种基于随机图模型的文本适配器VRGAdapter，用于捕获类别描述的多样性和类间关系，并通过不确定性引导的多分支融合提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有确定性文本适配器无法充分捕捉类别描述的多样性和类间关系，限制了视觉语言模型在下游任务中的潜力。

Method: 利用顶点随机知识图（VRKG）建模类别描述的多样性和类间关系，通过概率消息传播学习上下文感知的分布表示，并采用重参数化采样实现适配器学习。

Result: 在多个基准数据集上的实验验证了VRGAdapter的有效性。

Conclusion: VRGAdapter提供了一种更通用的适配器解决方案，并通过UMF提升了模型的鲁棒性。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [259] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出并解决了细粒度零样本目标检测（FG-ZSD）问题，开发了基于改进两阶段检测器的MSHC方法，并构建了首个FG-ZSD基准数据集FGZSD-Birds。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标检测（ZSD）方法主要针对粗粒度对象，而现实场景中常需处理细粒度对象（如不同鸟类、鱼类和花卉），其类别间差异微小，难以区分。

Method: 提出MSHC方法，基于改进的两阶段检测器，采用多级语义感知嵌入对齐损失，确保视觉与语义空间的紧密耦合。

Result: 在构建的FGZSD-Birds数据集上，MSHC方法优于现有ZSD模型。

Conclusion: FG-ZSD是一个重要且具有挑战性的新问题，MSHC方法在细粒度检测任务中表现优异。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [260] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL是一个测试时、数据驱动的框架，利用基础模型的互联网规模视觉先验，通过生成和优化候选变换来实现鲁棒感知，无需重新训练或架构更改。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖专用架构或预定义增强训练，限制了泛化能力，需要一种更通用的鲁棒感知方法。

Method: FOCAL通过生成和优化候选变换，使其趋向视觉典型的“规范”视图，从而提升鲁棒性。

Result: 实验显示FOCAL显著提升了CLIP和SAM在2D/3D旋转、光照变化和昼夜变化等挑战性变换中的鲁棒性。

Conclusion: FOCAL挑战了变换特定训练的必要性，提供了一种可扩展的鲁棒感知路径。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [261] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的管道，提升了遥感图像分类的性能。


<details>
  <summary>Details</summary>
Motivation: 解决卷积神经网络（CNN）偏向纹理特征的局限性，探索TDA在复杂数据集中的几何信息表达能力。

Method: 设计了一个TDA特征工程管道，并将其与ResNet18模型结合，应用于EuroSAT和RESISC45数据集。

Result: 在EuroSAT数据集上达到99.33%的准确率，超过ResNet50和XL Vision Transformers；在RESISC45数据集上比基线高1.82%。

Conclusion: TDA特征可以成功与深度学习模型结合，扩展了TDA的适用性，即使在没有显式拓扑结构的数据集上。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [262] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 论文探讨了代数、数值计算和计算机视觉中参数化代数方程组的求解问题，特别是与RanSaC相关的模型拟合。


<details>
  <summary>Details</summary>
Motivation: 研究参数化代数方程组的求解问题，因其在计算机视觉（如RanSaC）中的实际应用需求。

Method: 概述了过去5年多的工作，旨在衡量此类参数化系统的固有求解难度，并推动实用解决方案的发展。

Result: 提出了解决参数化代数方程组的新方法，并取得了实际进展。

Conclusion: 研究为参数化代数方程组的求解提供了理论框架和实用工具，对计算机视觉等领域有重要意义。

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [263] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为SC-AGIQA的统一框架，通过文本-视觉语义约束解决AI生成图像质量评估中的语义错位和细节感知缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估AI生成图像时存在语义错位和细节感知缺失的挑战，需要更全面的评估框架。

Method: 结合文本辅助语义对齐模块（TSAM）和频域细粒度退化感知模块（FFDPM），利用多模态大语言模型和频域分析提升评估效果。

Result: 在多个基准数据集上的实验表明，SC-AGIQA优于现有最先进方法。

Conclusion: SC-AGIQA通过语义约束和频域分析显著提升了AI生成图像质量评估的准确性和全面性。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [264] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal框架通过密集特征网络和分层对齐策略，无需稀疏关键点注释即可从视频重建可动画的3D动物模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀疏语义关键点，获取成本高且不可靠，因此提出无需关键点注释的解决方案。

Method: 使用密集特征网络将2D表示映射到SMAL参数，并结合分层对齐策略整合多种视觉线索。

Result: 实验表明4D-Animal优于基于模型和无模型的基线方法，生成的高质量3D资源可支持其他任务。

Conclusion: 4D-Animal在效率和稳定性上表现优异，具有大规模应用的潜力。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [265] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种新的盲人脸图像恢复方法RefSTAR，通过参考图像的选择、转移和重建，解决了身份保留问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份保留上表现不佳，主要因纹理特征引入不当。

Method: 构建RefSel模块选择参考图像，设计特征融合范式转移特征，并提出重建机制确保参考特征保留。

Result: 在多种骨干模型上表现优异，身份保留和特征转移质量更好。

Conclusion: RefSTAR方法在盲人脸恢复中有效提升了身份保留和特征转移效果。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [266] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc是一种新颖的检索方法，联合预测图像的拍摄时间（小时和月份）和地理位置（GPS坐标），通过共享高维特征空间对齐嵌入，优于现有时间预测方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间戳预测与地理定位的相互依赖问题，支持元数据校正、检索和数字取证等应用。

Method: 采用图像、时间和位置的独立编码器，在共享特征空间中对齐嵌入，提出基于循环环形表面的时间度量学习目标。

Result: GT-Loc在时间预测上超越现有方法，即使输入真实地理位置，同时在标准地理定位任务中表现优异。

Conclusion: 联合优化方法在时间预测和地理定位上均表现优异，统一嵌入空间支持组合和基于文本的图像检索。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [267] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出了一种基于置信度的自蒸馏方法，用于改进结肠镜息肉分割任务，减少了资源需求并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在息肉检测和分割中表现优异但参数量大，容易过拟合且泛化能力差。知识蒸馏和自蒸馏虽有效但资源消耗高。

Method: 采用动态置信系数计算批次内前后迭代的损失，仅需存储前次迭代数据，无需额外计算或内存。

Result: 在息肉分割任务中表现优于现有最优模型，并在多临床中心数据集上泛化良好。

Conclusion: 提出的方法高效且性能优越，代码将公开。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [268] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 利用高分辨率地球观测数据和深度迁移学习，首次绘制了莫桑比克全国2100万个农田边界，准确率达93%，揭示了小农户农业系统的复杂性和农田规模的社会经济与环境影响。


<details>
  <summary>Details</summary>
Motivation: 设计科学政策以提升小农户农业可持续性，需理解农田空间分布和规模等基本系统特性，但现有数据不足。

Method: 整合1.5米高分辨率地球观测数据和深度迁移学习，最小化参考数据需求并增强可迁移性，实现复杂农业系统的全国尺度农田边界绘制。

Result: 绘制了莫桑比克2023年2100万个农田边界，准确率93%，农田规模中位数为0.16公顷，83%小于0.5公顷，揭示了农田规模与可及性、人口密度和森林覆盖变化的关联。

Conclusion: 农田规模是农业社会经济与环境结果（如粮食生产、生计、森林砍伐）及其权衡的关键指标，研究为政策制定提供了重要数据支持。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [269] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ框架通过预训练的VAE快速训练VQ-VAE，显著降低计算成本，同时保持高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 高压缩率VQ-VAE训练计算成本高，需探索更高效的方法。

Method: 利用预训练VAE，结合通道多组量化和后矫正器，减少量化误差。

Result: 在ImageNet上压缩至512个token，重建质量高（rFID=1.06），训练成本降低两个数量级。

Conclusion: ReVQ在效率与重建质量间取得优越平衡，大幅减少训练资源需求。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [270] [Orchestration for Domain-specific Edge-Cloud Language Models](https://arxiv.org/abs/2507.09003)
*Prasoon Patidar,Alex Crown,Kevin Hsieh,Yifei Xu,Tusher Chakraborty,Ranveer Chandra,Yuvraj Agarwal*

Main category: cs.DB

TL;DR: ECO-LLM是一个边缘-云协作系统，通过联合优化LLM服务管道的组件配置和动态策略选择，显著提升性能、降低成本并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注选择最佳LLM模型，忽略了服务管道组件间的交互及动态约束，导致性能、成本和延迟的优化不足。

Method: ECO-LLM由两部分组成：ECO-LLM模拟器（通过查询聚类和帕累托最优路径探索配置空间）和ECO-LLM运行时（动态选择最优策略以满足用户SLO）。

Result: 在智能家居和智能汽车场景中，ECO-LLM在准确性（90% vs. 74%）、成本（降低90%）和延迟（减少55%）上优于GPT-4o。对于未见查询，成本降低62%或响应时间提升62%。

Conclusion: ECO-LLM通过联合优化和动态策略选择，显著提升了LLM服务的效率，同时满足用户定义的性能约束。

Abstract: The remarkable performance of Large Language Models (LLMs) has inspired many
applications, which often necessitate edge-cloud collaboration due to
connectivity, privacy, and cost considerations. Traditional methods primarily
focus on selecting the best LLM model for optimizing performance, while
neglecting the critical interplay between the components of the LLM serving
pipeline (context retrieval, query preprocessing, etc.) or the changing latency
and cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs),
a novel system that reframes this problem as a joint optimization challenge and
solves it by systematically exploring component configurations and dynamically
selecting optimal strategies at the query level. ECO-LLM consists of two
components: (1) the ECO-LLM Emulator, which efficiently explores the vast
configuration space utilizing query clustering and pareto-optimal path
selection, gathering domain-specific performance metrics without exhaustive
evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to
dynamically select optimal resolution strategies for user queries while meeting
user-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart
home and a smart car assistant scenarios. With an exhaustive exploration of all
possible configurations for seen queries, ECO-LLM outperforms cloud-based
models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing
costs by 90% and latency by 55%, demonstrating the value of its joint
optimization at the query level. In practical deployment for previously unseen
queries, ECO-LLM selects configurations that reduce costs by 62% or improve
response times by 62% on average compared to state-of-the-art model routing
approaches, while maintaining higher accuracy and consistently adhering to
specified latency and cost constraints.

</details>


### [271] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Main category: cs.DB

TL;DR: HedraRAG是一个基于图抽象的系统，通过动态图变换优化异构检索增强生成（RAG）服务，提升资源利用并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决异构RAG服务中多阶段工作流和多样化请求模式带来的系统级挑战。

Method: 使用图抽象和动态图变换（如节点分裂、重排序、边添加等）优化执行计划，并映射到混合CPU-GPU流水线。

Result: 在多种RAG工作流中实现1.5x至5x的加速。

Conclusion: HedraRAG通过协调生成与检索，显著提升了服务效率。

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


### [272] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer是一种新型VDBMS，通过自适应查询处理框架高效处理Re-ID查询，解决了Spactula的局限性，并在性能上优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有系统Spactula在大规模摄像头网络中时空过滤精度不足，且不支持自适应查询处理，无法满足高召回率需求。

Method: Tracer使用循环网络建模长期历史相关性，选择最优摄像头处理；引入概率自适应搜索模型，动态更新采样概率。

Result: Tracer在多样化数据集上平均性能优于现有系统3.9倍。

Conclusion: Tracer通过自适应处理和合成基准，显著提升了Re-ID查询的效率和准确性。

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


### [273] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.DB

TL;DR: THOR模块是一个将自然语言问题转化为安全SQL查询的引擎，适用于企业数据库，支持动态元数据注入、错误自纠正及结果解释。


<details>
  <summary>Details</summary>
Motivation: 为非技术用户提供简单、安全的企业数据访问方式，无需SQL知识。

Method: 采用解耦架构，包括查询路由、动态元数据注入、SQL生成及自纠正循环。

Result: 在金融、销售等场景中验证了可靠性和自动化报告能力。

Conclusion: THOR模块通过模式感知和容错设计，实现了零SQL复杂性的企业级数据访问。

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [274] [Rethinking LSM-tree based Key-Value Stores: A Survey](https://arxiv.org/abs/2507.09642)
*Yina Lv,Qiao Li,Quanqing Xu,Congming Gao,Chuanhui Yang,Xiaoli Wang,Chun Jason Xue*

Main category: cs.DB

TL;DR: 本文综述了LSM-tree优化的最新研究，重点讨论了过去五年中的代表性工作，包括性能挑战、解决方案及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LSM-tree在写密集型应用中广泛使用，但其不可预测的压缩行为带来性能波动、读写放大等问题，需要优化。

Method: 综述分析了现有解决方案，包括减少压缩对性能的影响、优化键值操作，并探讨分布式环境下的新挑战。

Result: 总结了LSM-tree优化的最新进展，并提出了未来研究方向。

Conclusion: 本文填补了现有综述的空白，为LSM-tree优化提供了全面视角和未来研究指导。

Abstract: LSM-tree is a widely adopted data structure in modern key-value store systems
that optimizes write performance in write-heavy applications by using append
writes to achieve sequential writes. However, the unpredictability of LSM-tree
compaction introduces significant challenges, including performance variability
during peak workloads and in resource-constrained environments, write
amplification caused by data rewriting during compactions, read amplification
from multi-level queries, trade-off between read and write performance, as well
as efficient space utilization to mitigate space amplification. Prior studies
on LSM-tree optimizations have addressed the above challenges; however, in
recent years, research on LSM-tree optimization has continued to propose. The
goal of this survey is to review LSM-tree optimization, focusing on
representative works in the past five years. This survey first studies existing
solutions on how to mitigate the performance impact of LSM-tree flush and
compaction and how to improve basic key-value operations. In addition,
distributed key-value stores serve multi-tenants, ranging from tens of
thousands to millions of users with diverse requirements. We then analyze the
new challenges and opportunities in these modern architectures and across
various application scenarios. Unlike the existing survey papers, this survey
provides a detailed discussion of the state-of-the-art work on LSM-tree
optimizations and gives future research directions.

</details>


### [275] [Efficient Temporal Simple Path Graph Generation](https://arxiv.org/abs/2507.10017)
*Zhiyang Tang,Yanping Wu,Xiangjun Zai,Chen Chen,Xiaoyang Wang,Ying Zhang*

Main category: cs.DB

TL;DR: 提出了一种高效方法（Verification in Upper-bound Graph）来生成时间简单路径图（tspG），避免直接枚举所有路径的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究时间图中顶点间基于时间路径的关系，填补了生成时间简单路径图问题的研究空白。

Method: 结合时间路径和简单路径约束排除无效边，生成紧上界图，再通过Escape Edges Verification算法精确构建tspG。

Result: 在10个真实世界图上验证了方法的效率和有效性。

Conclusion: 提出的方法能高效生成tspG，适用于时间图分析。

Abstract: Interactions between two entities often occur at specific timestamps, which
can be modeled as a temporal graph. Exploring the relationships between
vertices based on temporal paths is one of the fundamental tasks. In this
paper, we conduct the first research to propose and investigate the problem of
generating the temporal simple path graph (tspG), which is the subgraph
consisting of all temporal simple paths from the source vertex to the target
vertex within the given time interval. Directly enumerating all temporal simple
paths and constructing the tspG is computationally expensive. To accelerate the
processing, we propose an efficient method named Verification in Upper-bound
Graph. It first incorporates the temporal path constraint and simple path
constraint to exclude unpromising edges from the original graph, which obtains
a tight upper-bound graph as a high-quality approximation of the tspG in
polynomial time. Then, an Escape Edges Verification algorithm is further
applied in the upper-bound graph to construct the exact tspG without
exhaustively enumerating all temporal simple paths between given vertices.
Finally, comprehensive experiments on 10 real-world graphs are conducted to
demonstrate the efficiency and effectiveness of the proposed techniques.

</details>


### [276] [Breaking the Storage-Compute Bottleneck in Billion-Scale ANNS: A GPU-Driven Asynchronous I/O Framework](https://arxiv.org/abs/2507.10070)
*Yang Xiao,Mo Sun,Ziyu Song,Bing Tian,Jie Zhang,Jie Sun,Zeke Wang*

Main category: cs.DB

TL;DR: FlashANNS是一个GPU加速的基于图的ANNS系统，通过I/O计算重叠解决现有磁盘ANNS系统的性能问题，显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有磁盘ANNS系统因无法重叠SSD访问与距离计算以及I/O栈性能不足导致性能不佳。

Method: FlashANNS通过依赖松散的异步管道、Warp级并发SSD访问和计算-I/O平衡的图度选择三项创新实现I/O与计算同步协调。

Result: 实验表明，FlashANNS在单SSD和多SSD配置下分别实现2.3-5.9倍和2.7-12.2倍的吞吐量提升。

Conclusion: FlashANNS通过优化I/O与计算重叠，显著提升了ANNS系统的性能。

Abstract: With the advancement of information retrieval, recommendation systems, and
Retrieval-Augmented Generation (RAG), Approximate Nearest Neighbor Search
(ANNS) gains widespread applications due to its higher performance and
accuracy. While several disk-based ANNS systems have emerged to handle
exponentially growing vector datasets, they suffer from suboptimal performance
due to two inherent limitations: 1) failing to overlap SSD accesses with
distance computation processes and 2) extended I/O latency caused by suboptimal
I/O Stack. To address these challenges, we present FlashANNS, a GPU-accelerated
out-of-core graph-based ANNS system through I/O-compute overlapping. Our core
insight lies in the synchronized orchestration of I/O and computation through
three key innovations: 1) Dependency-Relaxed asynchronous pipeline: FlashANNS
decouples I/O-computation dependencies to fully overlap between GPU distance
calculations and SSD data transfers. 2) Warp-Level concurrent SSD access:
FlashANNS implements a lock-free I/O stack with warp-level concurrency control,
to reduce the latency-induced time overhead. 3) Computation-I/O balanced graph
degree Selection: FlashANNS selects graph degrees via lightweight
compute-to-I/O ratio sampling, ensuring optimal balance between computational
load and storage access latency across different I/O bandwidth configurations.
We implement FlashANNS and compare it with state-of-the-art out-of-core ANNS
systems (SPANN, DiskANN) and a GPU-accelerated out-of-core ANNS system
(FusionANNS). Experimental results demonstrate that at $\geq$95\% recall@10
accuracy, our method achieves 2.3-5.9$\times$ higher throughput compared to
existing SOTA methods with a single SSD, and further attains 2.7-12.2$\times$
throughput improvement in multi-SSD configurations.

</details>


### [277] [LogLite: Lightweight Plug-and-Play Streaming Log Compression](https://arxiv.org/abs/2507.10337)
*Benzhao Tang,Shiyu Yang,Zhitao Shen,Wenjie Zhang,Xuemin Lin,Zhihong Tian*

Main category: cs.DB

TL;DR: LogLite是一种轻量级、即插即用的无损日志压缩算法，针对TEXT和JSON日志设计，无需预定义规则或预训练，适应性强，压缩比和速度显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统和物联网设备日志量激增，存储成本高，无损压缩成为节省资源的有效解决方案。

Method: 通过公共日志数据集的特征研究，提出LogLite算法，支持流式处理，适应日志结构变化。

Result: LogLite在压缩比和速度上平均提升67.8%和2.7倍，多数场景下达到帕累托最优。

Conclusion: LogLite是一种高效、适应性强的日志压缩方案，适用于大规模日志处理。

Abstract: Log data is a vital resource for capturing system events and states. With the
increasing complexity and widespread adoption ofmodern software systems and IoT
devices, the daily volume of log generation has surged to tens of petabytes,
leading to significant collection and storage costs. To address this challenge,
lossless log compression has emerged as an effective solution, enabling
substantial resource savings without compromising log information. In this
paper, we first conduct a characterization study on extensive public log
datasets and identify four key observations. Building on these insights, we
propose LogLite, a lightweight, plug-and-play, streaming lossless compression
algorithm designed to handle both TEXT and JSON logs throughout their life
cycle. LogLite requires no predefined rules or pre-training and is inherently
adaptable to evolving log structures. Our evaluation shows that, compared to
state-of-the-art baselines, LogLite achieves Pareto optimality in most
scenarios, delivering an average improvement of up to 67.8% in compression
ratio and up to 2.7 $\times$ in compression speed.

</details>


### [278] [Instance-Optimized String Fingerprints](https://arxiv.org/abs/2507.10391)
*Mihail Stoian,Johannes Thürauf,Andreas Zimmerer,Alexander van Renen,Andreas Kipf*

Main category: cs.DB

TL;DR: 论文提出了一种轻量级二级索引结构——字符串指纹，用于近似处理LIKE谓词，减少计算和I/O开销，并通过混合整数优化适应特定工作负载。


<details>
  <summary>Details</summary>
Motivation: 云数据仓库中字符串列处理效率有限，现有技术如字典编码和前缀分区剪枝效果不足。

Method: 引入字符串指纹作为二级索引，结合混合整数优化，适应特定工作负载。

Result: 在DuckDB v1.3中测试IMDb列，表扫描速度提升最高达1.36倍。

Conclusion: 字符串指纹能有效提升字符串列处理效率，适用于列式查询引擎。

Abstract: Recent research found that cloud data warehouses are text-heavy. However,
their capabilities for efficiently processing string columns remain limited,
relying primarily on techniques like dictionary encoding and prefix-based
partition pruning. In recent work, we introduced string fingerprints - a
lightweight secondary index structure designed to approximate LIKE predicates,
albeit with false positives. This approach is particularly compelling for
columnar query engines, where fingerprints can help reduce both compute and I/O
overhead. We show that string fingerprints can be optimized for specific
workloads using mixed-integer optimization, and that they can generalize to
unseen table predicates. On an IMDb column evaluated in DuckDB v1.3, this
yields table-scan speedups of up to 1.36$\times$.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [279] [MQFQ-Sticky: Fair Queueing For Serverless GPU Functions](https://arxiv.org/abs/2507.08954)
*Alexander Fuerst,Siddharth Anil,Vishakha Dixit,Purushottam,Kulkarni,Prateek Sharma*

Main category: cs.DC

TL;DR: 论文提出了一种支持GPU加速的无服务器计算系统，解决了FaaS框架中GPU资源调度和管理的挑战，显著降低了函数延迟。


<details>
  <summary>Details</summary>
Motivation: 当前FaaS框架（如OpenWhisk）无法有效支持GPU加速，导致机器学习等应用无法充分利用GPU性能。

Method: 设计了MQFQ-Sticky调度策略，结合公平队列和GPU内存管理，优化资源分配和冷启动开销。

Result: 实验表明，该策略将函数延迟降低了2倍至20倍。

Conclusion: 通过创新的调度方法，实现了GPU在FaaS中的高效利用。

Abstract: Hardware accelerators like GPUs are now ubiquitous in data centers, but are
not fully supported by common cloud abstractions such as Functions as a Service
(FaaS). Many popular and emerging FaaS applications such as machine learning
and scientific computing can benefit from GPU acceleration. However, FaaS
frameworks (such as OpenWhisk) are not capable of providing this acceleration
because of the impedance mismatch between GPUs and the FaaS programming model,
which requires virtualization and sandboxing of each function. The challenges
are amplified due to the highly dynamic and heterogeneous FaaS workloads. This
paper presents the design and implementation of a FaaS system for providing GPU
acceleration in a black-box manner (without modifying function code). Running
small functions in containerized sandboxes is challenging due to limited GPU
concurrency and high cold-start overheads, resulting in heavy queueing of
function invocations. We show how principles from I/O scheduling, such as fair
queuing and anticipatory scheduling, can be translated to function scheduling
on GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory
management approach, which balances the tradeoffs between locality, fairness,
and latency. Empirical evaluation on a range of workloads shows that it reduces
function latency by 2x to 20x compared to existing GPU and CPU queueing
policies.

</details>


### [280] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: 本文提出了一种轻量级联邦学习（LTFL）框架，结合无线传输功率控制、模型剪枝和梯度量化，以解决无线网络中联邦学习的实际部署挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备连接无线网络的快速增长，数据量激增，需要机器学习技术提取价值。但集中式机器学习存在通信开销和隐私问题，联邦学习（FL）在网络边缘提供替代方案，但其在无线网络中的实际部署仍具挑战性。

Method: 提出LTFL框架，整合无线传输功率控制、模型剪枝和梯度量化，推导FL收敛间隙的闭式表达式，并基于此构建优化问题以最小化收敛间隙，同时满足延迟和能量约束。

Result: 通过真实数据集实验，LTFL优于现有最优方案。

Conclusion: LTFL框架通过优化模型剪枝、梯度量化和传输功率控制，显著提升了联邦学习在无线网络中的性能和实用性。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [281] [Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks](https://arxiv.org/abs/2507.09926)
*Zixuan Song,Zhishu Shen,Xiaoyu Zheng,Qiushi Zheng,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 提出了一种动态多区域划分框架，用于LEO卫星网络中的智能任务管理，结合遗传算法和MA-DDPG优化任务延迟和资源利用率。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络资源有限且计算负载分布不均，导致任务处理效率低下，亟需动态管理方法。

Method: 提出动态多区域划分框架，结合遗传算法调整区域大小，并采用MA-DDPG进行自适应路由和任务分配。

Result: 仿真显示，该框架在任务延迟、能耗和完成率上优于对比方法。

Conclusion: 该框架有效解决了LEO卫星网络中的任务管理挑战，提升了整体性能。

Abstract: As a key complement to terrestrial networks and a fundamental component of
future 6G systems, Low Earth Orbit (LEO) satellite networks are expected to
provide high-quality communication services when integrated with ground-based
infrastructure, thereby attracting significant research interest. However, the
limited satellite onboard resources and the uneven distribution of
computational workloads often result in congestion along inter-satellite links
(ISLs) that degrades task processing efficiency. Effectively managing the
dynamic and large-scale topology of LEO networks to ensure balanced task
distribution remains a critical challenge. To this end, we propose a dynamic
multi-region division framework for intelligent task management in LEO
satellite networks. This framework optimizes both intra- and inter-region
routing to minimize task delay while balancing the utilization of computational
and communication resources. Based on this framework, we propose a dynamic
multi-region division algorithm based on the Genetic Algorithm (GA), which
adaptively adjusts the size of each region based on the workload status of
individual satellites. Additionally, we incorporate an adaptive routing
algorithm and a task splitting and offloading scheme based on Multi-Agent Deep
Deterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving
tasks. Simulation results demonstrate that our proposed framework outperforms
comparative methods in terms of the task delay, energy consumption per task,
and task completion rate.

</details>


### [282] [EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning](https://arxiv.org/abs/2507.10026)
*Zhifei Xu,Zhiqing Tang,Jiong Lou,Zhi Yao,Xuan Xie,Tian Wang,Yinglong Wang,Weijia Jia*

Main category: cs.DC

TL;DR: 论文提出了一种QoS感知的边缘协作AIGC任务调度算法（EAT），通过分段任务和强化学习优化边缘服务器资源利用，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决边缘服务器部署AIGC服务时资源利用率低、推理延迟与质量不平衡的问题。

Method: 1) 将AIGC任务分段并调度到不同边缘服务器，建模为群体调度问题；2) 提出基于强化学习的EAT算法，利用注意力层提取服务器负载信息，并通过扩散策略网络进行调度；3) 开发AIGC任务调度系统。

Result: 实验表明，EAT算法相比基线方法可降低推理延迟高达56%。

Conclusion: EAT算法有效优化了边缘服务器的资源利用，显著提升了AIGC任务的性能。

Abstract: The growth of Artificial Intelligence (AI) and large language models has
enabled the use of Generative AI (GenAI) in cloud data centers for diverse
AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce
unavoidable delays and substantial resource overhead, which are unsuitable for
users at the network edge with high QoS demands. Deploying AIGC services on
edge servers reduces transmission times but often leads to underutilized
resources and fails to optimally balance inference latency and quality. To
address these issues, this paper introduces a QoS-aware
\underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling
(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to
various edge servers, formulating it as a gang scheduling problem that balances
inference latency and quality while considering server heterogeneity, such as
differing model distributions and cold start issues. 2) We propose a
reinforcement learning-based EAT algorithm that uses an attention layer to
extract load and task queue information from edge servers and employs a
diffusion-based policy network for scheduling, efficiently enabling model
reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm
to divide tasks and distribute them across multiple edge servers for
processing. Experimental results based on our system and large-scale
simulations show that our EAT algorithm can reduce inference latency by up to
56\% compared to baselines. We release our open-source code at
https://github.com/zzf1955/EAT.

</details>


### [283] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: 论文提出了一种名为Elastic Multimodal Parallelism (EMP)的新服务范式，以解决多模态大语言模型(MLLMs)推理效率低下的问题，并开发了ElasticMM系统，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前紧密耦合的服务架构难以区分混合请求类型或适应不同推理阶段的并行策略，导致延迟高和资源利用率低。

Method: 通过EMP范式，ElasticMM系统将请求分离为独立的模态组，动态分配资源，解耦推理阶段并调整并行性，同时采用统一的多模态前缀缓存和非阻塞编码。

Result: 实验表明，ElasticMM在真实数据集上优于现有系统，TTFT延迟降低4.2倍，吞吐量提高3.2-4.5倍。

Conclusion: EMP和ElasticMM为高效服务MLLMs提供了有效解决方案，显著提升了性能和资源利用率。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [284] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: 论文介绍了Dynamic Grale Using ScaNN (Dynamic GUS)，一个在动态环境中高效构建图的系统，继承了Grale的优点并实现了低延迟更新。


<details>
  <summary>Details</summary>
Motivation: 现有工具Grale适用于离线环境，但无法满足动态数据快速更新的需求；而现有的近似最近邻系统（ANN）又局限于单一嵌入的相似性。

Method: 提出了Dynamic GUS系统，结合Grale的质量和低延迟动态更新的能力。

Result: 系统在Google内部有超过10个部署，其中一个应用在Android安全与隐私领域，能比之前快4倍捕获有害应用。

Conclusion: Dynamic GUS成功解决了动态数据环境下图构建的低延迟需求，具有广泛的应用前景。

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


### [285] [Past-Future Scheduler for LLM Serving under SLA Guarantees](https://arxiv.org/abs/2507.10150)
*Ruihao Gong,Shihao Bai,Siyu Wu,Yunqian Fan,Zaijun Wang,Xiuhong Li,Hailong Yang,Xianglong Liu*

Main category: cs.DC

TL;DR: 论文提出了一种名为Past-Future的调度器，通过精确估计请求的内存需求，优化LLM服务框架的吞吐量，并开发了高性能框架LightLLM。


<details>
  <summary>Details</summary>
Motivation: 现有调度器因对请求输出长度多样性的估计不准确，导致内存消耗过高或过低，影响服务吞吐量。

Method: 提出Past-Future调度器，结合历史请求输出长度分布和未来时间点的内存占用计算，精确估计峰值内存需求。

Result: LightLLM框架在重负载下比其他调度器实现2-3倍更高的吞吐量。

Conclusion: Past-Future调度器能适应不同输入输出长度分布，显著提升LLM服务性能。

Abstract: The exploration and application of Large Language Models (LLMs) is thriving.
To reduce deployment costs, continuous batching has become an essential feature
in current service frameworks. The effectiveness of continuous batching relies
on an accurate estimate of the memory requirements of requests. However, due to
the diversity in request output lengths, existing frameworks tend to adopt
aggressive or conservative schedulers, which often result in significant
overestimation or underestimation of memory consumption. Consequently, they
suffer from harmful request evictions or prolonged queuing times, failing to
achieve satisfactory throughput under strict Service Level Agreement (SLA)
guarantees (a.k.a. goodput), across various LLM application scenarios with
differing input-output length distributions. To address this issue, we propose
a novel Past-Future scheduler that precisely estimates the peak memory
resources required by the running batch via considering the historical
distribution of request output lengths and calculating memory occupancy at each
future time point. It adapts to applications with all types of input-output
length distributions, balancing the trade-off between request queuing and
harmful evictions, thereby consistently achieving better goodput. Furthermore,
to validate the effectiveness of the proposed scheduler, we developed a
high-performance LLM serving framework, LightLLM, that implements the
Past-Future scheduler. Compared to existing aggressive or conservative
schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\times$
higher goodput than other schedulers under heavy loads. LightLLM is open source
to boost the research in such direction (https://github.com/ModelTC/lightllm).

</details>


### [286] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: 论文提出了一种名为TORTA的两层时空调度框架，通过结合长期工作负载模式和短期执行约束，优化分布式GPU推理基础设施的资源调度。


<details>
  <summary>Details</summary>
Motivation: 现有调度系统仅依赖当前系统状态进行决策，缺乏对任务需求和资源可用性随时间变化的考虑，导致GPU利用率低、任务迁移开销大和系统响应性差。

Method: TORTA采用两层设计：宏观调度器利用强化学习和最优传输协调区域间任务分配，微观分配器优化区域内任务到服务器的分配以减少延迟和切换成本。

Result: 实验表明，TORTA在多种网络拓扑下将平均推理响应时间降低15%，负载均衡提升4-5%，总运营成本减少10-20%。

Conclusion: TORTA通过时空调度框架显著提升了分布式GPU推理基础设施的效率和性能。

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


### [287] [FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline](https://arxiv.org/abs/2507.10367)
*Jingwei Xu,Junbin Kang,Mingkai Dong,Mingyu Liu,Lu Zhang,Shaohong Guo,Ziyan Qiu,Mingzhen You,Ziyi Tian,Anqi Yu,Tianhong Ding,Xinwei Hu,Haibo Chen*

Main category: cs.DC

TL;DR: FalconFS是一种针对深度学习管道的分布式文件系统，采用无状态客户端架构，通过服务器端路径解析和混合元数据索引优化性能，显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统客户端元数据缓存在深度学习管道中不仅效果不佳，还浪费内存资源，因此需要一种更高效的解决方案。

Method: FalconFS采用服务器端路径解析、混合元数据索引、惰性命名空间复制、并发请求合并和VFS快捷部署。

Result: 相比CephFS和Lustre，FalconFS在小文件读写和深度学习模型训练中分别实现了5.72倍和12.81倍的吞吐量提升。

Conclusion: FalconFS在华为自动驾驶系统中成功部署并运行一年，验证了其高效性和实用性。

Abstract: Client-side metadata caching has long been considered an effective method for
accelerating metadata operations in distributed file systems (DFSs). However,
we have found that client-side state (e.g., caching) is not only ineffective
but also consumes valuable memory resources in the deep learning pipelines. We
thus propose FalconFS, a DFS optimized for deep learning pipelines with the
stateless-client architecture. Specifically, instead of performing client-side
path resolution and caching, FalconFS efficiently resolves paths on the server
side using hybrid metadata indexing and lazy namespace replication. FalconFS
also boosts server concurrency with concurrent request merging and provides
easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show
that FalconFS achieves up to 5.72$\times$ throughput for small file read/write
and up to 12.81$\times$ throughput for deep learning model training. FalconFS
has been running in Huawei autonomous driving system's production environment
with 10,000 NPUs for one year.

</details>


### [288] [Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](https://arxiv.org/abs/2507.10392)
*Runsheng Benson Guo,Utkarsh Anand,Khuzaima Daudjee,Rathijit Sen*

Main category: cs.DC

TL;DR: 论文提出Zorse系统，解决在异构GPU集群上高效训练大语言模型的挑战，通过结合流水线并行和数据并行，并自动配置训练策略。


<details>
  <summary>Details</summary>
Motivation: 由于GPU资源有限且成本高，异构集群成为训练大语言模型的可行方案，但面临负载均衡、内存优化和通信效率等挑战。

Method: 提出Zorse系统，整合流水线并行和数据并行，支持灵活分区异构GPU，并引入自动配置训练策略的规划器。

Result: Zorse在异构训练场景中显著优于现有系统。

Conclusion: Zorse为异构集群上的高效训练提供了可行方案，解决了现有方法的局限性。

Abstract: Large language models (LLMs) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring communication-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both communication- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.

</details>


### [289] [Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?](https://arxiv.org/abs/2507.10413)
*Gabriel Rocha*

Main category: cs.DC

TL;DR: 本文探讨了FLP不可能定理在广义计算定义下的适用性，并利用复杂系统理论分析了一致性问题，提出了不一致性可能是分布式共识的涌现特征，同时讨论了矛盾逻辑的平凡性及其在共识算法中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究FLP不可能定理在广义计算定义下的扩展，以及分布式系统中一致性的涌现特征，探索矛盾逻辑在共识问题中的潜在应用。

Method: 使用复杂系统理论分析系统相变，探讨矛盾逻辑的性质，并研究其在共识算法中的可能性。

Result: FLP不可能定理在广义计算定义下仍然成立，不一致性可能是分布式共识的涌现特征，矛盾逻辑可能导致平凡性。

Conclusion: 分布式系统中的一致性问题具有复杂性，矛盾逻辑可能为解决共识问题提供新思路，但需进一步研究其实际应用。

Abstract: The consensus problem, briefly stated, consists of having processes in an
asynchronous distributed system agree on a value. It is widely known that the
consensus problem does not have a deterministic solution that ensures both
termination and consistency, if there is at least one faulty process in the
system. This result, known as the FLP impossibility theorem, led to several
generalizations and developments in theoretical distributed computing. This
paper argues that the FLP impossibility theorem holds even under a generalized
definition of computation through oracles. Furthermore, using a theoretical
machinery from complex systems, this paper also posits that inconsistency may
be an emergent feature of consensus over distributed systems by examining how a
system transitions phases. Under the same complex systems framework, this paper
examines paraconsistent logics, arguing that while inconsistency is not an
emergent feature for these logics, triviality may be. Lastly, some attention is
given to the possibility of developing consensus algorithms capable of
paraconsistent reasoning.

</details>


### [290] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: FedDHAD框架通过动态异构模型聚合和自适应丢弃方法，解决了联邦学习中数据异构和设备性能差异的问题，显著提升了准确性、效率和计算成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在边缘设备上训练全局模型时，面临数据分布不均（非IID）和设备性能差异的挑战，导致准确性下降和收敛速度慢。

Method: 提出FedDHAD框架，包含动态异构模型聚合（FedDH）和自适应丢弃（FedAD）两种方法，分别处理数据异构和设备性能差异。

Result: FedDHAD在准确性（最高提升6.7%）、效率（最高快2.02倍）和计算成本（最高降低15.0%）上优于现有方案。

Conclusion: FedDHAD有效解决了联邦学习中的数据异构和设备性能问题，显著提升了模型性能。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [291] [Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning](https://arxiv.org/abs/2507.09094)
*Xiaoren Xu,Hao Xu,Dongyu Wei,Walid Saad,Mehdi Bennis,Mingzhe Chen*

Main category: cs.NI

TL;DR: 提出了一种基于流体天线系统（FAS）的无人机（UAV）三维定位框架，通过多智能体强化学习优化轨迹和天线端口选择，显著降低定位误差。


<details>
  <summary>Details</summary>
Motivation: 研究目标是为移动的目标无人机提供实时三维定位，解决传统方法在动态环境中的性能不足问题。

Method: 采用注意力机制的循环多智能体强化学习（AR-MARL），优化控制无人机的轨迹和天线端口选择，以提高定位精度。

Result: 仿真结果显示，AR-MARL方案比VD-MARL和无FAS的方法分别降低了17.5%和58.5%的平均定位误差。

Conclusion: 提出的AR-MARL框架在动态环境中显著提升了目标无人机的定位精度，为未来无人机定位技术提供了新思路。

Abstract: In this paper, a novel Three dimensional (3D) positioning framework of fluid
antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In
the proposed framework, a set of controlled UAVs cooperatively estimate the
real-time 3D position of a target UAV. Here, the active UAV transmits a
measurement signal to the passive UAVs via the reflection from the target UAV.
Each passive UAV estimates the distance of the active-target-passive UAV link
and selects an antenna port to share the distance information with the base
station (BS) that calculates the real-time position of the target UAV. As the
target UAV is moving due to its task operation, the controlled UAVs must
optimize their trajectories and select optimal antenna port, aiming to estimate
the real-time position of the target UAV. We formulate this problem as an
optimization problem to minimize the target UAV positioning error via
optimizing the trajectories of all controlled UAVs and antenna port selection
of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement
learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use
the local Q function to determine its trajectory and antenna port while
optimizing the target UAV positioning performance without knowing the
trajectories and antenna port selections of other controlled UAVs. Different
from current MARL methods, the proposed method uses a recurrent neural network
(RNN) that incorporates historical state-action pairs of each controlled UAV,
and an attention mechanism to analyze the importance of these historical
state-action pairs, thus improving the global Q function approximation accuracy
and the target UAV positioning accuracy. Simulation results show that the
proposed AR-MARL scheme can reduce the average positioning error by up to 17.5%
and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.

</details>


### [292] [Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks](https://arxiv.org/abs/2507.09124)
*Syed Danial Ali Shah,Maryam Hafeez,Abdelaziz Salama,Syed Ali Raza Zaidi*

Main category: cs.NI

TL;DR: 提出了一种基于O-RAN规范的CAORA框架，实现实时RAN与计算密集型AI工作负载的动态共存，并通过实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: AI-RAN融合的愿景需要探索架构框架和资源编排策略，以实现6G平台对AI和RAN工作负载的无缝支持。

Method: 设计了基于O-RAN的CAORA框架，利用NRT-RIC中的xApps监控RAN KPIs，并通过Y1接口与E2E编排器交互，结合SAC强化学习管理资源分配。

Result: 实验表明，CAORA在动态条件下实现了近99%的RAN需求满足率，并支持动态AI工作负载，提升了资源利用率和系统适应性。

Conclusion: CAORA为未来AI-RAN融合的6G系统提供了可行的蓝图，显著提升了资源效率和连续性。

Abstract: The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims
to unlock a unified 6G platform capable of seamlessly supporting AI and RAN
workloads over shared infrastructure. However, the architectural framework and
intelligent resource orchestration strategies necessary to realize this vision
remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN
Architectural (CAORA) framework based on O-RAN specifications, enabling the
dynamic coexistence of real-time RAN and computationally intensive AI
workloads. We design custom xApps within the Near-Real-Time RAN Intelligent
Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an
End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The
orchestrator incorporates workload forecasting and anomaly detection modules,
augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that
proactively manages resource allocation, including Multi-Instance GPU (MIG)
partitioning. Using real-world 5G traffic traces from Barcelona, our
trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment
of RAN demands, supports dynamic AI workloads, and maximizes infrastructure
utilization even under highly dynamic conditions. Our results reveal that
predictive orchestration significantly improves system adaptability, resource
efficiency, and service continuity, offering a viable blueprint for future
AI-and-RAN converged 6G systems.

</details>


### [293] [On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response](https://arxiv.org/abs/2507.09153)
*Bilal Karaman,Ilhan Baştürk,Ferdi Kara,Engin Zeydan,Esra Aycan Beyazıt,Sezai Taşkın,Emil Björnson,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 提出了一种基于高空平台站（HAPS）的需求驱动通信系统，用于在自然灾害中恢复通信，提升应急响应能力。


<details>
  <summary>Details</summary>
Motivation: 自然灾害常导致通信网络中断，现有解决方案在无线接入网（RAN）和回传基础设施同时失效时表现不足。

Method: 利用HAPS支持的混合光/太赫兹链路提升回传容量和韧性，并在S和Ka波段实现可靠通信。

Result: 模拟显示HAPS在恶劣条件下仍能提供高容量和韧性的通信支持。

Conclusion: HAPS可显著增强应急通信网络的韧性，支持高效的灾害管理。

Abstract: Natural disasters often disrupt communication networks and severely hamper
emergency response and disaster management. Existing solutions, such as
portable communication units and cloud-based network architectures, have
improved disaster resilience but fall short if both the Radio Access Network
(RAN) and backhaul infrastructure become inoperable. To address these
challenges, we propose a demand-driven communication system supported by High
Altitude Platform Stations (HAPS) to restore communication in an affected area
and enable effective disaster relief. The proposed emergency response network
is a promising solution as it provides a rapidly deployable, resilient
communications infrastructure. The proposed HAPS-based communication can play a
crucial role not only in ensuring connectivity for mobile users but also in
restoring backhaul connections when terrestrial networks fail. As a bridge
between the disaster management center and the affected areas, it can
facilitate the exchange of information in real time, collect data from the
affected regions, and relay crucial updates to emergency responders. Enhancing
situational awareness, coordination between relief agencies, and ensuring
efficient resource allocation can significantly strengthen disaster response
capabilities. In this paper, simulations show that HAPS with hybrid optical/THz
links boosts backhaul capacity and resilience, even in harsh conditions.
HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first
responders and disaster-affected populations. This paper also explores the
integration of HAPS into emergency communication frameworks and standards, as
it has the potential to improve network resilience and support effective
disaster management.

</details>


### [294] [Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications](https://arxiv.org/abs/2507.09270)
*Songhan Zhao,Yusi Long,Lanhua Li,Bo Gu,Shimin Gong,Zehui Xiong*

Main category: cs.NI

TL;DR: 论文提出了一种语义感知的可重构智能表面（RIS）辅助无线网络，通过联合优化语义用户（SUs）的解码顺序、语义控制和RIS的被动波束成形策略，最小化系统能耗。


<details>
  <summary>Details</summary>
Motivation: 在非正交多址（NOMA）网络中，通过语义提取因子和RIS的被动波束成形，灵活满足用户需求并优化能耗。

Method: 将原问题分解为两个子问题，采用近似方法求解。

Result: 数值结果表明，联合流量重塑和信道重构方案显著提升了NOMA传输的节能性能。

Conclusion: 该方法通过语义和信道联合优化，有效降低了系统能耗。

Abstract: In this paper, we consider a semantic-aware reconfigurable intelligent
surface (RIS)-assisted wireless network, where multiple semantic users (SUs)
simultaneously transmit semantic information to an access point (AP) by using
the non-orthogonal multiple access (NOMA) method. The SUs can reshape their
traffic demands by modifying the semantic extraction factor, while the RIS can
reconfigure the channel conditions via the passive beamforming. This provides
the AP with greater flexibility to decode the superimposed signals from the
SUs. We aim to minimize the system's overall energy consumption, while ensuring
that each SU's traffic demand is satisfied. Hence, we formulate a joint
optimization problem of the SUs' decoding order and semantic control, as well
as the RIS's passive beamforming strategy. This problem is intractable due to
the complicated coupling in constraints. To solve this, we decompose the
original problem into two subproblems and solve them by using a series of
approximate methods. Numerical results show that the joint traffic reshaping
and channel reconfiguration scheme significantly improves the energy saving
performance of the NOMA transmissions compared to the benchmark methods.

</details>


### [295] [Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks](https://arxiv.org/abs/2507.09341)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 本文探讨了车辆移动边缘计算（VEC）中的任务卸载问题，通过静态和动态优化方法（PSO、DQN、PPO）减少任务丢弃和延迟，结果显示DQN表现最佳。


<details>
  <summary>Details</summary>
Motivation: VEC虽能实现低延迟数据处理，但车辆在RSU覆盖范围内停留时间短，导致任务卸载面临严格时间限制的挑战。

Method: 首先在静态环境中用PSO建立理论极限，后在动态场景中实施PSO、DQN和PPO模型，以最小化任务丢弃和端到端延迟。

Result: DQN模型显著优于动态PSO，执行时间减少99.2%，任务丢弃减少2.5%，端到端延迟降低18.6%。

Conclusion: 深度强化学习（DRL）在VEC系统中展现出高效、可扩展的任务管理潜力。

Abstract: Vehicular Mobile Edge Computing (VEC) drives the future by enabling
low-latency, high-efficiency data processing at the very edge of vehicular
networks. This drives innovation in key areas such as autonomous driving,
intelligent transportation systems, and real-time analytics. Despite its
potential, VEC faces significant challenges, particularly in adhering to strict
task offloading deadlines, as vehicles remain within the coverage area of
Roadside Units (RSUs) for only brief periods. To tackle this challenge, this
paper evaluates the performance boundaries of task processing by initially
establishing a theoretical limit using Particle Swarm Optimization (PSO) in a
static environment. To address more dynamic and practical scenarios, PSO, Deep
Q-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented
in an online setting. The objective is to minimize dropped tasks and reduce
end-to-end (E2E) latency, covering both communication and computation delays.
Experimental results demonstrate that the DQN model considerably surpasses the
dynamic PSO approach, achieving a 99.2% reduction in execution time.
Furthermore, It leads to a reduction in dropped tasks by 2.5% relative to
dynamic PSO and achieves 18.6\% lower E2E latency, highlighting the
effectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and
efficient task management for VEC systems.

</details>


### [296] [Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks](https://arxiv.org/abs/2507.09346)
*Arild Yonkeu,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 提出了一种基于指针网络的动态边缘计算任务调度方法，显著降低了延迟和任务丢弃率，同时提高了实时性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决传统任务调度方法在动态、时间敏感环境中的高计算开销问题，以及现有深度学习方法的可扩展性和适应性不足。

Method: 使用指针网络架构，通过遗传算法生成的合成数据集训练模型，优化任务排序。

Result: 模型在实验中表现出较低的丢弃率和等待时间，软序列准确率达89.2%，推理时间低于2秒，优于传统方法。

Conclusion: 该方法为边缘计算任务管理提供了高效、可扩展且适应性强的解决方案。

Abstract: Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for
meeting the low-latency demands of modern IoT and dynamic task scheduling
scenarios. MEC reduces the processing burden on resource-constrained devices by
enabling task execution at nearby edge servers. However, efficient task
scheduling remains a challenge in dynamic, time-sensitive environments.
Conventional methods -- such as heuristic algorithms and mixed-integer
programming -- suffer from high computational overhead, limiting their
real-time applicability. Existing deep learning (DL) approaches offer faster
inference but often lack scalability and adaptability to dynamic workloads. To
address these issues, we propose a Pointer Network-based architecture for task
scheduling in dynamic edge computing scenarios. Our model is trained on a
generated synthetic dataset using genetic algorithms to determine the optimal
task ordering. Experimental results show that our model achieves lower drop
ratios and waiting times than baseline methods, and a soft sequence accuracy of
up to 89.2%. Our model consistently achieves inference times under 2 seconds
across all evaluated task counts, whereas the integer and binary programming
approaches require approximately up to 18 seconds and 90 seconds, respectively.
It also shows strong generalization across varying scenarios, and adaptability
to real-time changes, offering a scalable and efficient solution for edge-based
task management.

</details>


### [297] [Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling](https://arxiv.org/abs/2507.09352)
*Ghazal Asemian,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 论文提出了一种动态MEC框架，结合传输多样性解决任务调度和资源分配问题，有效降低任务丢弃率并提升资源利用率。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）虽能降低延迟，但动态任务到达和通信干扰（如干扰）增加了可靠任务卸载和资源分配的复杂性。

Method: 提出了一种基于传输多样性的干扰感知卸载和资源块分配框架，通过分布式gNBs优化调度。

Result: 在4 dB的SJNR下，任务丢弃率为0.26，优于无传输多样性（0.50）及FCFS（0.63）和STF（0.52）策略。

Conclusion: 该方法显著减轻了干扰影响，适用于关键任务MEC应用。

Abstract: Mobile Edge Computing (MEC) enables low-latency applications by bringing
computation closer to the user, but dynamic task arrivals and communication
threats like jamming complicate reliable task offloading and resource
allocation. In this paper, we formulate a dynamic MEC framework considering the
transmission diversity that jointly addresses task scheduling and resource
block (RB) assignment in the presence of jamming. First, we define and evaluate
key network metrics-including dropped task ratio and bandwidth
utilization-while maintaining service continuity by accounting for the existing
commitments of the edge server to previously offloaded tasks. Then, we propose
a jamming-aware offloading and RB allocation framework that leverages
transmission diversity and optimal scheduling across distributed gNBs. The
proposed solution is compared to a similar scenario without transmission
diversity and two baseline strategies of first-come-first-served (FCFS) and
shortest task first (STF). The proposed algorithm effectively mitigates the
impact of jamming while enhancing resource utilization and minimizing task drop
rates, making it highly suitable for mission-critical MEC applications. At
signal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves
a $0.26$ task drop rate, outperforming the scenario without transmission
diversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52
and 0.63 task drop rates, respectively.

</details>


### [298] [MobiWorld: World Models for Mobile Wireless Network](https://arxiv.org/abs/2507.09462)
*Haoye Chai,Yuan Yuan,Yong Li*

Main category: cs.NI

TL;DR: MobiWorld是一个基于扩散模型的高保真移动网络仿真模型，通过整合多源异构数据支持网络规划和优化任务。


<details>
  <summary>Details</summary>
Motivation: 移动网络的精确建模和仿真对智能化和经济高效的网络优化至关重要。

Method: MobiWorld整合传感器、移动设备和基站等多源数据，利用扩散模型生成网络元素级和系统级性能指标。

Result: 实验证明MobiWorld在可控生成和能源优化方面优于传统方法。

Conclusion: MobiWorld为移动网络规划和优化提供了高效、精确的仿真环境。

Abstract: Accurate modeling and simulation of mobile networks are essential for
enabling intelligent and cost-effective network optimization. In this paper, we
propose MobiWorld, a generative world model designed to support high-fidelity
and flexible environment simulation for mobile network planning and
optimization. Unlike traditional predictive models constrained by limited
generalization capabilities, MobiWorld exhibits strong universality by
integrating heterogeneous data sources, including sensors, mobile devices, and
base stations, as well as multimodal data types such as sequences and images.
It is capable of generating both network element-level observations (e.g.,
traffic load, user distribution) and system-level performance indicators (e.g.,
throughput, energy consumption) to support a wide range of planning and
optimization tasks. Built upon advanced diffusion models, MobiWorld offers
powerful controllable generation capabilities by modeling the joint
distribution between mobile network data and diverse conditional factors
including spatio temporal contexts, user behaviors, and optimization policies.
This enables accurate simulation of dynamic network states under varying policy
configurations, providing optimization agents with precise environmental
feedback and facilitating effective decision-making without relying on costly
real-network interactions. We demonstrate the effectiveness of MobiWorld in a
collaborative energy-saving scenario, where an agent uses observations and
rewards generated by MobiWorld to optimize base station sleep and user
offloading policies. Experimental results show that MobiWorld exhibits strong
controllable generation performance and outperforms traditional methods in
energy optimization.

</details>


### [299] [Wi-Fi: Twenty-Five Years and Counting](https://arxiv.org/abs/2507.09613)
*Giovanni Geraci,Francesca Meneghello,Francesc Wilhelmi,David Lopez-Perez,Iñaki Val,Lorenzo Galati Giordano,Carlos Cordeiro,Monisha Ghosh,Edward Knightly,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文全面回顾了Wi-Fi从802.11b（Wi-Fi 1）到802.11bn（Wi-Fi 8）的技术演进，重点介绍了频谱分配、物理层、MAC层、多用户接入、节能机制、频谱聚合和AP协调等关键机制。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi技术在过去25年经历了巨大变革，本文旨在提供一个跨越八代Wi-Fi的技术和历史教程，填补现有文献的空白。

Method: 通过分析Wi-Fi的频谱分配、物理层、MAC层、多用户接入、节能机制、频谱聚合和AP协调等关键机制，全面梳理其技术演进。

Result: Wi-Fi的数据速率提升了1000倍以上，支持多用户接入、节能优化、频谱聚合和AP协调等新功能。

Conclusion: Wi-Fi技术持续创新，未来可能集成毫米波、传感、安全隐私扩展和AI/ML等新方向。

Abstract: Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding
name, today's Wi-Fi boasts entirely new capabilities that were not even on the
roadmap 25 years ago. This article aims to provide a holistic and comprehensive
technical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi
1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial
article to span these eight generations. Rather than a generation-by-generation
exposition, we describe the key mechanisms that have advanced Wi-Fi. We begin
by discussing spectrum allocation and coexistence, and detailing the IEEE
802.11 standardization cycle. Second, we provide an overview of the physical
layer and describe key elements that have enabled data rates to increase by
over 1,000x. Third, we describe how Wi-Fi Medium Access Control has been
enhanced from the original Distributed Coordination Function to now include
capabilities spanning from frame aggregation to wideband spectrum access.
Fourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and
introduced multi-user access. Fifth, given the increasing use of mobile,
battery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the
generations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate
spectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput,
reliability, and latency. Finally, we describe how Wi-Fi enables nearby Access
Points to coordinate in order to improve performance and efficiency. In the
Appendix, we further discuss Wi-Fi developments beyond 802.11bn, including
integrated mmWave operations, sensing, security and privacy extensions, and the
adoption of AI/ML.

</details>


### [300] [Towards Robust RTC in Sparse LEO Constellations](https://arxiv.org/abs/2507.09798)
*Aashish Gottipati,Lili Qiu*

Main category: cs.NI

TL;DR: 论文研究了稀疏低地球轨道（LEO）网络中视频会议系统的性能问题，提出了一种数据驱动的队列管理机制，显著提升了实时通信（RTC）的质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏LEO网络中的高延迟和网络不稳定性导致视频质量下降，传统WebRTC的队列管理机制无法适应这种环境。

Method: 引入了一种动态调整最大队列容量的机制，根据预测的切换活动调整队列大小，以优化延迟和稳定性。

Result: 相比默认WebRTC，视频比特率提升了3倍，冻结率降低了62%。

Conclusion: 数据驱动的队列管理机制能有效提升稀疏LEO网络中的实时通信性能。

Abstract: Google's congestion control (GCC) has become a cornerstone for real-time
video and audio communication, yet its performance remains fragile in emerging
Low Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer
longer duration links and reduced handover frequency compared to dense
deployments, presenting a unique opportunity for high-quality real-time
communication (RTC) in environments with limited terrestrial network
infrastructure. In this paper, we study the behavior of videoconferencing
systems in sparse LEO constellations. We observe that video quality degrades
due to inherent delays and network instability introduced by the high altitude
and rapid movement of LEO satellites, with these effects exacerbated by
WebRTC's conventional ``one-size-fits-all'' sender-side pacing queue
management. To boost RTC performance, we introduce a data-driven queue
management mechanism that adapts the maximum pacing queue capacity based on
predicted handover activity. Specifically, our approach employs shorter queue
limits during stable, no-handover phases to prioritize low latency
communication, and preemptively increases pacing queue capacity when entering
periods of increased handover activity to absorb disruptions. Our method yields
up to $3$x improvements in video bitrate and reduces freeze rate by $62\%$
compared to default WebRTC.

</details>


### [301] [UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks](https://arxiv.org/abs/2507.09852)
*Zihao Zhou,Zipeng Dai,Linyi Huang,Cui Yang,Youjun Xiang,Jie Tang,Kai-kit Wong*

Main category: cs.NI

TL;DR: 介绍了一个名为UavNetSim-v1的开源Python仿真平台，用于无人机网络的协议和算法开发与测试。


<details>
  <summary>Details</summary>
Motivation: 无人机网络中的通信协议和算法需要高效开发与测试工具，而实地实验成本过高，仿真成为经济有效的替代方案。

Method: 开发了UavNetSim-v1平台，提供路由/MAC协议、拓扑控制算法、移动/能量模型等功能，并支持性能评估和可视化分析。

Result: UavNetSim-v1是一个轻量级但功能强大的仿真平台，适用于快速原型设计和教育用途。

Conclusion: UavNetSim-v1为无人机通信研究提供了一个高效、易用的仿真工具，可作为成熟网络仿真器的替代方案。

Abstract: In unmanned aerial vehicle (UAV) networks, communication protocols and
algorithms are essential for cooperation and collaboration between UAVs.
Simulation provides a cost-effective solution for prototyping, debugging, and
analyzing protocols and algorithms, avoiding the prohibitive expenses of field
experiments. In this paper, we present ``UavNetSim-v1'', an open-source
Python-based simulation platform designed for rapid development, testing, and
evaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1''
provides most of the functionalities developers may need, including
routing/medium access control (MAC) protocols, topology control algorithms and
mobility/energy models, while maintaining ease of use. Furthermore, the
platform supports comprehensive performance evaluation and features an
interactive visualization interface for in-depth algorithm analysis. In short,
``UavNetSim-v1'' lends itself to both rapid prototyping and educational
purposes, and can serve as a lightweight yet powerful alternative to mature
network simulators for UAV communication research.

</details>


### [302] [Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference](https://arxiv.org/abs/2507.09942)
*Jiaming Cheng,Duong Tung Nguyen*

Main category: cs.NI

TL;DR: 研究如何在异构边缘数据中心优化分配大型语言模型推理任务，以降低能耗、碳排放和水资源使用，同时提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决动态电价和可再生能源时空变化下，如何高效分配推理任务以减少运营成本和环境影响的问题。

Method: 提出一种新颖的优化模型，用于任务分配。

Result: 数值结果验证了该方法的有效性。

Conclusion: 该方法能显著降低运营成本和环境影响，同时优化用户体验。

Abstract: This letter investigates the optimal allocation of large language model (LLM)
inference workloads across heterogeneous edge data centers (DCs) over time.
Each DC features on-site renewable generation and faces dynamic electricity
prices and spatiotemporal variability in renewable availability. The central
question is: how can inference workloads be optimally distributed to the DCs to
minimize energy consumption, carbon emissions, and water usage while enhancing
user experience? This letter proposes a novel optimization model for LLM
service providers to reduce operational costs and environmental impacts.
Numerical results validate the efficacy of the proposed approach.

</details>


### [303] [Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit](https://arxiv.org/abs/2507.10210)
*Thijs Havinga,Xianjun Jiao,Wei Liu,Baiheng Chen,Robbe Gaeremynck,Ingrid Moerman*

Main category: cs.NI

TL;DR: 论文提出了一种基于有线回程的细粒度Co-OFDMA实现方法，解决了Wi-Fi 8中多AP协调的资源分配问题，并展示了优于无线标准的性能。


<details>
  <summary>Details</summary>
Motivation: 密集部署中无线网络的性能需要协调，但传统竞争机制在高负载下延迟高，Co-OFDMA虽能提高效率，但细粒度资源分配存在调度复杂性问题。

Method: 利用光纤回程和开源平台openwifi与White Rabbit，实现了Wi-Fi 6兼容的细粒度Co-OFDMA，避免了空中调度开销。

Result: 实验显示，两个AP间的载波频率偏移预补偿和时间同步性能超过无线标准要求，且Co-OFDMA帧的接收质量优于单独AP发送的帧。

Conclusion: 有线回程支持的细粒度Co-OFDMA是可行的，为未来多AP协调技术（如波束成形和联合传输）奠定了基础。

Abstract: Proper coordination is needed to guarantee the performance of wireless
networks in dense deployments. Contention-based systems suffer badly in terms
of latency when multiple devices compete for the same resources. Coordinated
Orthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi
8 to remedy this, as it enables multiple Access Points (APs) to share spectrum
more efficiently. However, fine-grained resource allocation, namely within
20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling
overhead and complexity in terms of physical layer signaling. A wired backhaul
mitigates the need for over-the-air scheduling and synchronization, and it
allows for coordination even if APs are not in each others' range. Furthermore,
it forms the basis for more advanced multi-AP coordination schemes like
coordinated beamforming and joint transmission. In this work we demonstrate the
realization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,
enabled by the open-source platforms openwifi and White Rabbit. We show that
the performance in terms of carrier frequency offset pre-compensation and time
synchronization between two APs exceeds related wireless standard requirements.
Furthermore, the quality of the received constellation of the Co-OFDMA frame as
reported by a wireless connectivity tester is better than individual frames
sent by the APs.

</details>


### [304] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: AI Video Chat introduces a new RTC paradigm with MLLMs, addressing latency challenges through Artic, Context-Aware Video Streaming, and Loss-Resilient Adaptive Frame Rate, while evaluating impact via DeViBench.


<details>
  <summary>Details</summary>
Motivation: To make human-AI interaction as intuitive as face-to-face chat, despite latency challenges from MLLM inference and network instability.

Method: Proposes Artic framework, Context-Aware Video Streaming for bitrate reduction, and Loss-Resilient Adaptive Frame Rate to handle packet loss.

Result: Develops DeViBench to assess video streaming quality's impact on MLLM accuracy.

Conclusion: AI Video Chat's challenges are addressed through novel methods, with ongoing solutions for further improvement.

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [305] [Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams](https://arxiv.org/abs/2507.09539)
*Anna Bolotina,Christoph M. Kirsch,Stefanie Muroya Lei,Matthias Pleschinger*

Main category: cs.PL

TL;DR: 论文探讨了符号执行在机器代码层面的可扩展性，提出了两种工具rotor和bitme，分别用于模型生成和有界模型检查，通过BDDs优化SMT求解效率。


<details>
  <summary>Details</summary>
Motivation: 解决符号执行在控制流和数据流中的状态爆炸问题，并探索在机器代码层面实现高效推理的可能性。

Method: 开发了rotor和bitme工具，利用BDDs（ADDs和CFLOBDDs）优化SMT求解，减少状态爆炸。

Result: 实验表明，尽管现有SMT求解器表现不佳，但BDDs（尤其是CFLOBDDs）能显著提升效率。

Conclusion: 在机器代码层面实现高效的符号执行是可行的，BDDs技术为优化SMT求解提供了新方向。

Abstract: Symbolic execution is a powerful technique for analyzing the behavior of
software yet scalability remains a challenge due to state explosion in control
and data flow. Existing tools typically aim at managing control flow
internally, often at the expense of completeness, while offloading reasoning
over data flow to SMT solvers. Moreover, reasoning typically happens on source
code or intermediate representation level to leverage structural information,
making machine code generation part of the trust base. We are interested in
changing the equation in two non-trivial ways: pushing reasoning down to
machine code level, and then offloading reasoning entirely into SMT solvers and
other, possibly more efficient solver technology. In more abstract terms, we
are asking if bit-precise reasoning technology can be made scalable on
software, and not just hardware. For this purpose, we developed two tools
called rotor and bitme for model generation and bounded model checking,
respectively. We chose RISC-V restricted to integer arithmetic as modeling
target for rotor since RISC-V integer semantics is essentially equivalent to
established SMT semantics over bitvectors and arrays of bitvectors. While
state-of-the-art SMT solvers struggle in our experiments, we have evidence that
there is potential for improvement. To show the potential, we have slightly
generalized and then implemented in bitme two types of binary decision diagrams
(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered
binary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input
through models, essentially generalizing constant propagation to domain
propagation. SMT solvers only get involved when model input cannot be
propagated, significanly speeding up SMT solving. We then study the impact on
state explosion of CFLOBDDs, which are potentially more scalable than ADDs.

</details>


### [306] [BeePL: Correct-by-compilation kernel extensions](https://arxiv.org/abs/2507.09883)
*Swarn Priya,Frédéric Besson,Connor Sughrue,Tim Steenvoorden,Jamie Fulford,Freek Verbeek,Binoy Ravindran*

Main category: cs.PL

TL;DR: BeePL是一种针对eBPF的领域特定语言，通过形式化验证的类型系统确保程序安全，解决了现有eBPF验证器的保守性和不健全问题。


<details>
  <summary>Details</summary>
Motivation: eBPF验证器在安全性和正确性上存在不足，既可能拒绝有效程序，也可能允许不安全行为。

Method: 设计BeePL语言及其形式化验证的类型系统，静态检查类型正确性、内存安全和控制流等，并在编译时插入运行时检查。

Result: BeePL确保类型安全的程序满足eBPF执行环境的安全要求，并通过形式化证明支持其正确性。

Conclusion: BeePL为安全内核扩展提供了一个端到端可验证的工具链基础。

Abstract: eBPF is a technology that allows developers to safely extend kernel
functionality without modifying kernel source code or developing loadable
kernel modules. Since the kernel governs critical system operations and
enforces isolation boundaries between user space and privileged data, any
mechanism that modifies its behavior must meet the highest standards of safety
and correctness. To this end, the eBPF toolchain includes a verifier, which
statically checks safety properties such as memory access validity, bounded
loops, and type correctness before loading the program into the kernel.
However, the existing verifier is both overly conservative in some
cases-rejecting valid programs-and unsound in others, permitting unsafe
behavior that violates the intended semantics of the kernel interface.
  To address these challenges, we introduce BeePL, a domain-specific language
for eBPF with a formally verified type system. The BeePL type system, along
with the language design, statically enforces key safety properties such as
type-correct memory access, safe pointer usage, absence of unbounded loops, and
structured control flow. These guarantees are backed by formal type soundness
proofs, ensuring that well-typed programs satisfy the safety invariants
required by the eBPF execution environment. BeePL also proves that well-typed
source programs meet critical eBPF-specific properties related to memory
safety, termination, and control flow, enabling high-level reasoning prior to
compilation. For properties not fully enforceable statically-such as dynamic
bounds and undefined behavior-BeePL inserts semantics-preserving runtime checks
during compilation. We develop a verified compilation strategy that extends
CompCert to generate BPF bytecode from BeePL programs, establishing a
principled foundation for an end-to-end verifiable toolchain for safe kernel
extensions.

</details>


### [307] [Rows and Capabilities as Modal Effects](https://arxiv.org/abs/2507.10301)
*Wenhao Tang,Sam Lindley*

Main category: cs.PL

TL;DR: 提出了一种统一的框架，用于编码、分析和比较基于行多态和能力的效果系统，通过模态效果类型解耦效果跟踪与函数。


<details>
  <summary>Details</summary>
Motivation: 理解基于行多态和能力的效果系统之间的精确关系存在困难，因为效果跟踪通常与其他特性（如函数）纠缠在一起。

Method: 利用模态效果类型构建统一框架，通过宏翻译将现有系统编码到框架中，并验证类型和语义的保持。

Result: 编码揭示了不同效果系统中效果跟踪机制的本质，支持直接分析差异，并为语言设计提供见解。

Conclusion: 该框架为效果系统的比较和分析提供了新视角，有助于语言设计的优化。

Abstract: Effect handlers allow programmers to model and compose computational effects
modularly. Effect systems statically guarantee that all effects are handled.
Several recent practical effect systems are based on either row polymorphism or
capabilities. However, there remains a gap in understanding the precise
relationship between effect systems with such disparate foundations. The main
difficulty is that in both row-based and capability-based systems, effect
tracking is typically entangled with other features such as functions.
  We propose a uniform framework for encoding, analysing, and comparing effect
systems. Our framework exploits and generalises modal effect types, a recent
novel effect system which decouples effect tracking from functions via
modalities. Modalities offer fine-grained control over when and how effects are
tracked, enabling us to express different strategies for effect tracking. We
give encodings as macro translations from existing row-based and
capability-based effect systems into our framework and show that these
encodings preserve types and semantics. Our encodings reveal the essence of
effect tracking mechanisms in different effect systems, enable a direct
analysis on their differences, and provide valuable insights on language
design.

</details>


### [308] [Orthologic Type Systems](https://arxiv.org/abs/2507.10482)
*Simon Guilloud,Viktor Kunčak*

Main category: cs.PL

TL;DR: 论文提出基于正交逻辑设计支持交、并、否定类型及子类型假设的类型系统，扩展逻辑以支持单调和反单调函数，并给出部分消解证明系统。最终提出O(n²(1+m))的子类型判定算法和O(n²)的规范化算法。


<details>
  <summary>Details</summary>
Motivation: 为支持复杂类型操作（如交、并、否定）及子类型假设，需设计高效的类型系统，正交逻辑为此提供了理论基础。

Method: 扩展正交逻辑以支持单调和反单调函数，构建包含函数符号的证明系统，并证明其部分消解性。

Result: 提出O(n²(1+m))的子类型判定算法和O(n²)的类型规范化算法。

Conclusion: 正交逻辑为复杂类型系统设计提供了有效框架，支持高效算法实现。

Abstract: We propose to use orthologic as the basis for designing type systems
supporting intersection, union, and negation types in the presence of subtyping
assumptions. We show how to extend orthologic to support monotonic and
antimonotonic functions, supporting the use of type constructors in such type
systems. We present a proof system for orthologic with function symbols,
showing that it admits partial cut elimination. Using these insights, we
present an $\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation
under $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization
algorithm, allowing simplification of types to their minimal canonical form.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [309] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: 论文研究了Git工作流（分支式和主干式）在巴西开发者中的使用情况，发现主干式适合快节奏、经验丰富的小团队，而分支式更适合经验较少的大团队。


<details>
  <summary>Details</summary>
Motivation: 探讨如何选择Git工作流以最大化团队生产力和软件质量，填补科学文献在此领域的空白。

Method: 通过半结构化访谈和问卷调查收集巴西开发者的数据。

Result: 主干式开发适合快节奏、经验丰富的小团队；分支式开发更适合经验较少的大团队，但管理挑战更大。

Conclusion: 团队应根据项目节奏、规模和经验选择合适的工作流。

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [310] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 本文提出了一种自动化方法，用于研究R代码的分割，使用大型和小型语言模型（LLMs/SLMs），并比较了两种方法：基于行的分析和基于范围的分割。结果表明，基于行的方法更优，且小型模型表现优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 随着代码库的增长，尤其是低资源语言（如R）的研究领域，手动和语法分析方法变得不切实际，需要自动化解决方案。

Method: 提出了两种方法：基于行的分析和基于范围的分割，并实验了LLMs和微调的SLMs。还使用了Python代码验证方法的通用性。

Result: 基于行的分析优于基于范围的分割，且小型模型（如CodeBERT和CodeT5+）表现优于LLMs，尽管它们未在R代码上预训练。

Conclusion: 小型语言模型在有限数据下的微调效果优于大型模型，为低资源语言代码分割提供了高效解决方案。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [311] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: 本文介绍了一种名为Tippy的新型AI框架，通过多智能体系统自动化药物发现中的DMTA循环，显著提升效率和决策速度。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法难以满足现代治疗开发需求，需要AI驱动的自动化解决方案。

Method: Tippy框架包含五个专门设计的AI智能体（Supervisor、Molecule、Lab、Analysis、Report）和Safety Guardrail监督，协同完成DMTA循环。

Result: Tippy显著提高了工作流效率、决策速度和跨学科协作，为AI辅助药物发现提供了新范式。

Conclusion: Tippy是首个生产就绪的AI智能体系统，展示了AI如何通过自动化DMTA循环改变实验室工作流程。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [312] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 论文提出了一种基于序列到序列（Seq2seq）生成方法的命名实体识别（NER）任务框架，用于从移动应用评论中提取需求，解决了现有方法在处理非正式语言和无关信息时的不足。


<details>
  <summary>Details</summary>
Motivation: 移动应用评论是改进软件的重要数据源，但现有方法因评论中的非正式语言、语法错误和无关信息而效果不佳。

Method: 提出了一种结合BiLSTM编码器、LSTM解码器、自注意力机制、GloVe嵌入和CRF模型的Seq2seq框架。

Result: 在两个数据集上评估，F1分数分别为0.96（Dataset 2）和0.47（Dataset 1），优于现有方法。

Conclusion: 该框架能有效从非结构化评论中提取需求，支持软件改进。

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [313] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: CMER结合NLI和LLM，高效提取应用评论中的伦理问题，如隐私和安全。


<details>
  <summary>Details</summary>
Motivation: 移动应用伦理问题日益突出，但相关评论因语言专业且被通用反馈掩盖，难以自动化提取。

Method: 提出CMER方法，结合NLI提供领域上下文，LLM无需标注数据分类。

Result: 从38.2万条评论中提取2178条被忽视的隐私安全相关评论，验证CMER有效性。

Conclusion: CMER能高效提取伦理问题评论，为需求工程提供可操作依据。

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [314] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 该论文提出了SAGE方法，利用自然语言推理和GPT模型自动从心理健康应用评论中挖掘隐私相关评论，无需微调即可达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 心理健康应用收集敏感数据引发隐私担忧，但隐私评论常被其他反馈类别掩盖，难以自动识别。

Method: 结合自然语言推理（NLI）和GPT模型，利用领域特定的隐私假设，自动挖掘隐私评论。

Result: SAGE在204K评论数据集上F1得分0.85，优于微调的BERT和T5，并发现748条被关键词方法忽略的隐私评论。

Conclusion: SAGE能有效识别隐私评论，为后续提炼隐私需求提供支持。

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [315] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: SetupBench是一个93实例的基准测试，用于评估LLM代理在裸Linux沙盒中完成环境初始化任务的能力，发现当前代理在依赖解决、数据库配置等方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要在预装环境中评估LLM代理，无法反映真实世界任务的复杂性，因此需要一种新的基准测试来评估环境初始化能力。

Method: 通过SetupBench基准测试，覆盖七种语言生态系统、五种数据库引擎和多服务编排场景，评估代理在裸Linux沙盒中的表现。

Result: OpenHands代理在仓库设置（38.9-57.4%）和本地数据库配置（20.0-53.3%）任务中成功率低，存在工具安装不完整、幻觉任务约束等问题。

Conclusion: SetupBench揭示了当前代理在环境初始化能力上的不足，为下一代开发者代理提供了严格的评估标准。

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [316] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SPICE是一个自动化标注软件工程数据集的工具，显著降低了标注成本并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据集对训练和评估基础模型至关重要，但手动标注成本高昂且耗时。

Method: SPICE结合上下文感知代码导航、基于理由的提示和多轮共识，生成接近专家标注的标签。

Result: SPICE在降低标注成本（从10万美元降至5.10美元）的同时，保持了与人工标注的高一致性。

Conclusion: SPICE为软件工程领域提供了高效、低成本的大规模数据集创建方案，并发布了工具和数据集支持社区。

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [317] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: 论文主张通过编程语言（PL）技术弥补大语言模型（LLM）在代码生成中的语义鸿沟，提升代码的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中表现出色，但其统计性和黑盒特性导致语法错误、语义幻觉和可靠性问题，需要PL技术填补这些缺陷。

Method: 提出通过结构化程序表示、形式化正确性保证和鲁棒验证机制，将PL技术与LLM结合。

Result: PL技术能将LLM生成的代码从统计模式匹配提升到可靠可信的水平。

Conclusion: PL技术与LLM的整合对生成功能正确、可解释、可验证且可信的代码至关重要。

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [318] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: OpenCAMS是一个开源、同步且可扩展的联合仿真平台，整合了SUMO、CARLA和OMNeT++，用于交通、感知和通信领域的研究。


<details>
  <summary>Details</summary>
Motivation: 支持交通安全性、移动性和网络安全的高级研究，结合各仿真工具的优势。

Method: 采用时间同步的双向耦合架构，整合SUMO（交通建模）、CARLA（3D感知与控制）和OMNeT++（网络通信）。

Result: 提供了一个模块化、可扩展且可复现的仿真平台，支持C-V2X通信等应用。

Conclusion: OpenCAMS为智能交通系统的研究提供了灵活、协作的开源环境。

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [319] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: 论文提出了Realistic Distribution Setting (RDS)评估方法，发现现有技术在真实场景下性能显著下降，并提出了基于向量数据库和大语言模型的EasyLink方法，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有问题-提交链接评估方法忽略了真实场景中大量无关提交的干扰，导致性能评估不准确。

Method: 提出RDS评估数据集，并设计EasyLink方法，结合向量数据库和大语言模型进行重新排序。

Result: 在RDS数据集上，现有深度学习方法的性能下降超过一半，而EasyLink的Precision@1达到75.91%，提升显著。

Conclusion: EasyLink在真实场景下表现优异，为问题-提交链接研究提供了实用指导。

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [320] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: 论文探讨了AI工具在需求工程中生成设计工件时的可解释性不足问题，通过访谈揭示了其局限性，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具在受监管行业中应用受限的原因，特别是透明度和可追溯性不足的问题。

Method: 通过半结构化访谈，调查了十位来自安全关键行业的从业者，分析AI工具的集成与挑战。

Result: 发现不可解释的AI输出导致手动验证增加、信任降低、术语处理困难、团队协作受阻及合规风险。

Conclusion: 提出改进建议，如来源追踪、决策解释、领域适应和合规验证，以提升AI工具的透明度和可靠性。

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [321] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: SCELM是一个端到端的自动化框架，用于高效管理软件变更，减少服务故障和经济损失。


<details>
  <summary>Details</summary>
Motivation: 现代在线服务中频繁的软件变更带来重大风险，需要有效管理。

Method: 提出SCELM框架，实现软件变更的自动化评估和生命周期管理。

Result: 显著减少服务故障和经济损失。

Conclusion: SCELM能高效且精确地管理软件变更，提升服务稳定性。

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [322] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: 论文提出了一种基于分支覆盖的适应度函数，用于增强游戏自动化测试的效果，并通过实验验证了其优于基于语句覆盖的方法。


<details>
  <summary>Details</summary>
Motivation: 游戏程序的非确定性和复杂控制结构使得自动化测试生成具有挑战性，仅依赖语句覆盖难以检测所有逻辑分支的故障。

Method: 扩展了NEATEST框架，引入分支覆盖适应度函数，优先处理控制依赖分支，以最大化分支探索。

Result: 在25个Scratch游戏上的实验表明，基于分支覆盖的方法（NBC）在13个游戏中实现了更高的分支覆盖率，且突变测试中假阳性率更低。

Conclusion: 基于分支覆盖的测试生成能显著提高Scratch程序的测试覆盖率和故障检测能力。

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [323] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: StateGen是一个自动化框架，用于生成涉及顺序API交互的多样化编码任务，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖手动收集的测试用例，无法自动检查语义正确性，且忽略了顺序API调用的复杂交互。

Method: StateGen结合状态机约束求解、能量采样和控制流注入生成可执行程序，并通过两个LLM代理将其转化为自然语言任务描述。

Result: 构建了StateEval基准，包含120个已验证测试用例，覆盖三个典型场景，实验证明StateGen能有效生成具有挑战性的API任务。

Conclusion: StateGen为LLM工具使用的测试和评估提供了新方法，揭示了当前LLM在API集成中的改进空间。

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [324] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的自动化游戏测试方法Lap，利用ChatGPT测试三消游戏，通过处理游戏环境、生成动作和执行动作三个步骤，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 手动游戏测试耗时且昂贵，而现有自动化工具缺乏领域知识和问题解决能力。LLM为自动化测试提供了新可能，但视觉感知和API限制是主要挑战。

Method: Lap方法包括游戏环境处理、基于提示的动作生成和动作执行三个步骤，利用ChatGPT-O1-mini API迭代测试三消游戏。

Result: 在开源游戏CasseBonbons上的实验表明，Lap在代码覆盖率和触发程序崩溃方面优于现有工具。

Conclusion: Lap为自动化测试和LLM应用提供了新方向，展示了其在游戏测试中的潜力。

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [325] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: VulTegra框架评估了深度学习的漏洞检测模型，发现现有模型在一致性、实际效果和扩展性方面存在问题，并揭示了预训练模型并非总是优于从头训练的模型。


<details>
  <summary>Details</summary>
Motivation: 随着软件漏洞威胁的增加，基于深度学习的检测器被广泛使用，但其一致性、实际效果和跨场景适用性仍存疑，可能导致不可靠的检测结果。

Method: 提出了VulTegra框架，对从头训练和预训练的深度学习模型进行多维比较分析。

Result: 实验表明，调整关键因素可显著提升检测器的召回率和F1分数，同时揭示了CWE分类的局限性。

Conclusion: 研究强调了在漏洞检测中需综合考虑漏洞类型和代码特征，为模型设计和部署提供了改进方向。

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [326] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: 论文介绍了一种基于Google Gemini API的服务器实时股票分析系统，通过自动化数据处理和静态前端展示，实现了低成本的高效金融分析工具。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）如Gemini，为金融数据分析提供民主化解决方案，降低技术门槛。

Method: 设计并实现了一个服务器系统，结合Gemini API进行定性分析，通过GitHub Actions自动化数据处理，采用解耦的静态前端展示结果。

Result: 系统最终以接近零成本运行，展示了个人构建AI金融工具的可行性，并公开了源代码和操作应用。

Conclusion: 探讨了LLMs在金融分析中的作用、调试方法的重要性，以及人机协作在软件开发中的新兴范式。

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [327] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文探讨了设计在工程中的重要性，指出缺乏科学基础会导致主观决策，影响效率和创新。研究回顾了软件行业的设计定义，挑战了常见误解，并基于设计理论提出了科学的设计定义和评价标准。


<details>
  <summary>Details</summary>
Motivation: 设计在工程中至关重要，但缺乏科学基础导致主观决策，影响效率和创新。特别是在软件行业和工业控制与自动化系统（iCAS）领域，这一问题尤为突出。

Method: 研究回顾了软件行业的设计定义，挑战了常见误解，并基于设计理论探讨了设计的科学定义、开始时间、好设计的标准以及设计与设计语言的区别。

Result: 研究提出了科学的设计定义和评价标准，区分了临时设计和系统化设计方法，并探讨了如何平衡操作和进化需求。

Conclusion: 通过科学的设计定义和方法，可以提高设计效率和创新性，特别是在软件和iCAS领域。

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [328] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文指出高内聚低耦合的设计原则在某些情况下可能不完美，甚至有害，并探讨其背后的哲学意义。


<details>
  <summary>Details</summary>
Motivation: 澄清高内聚低耦合设计原则的局限性及其潜在误导性，帮助读者理解其真正含义。

Method: 通过理论分析和哲学探讨，解析高内聚低耦合的优缺点及其适用场景。

Result: 高内聚低耦合并非绝对理想的设计目标，需结合具体场景权衡其成本与收益。

Conclusion: 设计原则应灵活应用，避免盲目追求高内聚低耦合，需考虑实际需求和成本。

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [329] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: 本文为复杂性和耦合提供了基于功能域的精确科学定义，强调现有定义的模糊性，并探讨如何减少复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂性和耦合定义在工业控制和自动化系统中的模糊性问题，澄清物理属性定义的不足。

Method: 通过跨学科案例（如软件工程、工业自动化、机械设计）分析复杂性与耦合的关系。

Result: 复杂性不依赖于系统规模或组件数量，耦合发生在功能域而非物理域。

Conclusion: 有效设计需在功能域内解决耦合和复杂性问题。

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [330] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: 论文提出了一种基于认知过程的代码审查模型（CRDM），通过人种志研究和数据分析，揭示了代码审查与决策过程的相似性，旨在优化工具支持并保留人际效益。


<details>
  <summary>Details</summary>
Motivation: 代码审查在软件工程中具有重要价值，但现有工具和流程存在挑战，自动化可能牺牲人际效益。研究旨在通过理解认知过程改进工具支持。

Method: 采用人种志的"有声思考"研究，涉及10名参与者和34次代码审查，通过主题、统计、时间和顺序分析构建认知模型。

Result: 提出了CRDM模型，显示开发者分为两个阶段进行代码审查：导向阶段和分析阶段，涉及多种决策。

Conclusion: CRDM模型为优化代码审查工具提供了理论基础，同时保留了人际效益，如知识传递和共享所有权。

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [331] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 模型量化是减少大型语言模型资源需求的有效方法，但对代码质量的影响尚未充分研究。本文通过量化CodeLlama和DeepSeekCoder模型生成代码，发现量化不仅保持功能正确性，还保留了代码的可维护性和结构简洁性。


<details>
  <summary>Details</summary>
Motivation: 量化虽能减少资源需求，但其对代码质量（如可靠性、可维护性、安全性）的影响尚未明确。本文旨在填补这一研究空白。

Method: 应用激活感知权重量化（AWQ）于CodeLlama和DeepSeekCoder模型，生成Java和Python代码，并使用静态分析工具评估代码质量指标。

Result: 量化不仅保持功能正确性，还保留了代码的可维护性和结构简洁性等关键质量属性。

Conclusion: 量化是一种稳健的技术，适用于优化代码模型，同时保持高质量的代码生成。

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [332] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: OrQstrator是一个基于深度强化学习的模块化框架，用于在NISQ时代优化量子电路。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，量子电路的优化面临噪声和规模限制的挑战，需要智能化的解决方案。

Method: 框架包含三个互补的优化器：基于DRL的电路重写器、领域特定优化器和参数化电路实例化器，由中央协调引擎管理。

Result: 系统输出优化的电路，适应硬件约束，提升性能指标如门数、深度和保真度。

Conclusion: OrQstrator通过智能协调和优化，为NISQ设备提供了高效的量子电路优化方案。

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [333] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在性能导向的软件配置中的潜力，初步结果显示其能力与局限性并存。


<details>
  <summary>Details</summary>
Motivation: 软件系统配置选项繁多且影响性能，传统机器学习方法计算成本高，LLMs可能提供辅助。

Method: 通过提示评估LLMs在识别相关选项、排序配置和推荐高性能配置等任务上的表现。

Result: LLMs在某些任务和系统中表现与专家知识一致，但也存在幻觉或浅层推理的问题。

Conclusion: 研究为LLMs在软件配置中的系统评估和解决方案设计提供了初步探索。

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [334] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: 该论文提出了一个评估大型语言模型（LLM）应用级安全性的实用框架，包括定制安全风险分类的原则和评估实践，并通过实际部署验证。


<details>
  <summary>Details</summary>
Motivation: 当前的安全测试主要关注基础模型，但应用级组件（如系统提示、检索管道等）对整体安全性有显著影响，需要专门的评估方法。

Method: 框架分为两部分：开发定制安全风险分类的原则，以及评估LLM应用安全风险的实践。

Result: 框架在组织内部多个用例中实际部署并验证，提供了可扩展的安全测试参考。

Conclusion: 该研究旨在弥合AI安全理论与实际LLM应用保护之间的差距，为安全且可扩展的部署提供实用指导。

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [335] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 论文介绍了LiveRepoReflection基准和RepoReflection-Instruct数据集，用于评估和改进代码LLM在多文件仓库环境中的理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略了代码仓库修改场景，且在动态基准测试中存在数据污染和反思能力不足的挑战。

Method: 提出LiveRepoReflection基准（1,888个测试案例）和RepoReflection-Instruct数据集，通过两轮对话训练RepoReflectionCoder。

Result: 评估了40多个LLM，展示了模型在仓库代码反思任务中的性能。

Conclusion: LiveRepoReflection和RepoReflection-Instruct为代码LLM在多文件环境中的能力评估和改进提供了有效工具。

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [336] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: PathFuzzing结合模糊测试和符号执行的优点，通过将程序转换为符号程序并应用进化模糊测试技术，以解决最坏情况分析中的代码覆盖率和路径爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 最坏情况资源消耗估计是软件开发中的关键任务，但模糊测试和符号执行在代码覆盖率和路径爆炸方面存在挑战。

Method: 将程序转换为符号程序，利用二进制字符串编码执行路径，并通过进化模糊测试搜索高资源消耗的路径。

Result: 实验结果表明，PathFuzzing在基准测试中优于模糊测试和符号执行的基线方法。

Conclusion: PathFuzzing通过结合模糊测试和符号执行的优点，有效解决了最坏情况分析中的问题。

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [337] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: 论文提出了Agile Map模型，用于系统化描述敏捷实践间的相互关系，帮助从业者更有效地选择和组合敏捷实践。


<details>
  <summary>Details</summary>
Motivation: 敏捷实践在广泛使用中被大量调整和定制，导致实践间关系不明确，组合使用时效果受限。

Method: 研究通过系统化方法识别敏捷实践间的相互关系，并构建Agile Map理论模型。

Result: 核心贡献是Agile Map模型，为敏捷实践间的关联提供了系统化描述。

Conclusion: Agile Map模型有助于从业者更合理地选择和组合敏捷实践，提升实践效果。

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [338] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: 本文研究了压缩工作制（如四天工作周）对IT企业运营效率的影响，并提出了一个综合框架。


<details>
  <summary>Details</summary>
Motivation: 敏捷IT组织需要高效灵活的工作环境，压缩工作制可能带来组织和员工的双赢。

Method: 通过系统文献综述和网络内容分析，研究四天工作周的概念化及其组织和社会影响。

Result: 提出了一个元框架，指导根据管理前提和情境采用压缩工作制。

Conclusion: 压缩工作制在IT企业中的实施需结合具体情况，框架为其提供了指导。

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [339] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 研究探讨开源大语言模型（LLMs）在被直接或间接要求生成不安全代码时的行为，发现模型常生成漏洞代码，且用户角色和提示方式影响成功率。


<details>
  <summary>Details</summary>
Motivation: 理解开源LLMs在明确要求生成不安全代码时的行为，填补现有研究对直接威胁场景的空白。

Method: 采用动态提示和反向提示两种实验设计，评估三种开源模型生成代码的漏洞情况。

Result: 所有模型均频繁生成有漏洞代码，Qwen2正确率最高；用户角色和提示方式显著影响结果。

Conclusion: 开源模型的安全机制存在局限性，尤其在看似无害的教育请求中。

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [340] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: LLMShot利用视觉大语言模型自动分析快照测试失败，通过分层分类UI变化，显著减少手动检查工作。


<details>
  <summary>Details</summary>
Motivation: 快照测试在UI验证中至关重要，但频繁的UI变化导致测试失败需要手动检查，维护成本高。

Method: LLMShot框架利用Gemma3模型对快照测试失败进行分层分类，评估基于iOS应用的全面数据集。

Result: 12B模型在识别失败根因时召回率超过84%，4B模型适合持续集成环境。

Conclusion: LLMShot是首个自动化语义快照测试分析方法，显著减少手动检查，推动更智能的UI测试。

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [341] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: SelRepair是一种结合微调LLM和双RAG模块的新型APR方法，显著提升了修复效果并减少了推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法受限于缺陷类型、训练数据质量和模型参数规模，且当前代码LLM和RAG设计未充分考虑代码修复任务和代码特性。

Method: 通过微调LLM并结合新设计的双RAG模块，利用bug-fix对数据集，并通过RAG选择门引入语义和语法/结构相似性信息。

Result: 在Java数据集上，SelRepair在精确匹配（EM）指标上分别达到26.29%和17.64%，推理时间减少至少6.42%。

Conclusion: SelRepair有效解决了现有APR方法的局限性，显著提升了修复性能和效率。

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [342] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: 研究表明，小型微调语言模型在生成高质量后置条件时，可以媲美或超越大型模型，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 手动编写形式化规范既繁琐又容易出错，而大型语言模型（LLMs）虽然表现良好，但其庞大的规模和计算需求引发了是否真正需要大型模型的疑问。

Method: 构建一个包含提示、推理日志和后置条件的专用数据集，并对一个70亿参数的代码模型进行监督微调，解决了实际仓库依赖性和保留预状态信息的问题。

Result: 在真实Java缺陷基准（Defects4J）上评估，小型模型在语法正确性、语义正确性和区分缺陷能力上媲美或超越大型模型。

Conclusion: 通过针对性微调和适度数据集，小型模型可以实现以往仅大型模型才能达到的效果，为自动化规范生成提供了实用且高效的路径。

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [343] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 提出将AMDiRE和PerSpecML两种方法结合，以解决AI系统可信赖性需求工程中的挑战。


<details>
  <summary>Details</summary>
Motivation: AI系统的可信赖性问题需要结构化方法来解决，尤其是针对ML系统的非确定性行为。

Method: 整合AMDiRE（基于工件的需求工程方法）和PerSpecML（多视角的ML需求分析方法）。

Result: 提出了一种结合两种方法的路径，以操作化可信赖性需求。

Conclusion: 总结了关键研究方向和开放挑战，供需求工程社区讨论。

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [344] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文研究了机器人系统中的交互错误（iBugs），分析了ROS框架中的121个iBugs，分类并探讨了其根源、修复策略及影响，提出了改进系统鲁棒性的建议。


<details>
  <summary>Details</summary>
Motivation: 机器人系统的可靠性不仅依赖单个组件的正确性，还依赖组件间交互的正确性，但目前交互错误的研究较少。

Method: 对十个ROS项目中的121个iBugs进行实证研究，分类为系统内、硬件和环境iBugs，分析其根源和修复策略。

Result: 揭示了iBugs的性质，提出了改进预防和检测的方向。

Conclusion: 研究结果为设计更鲁棒和安全的机器人系统提供了参考。

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [345] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: 论文提出了一种改进的交互式可视化工具Helveg，用于帮助开发者理解代码库，解决了传统文档的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统文档形式不适合代码库的高层次探索分析，因其固定路径限制了用户灵活性。

Method: 设计了一种高度交互的节点链接图，结合灵活的过滤功能，并通过改进Helveg工具的原型实现。

Result: 用户测试证实了工具的潜力，但也揭示了可读性、直观性和用户体验的问题。

Conclusion: 通过改进Helveg的符号设计、交互方式和用户界面，解决了之前的问题，并通过用户评估验证了改进效果。

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [346] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: 研究发现，在结对编程中，知识转移可能因开发者忽视伙伴需求或权力差距而产生负面影响，导致防御行为和恶性循环。


<details>
  <summary>Details</summary>
Motivation: 探讨在结对编程中，知识转移在何种情况下可能对会话产生负面影响。

Method: 采用扎根理论方法，分析17次结对编程会话记录和6次开发者访谈。

Result: 定义了学生和教师角色以应对知识差距，提出了权力差距理论，并指出防御行为可能导致的恶性循环。

Conclusion: 若开发者忽视伙伴需求和权力差距，知识转移可能对团队协作和代码质量产生负面影响。

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [347] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的流程和工具链，旨在优化数字接口和机载软件的开发，强调自动化、灵活性并满足设计保证要求。


<details>
  <summary>Details</summary>
Motivation: 航空航天行业的技术进步和创新解决方案带来了新挑战，特别是数字设备间通信接口的高效处理问题，需要自动化解决方案以减少人工投入。

Method: 提出了一种结合自动化和灵活性的流程及工具链，应用于多个已完成项目中。

Result: 该方法成功应用于多个项目，优化了开发过程。

Conclusion: 该流程和工具链有效解决了数字接口开发的挑战，同时满足设计保证要求。

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [348] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: AssertCoder是一个自动化生成高质量SVAs的统一框架，通过多模态设计规范和语义分析提升硬件验证效率。


<details>
  <summary>Details</summary>
Motivation: 手动编写高质量SVAs耗时且易错，需要自动化解决方案。

Method: 采用多模态敏感预处理和语义分析，结合多步链式思考提示生成断言，并通过变异评估优化。

Result: 在三个RTL设计中，AssertCoder功能正确性提升8.4%，变异检测提升5.8%。

Conclusion: AssertCoder显著优于现有方法，为硬件验证提供高效自动化工具。

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [349] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: 论文研究了生成式AI（GenAI）在开源软件开发中的使用情况，通过分析GitHub仓库中的自我承认使用案例，揭示了开发者的任务、内容和目的分类，并探讨了伦理、法律和实践问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具（如GitHub Copilot和ChatGPT）的广泛使用正在改变软件开发，但其实际影响和开源项目中的使用情况尚不明确。

Method: 通过分析25万个GitHub仓库中的1,292个自我承认案例，采用混合方法（定性和定量）对GenAI的使用进行分类，并调查开发者的伦理和法律担忧。

Result: 研究发现开发者主动管理GenAI的使用，强调了透明度和质量控制的重要性，同时发现GenAI的采用并未显著增加代码变更频率。

Conclusion: 研究呼吁在AI辅助软件开发中加强项目级透明度和质量控制，同时挑战了GenAI对代码变更影响的流行观点。

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [350] [Correlated Synthetic Controls](https://arxiv.org/abs/2507.08918)
*Tzvetan Moev*

Main category: econ.EM

TL;DR: 论文提出了一种名为Correlated Synthetic Controls (CSC)的估计器，适用于多处理单元的情况，相比传统方法具有更好的理论性质，并在实际案例中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统合成控制方法仅适用于单一处理单元，而现实中常有多处理单元的情况，需要一种更通用的方法来预测反事实。

Method: 提出CSC估计器，通过创建跨个体的相关合成控制来解决多处理单元问题，并利用相似可观测变量进行预测。

Result: CSC在理论上优于差异中的差异（DID）估计器，尤其是在处理分配与不可观测变量相关时。实际应用中，CSC成功用于Mariel Boatlift研究，获得了异质性处理效应。

Conclusion: CSC为多处理单元的反事实预测提供了一种有效方法，具有理论和实践上的优势。

Abstract: Synthetic Control methods have recently gained considerable attention in
applications with only one treated unit. Their popularity is partly based on
the key insight that we can predict good synthetic counterfactuals for our
treated unit. However, this insight of predicting counterfactuals is
generalisable to microeconometric settings where we often observe many treated
units. We propose the Correlated Synthetic Controls (CSC) estimator for such
situations: intuitively, it creates synthetic controls that are correlated
across individuals with similar observables. When treatment assignment is
correlated with unobservables, we show that the CSC estimator has more
desirable theoretical properties than the difference-in-differences estimator.
We also utilise CSC in practice to obtain heterogeneous treatment effects in
the well-known Mariel Boatlift study, leveraging additional information from
the PSID.

</details>


### [351] [Breakdown Analysis for Instrumental Variables with Binary Outcomes](https://arxiv.org/abs/2507.10242)
*Pedro Picchetti*

Main category: econ.EM

TL;DR: 本文研究了在违反独立性假设的工具变量（IV）设置中，二元结果下处理效应的部分识别问题。作者推导了感兴趣的处理参数的识别集，以及关于真实处理效应的结论的崩溃值。


<details>
  <summary>Details</summary>
Motivation: 在观测数据中使用IV方法时，工具变量与潜在量的独立性假设普遍存在问题，本文旨在评估在这种假设下得出的经验结论的稳健性。

Method: 作者推导了处理效应边界和崩溃值的√N-一致非参数估计量。

Result: 在实证应用中，作者发现使用同性兄弟姐妹作为工具变量时，家庭规模对女性失业影响的结论对独立性假设的违反高度敏感。

Conclusion: 本文的结果可用于评估在工具变量独立性假设下得出的经验结论的稳健性，特别是在观测数据中。

Abstract: This paper studies the partial identification of treatment effects in
Instrumental Variables (IV) settings with binary outcomes under violations of
independence. I derive the identified sets for the treatment parameters of
interest in the setting, as well as breakdown values for conclusions regarding
the true treatment effects. I derive $\sqrt{N}$-consistent nonparametric
estimators for the bounds of treatment effects and for breakdown values. These
results can be used to assess the robustness of empirical conclusions obtained
under the assumption that the instrument is independent from potential
quantities, which is a pervasive concern in studies that use IV methods with
observational data. In the empirical application, I show that the conclusions
regarding the effects of family size on female unemployment using same-sex
sibling as the instrument are highly sensitive to violations of independence.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [352] [Central Bank Digital Currencies: A Survey](https://arxiv.org/abs/2507.08880)
*Qifeng Tang,Yain-Whar Si*

Main category: econ.GN

TL;DR: 本文综述了央行数字货币（CBDC）系统设计与实施的最新进展，分析了135篇研究论文，提出了CBDC设计分类和生态系统框架，并对26个现有CBDC系统进行了比较分析。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付技术的发展，全球央行开始探索CBDC的实施，以解决现有支付系统的低效和结构性问题。

Method: 基于CBDC设计金字塔，研究了账本技术、共识机制选择和离线支付等关键架构元素，并对26个CBDC系统进行了四维比较分析。

Result: 研究发现最常见的配置是双层架构、分布式账本技术和基于令牌的访问模型，但应用领域尚未形成主导趋势。

Conclusion: 论文提出了未来研究的建议，并指出CBDC在跨境支付中的应用潜力。

Abstract: With the advancement of digital payment technologies, central banks worldwide
have increasingly begun to explore the implementation of Central Bank Digital
Currencies (CBDCs). This paper presents a comprehensive review of the latest
developments in CBDC system design and implementation. By analyzing 135
research papers published between 2018 and 2025, the study provides an in-depth
examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the
CBDC Design Pyramid, the paper refines and expands key architectural elements
by thoroughly investigating innovations in ledger technologies, the selection
of consensus mechanisms, and challenges associated with offline payments and
digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A
detailed comparative analysis of 26 existing CBDC systems is conducted across
four dimensions: system architecture, ledger technology, access model, and
application domain. The findings reveal that the most common configuration
consists of a two-tier architecture, distributed ledger technology (DLT), and a
token-based access model. However, no dominant trend has emerged regarding
application domains. Notably, recent research shows a growing focus on
leveraging CBDCs for cross-border payments to resolve inefficiencies and
structural delays in current systems. Finally, the paper offers several
forward-looking recommendations for future research.

</details>


### [353] [Comrades and Cause: Peer Influence on West Point Cadets' Civil War Allegiances](https://arxiv.org/abs/2507.09419)
*Yuchen Guo,Matthew O. Jackson,Ruixue Jia*

Main category: econ.GN

TL;DR: 研究探讨了社会网络和同伴影响在高度极化环境中对重大人生决策的影响，以西点军校学员在美国内战中的选择为例。


<details>
  <summary>Details</summary>
Motivation: 探究在极端对立背景下，同伴如何影响个人重大决策，如选择加入哪一方军队。

Method: 利用来自自由州学员比例的准随机变化，分析其对奴隶州学员选择的影响。

Result: 自由州同学比例越高，奴隶州学员加入联邦军队的可能性越大；自由州学员几乎全部选择联邦军队。

Conclusion: 即使在极端对立时期，同伴仍对人生重大选择有显著影响。

Abstract: Do social networks and peer influence shape major life decisions in highly
polarized settings? We explore this question by examining how peers influenced
the allegiances of West Point cadets during the American Civil War. Leveraging
quasi-random variations in the proportion of cadets from Free States, we
analyze how these differences affected decisions about which army to join. We
find that a higher proportion of classmates from Free States significantly
increased the likelihood that cadets from Slave States joined the Union Army,
while almost all cadets from Free States joined the Union Army (if they decided
to join the war). We further examine how cadets' decisions affected their
military rank and career outcomes. Our findings highlight that peers still
influence choices even when they are life-altering and occur during periods of
extreme polarization.

</details>


### [354] [Integrated Warehouse Location and Inventory Decisions in a Multi-location Newsvendor Problem](https://arxiv.org/abs/2507.09631)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: econ.GN

TL;DR: 研究了供应链网络中供应商与多个零售商的互动，比较了直接订单与仓库集中订单的效益，并考虑了三种运输成本模式。


<details>
  <summary>Details</summary>
Motivation: 探讨供应链网络中供应商与零售商的互动模式，以及不同运输成本对网络效益的影响。

Method: 构建六种模型（三种分散式、三种集中式），提供封闭解或算法（如Q-search）求解最优订单量和仓库位置。

Result: 集中式模型在大网络中表现更优，利润和服务水平更高。

Conclusion: 集中式供应链网络模式在大规模网络中更具优势。

Abstract: In this paper, we investigate a supply chain network with a supplier and
multiple retailers. The supplier can either take orders from retailers
directly, or choose to build a warehouse somewhere in the network to centralize
the ordering from retailers. Meanwhile, we take three modes of transportation
cost into account, including distance-dependent, quantity-dependent, and
distance-quantity-dependent costs, to formulate six models. For the three
decentralized models, we provide closed-form solutions to compute the optimal
order quantity of each retailer. For the centralized models, we develop
closed-form solutions for the first two models as the transportation cost only
depends on either distance or order quantity; but when it depends on both, the
model becomes a non-linear programming problem. We develop a solution algorithm
named Q-search to find a high-quality solution that includes the order quantity
and the warehouse location. The experiment results show that the centralized
model outperforms the decentralized model in large networks in terms of profit
and service level.

</details>


### [355] [Selective Newsvendor Problem with Dependent Leadtime and Joint Marketing Decisions](https://arxiv.org/abs/2507.09635)
*Jianing Zhi,Guanqiu Qi,Xinghua Li*

Main category: econ.GN

TL;DR: 论文研究了两阶段供应链网络的联合决策模式，提出了两种新模型（AON-SNP和DLSNP），并开发了高效的R-search算法。模型适用于时尚、家具和电子行业，通过敏感性分析验证了模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 研究供应链网络中供应商、公司和客户的联合决策问题，特别是在需求模式与交货时间和服务水平相关的情况下。

Method: 提出了两种新模型（AON-SNP和DLSNP），并开发了R-search算法以解决DL-selectivity问题。

Result: 模型表明，当总需求低于订单量上限时，最佳策略是匹配需求；市场需求增加时，公司需寻找本地供应商填补额外需求。

Conclusion: 模型能有效解释网络中各方行为，并为公司提供智能决策指导。

Abstract: In this paper, we investigate a joint decision-making pattern for a two-stage
supply chain network, including a supplier, a company, and its customers. We
investigate two types of demand patterns, associated with dependent lead time
and service level considerations. We define two novel models, including
all-or-nothing selective newsvendor problem (AON-SNP) and selective news Vendor
Problem with Dependent Lead Time and Price Related Demands (DLSNP). The
proposed models are applicable to numerous areas such as the fashion,
furniture, and electronic industries. We develop an efficient solution
algorithm, referred to as R-search, to identify an optimal solution for the
DL-selectivity problem. We examine various responses of the system through
parameter sensitivity analysis. Our model proves that if the total demand is
lower than the upper limit of the order quantity, the best strategy for Q is to
match that demand. If the market increases, more demand comes in, leading to a
shortage, and forcing the company to find other local suppliers to fill the
additional demand. The results demonstrate that our model well explains various
behavior for all involved parties in the network, and provide guidance on
intelligent decision making for the company.

</details>


### [356] [The Effects of Flipped Classrooms in Higher Education: A Causal Machine Learning Analysis](https://arxiv.org/abs/2507.10140)
*Daniel Czarnowske,Florian Heiss,Theresa M. A. Schmitz,Amrei Stammann*

Main category: econ.GN

TL;DR: 研究使用双重/去偏机器学习评估从讲座式混合教学转向翻转课堂的效果，发现学生自我认知改善和拖延行为减少，但课堂享受度下降，且对考试成绩、通过率或知识保留无显著影响。翻转课堂实施不足是主要原因。


<details>
  <summary>Details</summary>
Motivation: 评估翻转课堂对教学效果的实际影响，尤其是与传统混合教学的对比。

Method: 采用双重/去偏机器学习方法，分析大规模必修统计学课程的队列数据，利用翻转课堂的详细使用数据（如视频观看、测验参与）。

Result: 学生自我认知和拖延行为改善，但课堂享受度下降；考试成绩、通过率和知识保留无显著提升，翻转课堂实施不足是原因。

Conclusion: 翻转课堂需额外策略确保学生充分参与，才能真正受益。

Abstract: This study uses double/debiased machine learning to evaluate the impact of
transitioning from lecture-based blended teaching to a flipped classroom
concept in a cohort comparison of a large compulsory introductory statistics
course at a German tuition-free university. Our findings indicate positive
changes in students' self-conception and a reduction in procrastination
behaviors. However, we also observe a decline in the enjoyment of classroom
sessions. Contrary to theoretical expectations, we do not find significant
positive effects on exam scores, passing rates, or knowledge retention. Unlike
most studies, however, we can leverage detailed usage data from the flipped
cohort, including the timeliness and completeness of pre-class video watching,
as well as quiz participation patterns, to check how well students implemented
each part of the curriculum. Our findings suggest that, on average, students in
the flipped cohort implemented the instructional approach insufficiently,
explaining the mechanism of our null results in exam performance and knowledge
retention. This highlights the need for additional strategies to ensure that
students actually benefit from a flipped curriculum.

</details>


### [357] [The Green Premium Puzzle: Empirical Evidence from Climate-Friendly Food Products](https://arxiv.org/abs/2507.10333)
*Voraprapa Nakavachara,Chanon Thongtai,Thanarat Chalidabhongse,Chanathip Pharino*

Main category: econ.GN

TL;DR: 研究发现，气候友好型食品在瑞典超市中并未显示出价格溢价，甚至在某些类别中价格更低，形成了“绿色溢价之谜”。


<details>
  <summary>Details</summary>
Motivation: 探讨气候友好型食品是否在消费者市场中享有价格溢价，以验证绿色消费的经济激励效果。

Method: 使用瑞典超市的产品级数据，分析包装前气候影响评分与零售价格的关系，并控制产品大小、营养内容和固定效应。

Result: 未发现气候友好型产品价格更高的证据，某些类别中气候评分更好的产品价格更低。

Conclusion: 市场摩擦（如消费者优先级冲突、对气候问题的心理距离和环保标签的怀疑）可能抑制了可持续消费的价格信号，这对生产者、零售商和政策制定者具有重要启示。

Abstract: This paper investigates whether climate-friendly food products command a
price premium in consumer markets. Using product-level data from a supermarket
in Sweden, we examine the relationship between front-of-package climate impact
scores and retail prices, controlling for product size, nutritional content,
and fixed effects. Contrary to the intuitive expectation of a positive green
premium, we find no evidence that climate-friendly products are priced higher.
In some product categories, products with better climate scores are in fact
associated with lower prices, suggesting a negative premium, an outcome that
gives rise to what we refer to as the green premium puzzle. We argue that
market frictions such as competing consumer priorities, psychological distance
from climate issues, and skepticism toward environmental labeling may suppress
the price signals intended to reward sustainable consumption. These findings
offer important insights for producers, retailers, and policymakers seeking to
align climate goals with effective market incentives in the transition toward a
more sustainable society.

</details>


### [358] [Intimate partner violence and women's economic preferences](https://arxiv.org/abs/2507.10416)
*Dan Anderberg,Rachel Cassidy,Anaya Dam,Melissa Hidrobo,Jessica Leight,Karlijn Morsink*

Main category: econ.GN

TL;DR: 研究发现，亲密伴侣暴力（IPV）会导致女性更倾向于短期决策，影响其储蓄、投资和劳动力供给行为。


<details>
  <summary>Details</summary>
Motivation: 全球三分之一的女性经历过IPV，但其对经济决策的影响尚不明确。

Method: 结合随机回忆实验和随机干预实验，分析IPV对女性时间偏好的因果影响。

Result: IPV经历导致女性更缺乏耐心，而减少IPV的干预则使其更倾向于长期决策。

Conclusion: IPV通过心理机制加剧经济劣势，限制女性采取长期规划行动的能力。

Abstract: One in three women globally experiences intimate partner violence (IPV), yet
little is known about how such trauma affects economic decision-making. We
provide causal evidence that IPV influences women's time preferences - a key
parameter in models of savings, investment, and labor supply. We combine two
empirical strategies using four distinct datasets. First, in two randomized
recall experiments in Ethiopia, we randomly assigned women to recall specific
acts of abuse before eliciting their intertemporal choices. Women with IPV
experiences prompted to recall IPV display significantly greater impatience
than otherwise similar women who are not prompted. Second, we exploit exogenous
reductions in IPV generated by two randomized interventions - one involving
cash transfers, the other psychotherapy - and use treatment assignment as an
instrument for IPV exposure. Women who experience reduced IPV as a result of
treatment exhibit more patient time preferences. Together, these results
provide consistent, novel causal evidence that exposure to IPV induces
individuals to discount the future more heavily. This evidence suggests a
psychological channel through which violence can perpetuate economic
disadvantage and constrain women's ability to take actions - such as saving,
investing, or exiting abusive relationships - that require planning over time.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [359] [Egalitarian-equivalent and strategy-proof mechanisms in homogeneous multi-object allocation problems](https://arxiv.org/abs/2507.09152)
*Hinata Kurashita,Ryosuke Sakai*

Main category: econ.TH

TL;DR: 研究了在金钱条件下分配同质不可分物品的问题，探讨了公平性与效率的关系。


<details>
  <summary>Details</summary>
Motivation: 探索公平性（平等等价）与效率在激励机制下的关系。

Method: 首先描述满足平等等价、策略证明、个体合理性和无补贴的机制类别，然后放宽激励约束以设计更高效的机制。

Result: 在严格条件下机制分配有限，放宽激励约束后可实现效率与公平。

Conclusion: 放宽激励约束能实现效率与公平的平衡，并找到最优机制。

Abstract: We study the problem of allocating homogeneous and indivisible objects among
agents with money. In particular, we investigate the relationship between
egalitarian-equivalence (Pazner and Schmeidler, 1978), as a fairness concept,
and efficiency under agents' incentive constraints. As a first result, we
characterize the class of mechanisms that satisfy egalitarian-equivalence,
strategy-proofness, individual rationality, and no subsidy. Our
characterization reveals a strong tension between egalitarian-equivalence and
efficiency: under these properties, the mechanisms allocate objects only in
limited cases. To address this limitation, we replace strategy-proofness with
the weaker incentive property, non-obvious manipulability (Troyan and Morrill,
2020). We show that this relaxation allows us to design mechanisms that achieve
efficiency while still ensuring egalitarian-equivalence. Furthermore, upon
achieving efficiency, we identify the agent optimal mechanism in the
characterized class.

</details>


### [360] [Contracting a crowd of heterogeneous agents](https://arxiv.org/abs/2507.09415)
*Guillermo Alonso Alvarez,Erhan Bayraktar,Ibrahim Ekren*

Main category: econ.TH

TL;DR: 研究了一个涉及大量异质交互主体的委托-代理模型，通过扩展现有方法，找到了连续主体假设下的最优合同，并证明当主体数量足够大时，这些合同对有限主体问题也是近似最优的。通过比较统计和数值模拟分析了主体连通性对委托方价值、主体努力和最优合同的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在大量异质交互主体情况下，如何设计最优合同，并分析主体连通性对合同效果的影响。

Method: 扩展现有方法，假设主体连续分布，推导最优合同，并通过比较统计和数值模拟验证结果。

Result: 在主体数量足够大时，连续主体假设下的最优合同对有限主体问题近似最优。主体连通性显著影响委托方价值、主体努力和合同设计。

Conclusion: 连续主体假设为大规模异质交互主体问题提供了有效的合同设计框架，主体连通性是关键影响因素。

Abstract: We study a principal-agent model involving a large population of
heterogeneously interacting agents. By extending the existing methods, we find
the optimal contracts assuming a continuum of agents, and show that, when the
number of agents is sufficiently large, the optimal contracts for the problem
with a continuum of agents are near-optimal for the finite agents problem. We
make comparative statistics and provide numerical simulations to analyze how
the agents' connectivity affects the principal's value, the effort of the
agents, and the optimal contracts.

</details>


### [361] [On Probabilistic Assignment Rules](https://arxiv.org/abs/2507.09550)
*Sreedurga Gogulapati,Yadati Narahari,Souvik Roy,Soumyarup Sadhukhan*

Main category: econ.TH

TL;DR: 论文研究了带有初始禀赋的经典分配问题，在概率框架下扩展了Ma (1994)的确定性结果，证明了在确定性禀赋下，TTC规则是唯一满足SD效率、SD个体理性和SD-top-策略证明性的概率规则。


<details>
  <summary>Details</summary>
Motivation: 研究在概率分配中如何满足策略证明性、帕累托效率和个体理性等理想性质，填补了确定性TTC规则在概率领域的空白。

Method: 在概率框架下，引入SD效率、SD个体理性和SD-top-策略证明性，分析TTC规则在这些性质下的唯一性。

Result: 证明在确定性禀赋下，TTC规则是唯一满足SD效率、SD个体理性和SD-top-策略证明性的概率规则。

Conclusion: 研究为概率分配中的理想性质提供了积极可能性，并为未来研究奠定了基础。

Abstract: We study the classical assignment problem with initial endowments in a
probabilistic framework. In this setting, each agent initially owns an object
and has strict preferences over the entire set of objects, and the goal is to
reassign objects in a way that satisfies desirable properties such as
strategy-proofness, Pareto efficiency, and individual rationality. While the
celebrated result by Ma (1994) shows that the Top Trading Cycles (TTC) rule is
the unique deterministic rule satisfying these properties, similar positive
results are scarce in the probabilistic domain. We extend Ma's result in the
probabilistic setting, and as desirable properties, consider SD-efficiency,
SD-individual rationality, and a weaker notion of SD-strategy-proofness --
SD-top-strategy-proofness -- which only requires agents to have no incentive to
misreport if doing so increases the probability of receiving their top-ranked
object. We show that under deterministic endowments, a probabilistic rule is
SD-efficient, SD-individually rational, and SD-top-strategy-proof if and only
if it coincides with the TTC rule. Our result highlights a positive possibility
in the face of earlier impossibility results for fractional endowments
(Athanassoglou and Sethuraman (2011)) and provides a first step toward
reconciling desirable properties in probabilistic assignments with endowments.

</details>


### [362] [On the existence of EFX allocations for goods](https://arxiv.org/abs/2507.09600)
*Ujjwal Kumar,Souvik Roy*

Main category: econ.TH

TL;DR: 论文证明了在特定估值函数条件下，EFX分配总是存在的。


<details>
  <summary>Details</summary>
Motivation: 研究在何种估值函数条件下可以保证EFX（无嫉妒且公平）分配的存在性。

Method: 假设两个代理具有任意集合单调估值函数，其余代理具有大小单调估值函数，分析EFX分配的存在性。

Result: 在上述条件下，EFX分配总是存在。

Conclusion: 为EFX分配的存在性提供了新的理论支持。

Abstract: We consider a set $E$ of $m$ indivisible goods and a set $N$ consisting of $n
\geq 2$ agents. The paper shows that if two agents have \textit{arbitrary} set
monotonic valuation functions and the remaining agents have size monotonic
valuation functions, then EFX allocations always exist.

</details>


### [363] [Ranked Pairs minimizes the $p$-norm as $p \to \infty$](https://arxiv.org/abs/2507.09654)
*Amir Babak Aazami,Hubert L. Bray*

Main category: econ.TH

TL;DR: Ranked Pairs方法在$p \to \infty$时，最小化违背其排序的头对头胜利边际的$p$-范数。


<details>
  <summary>Details</summary>
Motivation: 研究Ranked Pairs方法在排序候选者时的数学性质，特别是其在最小化特定范数下的表现。

Method: 通过分析Ranked Pairs排序与头对头胜利边际的关系，探讨其在$p \to \infty$时的极限行为。

Result: 证明Ranked Pairs方法在$p \to \infty$时能够最小化违背其排序的头对头胜利边际的$p$-范数。

Conclusion: Ranked Pairs方法在特定条件下表现出最优的数学性质，适用于需要最小化冲突边际的排序场景。

Abstract: We prove that Ranked Pairs orders candidates in such a way as to minimize the
$p$-norm, in the limit as $p \to \infty$, of those head-to-head margins of
victory which go against its ordering.

</details>


### [364] [A Folk Theorem for Indefinitely Repeated Network Games](https://arxiv.org/abs/2507.10148)
*Andrea Benso*

Main category: econ.TH

TL;DR: 论文研究了网络中的重复博弈，玩家只能观察邻居的行为，且通信是局部的。结果表明，当且仅当网络是2-连通时，Folk定理成立。


<details>
  <summary>Details</summary>
Motivation: 探讨在网络结构中，玩家如何通过局部观察和通信实现合作，以及网络连通性对博弈结果的影响。

Method: 通过重复博弈模型，玩家仅能观察邻居行为，通信为局部且混合公开与私有，使用完美贝叶斯均衡作为解概念。

Result: 证明Folk定理成立的条件是网络必须是2-连通的。

Conclusion: 网络连通性（2-连通）是实现合作均衡的关键条件。

Abstract: We consider a repeated game in which players, considered as nodes of a
network, are connected. Each player observes her neighbors' moves only. Thus,
monitoring is private and imperfect. Players can communicate with their
neighbors at each stage; each player, for any subset of her neighbors, sends
the same message to any player of that subset. Thus, communication is local and
both public and private. Both communication and monitoring structures are given
by the network. The solution concept is perfect Bayesian equilibrium. In this
paper we show that a folk theorem holds if and only if the network is
2-connected for any number of players.

</details>


### [365] [Machine-Learning to Trust](https://arxiv.org/abs/2507.10363)
*Ran Spiegler*

Main category: econ.TH

TL;DR: 研究探讨了在机器学习方法惩罚复杂性的情况下，玩家是否能维持长期信任。通过一个无限序列的代理游戏，发现均衡解显著缩小了信任的范围。


<details>
  <summary>Details</summary>
Motivation: 探讨在机器学习方法影响下，玩家信任行为的长期可持续性。

Method: 使用一个无限序列代理游戏，玩家基于对他人行为的粗粒度信念做出信任决策，均衡解最小化预测误差和复杂性惩罚。

Result: 相对于对称混合策略纳什均衡，该方法显著缩小了信任的范围。

Conclusion: 在机器学习方法惩罚复杂性的情况下，玩家的信任行为受到显著限制。

Abstract: Can players sustain long-run trust when their equilibrium beliefs are shaped
by machine-learning methods that penalize complexity? I study a game in which
an infinite sequence of agents with one-period recall decides whether to place
trust in their immediate successor. The cost of trusting is state-dependent.
Each player's best response is based on a belief about others' behavior, which
is a coarse fit of the true population strategy with respect to a partition of
relevant contingencies. In equilibrium, this partition minimizes the sum of the
mean squared prediction error and a complexity penalty proportional to its
size. Relative to symmetric mixed-strategy Nash equilibrium, this solution
concept significantly narrows the scope for trust.

</details>
