<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 48]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 9]
- [econ.EM](#econ.EM) [Total: 2]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent是一种新型AI代理，结合大型语言模型（LLMs）和形式化证明助手Lean，实现自动化定理证明，成功率达到86.1%。


<details>
  <summary>Details</summary>
Motivation: 通过整合LLMs和形式化证明工具，提高自动化定理证明的效率和成功率。

Method: 结合非正式推理LLM、形式化证明模型和Lean的反馈，生成辅助引理以帮助发现整体证明策略。

Result: 在MiniF2F基准测试中达到86.1%的成功率，成为使用小语言模型（SLMs）且样本预算更低的新最优方法。

Conclusion: Prover Agent通过生成辅助引理，显著提升了自动化定理证明的性能，展示了其在实际问题中的潜力。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [2] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: 提出了一种基于组合多臂老虎机（CMAB）的框架，用于高效识别检索上下文对语言模型生成答案的贡献。


<details>
  <summary>Details</summary>
Motivation: 提高生成式问答系统的可解释性和可信度，通过明确上下文各部分对答案的贡献。

Method: 将上下文片段视为老虎机臂，使用组合汤普森采样（CTS）在有限查询预算下高效探索上下文子集。

Result: 在多种数据集和语言模型上验证，该方法以更少的查询实现了竞争性的归因质量。

Conclusion: 该方法显著提高了查询效率，同时保持了高归因保真度，优于传统扰动方法。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [3] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: 论文评估了大型语言模型（LLMs）在PennyLane量子代码生成中的表现，引入了QHackBench数据集，并比较了标准提示与检索增强生成（RAG）的效果。结果显示RAG在复杂量子算法中表现接近标准提示，同时提出了多代理评估管道以提升成功率。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在量子计算代码生成中的潜力，填补现有研究的空白。

Method: 使用QHack竞赛的真实挑战构建QHackBench数据集，评估标准提示和RAG的效果，并引入多代理评估管道。

Result: RAG增强模型在复杂量子算法中表现接近标准提示，多代理管道进一步提升了执行成功率。

Conclusion: 论文为AI辅助量子编程提供了新工具和数据集，推动了该领域的进一步发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [4] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 论文提出了一种可定制的RAG框架，用于医疗任务，其性能和能耗优于商业LLM模型，同时减少环境影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在医疗领域的应用引发的环境和伦理问题，开发可持续的解决方案。

Method: 开发了可监控能耗和CO2排放的RAG框架，基于开源LLM构建模型，并与商业模型对比。

Result: 自定义RAG模型在准确性和能耗上优于商业模型，llama3.1:8B表现最佳。

Conclusion: 本地LLM开发的RAG在医疗任务中优于商业模型，且更环保，符合可持续发展目标。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [5] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 论文探讨了利用低延迟AI模型的实时决策支持系统，结合了AI驱动决策工具、Edge-IoT技术和人机协作方法，特别关注资源有限时大语言模型的辅助作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在整合AI、边缘计算和人机协作的最新进展，以提升实时决策支持系统的效率和适应性。

Method: 通过详细综述，分析了DeLLMa、模型压缩技术和边缘设备分析改进等技术发展，并探讨了资源限制和框架适应性等问题。

Result: 论文提供了开发策略和应用领域的实用视角，指出了高效灵活AI支持系统的机会。

Conclusion: 结论为这一快速变化领域的未来突破奠定了基础，强调了AI如何重塑实时决策支持。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [6] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型（LLMs）在赋予特定政治和社会身份后，会表现出类似人类的动机性推理，且难以通过常规提示方法消除偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否会在赋予政治和社会身份后表现出动机性推理，以及这种偏见是否难以消除。

Method: 通过为8种LLMs赋予4种政治和社会属性的人设，测试其在信息真实性辨别和科学证据评估任务中的表现。

Result: 赋予人设的LLMs在信息辨别能力上降低9%，政治人设的模型在科学证据评估中表现出90%的偏见倾向。

Conclusion: LLMs表现出类似人类的动机性推理，且常规去偏见方法效果有限，可能加剧身份认同偏见。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [7] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM是一种新型医疗大语言模型，通过整合电子健康记录（EHR）提升临床对话的实用性，支持临床测试推荐、结果解释和诊断预测。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型忽视电子健康记录（EHR）的作用，且仅关注诊断推荐，限制了临床实用性。

Method: 提出DiaLLM模型，采用临床测试参考（CTR）策略处理EHR数据，并结合强化学习框架进行证据获取和自动诊断。

Result: 实验表明DiaLLM在临床测试推荐和诊断预测上优于基线模型。

Conclusion: DiaLLM通过整合EHR和改进的强化学习策略，显著提升了医疗大语言模型的临床适用性。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [8] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub是一个AI驱动的平台，通过模块化助手（如Reproducibility Copilot）支持开放科学任务，显著减少研究复现时间并提高透明度。


<details>
  <summary>Details</summary>
Motivation: 解决开放科学中研究结果难以独立复现的挑战。

Method: 使用AI分析论文、代码和补充材料，生成结构化Jupyter Notebook和建议，以促进计算复现。

Result: 测试显示，复现时间从30小时缩短至1小时，且能高覆盖率复现图表和结果。

Conclusion: AI工具可有效减轻复现负担，提升科学交流的透明度和可验证性。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [9] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: Genesys是一个多代理LLM系统，通过模拟研究流程（从提案到验证）和遗传编程，高效发现新型语言模型架构，性能优于已知架构。


<details>
  <summary>Details</summary>
Motivation: 利用LLM模拟研究流程，探索新型语言模型架构的发现过程，以提高效率和性能。

Method: 采用多代理LLM方法，结合遗传编程和Ladder of Scales策略，逐步验证设计。

Result: 生成了1,162个新设计，其中1,062个通过预训练验证，部分设计在6/9基准测试中优于GPT2和Mamba2。

Conclusion: Genesys展示了自主发现系统的有效性，为高效架构设计提供了新思路。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [10] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: 提出一个基于Bloom分类法的14任务框架，评估LLM在企业环境中的能力，开发了可扩展的标注流程，并构建了9700样本的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如MMLU）未能充分评估企业特定任务的复杂性，需要更全面的评估方法。

Method: 结合LLM-as-a-Labeler、LLM-as-a-Judge和CRAG技术，构建可扩展的标注流程和基准测试。

Result: 开源模型（如DeepSeek R1）在推理任务中表现接近专有模型，但在判断任务中落后，可能因过度思考。

Conclusion: 该研究为企业提供了定制化评估的蓝图，并推动了LLM的实际部署。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [11] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 论文提出Mobile-R1方法，通过多轮交互式强化学习与任务级奖励，提升移动代理的探索与纠错能力，显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于离线强化学习或动作级奖励的在线优化，限制了代理与环境的动态交互，导致局部最优和探索能力不足。

Method: 采用三阶段训练框架：初始格式微调、基于动作级奖励的单步在线训练、基于任务级奖励的多轮轨迹在线训练。

Result: 收集了28个中国应用的24,521条高质量标注数据，并建立500条轨迹的新基准，性能显著提升。

Conclusion: Mobile-R1通过任务级奖励和多轮交互强化学习，有效提升了移动代理的探索与纠错能力，资源将开源。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [12] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: REFeat是一种新方法，通过多类型推理引导LLM生成多样且信息丰富的特征，提升表格数据的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的特征生成方法常因固有偏见和缺乏结构化推理指导，生成过于简单或重复的特征。

Method: 提出REFeat方法，利用多类型推理引导LLM生成特征。

Result: 在59个基准数据集上，REFeat不仅平均预测准确率更高，还发现了更多样且有意义的特征。

Conclusion: 结合丰富推理范式和自适应策略选择，REFeat为LLM驱动的表格数据特征发现提供了新方向。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [13] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 本文介绍了Paladin-mini（3.8B参数的开源分类器）和grounding-benchmark（新评估数据集），用于解决上下文中的声明验证问题。


<details>
  <summary>Details</summary>
Motivation: 解决在给定上下文中验证声明是否得到支持的问题。

Method: 提出Paladin-mini分类器和grounding-benchmark数据集，用于评估性能。

Result: 展示了Paladin-mini在当前SOTA基准上的表现，并提供可复现结果。

Conclusion: Paladin-mini和grounding-benchmark为声明验证任务提供了高效工具。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [14] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: 论文提出了一种考虑V2G技术的电动汽车路径优化问题（EVOP-V2G），旨在最大化司机利润，并通过MIP模型和两种元启发式算法（EA和LNS）实现高效求解。实验表明，该方法在真实数据上显著提升了利润。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车（EVs）的普及和V2G技术的发展，如何在满足客户需求的同时优化充放电策略成为EV服务系统的新挑战。

Method: 将问题建模为混合整数规划（MIP），并提出基于进化算法（EA）和大邻域搜索（LNS）的两种元启发式算法。

Result: 实验证明，该方法在真实数据上可将司机利润翻倍，并在小规模实例中接近最优解，大规模实例中表现良好。

Conclusion: 研究为支持电网的智能EV移动系统提供了高效且可扩展的解决方案。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [15] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: GymPN是一个基于深度强化学习的软件库，用于优化业务流程中的决策，支持部分流程可观察性和多决策建模，解决了先前工作的局限性。


<details>
  <summary>Details</summary>
Motivation: 业务流程管理系统需要优化任务分配决策，现有工具无法完全支持部分流程可观察性和多决策建模。

Method: GymPN基于深度强化学习，扩展了先前任务分配工作的功能，引入部分流程可观察性和多决策建模能力。

Result: 在八种典型业务流程决策问题模式上验证，GymPN能轻松建模问题并学习最优决策策略。

Conclusion: GymPN通过解决先前工作的局限性，实现了更现实的业务流程决策建模和优化。

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [16] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: 提出了一种混合神经细胞自动机（MNCA）框架，通过结合概率规则和内在噪声，增强了传统神经细胞自动机（NCA）的随机性建模能力，并在多个生物模拟任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统神经细胞自动机（NCA）的确定性限制了其对真实生物系统中随机性的建模能力。

Method: 提出MNCA框架，结合概率规则和内在噪声，模拟多样化的局部行为和生物过程的随机动态。

Result: 在组织生长模拟、图像形态生成鲁棒性和显微镜图像分割等任务中，MNCA表现出更强的鲁棒性、更真实的生物生长模式以及可解释的规则分割。

Conclusion: MNCA为建模随机动力系统和研究自生长过程提供了有前景的工具。

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [17] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: 论文提出了一个适用于机器设计的感知定义，强调功能性和计算性实现，同时保留主观性。


<details>
  <summary>Details</summary>
Motivation: 为AI设计有意义的感知能力，需明确其功能性和主观性，避免无意中创造具有感知的AI。

Method: 提出感知需具备断言性和质性特征，并结合当前技术探讨实现方法。

Result: 明确了功能性感知的定义及其在AI中的潜在实现路径。

Conclusion: 理解功能性感知有助于避免无意中创造具有感知的AI，或及时发现其存在。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [18] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 论文提出了一种基于案例推理增强的大型语言模型（CBR-LLM）框架，用于复杂风险场景中的规避机动决策，结合语义场景理解和历史驾驶案例检索，提升决策准确性和人类对齐性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，快速、基于情境的决策需要结合情境理解和经验推理。尽管大型语言模型（LLMs）具有强大的通用推理能力，但其在自动驾驶中的直接应用仍受限于领域适应、情境基础和缺乏经验知识等问题。

Method: 提出CBR-LLM框架，通过结合语义场景理解（来自行车记录仪视频）和相关历史驾驶案例检索，使LLMs生成情境敏感且符合人类行为的机动建议。

Result: 实验表明，该框架提高了决策准确性、理由质量和与人类专家行为的一致性。风险感知提示策略进一步提升了性能，基于相似性的案例检索优于随机采样。

Conclusion: CBR-LLM框架在复杂场景中表现出鲁棒性，有望成为智能驾驶系统的自适应且可信的决策支持工具。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [19] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: 多智能体AI框架用于可持续蛋白质生产研究，结合检索增强生成（RAG）技术，优化文献搜索和信息提取。


<details>
  <summary>Details</summary>
Motivation: 全球对可持续蛋白质的需求推动了智能工具的开发，以快速处理领域知识。

Method: 使用两个基于GPT的LLM代理：文献搜索代理和信息提取代理，探索微调和提示工程优化方法。

Result: 微调和提示工程均有效提升性能，微调效果更优（平均余弦相似度≥0.94）。

Conclusion: 多智能体AI系统在可持续蛋白质研究中具有潜力，已开发用户界面并探索扩展功能。

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [20] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen是一个学习者中心的AI架构，将编程视频转化为交互式、自适应的学习体验，结合学生建模和生成式AI辅导。


<details>
  <summary>Details</summary>
Motivation: 通过结合学生建模和生成式AI，提升视频编程教育的交互性和适应性。

Method: 采用三部分架构：视频按学习目标分段、基于认知学徒制的对话辅导引擎、贝叶斯知识追踪的学生模型。

Result: 技术评估显示视频分段准确且教学策略有效，消融实验验证各组件必要性。

Conclusion: CogGen通过结合结构化学生建模和交互式AI对话，推动了AI辅导的发展，为视频编程教育提供了可扩展方案。

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [21] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: PETSc团队利用LLM（如RAG和重排序算法）构建系统，整合分散知识库，提升用户和开发者对PETSc资源的访问与利用。


<details>
  <summary>Details</summary>
Motivation: PETSc积累了30年的分散知识（代码、文档、邮件等），但大多非正式且难以访问，需通过AI工具激活这些资源。

Method: 结合RAG、重排序算法和聊天机器人，设计系统架构，评估不同LLM和嵌入模型，优化用户界面。

Result: 初步验证LLM工具可提升数值软件开发和使用效率，重点关注可扩展Krylov求解器。

Conclusion: 目标是建立可扩展的AI框架，支持科学软件开发，未来将扩展为更强大的知识平台。

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [22] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: MLE-Live框架评估ML代理在模拟Kaggle社区中的协作能力，CoMind代理在社区环境中表现出色，超越79.2%的人类竞争者。


<details>
  <summary>Details</summary>
Motivation: 现有ML代理通常孤立工作，缺乏与社区的互动，而人类研究者通过分享知识获得洞察。

Method: 引入MLE-Live框架评估代理的协作能力，并提出CoMind代理，在模拟社区中交换见解和开发新方案。

Result: CoMind在MLE-Live上表现优异，平均超越79.2%的人类竞争者。

Conclusion: CoMind展示了在社区环境中协作的潜力，为自动化ML研究提供了新方向。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [23] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: Decrypto是一个基于游戏的多智能体推理和心智理论（ToM）基准测试，旨在解决现有基准测试的局限性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）具备代理能力，需要评估其在多智能体场景中的推理和心智理论能力，但现有基准测试存在范围狭窄、数据泄露等问题。

Method: 提出Decrypto基准测试，结合认知科学、计算语用学和多智能体强化学习，设计交互式ToM实验平台，并通过实验验证其有效性。

Result: 实验发现，前沿LLMs在多智能体推理和ToM任务中表现不如人类和简单词嵌入基线模型，且最新推理模型表现不如旧版本。

Conclusion: Decrypto填补了当前推理和ToM评估的关键空白，为开发更优的人工智能代理铺平了道路。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: CycleDistill利用LLMs和少样本翻译，通过迭代生成合成平行语料库，无需大量平行语料即可提升低资源语言的机器翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言因缺乏平行语料库而难以实现高质量机器翻译的问题。

Method: 提出CycleDistill方法，通过零样本或少样本翻译从单语语料库生成合成平行语料库，并用于微调模型。

Result: 在三种印度语言上，CycleDistill显著提升翻译质量，首次迭代平均提高20-30 chrF点。

Conclusion: CycleDistill为低资源语言提供了一种高效的机器翻译解决方案，且通过软激活进一步优化翻译质量。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [25] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 论文提出了一种名为Inference-Scaled GraphRAG的新框架，通过推理时计算扩展增强LLM在图推理中的表现，显著提升了多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在知识密集型推理任务中表现不佳，主要因为缺乏对结构化上下文和多跳信息的访问。传统的RAG和GraphRAG方法难以捕捉知识图谱中节点间的关联结构。

Method: 结合了顺序扩展（深度链式图遍历）和并行扩展（多数投票采样轨迹），在交替推理-执行循环中实现推理时计算扩展。

Result: 在GRBench基准测试中，该方法显著优于传统GraphRAG和其他图遍历基线，提升了多跳问答性能。

Conclusion: 推理时扩展是一种实用且与架构无关的解决方案，适用于LLM的结构化知识推理。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [26] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent是一个可扩展的流程，用于从API文档构建工具代理，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有API代理工具集单一且无法应对现实API复杂性的问题。

Method: 通过从API文档生成可执行工具，并利用代码代理迭代优化。

Result: 在WebArena基准测试中性能提升55%，成本降低90%，并在特定领域任务中验证了适应性。

Conclusion: Doc2Agent为从非结构化API文档大规模构建工具代理提供了通用解决方案。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [27] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: 结合大型语言模型和主题模型动态分析叙事变化，提出一种高效方法，但区分内容与叙事变化的能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着媒体叙事的快速演变，需要动态分析叙事发展，而现有方法在成本或规模上存在限制。

Method: 结合主题模型和变化点检测方法筛选代表性文档，再用大型语言模型自动解析变化类型。

Result: 大型语言模型能高效提取叙事变化，但在区分内容与叙事变化时表现不佳。

Conclusion: 提出的方法在动态叙事分析中有效，但需进一步改进区分能力。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [28] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason框架结合大语言模型和时空模型，通过上下文学习分解复杂查询为模块化程序，显著提升多任务时空推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有时空数据挖掘模型局限于单一任务，缺乏多任务推理和深度解释能力，难以应对复杂现实场景。

Method: STReason利用大语言模型的推理能力和时空模型的分析能力，通过上下文学习分解查询为模块化程序，生成解决方案和详细解释。

Result: 实验表明STReason在复杂时空推理任务中显著优于基线模型，并通过人工评估验证其实用性。

Conclusion: STReason为开发更通用、强大的时空推理系统提供了新方向。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [29] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 论文分析了代码检索中的问题，提出了一种减少偏见并增强语义信息的框架SACL，显著提升了代码检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前代码检索器过度依赖表面文本特征（如文档字符串、标识符名称）并对文档丰富的代码存在偏见，影响了代码生成的效果。

Method: 通过系统性地屏蔽特定特征并保留代码功能，分析代码检索问题，提出SACL框架，结合语义信息增强文本和结构知识。

Result: SACL显著提升了代码检索性能（如HumanEval上Recall@1提升12.8%），并改善了代码生成效果（如HumanEval上Pass@1提升4.88%）。

Conclusion: SACL通过减少偏见和增强语义信息，有效提升了代码检索和生成性能，为相关领域提供了新思路。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [30] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 该论文探讨了如何通过结合组合性和符号性属性来提升Transformer自回归语言模型的解释性、可控性和泛化能力，提出了一种称为“语义表示学习”的新方向。


<details>
  <summary>Details</summary>
Motivation: 旨在缩小符号语义和分布语义之间的差距，增强语言模型的语义表示能力。

Method: 综述并比较了三种主流自编码器架构（VAE、VQVAE和SAE）及其在语义结构和解释性方面的潜在几何特性。

Result: 通过分析不同自编码器的潜在几何特性，为语义表示学习提供了新的视角。

Conclusion: 语义表示学习为连接符号和分布语义提供了桥梁，有望提升语言模型的语义能力。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [31] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: 论文提出了Time-Series QA任务和EngineMT-QA数据集，并开发了ITFormer框架，用于高效整合时间序列数据与自然语言，显著提升了QA准确性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在工业、医疗和气候研究中至关重要，但如何将其与自然语言动态结合仍具挑战性。

Method: 提出ITFormer框架，结合时间序列编码器和冻结的大型语言模型，提取、对齐和融合时空与文本特征。

Result: ITFormer在QA任务中显著优于基线模型，且仅需不到1%的可训练参数。

Conclusion: 该研究为多模态AI中时间序列与自然语言的整合提供了高效范例，推动了相关研究和应用。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [32] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode框架通过强化学习提升LLMs在动态API环境中的代码生成能力，减少对过时API知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因依赖过时API知识而无法适应动态库更新的问题。

Method: 构建数据集训练LLMs进行版本迁移，引入修改后的字符串相似度指标作为强化学习的奖励。

Result: ReCode显著提升LLMs在动态API场景中的性能，且对通用代码生成能力影响较小。

Conclusion: ReCode是一种有效的解决方案，适用于多种LLMs和强化学习算法。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [33] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: 三阶段LLM框架显著提升放射报告校对的正预测值（PPV）并降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 由于错误率低，现有LLM校对放射报告的正预测值有限，需改进方法以提高效率和准确性。

Method: 研究比较了三种LLM框架（单提示检测器、提取器加检测器、三阶段框架），通过回顾性分析和外部数据集验证其性能。

Result: 三阶段框架PPV显著提升至0.159，运营成本降低42.6%，同时保持检测性能稳定。

Conclusion: 三阶段LLM框架为AI辅助放射报告质量保证提供了高效策略。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [34] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 提出了一种利用自动评分技术填补缺失分数的新方法，以提高IRT能力估计的准确性，同时显著减少人工评分工作量。


<details>
  <summary>Details</summary>
Motivation: 评估学习者的高阶能力（如表达能力和逻辑思维）需求增加，但构建性反应测试（如简答和论述题）需要大量人工评分，成本高且耗时。IRT虽能从不完整分数数据估计能力，但随着缺失分数比例增加，准确性下降。

Method: 提出一种新方法，利用自动评分技术填补缺失分数，以改善IRT能力估计的准确性。

Result: 该方法在能力估计中实现了高准确性，并显著减少了人工评分的工作量。

Conclusion: 该方法为解决IRT在稀疏或异构数据中准确性不足的问题提供了有效解决方案，同时降低了人工评分成本。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [35] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: 论文提出了一种名为CCRS的新型评估框架，利用预训练LLM作为零样本端到端评判器，用于评估RAG系统的多维度质量，包括上下文连贯性、问题相关性等五个指标。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估方法存在局限性，如依赖简单词汇重叠指标或复杂多阶段流程，难以全面捕捉输出质量。

Method: 提出CCRS框架，包含五个指标（CC、QR、ID、AC、IR），使用单一预训练LLM进行零样本评估。

Result: 在BioASQ数据集上验证，CCRS能有效区分不同RAG系统性能，且计算效率优于现有方法。

Conclusion: CCRS为RAG系统提供了一种实用、全面且高效的评估框架。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [36] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: AALC是一种轻量级、准确性感知的长度奖励方法，通过动态平衡正确性和简洁性，显著减少推理模型的响应长度，同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过生成冗长的思维链实现强大推理能力，但这种方法导致高延迟和成本，且准确性提升有限。

Method: AALC将验证准确性纳入奖励，并采用动态调度的长度惩罚，延迟长度惩罚直至达到目标性能。

Result: 实验表明，AALC将响应长度减少50%以上，同时保持或提升准确性，并减少冗余推理模式。

Conclusion: AALC展示了基于奖励的策略在引导LRMs实现更高效、通用推理路径方面的潜力，但也可能牺牲部分可解释性。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [37] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED是一种结构编码器，通过四个阶段整合时间序列与语言模型，解决了结构依赖与语义推理之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有结构编码器无法支持语义级推理或任务适应，而大型语言模型（LLMs）无法直接处理原始时间序列数据，限制了统一预测系统的发展。

Method: SEED包括四个阶段：标记感知编码器提取补丁、投影模块对齐补丁与语言模型嵌入、语义重编程机制将补丁映射到任务感知原型、冻结语言模型进行预测。

Result: 实验表明，SEED在多个数据集上优于基线方法，有效填补了结构-语义建模的空白。

Conclusion: SEED通过解耦表示学习与推理，实现了数值模式与语义推理的高效对齐，为统一预测系统提供了可行方案。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [38] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN是一个不确定性保护选择框架，通过统计校准阈值在用户指定的FDR约束下筛选生成答案，显著提高样本保留率。


<details>
  <summary>Details</summary>
Motivation: 解决现有启发式不确定性量化方法缺乏形式化保证的问题，特别是选择性预测中的假发现率（FDR）控制。

Method: 采用校准集估计经验错误率，并应用Clopper-Pearson等置信区间方法建立真实错误率的高概率上界，以选择最大不确定性阈值。

Result: COIN在风险控制、测试时保留可接受答案的能力以及有限校准数据下的预测效率方面表现出色。

Conclusion: COIN框架具有扩展性和适应性，适用于多样化应用场景，并能通过替代上界构建和不确定性量化策略进一步提升性能。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [39] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本文研究了如何通过检索高质量示例来提升大型语言模型在对话情感识别任务中的表现，发现增强示例检索方法优于其他技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多个领域有广泛应用，但在主观任务（如情感识别）中实现高准确性仍具挑战性。

Method: 研究了随机和增强示例检索策略，并分析了对话上下文对情感识别准确性的影响。

Result: 实验表明，增强示例检索方法在所有数据集上均优于其他技术。

Conclusion: 检索具有相关性和针对性的示例并通过改写增强，对提升对话情感识别准确性至关重要。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [40] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: 比较捷克语特定和多语言句子嵌入模型，发现内在语义相似性测试表现好的模型在下游翻译任务中不一定表现最佳，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 探讨句子嵌入模型在内在和外在评估中的表现差异，揭示语义属性与下游任务之间的复杂关系。

Method: 使用Costra数据集和STS基准进行内在评估，通过COMET指标进行机器翻译评估的外在评估。

Result: 内在表现优异的模型在外在任务中不一定表现好，而某些看似平滑的嵌入模型通过微调却能取得优异结果。

Conclusion: 需要更多关于句子嵌入中'可操作语义'的研究，或更深入的下游任务数据集。

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [41] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: 论文提出了一种多视角的软标签方法，以更好地捕捉人类标注者的多样性观点，提升主观文本分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过聚合标注者观点生成单一真实标签，但可能忽视少数观点。本研究旨在开发更包容和多元的模型。

Method: 采用多视角软标签方法，分析多个主观文本分类任务（如仇恨言论、讽刺等），并与传统方法对比。

Result: 多视角方法在Jensen-Shannon Divergence和F1分数上优于传统方法，但在讽刺和立场检测任务中置信度较低。

Conclusion: 多视角方法能更好地反映人类观点多样性，但需进一步解决高主观性任务中的不确定性。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [42] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 论文提出了一种通过结构化推理增强大语言模型（LLMs）的新方法，结合了监督微调和GRPO优化算法，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂逻辑推理和系统规划任务中存在困难，主要因其依赖隐式统计关系而缺乏结构化知识表示。

Method: 将非结构化数据转换为结构化格式，通过监督微调（SFT）训练LLMs，并利用GRPO优化算法（MAX-Flow和LCS）增强推理能力。

Result: 实验结果显示，该方法在DeepSeek-R1-Distill-Qwen-1.5B模型上实现了简洁推理、鲁棒性能和优化兼容性。

Conclusion: 结构化推理的集成有效提升了LLMs的推理能力，验证了方法的有效性。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [43] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 提出了一种基于分块的自动流畅度评估方法，结合多种自监督学习模型和分层CNN-BiLSTM框架，显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 自动流畅度评估（AFA）在非母语者中难以捕捉语音节奏、停顿和不流畅现象，需要更精细的方法。

Method: 使用Silero-VAD将语音分段为呼吸组块，融合多种SSL模型的嵌入，并通过CNN-BiLSTM框架分析局部和长期依赖关系。

Result: 在Speechocean762和Avalinguo数据集上，F1分数和Pearson相关系数分别提升了2.8/6.2和4.2/4.0。

Conclusion: 分块多SSL融合方法在流畅度评估中表现优异，但未来需研究其在非规则韵律方言中的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [44] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: Biomed-Enriched是一个通过两阶段标注过程从PubMed构建的生物医学文本数据集，包含临床案例等高质量段落，为生物医学和临床NLP提供资源。


<details>
  <summary>Details</summary>
Motivation: 临床文本通常因隐私问题难以获取，Biomed-Enriched提供了一个公开的大规模临床案例数据集。

Method: 通过大语言模型标注PubMed段落的类型、领域和教育质量，再用小模型扩展标注至整个PMC-OA语料库，提取高质量子集。

Result: 实验表明，临床案例子集和高质量过滤能提升模型性能，组合方法还能加速收敛。

Conclusion: Biomed-Enriched为生物医学预训练提供了高效且有效的资源和方法。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [45] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: 论文提出了一种新方法TAPS，通过结构化标记工具和基于不确定性的工具检测器，提升大型语言模型在个性化工具使用上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了个性化在工具使用中的作用，研究旨在探索如何有效整合用户偏好到目标导向对话代理中。

Method: 引入TAPS，结合结构化标记工具和不确定性工具检测器，优化个性化工具使用。

Result: TAPS显著提升了LLMs整合用户偏好的能力，在NLSI任务上达到开源模型的新SOTA。

Conclusion: TAPS为个性化工具使用提供了有效解决方案，推动了相关领域的发展。

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [46] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare是一种基于大型语言模型（LLM）的罕见病诊断系统，通过处理异构临床输入生成诊断假设，并在多个数据集中表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临临床异质性、低患病率和医生知识有限的挑战，亟需高效准确的诊断工具。

Method: DeepRare由中央主机、长期记忆模块和专用代理服务器组成，整合40多种工具和最新医学知识，支持复杂诊断推理。

Result: 在2919种疾病中，DeepRare对1013种疾病达到100%准确率，显著优于其他15种方法，并在多模态输入场景中表现优异。

Conclusion: DeepRare是一种高效、透明的罕见病诊断系统，已实现为易用的网络应用，具有广泛临床潜力。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [47] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: 论文提出了一种名为Code of Thought (CoDoT)的提示策略，用于评估大型语言模型（LLMs）的安全性，发现当前模型在安全性方面存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在安全关键应用中广泛使用，需要提升其能力的同时确保安全性，但现有模型在安全性方面表现不佳。

Method: 通过CoDoT将自然语言输入转换为代码形式，以评估模型的安全性。例如，将“使文本更具毒性”转换为“make_more_toxic({text})”。

Result: 实验显示，CoDoT导致多种先进LLMs的安全性显著下降，例如GPT-4 Turbo的毒性增加16.5倍，DeepSeek R1失败率100%。

Conclusion: CoDoT揭示了当前LLMs在安全性上的严重缺陷，强调了从基本原理出发评估安全性的重要性，以确保安全性与能力同步提升。

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [48] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 论文提出了一种计算框架，用于量化对话中说话时间的分布及其动态过程，并通过实验验证了平衡对话更受欢迎，同时揭示了不同动态类型对参与者感知的影响。


<details>
  <summary>Details</summary>
Motivation: 研究对话中说话时间分配的动态过程及其对参与者感知的影响，为设计计算机辅助沟通平台提供新工具。

Method: 引入计算框架量化说话时间分布及其动态，通过视频聊天数据集验证框架有效性。

Result: 平衡对话更受欢迎，不同动态类型对参与者感知有显著影响。

Conclusion: 该框架为设计沟通平台提供了新视角，适用于人机交互和人际沟通。

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [49] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: Team Marikarp在SIGIR 2025 LiveRAG竞赛中获胜，其知识感知多样化重排RAG管道在15M文档子集中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 竞赛旨在公平评估从大规模文档中检索问题相关支持文档的能力，涵盖多样化的主题、问题类型和受众。

Method: 提出知识感知多样化重排RAG管道。

Result: 在竞赛中取得第一名。

Conclusion: 该方法在多样化文档检索任务中表现出色。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [50] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: 提出一种通过合并微调模型变体的层来压缩大型语言模型的新策略，显著减少参数并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然能力强，但模型规模大，部署和推理成本高。现有剪枝方法主要针对单一模型，无法充分利用微调变体的能力。

Method: 将模型压缩问题转化为零阶优化问题，支持三种操作：层移除、从候选模型中选择层、层合并。

Result: 在Llama2-13B模型上，压缩后模型参数减少约25%，性能保留97.3%，优于现有方法。

Conclusion: 该方法通过合并微调变体的层，实现了高效模型压缩，为LLMs的轻量化部署提供了新思路。

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [51] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 研究探讨了不同基础语言模型（如Llama和Qwen）在强化学习（RL）训练中的表现差异，提出了两阶段中训练策略Stable-then-Decay，并开发了OctoThinker模型家族，显著提升了RL兼容性。


<details>
  <summary>Details</summary>
Motivation: 探究基础语言模型在强化学习中的适用性，为下一代RL可扩展的基础模型开发提供指导。

Method: 比较Qwen和Llama模型家族，分析中训练策略对RL动态的影响，并提出两阶段训练策略Stable-then-Decay。

Result: 高质量数学语料（如MegaMath-Web-Pro）显著提升模型性能；长链推理数据增强RL效果，但需注意数据格式化；中训练规模扩大持续改善RL表现。

Conclusion: 提出的Stable-then-Decay策略和OctoThinker模型家族有效缩小了与RL友好模型（如Qwen）的性能差距，为RL时代的基础模型预训练策略提供了新方向。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [52] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: 论文研究了如何通过调整采样和选择策略，在多语言和多任务环境中高效扩展推理计算，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对英语和特定领域（如数学和代码），缺乏对多语言和开放任务的泛化能力。本文旨在填补这一空白。

Method: 提出新的采样和选择策略，适应多语言和多任务场景，并在8B和111B模型上进行验证。

Result: 新策略在m-ArenaHard-v2.0基准测试中显著提升胜率（8B模型+6.8，111B模型+9.0）。

Conclusion: 强调需要针对语言和任务定制推理计算策略，以提升非主流语言的性能。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [53] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: 论文提出了一种名为Behavior Editing的方法，通过模型编辑技术动态调整LLM代理的行为，以解决伦理和安全风险，并引入了多层次的评估基准BehaviorBench。


<details>
  <summary>Details</summary>
Motivation: LLM代理在高风险领域部署时存在伦理和安全风险，可能导致严重后果，因此需要一种高效的方法来引导其行为。

Method: 将代理行为引导问题建模为模型编辑任务，提出Behavior Editing方法，并开发了基于心理学道德理论的多层次基准BehaviorBench。

Result: Behavior Editing能动态调整代理行为，支持局部和全局道德对齐的调整，且在不同模型和场景中均有效。

Conclusion: Behavior Editing为代理行为引导提供了新范式，展示了其潜力与风险。

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [54] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: DiffuCoder是一种7B参数的扩散大语言模型（dLLM），用于代码生成，通过系统研究其去噪过程和强化学习方法，揭示了其与自回归模型的不同之处，并提出了一种新的采样方案coupled-GRPO以提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索扩散大语言模型（dLLMs）在代码生成中的潜力，解决其训练和推理机制尚未充分研究的问题。

Method: 训练7B参数的dLLM（DiffuCoder），分析其解码行为，并提出coupled-GRPO采样方案以减少方差并提升训练效率。

Result: DiffuCoder在代码生成基准测试中性能提升4.4%，并减少了对自回归因果性的依赖。

Conclusion: 研究揭示了dLLM的生成机制，并提供了一个有效的扩散原生强化学习训练框架。

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [55] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: Memento是一种提示策略，通过分解问题、动态构建事实数据库并整合信息，显著提升了多跳问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在需要紧密耦合检索与推理的任务（如多跳问答）中的局限性。

Method: 采用三阶段策略：分解问题、动态构建事实数据库、整合信息。

Result: 在多个基准测试中显著提升性能，如PhantomWiki（性能翻倍）、2WikiMultiHopQA（F1提升20%以上）、MuSiQue（F1提升3%以上）。

Conclusion: Memento策略在多跳问答任务中表现出色，能够有效提升现有提示策略的性能。

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [56] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: 论文探讨了如何在LLMs中模拟人类在社交情境中的价值权衡，通过认知模型评估LLMs是否体现类似人类的权衡行为。


<details>
  <summary>Details</summary>
Motivation: 当前工具难以捕捉LLMs中动态、多方面的价值权衡，而人类社交决策中的价值权衡是重要研究课题。

Method: 采用认知模型分析LLMs的价值权衡，评估不同推理努力和RL后训练动态下的表现。

Result: 发现推理模型更注重信息效用而非社交效用，训练早期价值权衡变化显著，受基础模型和预训练数据影响较大。

Conclusion: 该方法有助于理解LLMs的高层行为，优化训练策略，并控制价值权衡。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文评估了五种先进的目标检测架构在三个历史文档数据集上的性能，发现Transformer和CNN-OBB模型各有优势，OBB对非笛卡尔布局至关重要。


<details>
  <summary>Details</summary>
Motivation: 历史文档布局复杂，自动化处理需要稳健的文档布局分析（DLA），本文旨在比较不同模型在此任务上的表现。

Method: 在三个数据集（e-NDP、CATMuS、HORAE）上测试了两种Transformer模型（Co-DETR、Grounding DINO）和三种YOLO变体（AABB、OBB、YOLO-World）。

Result: Co-DETR在e-NDP上表现最佳（0.752 mAP），而YOLOv11x-OBB在更复杂的CATMuS和HORAE上显著优于其他模型（0.564和0.568）。

Conclusion: Transformer适合结构化布局，而CNN-OBB模型在复杂文档上泛化能力更强，OBB对历史文档分析至关重要。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [58] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: 开发了一种基于计算机视觉的系统，用于量化农业喷雾器的喷杆运动，以提高喷雾精度。


<details>
  <summary>Details</summary>
Motivation: 喷雾器喷杆的不稳定性是喷雾应用误差的主要因素之一，但目前缺乏定量数据来指导喷杆设计和控制系统开发。

Method: 使用YOLO V7、V8和V11神经网络模型实时跟踪喷杆边缘的目标，并结合倾角传感器验证模型输出。

Result: 模型检测目标的准确率超过90%，距离估计与传感器数据误差在0.026米内。

Conclusion: 该系统可量化喷杆运动，为喷杆设计和稳定性改进提供数据支持。

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [59] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: 论文提出EBC-ZIP框架，通过零膨胀泊松回归改进人群计数中的密度图估计，解决现有方法对稀疏区域处理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数方法忽视了真实场景中密度图的极端稀疏性（95%以上区域无人），导致模型在稀疏区域表现不佳。此外，传统损失函数（如MSE）假设高斯分布，不适合离散计数数据。

Method: 提出EBC-ZIP框架，采用零膨胀泊松（ZIP）回归建模计数分布，取代传统回归损失，并结合增强块分类（EBC）框架的优势。

Result: 在四个基准测试中，EBC-ZIP表现优于EBC，并达到最先进水平。

Conclusion: EBC-ZIP通过更合理的概率损失函数，显著提升了稀疏区域的计数准确性，同时保持了模型的稳定性和可扩展性。

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [60] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA是一种结合语义和空间感知的Token合并方法，通过深度图像生成伪空间Token，优化ViT的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有Token合并方法主要依赖特征相似性，忽略了早期层中空间信息的重要性。

Method: 利用深度图像生成伪空间Token，结合语义和空间信息指导Token合并。

Result: 在多个视觉和具身问答基准上表现优于现有方法，同时显著减少ViT运行时间。

Conclusion: ToSA是一种高效的ViT加速解决方案，能更好地保留关键场景结构。

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [61] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文提出了BrokenVideos数据集，用于定位AI生成视频中的视觉伪影，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: AI生成视频中存在视觉伪影（如运动不一致、物体变形等），影响真实性和用户信任，但缺乏专门用于伪影定位的基准数据集。

Method: 构建了包含3,254个AI生成视频的数据集BrokenVideos，提供像素级标注的伪影区域，并通过人工验证确保标注质量。

Result: 实验表明，在BrokenVideos上训练的模型能显著提升伪影定位能力。

Conclusion: BrokenVideos为生成视频模型的伪影定位研究提供了重要基准。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [62] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: 该论文综述了从2D感知到3D认知的世界模型发展，提出了一个概念框架，重点分析了3D表示和世界知识两大技术驱动力，并探讨了3D世界建模的三大核心能力及其实际应用。


<details>
  <summary>Details</summary>
Motivation: 填补3D认知世界模型领域缺乏系统性分析的空白，推动从2D感知到3D认知的过渡。

Method: 引入概念框架，分类新兴技术，分析3D表示和世界知识两大驱动力，并探讨3D世界建模的三大核心能力。

Result: 总结了3D世界模型在物理场景生成、空间推理和交互方面的进展，并分析了其在具体应用中的部署。

Conclusion: 指出了数据、建模和部署方面的挑战，并提出了未来研究方向，以推动更稳健和通用的3D世界模型发展。

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [63] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EAR的微调方法，用于在自回归模型中实现高效且保留性能的概念擦除，并提出了新的评估基准ECGVF。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在视觉理解和图像生成任务中表现优异，但如何在保持生成质量的同时去除不需要的概念仍是一个挑战。

Method: EAR方法通过窗口梯度累积（WGA）策略和对阈值损失掩码（TLM）策略，实现概念擦除并保护无关内容。

Result: 在ECGVF基准测试中，EAR显著提升了概念擦除效果和模型性能保留。

Conclusion: EAR为自回归模型中的概念擦除提供了有效且实用的解决方案。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [64] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: 本文提出了一种高效的损失感知自动选择结构化剪枝标准（LAASP）方法，用于压缩和加速深度神经网络。该方法通过剪枝与训练结合的方式，自动选择剪枝标准和层，显著减少了计算量并保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的边缘设备上部署神经网络，需要一种高效的剪枝方法，既能减少计算量（FLOPs），又能保持模型精度。

Method: 提出了一种剪枝与训练结合的方法（LAASP），自动选择剪枝标准和层，并通过损失函数指导剪枝过程。每减少一定FLOPs后，网络会进行短暂重训练以缓解精度下降。

Result: 在CIFAR-10和ImageNet数据集上，ResNet56和ResNet110模型的FLOPs减少了52%，且精度优于现有方法；ResNet50的FLOPs减少了42%，仅损失0.33%的top-5精度。

Conclusion: LAASP方法在减少计算量的同时保持了高精度，适用于资源受限的设备部署。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [65] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: 本文提出了一种基于示例对的图像编辑方法，利用预训练的文本到图像扩散模型和多模态VLM，无需优化即可高效完成编辑任务。


<details>
  <summary>Details</summary>
Motivation: 仅通过文本描述难以捕捉所有类型的图像编辑需求，而示例对能更直观地表达模糊的编辑意图。

Method: 结合预训练的文本到图像扩散模型和多模态VLM，构建端到端无优化的编辑流程。

Result: 实验表明，该方法在多种编辑类型上优于基线方法，且速度快约4倍。

Conclusion: 该方法为基于示例的图像编辑提供了高效且性能优越的解决方案。

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [66] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: 论文提出KIE-HVQA基准和GRPO框架，用于评估和改进多模态大语言模型在视觉退化条件下的文档理解能力，减少幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 现有模型在视觉退化场景下表现不佳，容易产生幻觉内容，缺乏对不确定性的感知能力。

Method: 提出KIE-HVQA基准测试数据集，并设计GRPO框架，结合视觉不确定性自感知和拒绝回答机制。

Result: 实验显示，7B参数模型在KIE-HVQA上比GPT-4o幻觉减少22%，且标准任务性能无显著下降。

Conclusion: GRPO框架有效提升了模型在视觉退化条件下的鲁棒性，减少了幻觉生成。

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [67] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: 研究探讨了如何结合预训练的遥感与通用视觉基础模型，以提升地球观测任务的性能，发现特征级集成小型模型可媲美或超越大型模型，且更高效。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型的复用与组合策略，以替代从头训练大型模型，提高地球观测数据挖掘的效率和性能。

Method: 使用GEO-Bench基准测试，评估Prithvi、Hiera和DOFA等模型在11个数据集上的表现，采用特征级集成和知识蒸馏技术。

Result: 特征级集成小型预训练模型的性能可匹敌或超越大型模型，同时减少训练时间和计算资源。知识蒸馏可将集成优势转移至更紧凑模型。

Conclusion: 预训练模型的组合与知识蒸馏为地球观测应用提供了高效且实用的解决方案。

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [68] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 论文探讨了基于深度学习的全色锐化方法，发现Wald协议生成的数据限制了模型的泛化能力，提出PADM和HFreqdiff模块以改进性能。


<details>
  <summary>Details</summary>
Motivation: Wald协议生成的数据无法准确模拟真实世界的退化模式，限制了全色锐化模型的泛化能力。

Method: 提出PADM模块，通过两个子网络自适应学习退化过程；引入HFreqdiff模块，结合CFB和BACM模块提取高频细节。

Result: 实验表明，该方法在空间清晰度和图像质量上显著优于现有技术。

Conclusion: PADM和HFreqdiff模块有效提升了全色锐化性能，解决了Wald协议的局限性。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [69] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode²提出了一种级联码本框架，用于大规模、语义对齐且稳定的视觉标记化，解决了现有方法在词汇量小或训练不稳定上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于码本的方法要么词汇量小（约16K条目），缺乏细粒度语义，要么盲目扩展导致标记利用率低和训练不稳定。

Method: 通过聚类数百万SigLIP序列嵌入构建500K条目的码本，采用级联设计：冻结码本锚定嵌入空间，可训练码本细化任务特定语义。

Result: UniCode²在多样化基准测试中表现优异，支持高质量视觉合成，且无需牺牲稳定性、语义或模块性。

Conclusion: UniCode²证明了在不牺牲稳定性、语义或模块性的前提下扩展视觉标记空间的可行性。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [70] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: 提出了一种联合事件和图像（E-I）传输框架，通过贝叶斯建模和信息瓶颈方法消除冗余，优化带宽利用，同时实现实时去模糊。


<details>
  <summary>Details</summary>
Motivation: 混合系统中事件相机和RGB相机传输大量数据存在挑战，且两者捕获方式不同导致信息冗余。

Method: 使用贝叶斯建模和信息瓶颈方法分离共享和领域特定信息，动态分配传输带宽。

Result: 仿真结果表明，该方案在重建质量和去模糊性能上优于传统系统。

Conclusion: 提出的框架有效解决了混合系统的传输冗余问题，并提升了性能。

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [71] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA是一个轻量级框架，通过少量标注和自然语言定义，实现跨机构和跨手术的工作流理解，提升手术阶段识别的性能。


<details>
  <summary>Details</summary>
Motivation: 手术工作流的复杂性和多样性限制了通用模型的开发，现有基础模型在零样本性能上受限于领域偏移。

Method: SPA通过少样本空间适应、扩散建模确保时间一致性，以及动态测试时适应，调整基础模型以适应特定机构环境。

Result: SPA在少样本手术阶段识别中表现优异，甚至超过全样本模型。

Conclusion: SPA为医院提供了一种快速定制手术阶段识别模型的轻量级解决方案。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [72] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 提出一种融合手写识别中离线图像和在线笔画数据的端到端网络，通过共享潜在空间实现早期融合，提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 手写识别通常仅利用单一模态（图像或笔画轨迹），而忽略了互补信息。本文旨在通过融合两种模态提升性能。

Method: 设计端到端网络，将离线图像转换为视觉标记，在线笔画数据通过轻量级Transformer嵌入，并在共享潜在空间中融合。

Result: 在IAMOn-DB和VNOn-DB数据集上达到最优准确率，超过之前最佳结果1%。

Conclusion: 早期融合多模态数据能增强表示学习，提升手写识别的准确率和鲁棒性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [73] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: HMDRN提出了一种结合双层次特征重建和掩码增强的少样本细粒度图像分类方法，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少样本细粒度图像分类中存在空间信息丢失、局部特征不对齐、缺乏层次特征利用和判别区域聚焦机制等问题。

Method: HMDRN通过双层次特征重建与融合模块和空间二值掩码增强的自重建模块，平衡高层语义和中层结构信息，并增强判别区域聚焦。

Result: 在三个细粒度数据集上，HMDRN在Conv-4和ResNet-12架构上均优于现有方法，并通过消融实验验证了各模块的有效性。

Conclusion: HMDRN通过双层次重建增强类间区分，掩码增强减少类内差异，展示了优越的特征重建能力。

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [74] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的纺织品相似性评估新方法，用于艺术品中画布的鉴定与保护。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖线密度图匹配，无法适用于非连续位置的画布，需要一种更通用的方法。

Method: 设计并训练了一个Siamese深度学习模型，通过扫描图像的特征表示比较画布相似性，并提出了一种聚合预测的相似性评分方法。

Result: 在Museo Nacional del Prado的画布上验证了方法的可行性，证明即使线密度相似，平纹画布也能有效比较。

Conclusion: 该方法为艺术品分析提供了新途径，具有高准确性和实用性。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [75] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: 论文提出了DenseWorld基准和DenseDiT方法，用于解决密集预测任务在真实场景中的泛化问题，并通过实验验证了其高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 密集预测任务在计算机视觉中很重要，但现有方法在真实场景中泛化能力有限且数据稀缺。

Method: 提出DenseDiT方法，利用生成模型的视觉先验，通过参数重用机制和两个轻量级分支实现多尺度上下文自适应集成。

Result: 在DenseWorld基准上，DenseDiT表现优于现有基线方法，仅需不到0.01%的训练数据。

Conclusion: DenseDiT在真实场景中具有显著优势，为密集预测任务的实用部署提供了高效解决方案。

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [76] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: 提出了一种基于光谱域的未注册高光谱图像（HSI）和多光谱图像（MSI）融合方法，通过光谱特征学习和稀疏融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过空间变换对齐HSI和MSI，但由于分辨率差异大且处理耗时，效果不佳。

Method: 开发了轻量级光谱先验学习（SPL）网络提取光谱特征，提出盲稀疏融合（BSF）方法，并采用PAO算法求解。

Result: 实验验证了方法在注册和融合中的有效性，并提升了分类性能。

Conclusion: 该方法在光谱域解决了注册问题，降低了计算复杂度，提升了融合效果。

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [77] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 提出了一种名为Ctrl-Z Sampling的新采样策略，通过动态交替前向细化和后向探索，解决扩散模型在条件生成中陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成中容易陷入局部最优，导致全局不一致或条件不对齐。现有方法通过增强引导信号或调整初始噪声分布尝试解决，但效果有限。

Method: Ctrl-Z Sampling通过奖励模型识别局部最优，注入噪声并回退到更早状态以逃离。动态交替前向和后向步骤，提升生成质量。

Result: 实验表明，Ctrl-Z Sampling显著提升生成质量，仅增加约7.6倍函数评估。

Conclusion: Ctrl-Z Sampling是一种模型无关的方法，能有效提升扩散模型的条件生成质量。

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [78] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的扩散模型，用于图像恢复任务，在多个质量指标上优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决恶劣环境下图像退化（如噪声、色偏、模糊等）导致的质量下降问题，提升下游任务（如目标检测、分类）的适用性。

Method: 采用Transformer与扩散模型结合的方法，在公开数据集上评估水下图像增强、去噪和去雨任务。

Result: 模型在性能上超越现有方法，证明了扩散模型和Transformer在提升退化图像质量方面的有效性。

Conclusion: 该模型显著提升了退化图像的质量，扩展了其在需要高保真视觉数据的下游任务中的应用潜力。

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [79] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 论文提出了一种动态构建放射组学特征（指纹）的新框架，通过深度学习模型为每位患者个性化选择特征，解决了传统放射组学方法的局限性，并在多个膝关节诊断任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统放射组学方法使用固定的特征集，无法充分捕捉个体病理差异，导致性能受限。本文旨在通过个性化特征选择提升放射组学方法的泛化能力和解释性。

Method: 提出放射组学指纹框架，利用深度学习模型动态为每位患者选择特征，并与低维逻辑回归结合进行分类。

Result: 在膝关节异常、前交叉韧带撕裂和半月板撕裂等诊断任务中，该方法性能与端到端深度学习模型相当或更优，同时保持了可解释性。

Conclusion: 个性化放射组学特征选择不仅提升了诊断准确性，还增强了临床解释性，为生物标志物发现提供了新途径。

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [80] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: 论文研究了在人脸识别中普遍存在的“突发性”现象，并提出三种策略检测和抑制这种现象，以提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 突发性现象在基于集合的人脸识别中广泛存在，导致训练和评估阶段的性能下降，因此需要检测和抑制。

Method: 提出基于Quickshift++、特征自相似性和广义最大池化（GMP）的三种策略检测突发性，并在训练和评估阶段调整采样比例或贡献。评估阶段还提出质量感知GMP。

Result: 实验证明突发性普遍存在，抑制突发性显著提升了识别性能。

Conclusion: 通过检测和抑制突发性，可以有效提升基于集合的人脸识别性能。

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [81] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出了一种深度翻译动作识别框架，通过联合预测动作概念和辅助特征提升识别精度，并引入两种领域特定描述符（ODF和SDF）增强特征表示。


<details>
  <summary>Details</summary>
Motivation: 视频中的人类动作理解需要高级语义推理和多模态特征的有效整合，而现有方法在特征表示和计算效率上存在不足。

Method: 结合RGB视频帧预测动作概念和辅助特征，通过幻觉流推断缺失线索，引入ODF和SDF描述符，整合多种模态（如光流、骨架数据等），并采用不确定性建模和鲁棒损失函数。

Result: 在Kinetics-400、Kinetics-600和Something-Something V2等多个基准测试中达到最先进性能。

Conclusion: 该框架能有效捕捉细粒度动作动态，且兼容多种先进架构。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [82] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种基于失真不变特征学习的深度零水印框架，通过优化特征空间中的参考签名实现水印嵌入，同时保持原始图像不变。


<details>
  <summary>Details</summary>
Motivation: 解决传统水印方法在图像失真时鲁棒性不足的问题，同时避免对原始图像的修改。

Method: 框架包含两个模块：1）通过噪声对抗学习训练特征提取器，生成对失真不变且语义丰富的特征；2）设计基于学习的多比特零水印方案，将特征投影到优化的参考代码上以匹配目标二进制消息。

Result: 在多种图像数据集和失真条件下，该方法在特征稳定性和水印恢复方面达到最优鲁棒性，且优于现有自监督和深度水印技术。

Conclusion: 该框架在零水印任务中表现出卓越的泛化能力和鲁棒性，为图像水印提供了新思路。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [83] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT和DyHiT是高效的视觉跟踪模型，通过轻量级Transformer和动态路由技术，在保持高性能的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的视觉跟踪器在资源受限设备上速度慢的问题。

Method: HiT采用Bridge Module和双图像位置编码；DyHiT通过动态路由分类场景并选择计算路径。

Result: HiT在NVIDIA Jetson AGX上达到61 fps，AUC 64.6%；DyHiT达到111 fps，AUC 62.4%。

Conclusion: HiT和DyHiT在速度和性能上取得平衡，动态路由方法还能加速其他高性能跟踪器。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [84] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉基础模型（LVFM）的高分辨率冠层高度图（CHM）生成方法，用于低成本、高精度监测人工林地上生物量（AGB）。


<details>
  <summary>Details</summary>
Motivation: 传统激光雷达方法成本高昂，而基于RGB图像的深度学习方法在冠层高度特征提取上存在挑战，因此开发了一种更高效、准确的替代方案。

Method: 结合特征提取器、自监督特征增强模块和高度估计器，利用1米分辨率的Google Earth图像生成CHM。

Result: 在北京房山区的测试中，模型表现优于现有方法，平均绝对误差0.09米，均方根误差0.24米，与激光雷达CHM的相关系数为0.78。

Conclusion: 该方法为人工林和天然林的碳汇评估提供了可扩展的工具，具有广泛的应用潜力。

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [85] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art是一个针对医学图像生成的框架，解决了数据稀缺问题，通过结合视觉语言模型和扩散变换器，实现了高性能生成。


<details>
  <summary>Details</summary>
Motivation: 医学图像生成面临数据集小和文本数据稀缺的挑战，需要一种高效解决方案。

Method: 结合视觉语言模型生成医学图像描述，并基于PixArt-α模型提出混合级扩散微调方法（HLDF）。

Result: 在两个医学图像数据集上实现了最先进的性能（FID、KID和分类性能）。

Conclusion: Med-Art在数据有限的情况下高效生成医学图像，具有实际应用潜力。

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [86] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave是一种无需训练的方法，通过两阶段流程和基于小波的细节增强模块，显著提升预训练扩散模型在超高分辨率图像合成中的视觉保真度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本生成技术在高分辨率图像合成中常产生伪影（如物体重复和空间不连贯），HiWave旨在解决这一问题。

Method: 采用两阶段流程：1）生成基础图像；2）基于DDIM反演和小波域的细节增强模块，保留低频结构并增强高频细节。

Result: 在Stable Diffusion XL上的评估显示，HiWave有效减少伪影，用户研究中80%以上案例优于现有方法。

Conclusion: HiWave无需重新训练或修改架构，即可实现高质量超高分辨率图像合成。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [87] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 论文提出了一种名为DeepBolt的两阶段深度学习架构，用于在复杂3D点云中自动高效识别岩石螺栓，解决了传统方法在噪声、环境变化和遮挡问题上的不足，性能显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 地下矿山中岩石螺栓的频繁评估对安全至关重要，但人工检测因环境恶劣和耗时而不切实际，自动化检测成为必要。现有方法在复杂点云中表现不佳，亟需更鲁棒的解决方案。

Method: 提出DeepBolt，一种两阶段深度学习架构，专门针对严重类别不平衡问题，用于在复杂3D点云中识别岩石螺栓。

Result: DeepBolt在岩石螺栓点的IoU上比现有语义分割模型提升42.5%，分类精度和召回率分别达到96.41%和96.96%。

Conclusion: DeepBolt在复杂地下环境中表现出色，为岩石螺栓的自动化检测提供了高效且鲁棒的解决方案。

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [88] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: 提出了一种基于AI的深度学习框架，用于自动检测和量化牙槽骨流失及其模式，通过IOPA放射影像实现高精度评估。


<details>
  <summary>Details</summary>
Motivation: 牙周炎严重影响口腔健康和生活质量，准确评估骨流失的严重程度和模式对诊断和治疗规划至关重要。

Method: 结合YOLOv8进行牙齿检测和Keypoint R-CNN模型识别解剖标志，利用YOLOv8x-seg模型分割骨水平和牙齿掩模，通过几何分析确定骨流失模式。

Result: 在1000张放射影像数据集上评估，骨流失严重程度检测的类内相关系数达0.80，骨流失模式分类准确率为87%。

Conclusion: 该自动化系统为牙周评估提供了快速、客观且可重复的工具，有望改善牙周炎的早期诊断和个性化治疗规划。

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [89] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA框架通过处理压缩图像的块效应和利用配对与非配对数据，显著提升了深度伪造检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法忽视压缩引入的块效应，且主要依赖原始图像，难以应对实际场景。

Method: PLADA包含块效应消除模块（B2E）和开放数据聚合模块（ODA），分别处理块效应和混合数据。

Result: 在26个数据集上，PLADA表现优异，优于现有方法，尤其在压缩和有限配对数据情况下。

Conclusion: PLADA不仅提出块效应作为关键因素，还为开放世界场景提供了鲁棒的深度伪造检测方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [90] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出了一种简单有效的策略，通过堆叠连续帧输入YOLO检测器，仅监督目标帧输出，以利用时间信息提升检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单帧检测方法忽略视频中的时间信息，而复杂的时间模块增加模型复杂度和计算成本。

Method: 堆叠连续帧输入YOLO检测器，仅监督目标帧输出，保留架构简单性和实时性。

Result: 在MOT20Det和BOAT360数据集上验证了方法有效性，提升了轻量模型的检测鲁棒性。

Conclusion: 该方法以最小修改利用时间信息，缩小了轻量与重型检测网络的差距，并贡献了BOAT360数据集。

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [91] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: 提出了一种对抗性掩码图像建模方法，用于半监督医学图像分割，通过增强监督信号和减少域差距，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在标注数据稀缺的半监督学习中表现不佳的问题，通过掩码建模和对抗训练增强监督信号。

Method: 构建掩码域，利用原始标签和伪标签训练Transformer预测完整分割掩码，并设计对抗训练损失以减少域差距。

Result: 在三个公开医学图像分割数据集上显著优于现有方法。

Conclusion: 对抗性掩码图像建模方法有效提升了半监督医学图像分割的性能，代码已开源。

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [92] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: 提出了一种基于分割与总结（DaS）的密集视频字幕框架，通过两阶段LSTM和分层注意力机制生成描述性句子。


<details>
  <summary>Details</summary>
Motivation: 解决未修剪长视频的密集字幕问题，通过分割视频为事件提案并利用视觉特征辅助句子总结。

Method: 1. 分割视频为事件提案，提取视觉特征；2. 使用两阶段LSTM（编码器-解码器）和分层注意力机制生成描述。

Result: 在ActivityNet Captions数据集上验证了DaS框架的有效性。

Conclusion: DaS框架通过结合视觉特征和语义信息，成功实现了密集视频字幕的生成。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [93] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 通过分组观察学习可识别的因果表示，提升胸部X光疾病分类的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，识别数据生成过程的真实因果关系可以提升任务特定潜在特征的泛化性和鲁棒性。

Method: 提出一种端到端框架，通过分组观察学习可识别的因果表示，用于胸部X光疾病分类。

Result: 实验表明，通过分组（如种族、性别和成像视角）强制不变性，因果表示在多分类任务中提升了泛化性和鲁棒性。

Conclusion: 分组观察学习因果表示是提升医学影像分类任务性能的有效方法。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [94] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于图的分区与总结（GPaS）框架，通过两阶段方法改进密集视频描述任务，特别关注语义词关系的总结。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景和对象变化较长的提案中表现不佳，未能充分探索事件提案内的场景演变。

Method: 采用分区阶段将事件提案分割为短片段进行描述，总结阶段通过图卷积网络（GCN）和长短期记忆（LSTM）结合总结语义词关系。

Result: 在两个基准数据集（ActivityNet Captions和YouCook II）上优于现有方法。

Conclusion: GPaS框架通过分区和总结两阶段有效提升了密集视频描述的性能。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [95] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出一种基于神经网络的单目360度鱼眼相机距离估计方法，优于传统几何方法。


<details>
  <summary>Details</summary>
Motivation: 解决全向成像中传统几何方法因镜头畸变和环境变化导致的距离估计不准确问题。

Method: 使用神经网络直接从原始全向输入中学习和推断物体距离，无需精确镜头校准。

Result: 在三个360度数据集（LOAF、ULM360、Boat360）上验证，模型在准确性和鲁棒性上优于传统方法和其他学习基线。

Conclusion: 深度学习方法在全向实时距离估计中具有潜力，适用于低成本机器人、自主导航和监控应用。

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [96] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出了一种自监督视频摘要模型，无需注意力机制或复杂架构，在SUMME和TVSUM数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频内容日益普及，需要高效访问有意义信息，但现有方法依赖监督标注或计算昂贵的注意力模型，跨域适用性差。

Method: 采用自监督学习范式，结合马尔可夫过程驱动的损失指标，避免注意力、RNN或Transformer的开销。

Result: 在SUMME和TVSUM数据集上达到最先进性能，优于所有无监督方法，媲美监督模型。

Conclusion: 展示了高效、无需标注的架构潜力，挑战了对复杂架构的依赖，推动了通用视频摘要技术的发展。

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [97] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree是一种交互式3D场景生成模型，解决了现有方法在视角探索中的限制，通过WorldRestorer和ConsistView提升新视角质量和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成方法在视角探索时存在渲染质量低和一致性差的问题，限制了沉浸式体验。

Method: 提出WonderFree模型，包含WorldRestorer（消除新视角中的视觉伪影）和ConsistView（确保多视角一致性）。

Result: 实验显示WonderFree显著提升了渲染质量和全局一致性，用户偏好率77.20%。

Conclusion: WonderFree为3D场景生成提供了高质量和一致性的探索体验，代码和模型将开源。

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [98] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: MMSearch-R1是一个基于强化学习的端到端框架，使大型多模态模型（LMMs）能够在真实互联网环境中按需进行多轮搜索，整合图像和文本搜索工具，并通过奖励机制优化搜索行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RAG和提示工程搜索代理）依赖固定流程，导致搜索效率低下或过度搜索，无法满足真实世界动态信息的需求。

Method: 提出MMSearch-R1框架，结合强化学习和多模态搜索工具，通过半自动化流程收集多模态搜索VQA数据集，并设计搜索平衡子集以训练模型。

Result: 实验表明，MMSearch-R1在知识密集和信息寻求任务中优于同规模RAG基线，且搜索调用减少30%，性能与更大RAG模型相当。

Conclusion: MMSearch-R1为多模态搜索研究提供了高效、按需的解决方案，并提出了可操作的改进方向。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [99] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: SFNet是一种新型的伪造检测框架，通过结合空间和频域特征来识别多样化的遥感图像伪造，显著提升了检测准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速发展导致伪造遥感图像难以检测，可能引发错误情报和虚假信息。现有方法依赖单一视觉特征，难以适应多样化的遥感数据和不断演变的伪造技术。

Method: SFNet采用两个独立的特征提取器分别捕获空间和频域特征，并通过域特征映射模块和混合域特征细化模块（CBAM注意力）对齐和融合多域特征，抑制冗余信息。

Result: 在三个数据集上的实验表明，SFNet比现有方法准确率提高了4%-15.18%，并展现出强大的泛化能力。

Conclusion: SFNet通过多域特征融合显著提升了伪造检测性能，适用于多样化的遥感数据，为未来研究提供了新方向。

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [100] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene利用视频生成模型的3D物理世界常识知识，通过结合视频生成、3D重建和开放词汇感知模型，实现高真实性和结构一致性的3D场景合成。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景合成需要专业知识且耗时，自动化可广泛应用于建筑设计、机器人仿真等领域。现有方法（如LLMs和图像生成模型）在3D空间推理和多视角一致性上存在局限。

Method: VIPScene结合文本和图像提示，整合视频生成、3D重建和开放词汇感知模型，并引入FPVScore评估一致性和合理性。

Result: 实验表明VIPScene显著优于现有方法，且能泛化到多样场景。

Conclusion: VIPScene通过视频生成模型的3D常识知识，解决了现有方法的局限性，实现了高质量的3D场景合成。

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [101] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal框架通过重新解释自然物体轮廓（如云、石头或火焰）为动物形态，模拟人类对模糊刺激的认知现象（pareidolia）。


<details>
  <summary>Details</summary>
Motivation: 模仿人类在模糊刺激中感知有意义模式的能力，为视觉叙事、教育内容、数字艺术和交互媒体设计提供新机会。

Method: 使用开放词汇分割提取物体轮廓，通过视觉语言模型解释语义合适的动物概念，并利用文本到图像扩散模型合成符合输入形状的动物图像。

Result: 在多样化真实输入上评估，展示了框架的鲁棒性和创造力。

Conclusion: Shape2Animal为视觉创意领域提供了新的可能性。

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [102] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: 论文利用NeRF技术从模拟图像中重建非合作空间物体的3D模型，重点优化相机姿态，实验表明逐帧训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 提升空间态势感知能力，支持主动碎片清除、在轨维护等应用。

Method: 采用NeRF进行3D重建，联合优化相机姿态，使用正则化防止姿态变化过大。

Result: 逐帧训练能实现最准确的3D重建。

Conclusion: 优化相机姿态对NeRF在空间物体重建中至关重要。

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [103] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 提出了一种解耦表示学习（DRL）方法，用于提升显微镜图像分类模型的可解释性，并在多个数据集上验证了其准确性与可解释性的平衡。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分析在多个领域至关重要，但深度学习模型的可解释性仍是一个挑战。

Method: 采用解耦表示学习（DRL）框架，通过从合成数据中学习表示并迁移到真实数据，提升模型的可解释性。

Result: 在浮游生物、酵母液泡和人类细胞三个显微镜图像数据集上，DRL方法在准确性和可解释性之间取得了良好平衡。

Conclusion: DRL方法为显微镜图像分类提供了一种兼具高准确性和可解释性的解决方案。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [104] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: IPFormer提出了一种基于视觉的3D全景场景完成方法，通过动态生成实例提案来提升场景理解的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在测试时无法动态适应场景的问题，并探索基于相机的全景场景完成方法。

Method: IPFormer利用图像上下文动态初始化实例提案，并通过注意力机制优化语义实例-体素关系。

Result: 在PQ$^\dagger$和PQ-All指标上超越现有方法，运行时间减少14倍以上，动态提案带来显著性能提升。

Conclusion: IPFormer是首个基于视觉的3D全景场景完成方法，动态实例提案显著提升了性能。

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [105] [Near Data Processing in Taurus Database](https://arxiv.org/abs/2506.20010)
*Shu Lin,Arunprasad P. Marathe,Per-Ȧke Larson,Chong Chen,Calvin Sun,Paul Lee,Weidong Yu*

Main category: cs.DB

TL;DR: 华为的GaussDB for MySQL（Taurus）通过近数据处理（NDP）将数据操作推送到存储层，减少网络传输数据量，释放计算层CPU资源，提升查询效率和系统吞吐量。实验显示，TPCH基准测试中18个查询受益于NDP，数据传输减少63%，CPU时间减少50%。


<details>
  <summary>Details</summary>
Motivation: 传统数据库系统中，数据操作通常在计算层完成，导致大量数据传输和CPU资源占用。通过将数据操作推送到存储层（NDP），可以减少网络负载和计算层压力，提升整体性能。

Method: 设计并实现了Taurus中的NDP功能，将选择、投影和聚合等数据操作推送到存储层执行，利用存储服务器的计算能力。

Result: TPCH基准测试（100GB）显示，22个查询中有18个受益于NDP，数据传输减少63%，CPU时间减少50%。Q15查询中，数据传输减少98%，CPU时间减少91%，运行时间减少80%。

Conclusion: NDP在Taurus中显著减少了数据传输和CPU资源占用，提升了查询效率和系统吞吐量，证明了其在云原生数据库系统中的有效性。

Abstract: Huawei's cloud-native database system GaussDB for MySQL (also known as
Taurus) stores data in a separate storage layer consisting of a pool of storage
servers. Each server has considerable compute power making it possible to push
data reduction operations (selection, projection, and aggregation) close to
storage. This paper describes the design and implementation of near data
processing (NDP) in Taurus. NDP has several benefits: it reduces the amount of
data shipped over the network; frees up CPU capacity in the compute layer; and
reduces query run time, thereby enabling higher system throughput. Experiments
with the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited
from NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.
On Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU
time by 91 percent; and run time by 80 percent.

</details>


### [106] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: 论文重新审视了误差有界的分段线性近似（ε-PLA）在机器学习索引中的应用，改进了理论下界，并实证评估了现有算法。


<details>
  <summary>Details</summary>
Motivation: 尽管ε-PLA在机器学习索引中广泛应用，但其拟合算法的设计和分析尚未充分研究。

Method: 从理论和实证角度重新审视ε-PLA，改进其理论下界，并对现有算法进行全面基准测试。

Result: 提出了改进的理论下界，并通过实验揭示了模型精度、大小和查询性能之间的权衡。

Conclusion: 为未来机器学习数据结构的原理设计提供了实用指南。

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [107] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Main category: cs.DC

TL;DR: MAIZX框架通过动态资源排名优化云操作，减少碳排放，测试中CO2排放减少85.68%。


<details>
  <summary>Details</summary>
Motivation: 云计算的高能耗和碳排放问题日益突出，尤其是私有云基础设施的能效和透明度需求迫切。

Method: MAIZX框架利用实时和预测的碳强度、PUE和能耗数据，通过灵活排名算法优化资源分配。

Result: 在分布式数据中心测试中，MAIZX显著减少碳排放，同时保持操作效率。

Conclusion: MAIZX为云计算提供了一种高效、可扩展的碳减排解决方案。

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [108] [On the $h$-majority dynamics with many opinions](https://arxiv.org/abs/2506.20218)
*Francesco d'Amore,Niccolò D'Archivio,George Giakkoupis,Emanuele Natale*

Main category: cs.DC

TL;DR: 论文首次提出了在同步设置下，针对非恒定的h和k，h-多数动态达成共识时间的上界。假设初始存在对某个多数意见的偏差，证明在特定条件下，过程会在O(log n)轮内以高概率收敛到多数共识。


<details>
  <summary>Details</summary>
Motivation: 研究h-多数动态在非恒定h和k下的收敛时间，填补现有理论空白，并改进已知下界。

Method: 假设初始存在对某个多数意见的偏差，通过理论分析证明在偏差和初始支持节点数的特定条件下，过程快速收敛。

Result: 证明在偏差ω(√x)且初始支持节点x=ω(log n)时，过程在O(log n)轮内收敛；若k=o(n/log n)且初始偏差ω(√n/k)，则h=ω(k log n)足以保证快速收敛。

Conclusion: 论文改进了已知下界，表明所需偏差比3-多数动态更小，适用于任意k≥2。

Abstract: We present the first upper bound on the convergence time to consensus of the
well-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,
for $h$ and $k$ that are both non-constant values.
  We suppose that, at the beginning of the process, there is some initial
additive bias towards some plurality opinion, that is, there is an opinion that
is supported by $x$ nodes while any other opinion is supported by strictly
fewer nodes.
  We prove that, with high probability, if the bias is $\omega(\sqrt{x})$ and
the initial plurality opinion is supported by at least $x = \omega(\log n)$
nodes, then the process converges to plurality consensus in $O(\log n)$ rounds
whenever $h = \omega(n \log n / x)$.
  A main corollary is the following: if $k = o(n / \log n)$ and the process
starts from an almost-balanced configuration with an initial bias of magnitude
$\omega(\sqrt{n/k})$ towards the initial plurality opinion, then any function
$h = \omega(k \log n)$ suffices to guarantee convergence to consensus in
$O(\log n)$ rounds, with high probability.
  Our upper bound shows that the lower bound of $\Omega(k / h^2)$ rounds to
reach consensus given by Becchetti et al.\ (2017) cannot be pushed further than
$\widetilde{\Omega}(k / h)$.
  Moreover, the bias we require is asymptotically smaller than the
$\Omega(\sqrt{n\log n})$ bias that guarantees plurality consensus in the
$3$-majority dynamics: in our case, the required bias is at most any
(arbitrarily small) function in $\omega(\sqrt{x})$ for any value of $k \ge 2$.

</details>


### [109] [PAT: a new algorithm for all-gather and reduce-scatter operations at scale](https://arxiv.org/abs/2506.20252)
*Sylvain Jeaugey*

Main category: cs.DC

TL;DR: 提出了一种名为PAT的新算法，用于实现all-gather和reduce-scatter操作，旨在提升NCCL库在环形算法效率低下时的性能。


<details>
  <summary>Details</summary>
Motivation: 解决环形算法在小规模或大规模操作时因线性延迟导致的性能问题。

Method: PAT算法适用于任意数量的节点，具有对数级网络传输次数，最小化长距离通信，且内部缓冲区需求与操作规模无关。

Result: 算法在小规模操作中表现出对数级网络传输效率，优化了通信性能。

Conclusion: PAT算法为NCCL库提供了一种高效的替代方案，特别适用于环形算法不适用的情况。

Abstract: This paper describes a new algorithm called PAT, for Parallel Aggregated
Trees, and which can be used to implement all-gather and reduce-scatter
operations. This algorithm works on any number of ranks, has a logarithmic
number of network transfers for small size operations, minimizes long-distance
communication, and requires a logarithmic amount of internal buffers,
independently from the total operation size. It is aimed at improving the
performance of the NCCL library in cases where the ring algorithm would be
inefficient, as its linear latency would show poor performance for small sizes
and/or at scale.

</details>


### [110] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Main category: cs.DC

TL;DR: WattsOnAI是一个用于测量、分析和可视化AI工作负载能耗、功率、硬件性能和碳排放的软件工具包，解决了现有工具的局限性，支持标准化报告和相关性分析。


<details>
  <summary>Details</summary>
Motivation: AI（尤其是大语言模型）的快速发展引发了对其能耗和碳排放的担忧，但现有工具缺乏系统性指标集成和相关性分析支持。

Method: WattsOnAI通过与现有AI框架无缝集成，提供标准化报告和细粒度时间序列数据，支持相关性分析和性能优化。

Result: WattsOnAI填补了现有工具的不足，支持环境影响的量化分析，促进了绿色AI实践的发展。

Conclusion: WattsOnAI为研究社区提供了一个全面的工具，鼓励在AI工作负载中权衡环境影响与性能，推动可持续AI的发展。

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


### [111] [SuperSONIC: Cloud-Native Infrastructure for ML Inferencing](https://arxiv.org/abs/2506.20657)
*Dmitry Kondratyev,Benedikt Riedel,Yuan-Tang Chou,Miles Cochran-Branson,Noah Paladino,David Schultz,Mia Liu,Javier Duarte,Philip Harris,Shih-Chieh Hsu*

Main category: cs.DC

TL;DR: SONIC通过将机器学习推理卸载到协处理器来优化资源利用，SuperSONIC项目进一步扩展为可扩展的服务器基础设施，支持在Kubernetes集群上部署计算密集型任务，已在多个科学实验中得到成功应用。


<details>
  <summary>Details</summary>
Motivation: 随着数据速率和复杂机器学习算法的计算需求增加，科学实验需要更高效的资源利用和加速推理的方法。

Method: 采用SONIC方法，通过SuperSONIC项目实现可扩展的服务器基础设施，利用NVIDIA Triton Inference Server标准化通信、优化吞吐量和负载均衡。

Result: SuperSONIC已在CERN LHC、IceCube和LIGO等实验中成功部署，并在多个Kubernetes集群上测试。

Conclusion: SuperSONIC为云原生时代提供了一种可重用、可配置的框架，提升了跨科学领域和行业的加速器推理部署效率。

Abstract: The increasing computational demand from growing data rates and complex
machine learning (ML) algorithms in large-scale scientific experiments has
driven the adoption of the Services for Optimized Network Inference on
Coprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it
to local or remote coprocessors to optimize resource utilization. Leveraging
its portability to different types of coprocessors, SONIC enhances data
processing and model deployment efficiency for cutting-edge research in high
energy physics (HEP) and multi-messenger astrophysics (MMA). We developed the
SuperSONIC project, a scalable server infrastructure for SONIC, enabling the
deployment of computationally intensive tasks to Kubernetes clusters equipped
with graphics processing units (GPUs). Using NVIDIA Triton Inference Server,
SuperSONIC decouples client workflows from server infrastructure, standardizing
communication, optimizing throughput, load balancing, and monitoring.
SuperSONIC has been successfully deployed for the CMS and ATLAS experiments at
the CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory
(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)
and tested on Kubernetes clusters at Purdue University, the National Research
Platform (NRP), and the University of Chicago. SuperSONIC addresses the
challenges of the Cloud-native era by providing a reusable, configurable
framework that enhances the efficiency of accelerator-based inference
deployment across diverse scientific domains and industries.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [112] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Main category: cs.NI

TL;DR: 论文提出了一种基于学习的信道占用预测方法MiLAAP，无需节点间状态共享，通过自注意力机制预测信道占用状态，显著提高了动态网络中的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 无线网络中信道跳频（CS）通信系统需适应干扰变化和节点移动以维持吞吐效率，但状态共享会引入通信开销。本文旨在避免状态共享，通过本地学习预测信道占用。

Method: 提出MiLAAP框架，利用自注意力机制和多头自注意力机制分别预测信道占用状态和节点移动轨迹，仅依赖本地被动观测数据。

Result: MiLAAP在动态网络中信道状态预测准确率接近100%，且具有零样本泛化能力。

Conclusion: MiLAAP通过本地学习预测信道占用和节点移动，显著提升了动态网络的吞吐效率，且无需额外通信开销。

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>


### [113] [Notes on Degeneracy and Robustness](https://arxiv.org/abs/2506.19974)
*Indrakshi Dey,Nicola Marchetti*

Main category: cs.NI

TL;DR: 论文定义了衡量网络（静态/移动/动态）中资源可替代性的新指标，以解决网络鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 研究退化性（Degeneracy）在网络中的作用，即不同结构元素在特定约束下执行相同功能的能力，以提高系统的适应性和恢复速度。

Method: 提出并公式化了几种新的资源可替代性指标，用于评估网络的鲁棒性。

Result: 通过定义这些指标，能够更好地理解和优化网络中的退化性，从而提升系统的鲁棒性和适应性。

Conclusion: 退化性在网络中具有重要意义，通过新定义的指标可以有效衡量和优化网络的鲁棒性。

Abstract: Degeneracy is the ability of structurally different elements to perform the
same function or yield the same output under certain constraints. In contrast
to redundancy, which implies identical backups, degeneracy allows diverse
components to step in and perform the same or similar role. Mathematically, it
is about mapping multiple distinct elements into the same function. In a
degenerate system, failure in one part can be compensated by others not
structurally linked. System functions are distributed within the system itself
or the entire network. This renders faster and more adaptive recovery. In this
work, we define and formulate several novel metrics for resource fungibility to
address robustness in networks (static/mobile/dynamic).

</details>


### [114] [A clusterability test for directed graphs](https://arxiv.org/abs/2506.20111)
*Mario R. Guarracino,Pierre Miasnikof,Alexander Y. Shestopaloff,Houyem Demni,Cristián Bravo,Yuri Lawryshyn*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this article, we extend a statistical test of graph clusterability, the
$\delta$ test, to directed graphs with no self loops. The $\delta$ test,
originally designed for undirected graphs, is based on the premise that graphs
with a clustered structure display a mean local density that is statistically
higher than the graph's global density. We posit that graphs that do not meet
this necessary (but not sufficient) condition for clusterability can be
considered unsuited to clustering. In such cases, vertex clusters do not offer
a meaningful summary of the broader graph. Additionally in this study, we aim
to determine the optimal sample size (number of neighborhoods). Our test,
designed for the analysis of large networks, is based on sampling subsets of
neighborhoods/nodes. It is designed for cases where computing the density of
every node's neighborhood is infeasible. Our results show that the $\delta$
test performs very well, even with very small samples of neighborhoods ($1\%$).
It accurately detects unclusterable graphs and is also shown to be robust to
departures from the underlying assumptions of the $t$ test.

</details>


### [115] [A Detailed Measurement View on IPv6 Scanners and Their Adaption to BGP Signals](https://arxiv.org/abs/2506.20383)
*Isabell Egloff,Raphael Hiesgen,Maynard Koch,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 分析IPv6扫描行为及其在不同网络条件下的表现，提出部署网络望远镜以增强IPv6扫描可见性的操作指南。


<details>
  <summary>Details</summary>
Motivation: IPv6扫描仍具挑战性，研究当前扫描行为以提升理解和应对能力。

Method: 在四个网络望远镜中观察11个月的扫描行为，分析扫描器的目标、网络选择策略等。

Result: 发现大前缀的静默子网不可见，而BGP前缀公告能快速吸引扫描器。

Conclusion: 基于研究结果，提出部署网络望远镜以增强IPv6扫描可见性的建议。

Abstract: Scanners are daily visitors of public IPv4 hosts. Scanning IPv6 nodes
successfully is still a challenge, which an increasing crowd of actors tries to
master. In this paper, we analyze current IPv6 scanning under various network
conditions. We observe scanner behavior during eleven months in four network
telescopes, one of which is periodically reconfigured by changing BGP
announcements. We analyze and classify the observed scanners w.r.t. their
temporal behavior, their target, and network selection strategy, as well as
their individual tools, fingerprints, and correlations across categories. We
find that silent subnets of larger prefixes remain invisible, whereas BGP
prefix announcements quickly attract attention by scanners. Based on our
findings, we derive operational guidance on how to deploy network telescopes to
increase visibility of IPv6 scanners.

</details>


### [116] [Semantic Caching for Improving Web Affordability](https://arxiv.org/abs/2506.20420)
*Hafsa Akbar,Danish Athar,Muhammad Ayain Fida Rana,Chaudhary Hammad Javed,Zartash Afzal Uzmi,Ihsan Ayyub Qazi,Zafar Ayyub Qazi*

Main category: cs.NI

TL;DR: 提出使用大型语言模型（LLMs）进行语义缓存以减少网页数据传输，显著降低数据成本。


<details>
  <summary>Details</summary>
Motivation: 解决发展中国家因数据成本高而难以负担的网页内容加载问题。

Method: 分析50个新闻和媒体网站的4264张图片及40000对图像，利用多模态LLMs评估语义可替换性。

Result: 部分网站类别中37%的图像可替换，用户可节省约10%的数据传输。

Conclusion: 语义缓存对用户和网站运营商均有显著益处，但需解决伦理和实际问题。

Abstract: The rapid growth of web content has led to increasingly large webpages,
posing significant challenges for Internet affordability, especially in
developing countries where data costs remain prohibitively high. We propose
semantic caching using Large Language Models (LLMs) to improve web
affordability by enabling reuse of semantically similar images within webpages.
Analyzing 50 leading news and media websites, encompassing 4,264 images and
over 40,000 image pairs, we demonstrate potential for significant data transfer
reduction, with some website categories showing up to 37% of images as
replaceable. Our proof-of-concept architecture shows users can achieve
approximately 10% greater byte savings compared to exact caching. We evaluate
both commercial and open-source multi-modal LLMs for assessing semantic
replaceability. GPT-4o performs best with a low Normalized Root Mean Square
Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA
3.1 model shows comparable performance, highlighting its viability for
large-scale applications. This approach offers benefits for both users and
website operators, substantially reducing data transmission. We discuss ethical
concerns and practical challenges, including semantic preservation, user-driven
cache configuration, privacy concerns, and potential resistance from website
operators

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [117] [Dynamic Race Detection With O(1) Samples](https://arxiv.org/abs/2506.20127)
*Mosaad Al Thokair,Minjian Zhang,Umang Mathur,Mahesh Viswanathan*

Main category: cs.PL

TL;DR: 提出了一种基于采样的随机化竞态检测器，首次实现亚线性时间动态竞态检测，显著降低运行时间。


<details>
  <summary>Details</summary>
Motivation: 传统基于happens-before的动态分析虽无假阳性，但计算成本高，仅适用于内部测试。

Method: 采用采样和随机化技术，仅处理常量数量的事件，实现亚线性时间（o(n)）检测。

Result: 算法无假阳性，对远离无竞态痕迹的输入，高概率检测竞态；实验显示运行时间显著降低。

Conclusion: 该算法在保持无假阳性的同时，显著提升了竞态检测效率，适用于大规模软件项目。

Abstract: Happens before-based dynamic analysis is the go-to technique for detecting
data races in large scale software projects due to the absence of false
positive reports. However, such analyses are expensive since they employ
expensive vector clock updates at each event, rendering them usable only for
in-house testing. In this paper, we present a sampling-based, randomized race
detector that processes only constantly many events of the input trace even in
the worst case. This is the first sub-linear time (i.e., running in o(n) time
where n is the length of the trace) dynamic race detection algorithm; previous
sampling based approaches like Pacer run in linear time (i.e., O(n)). Our
algorithm is a property tester for HB-race detection -- it is sound in that it
never reports any false positive, and on traces that are far, with respect to
hamming distance, from any race-free trace, the algorithm detects an HB-race
with high probability. Our experimental evaluation of the algorithm and its
comparison with state-of-the-art deterministic and sampling based race
detectors shows that the algorithm does indeed have significantly low running
time, and detects races quite often.

</details>


### [118] [Unfolding Iterators: Specification and Verification of Higher-Order Iterators, in OCaml](https://arxiv.org/abs/2506.20310)
*Ion Chirica,Mário Pereira*

Main category: cs.PL

TL;DR: 提出了一种用于OCaml语言中高阶迭代器的通用规范和演绎验证方法，结合Gospel规范语言和Cameleer框架，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在编程语言中，迭代是一个核心概念，但对高阶迭代进行形式化和模块化推理具有挑战性。

Method: 使用Gospel规范语言描述迭代模式的一般行为，并利用Cameleer框架对迭代客户端进行演绎验证。

Result: 开发了一系列验证案例，包括经典列表迭代器和OCamlGraph库中的图算法。

Conclusion: 该方法为高阶迭代器的规范和验证提供了一种有效途径。

Abstract: Albeit being a central notion of every programming language, formally and
modularly reasoning about iteration proves itself to be a non-trivial feat,
specially in the context of higher-order iteration. In this paper, we present a
generic approach to the specification and deductive verification of
higher-order iterators, written in the OCaml language. Our methodology follows
two key principles: first, the usage of the Gospel specification language to
describe the general behaviour of any iteration schema; second, the usage of
the Cameleer framework to deductively verify that every iteration client is
correct with respect to its logical specification. To validate our approach we
develop a set of verified case studies, ranging from classic list iterators to
graph algorithms implemented in the widely used OCamlGraph library.

</details>


### [119] [Deadlock-free Context-free Session Types](https://arxiv.org/abs/2506.20356)
*Andreia Mordido,Jorge A. Pérez*

Main category: cs.PL

TL;DR: 提出一种基于优先级和递归类型的类型系统，用于静态确保消息传递程序不会死锁。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文无关会话类型的技术无法确保死锁自由，需要一种新方法来解决这一问题。

Method: 扩展上下文无关会话类型，引入优先级机制，支持多态和递归类型。

Result: 证明类型良好的程序既遵守协议，运行时也不会死锁，并通过示例展示表达能力的提升。

Conclusion: 该方法有效解决了死锁问题，同时提升了表达能力。

Abstract: We consider the problem of statically ensuring that message-passing programs
never run into deadlocks. We focus on concurrent functional programs governed
by context-free session types, which can express rich tree-like structures not
expressible in standard session types. Existing techniques based on
context-free session types enforce protocol conformance but not deadlock
freedom. We propose a type system that enhances context-free session types with
a priority-based approach to deadlock freedom, considering polymorphic and
recursive types. Interestingly, the notions needed for avoiding deadlocks fit
nicely into this expressive setting. We prove that well-typed programs respect
their protocols and never run into deadlocks at run-time; we also demonstrate
the expressiveness gains with respect to prior work by means of examples.

</details>


### [120] [PhasePoly: An Optimization Framework forPhase Polynomials in Quantum Circuits](https://arxiv.org/abs/2506.20624)
*Zihan Chen,Henry Chen,Yuwei Jin,Minghao Guo,Enhyeok Jang,Jiakang Li,Caitlin Chan,Won Woo Ro,Eddy Z. Zhang*

Main category: cs.PL

TL;DR: 本文提出了一种针对量子电路中相位多项式优化的系统性方法，显著减少了逻辑和物理电路的门数量。


<details>
  <summary>Details</summary>
Motivation: 量子计算具有解决经典计算难以处理问题的潜力，但算法优化需要自动化工具。相位多项式作为量子电路中的常见模块，其优化对提升整体电路性能至关重要。

Method: 从单块到多块相位多项式优化，采用贪心等效子电路替换和系统奇偶矩阵优化方法，同时兼顾硬件友好性。

Result: 实验显示逻辑电路门数平均减少34.92%，CNOT门数平均减少28.53%；物理电路CNOT门数平均减少25.47%。

Conclusion: 相位多项式优化框架显著提升了量子电路的效率，为硬件友好模块生成提供了实用工具。

Abstract: Quantum computing has transformative computational power to make classically
intractable computing feasible. As the algorithms that achieve practical
quantum advantage are beyond manual tuning, quantum circuit optimization has
become extremely important and integrated into today's quantum software stack.
This paper focuses on a critical type of quantum circuit optimization --
phase-polynomial optimization. Phase polynomials represents a class of
building-block circuits that appear frequently in quantum modular exponentials
(the most time-consuming component in Shor's factoring algorithm), in quantum
approximation optimization algorithms (QAOA), and in Hamiltonian simulations.
Compared to prior work on phase polynomials, we focus more on the impact of
phase polynomial synthesis in the context of whole-circuit optimization, from
single-block phase polynomials to multiple block phase polynomials, from greedy
equivalent sub-circuit replacement strategies to a systematic parity matrix
optimization approach, and from hardware-oblivious logical circuit optimization
to hardware-friendly logical circuit optimization. We also provide a utility of
our phase polynomial optimization framework to generate hardware-friendly
building blocks. Our experiments demonstrate improvements of up to 50%-with an
average total gate reduction of 34.92%-and reductions in the CNOT gate count of
up to 48.57%, averaging 28.53%, for logical circuits. Additionally, for
physical circuits, we achieve up to 47.65% CNOT gate reduction with an average
reduction of 25.47% across a representative set of important benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [121] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: 该论文探讨了大型语言模型（LLMs）在政府遗留代码现代化中的应用，特别是针对ALC和MUMPS语言，解决了输入限制问题。研究发现LLMs在代码分块和文档生成任务中表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 政府企业软件常使用遗留语言（如MUMPS或ALC），而现有LLMs未针对这些语言进行充分测试，且其上下文窗口无法处理长代码。

Method: 研究了多种代码分块方法，评估了不同LLMs（如GPT-4o、Claude 3 Sonnet等）在生成遗留代码摘要注释时的表现。

Result: LLMs选择的分块点与人类专家接近，且其生成的文档在事实性和实用性上分别提升20%和10%。

Conclusion: LLMs可替代人类在大型代码库分块任务中的工作，适用于LLM辅助的现代化过程。

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [122] [When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration](https://arxiv.org/abs/2506.20063)
*Zixuan Feng,Thomas Zimmermann,Lorenzo Pisani,Christopher Gooley,Jeremiah Wander,Anita Sarma*

Main category: cs.SE

TL;DR: 研究探讨了跨学科软件开发（CDSD）中领域专家（DEs）与专业开发者（SDEs）的合作动态，揭示了双方期望差异导致的摩擦。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发团队日益多样化和跨学科化，DEs与SDEs的合作中因期望冲突和优先级差异产生摩擦，需深入研究。

Method: 采用活动理论（AT）作为分析框架，结合24次访谈和293人参与的验证调查进行混合方法研究。

Result: 识别了SDEs的8项期望和DEs的6项期望，并通过AT框架揭示了21种摩擦及其产生机制。

Conclusion: 研究为理解CDSD中的摩擦提供了理论视角，并为未来研究、实践和基础设施设计提供了实用建议。

Abstract: Background: Software development teams are increasingly diverse, embedded,
and cross-disciplinary. Domain experts (DEs) from different disciplines
collaborate with professional software developers (SDEs), bringing
complementary expertise in creating and maintaining complex production
software. However, contested expectations, divergent problem-solving
perspectives, and conflicting priorities lead to friction. Aims: This study
aims to investigate the dynamics of emerging collaboration of
cross-disciplinary software development (CDSD) by exploring the expectations
held by DEs and SDEs and understanding how these frictions manifest in
practice. Method: We utilize Activity Theory (AT), a well-established
socio-technical framework, as an analytical lens in a grounded, empirical
investigation, conducted through a mixed-method study involving 24 interviews
(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants
(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the
CDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By
mapping these expectations to AT components, we revealed 21 frictions in CDSD
and illustrated where and how they arise. Conclusions: This study offers a
theoretical lens for understanding the dynamics and frictions in CDSD and
provides actionable insights for future research, practitioners, and
infrastructure design.

</details>


### [123] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: 研讨会探讨了AI与敏捷开发的整合挑战，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决AI与敏捷开发结合中的实际问题和机遇。

Method: 通过互动会议识别并分析挑战，制定研究路线图。

Result: 明确了工具、治理、数据质量和技能差距等挑战，并提出了解决方案。

Conclusion: 制定了促进产学研合作的行动计划，以推动成功实施。

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [124] [Ten simple rules for PIs to integrate Research Software Engineering into their research group](https://arxiv.org/abs/2506.20217)
*Stuart M. Allen,Neil Chue Hong,Stephan Druskat,Toby Hodges,Daniel S. Katz,Jan Linxweiler,Frank Löffler,Lars Grunske,Heidi Seibold,Jan Philipp Thiele,Samantha Wittke*

Main category: cs.SE

TL;DR: 本文提出了十条简单规则，旨在提高研究软件工程（RSEng）的可访问性，并为研究团队领导者提供实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程（RSEng）对高质量研究软件至关重要，但许多研究者对其了解不足或面临技术复杂性。

Method: 通过十条简单规则，提供实用且可操作的建议。

Result: 遵循这些规则可以提高研究软件的质量、可重复性和可信度。

Conclusion: 通过推广RSEng，可以改善研究的可重复性和可信度。

Abstract: Research Software Engineering (RSEng) is a key success factor in producing
high-quality research software, which in turn enables and improves research
outcomes. However, as a principal investigator or leader of a research group
you may not know what RSEng is, where to get started with it, or how to use it
to maximize its benefit for your research. RSEng also often comes with
technical complexity, and therefore reduced accessibility to some researchers.
The ten simple rules presented in this paper aim to improve the accessibility
of RSEng, and provide practical and actionable advice to PIs and leaders for
integrating RSEng into their research group. By following these rules, readers
can improve the quality, reproducibility, and trustworthiness of their research
software, ultimately leading to better, more reproducible and more trustworthy
research outcomes.

</details>


### [125] [The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review](https://arxiv.org/abs/2506.20435)
*Mennatullah T. Khedr,John S. Fitzgerald*

Main category: cs.SE

TL;DR: 本文综述了数字孪生（DTs）在复杂系统中的组合与验证方法，发现形式化验证不足，需标准化框架。


<details>
  <summary>Details</summary>
Motivation: 研究数字孪生在复杂系统中的组合与验证方法，以解决模型不确定性和集成复杂性。

Method: 系统文献综述，分析了2022-2024年的21项研究，探讨组合机制、系统特性及验证方法。

Result: 组合讨论较多但形式化不足；验证方法多样，形式化验证较少；关键挑战包括模型不确定性和集成复杂性。

Conclusion: 需标准化、可扩展的验证框架和严格的组合方法，以支持复杂数字孪生实现。

Abstract: Digital Twins (DTs) are increasingly used to model complex systems,
especially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where
effective integration is key. This systematic literature review investigates DT
composition and verification and validation (V&V) methodologies. Analyzing 21
studies from 2022-2024, we examined composition mechanisms, SoS
characteristics, and V&V formality, scope, and challenges. While composition is
discussed, formalization is limited. V&V approaches vary, with semi-formal
methods and simulations dominating; formal verification is underutilized. Key
technical challenges include model uncertainty and integration complexity.
Methodological challenges highlight the lack of standardized DT-specific V&V
frameworks. There is a need to move beyond model validation to address
integration and cyber-physical consistency. This review contributes a
structured classification of V&V approaches and emphasizes the need for
standardized, scalable V&V and rigorous composition methodologies for complex
DT implementations.

</details>


### [126] [Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds](https://arxiv.org/abs/2506.20444)
*Xiang Lan,Tim Menzies,Bowen Xu*

Main category: cs.SE

TL;DR: 论文提出了一种基于数据集映射的方法，通过识别和消除“坏种子”样本来提升漏洞检测模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在漏洞检测中受限于低质量训练数据（如噪声、错误标签或样本不平衡），影响了模型效果。

Method: 采用数据集映射方法，分类训练样本的学习难度，并将其整合到主动学习框架中，优先过滤有害样本并强调信息丰富样本。

Result: 实验显示，该方法在Big-Vul数据集上显著提升了F1分数，优于随机选择和传统主动学习方法，同时增强了模型鲁棒性。

Conclusion: 该方法通过优化样本选择提升了漏洞检测的可靠性和成本效益，并为未来数据集构建提供了改进方向。

Abstract: Vulnerability detection is crucial for identifying security weaknesses in
software systems. However, the effectiveness of machine learning models in this
domain is often hindered by low-quality training datasets, which contain noisy,
mislabeled, or imbalanced samples. This paper proposes a novel dataset
maps-empowered approach that systematically identifies and mitigates
hard-to-learn outliers, referred to as "bad seeds", to improve model training
efficiency. Our approach can categorize training examples based on learning
difficulty and integrate this information into an active learning framework.
Unlike traditional methods that focus on uncertainty-based sampling, our
strategy prioritizes dataset quality by filtering out performance-harmful
samples while emphasizing informative ones. Our experimental results show that
our approach can improve F1 score over random selection by 45.36% (DeepGini)
and 45.91% (K-Means) and outperforms standard active learning by 61.46%
(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,
demonstrating the effectiveness of integrating dataset maps for optimizing
sample selection in vulnerability detection. Furthermore, our approach also
enhances model robustness, improves sample selection by filtering bad seeds,
and stabilizes active learning performance across iterations. By analyzing the
characteristics of these outliers, we provide insights for future improvements
in dataset construction, making vulnerability detection more reliable and
cost-effective.

</details>


### [127] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: 该研究提出了一种基于大语言模型（LLM）的半自动化方法，用于BIM中的代码合规性检查，显著减少了时间和错误。


<details>
  <summary>Details</summary>
Motivation: 解决BIM中手动代码合规性检查耗时且易出错的问题。

Method: 整合LLM（如GPT、Claude、Gemini和Llama）与Revit软件，生成Python脚本并执行半自动化合规性检查。

Result: 案例研究表明，该系统减少了合规性检查的时间和精力，提高了准确性，并自动识别违规行为。

Conclusion: 该方法为BIM合规性检查提供了全面、灵活且经济的解决方案，具有广泛应用潜力。

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [128] [CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency](https://arxiv.org/abs/2506.20558)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Jinxi Kuang,Zhihan Jiang,Guangba Yu,Yichen Li,David Lo,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出CCIBench数据集和CCISolver框架，用于解决代码注释不一致问题，显著提升了检测和修复性能。


<details>
  <summary>Details</summary>
Motivation: 代码注释不一致（CCI）对软件开发、测试和维护有负面影响，现有研究数据集不准确且解决方案不足。

Method: 引入高质量数据集CCIBench，并提出基于LLM的端到端框架CCISolver。

Result: CCISolver在检测任务中F1-score达89.54%，修复任务中GLEU分数相对提升18.84%，推理速度快36%。

Conclusion: CCISolver在性能和实用性上显著优于现有方法，具有实际应用潜力。

Abstract: Comments within code serve as a crucial foundation for software
documentation, facilitating developers to communicate and understand the code
effectively. However, code-comment inconsistency (CCI) can negatively affect
software development, testing, and maintenance. Recent efforts to mitigate this
issue have emerged, but existing studies often suffer from inaccurate datasets
and inadequate solutions, weakening their practical effectiveness. In this
study, we first conduct a quantitative analysis of existing datasets, revealing
a substantial portion of sampled data are mislabeled. To address these data
limitations, we introduce CCIBench, a refined dataset comprising high-quality
data, to support the training and evaluation of method-level CCI methods.
Furthermore, we present an innovative end-to-end LLM-based framework,
CCISolver, designed to improve code quality by identifying and rectifying CCIs.
Comprehensive evaluations demonstrate CCISolver's superior performance. For
detection, it establishes a new state-of-the-art with an F1-score of 89.54%. In
fixing task, it achieves a remarkable 18.84% relative improvement in GLEU score
over the strongest baseline. This superiority is confirmed by human evaluation,
where CCISolver's fixing success rate of 0.6533 significantly surpasses
existing methods. Critically, in a practical end-to-end setting, CCISolver's
innovative architecture is approximately 36% faster for inference than the
baseline model, underscoring its scalability and real-world applicability.

</details>


### [129] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Define-ML框架扩展了Lean Inception，通过结构化活动整合数据和ML技术约束，提升ML产品构思的清晰度和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统构思方法缺乏对ML特有挑战（如数据依赖性和技术可行性）的支持，导致产品愿景与业务目标不一致。

Method: 基于技术转移模型开发并验证Define-ML，包括静态验证（玩具问题）和动态验证（工业案例研究），结合定量与定性分析。

Result: 参与者认为Define-ML能有效澄清数据问题、对齐ML能力与业务目标，减少构思模糊性，但需专家指导降低学习曲线。

Conclusion: Define-ML是一个公开可用的验证方法，结合了Lean Inception的敏捷性和ML技术约束的考虑，提升了产品构思的可行性。

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [130] [A Sharp and Robust Test for Selective Reporting](https://arxiv.org/abs/2506.20035)
*Stefan Faridani*

Main category: econ.EM

TL;DR: 本文提出了一种新的测试方法，能够检测选择性报告行为，并在t分数不完全正态时仍保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有测试方法在检测选择性报告行为时的局限性，尤其是在t分数非正态情况下的解释性问题。

Method: 通过计算平滑经验t曲线与无选择性报告情况下所有可能t曲线之间的距离，提出了一种新的投影测试方法。

Result: 在Brodeur等人的元数据中，RCTs、IVs和DIDs的t曲线扭曲程度超出偶然范围，但Edgeworth展开显示这些扭曲可能由t分数的近似正态性解释。

Conclusion: 选择性报告的检测结果比之前认为的更脆弱，扭曲程度较小，可能由t分数的近似正态性导致。

Abstract: This paper proposes a test that is consistent against every detectable form
of selective reporting and remains interpretable even when the t-scores are not
exactly normal. The test statistic is the distance between the smoothed
empirical t-curve and the set of all t-curves that would be possible in the
absence of any selective reporting. This novel projection test can only be
evaded in large meta-samples by selective reporting that also evades all other
valid tests of restrictions on the t-curve. A second benefit of the projection
test is that under the null we can interpret the projection residual as noise
plus bias incurred from approximating the t-score's exact distribution with the
normal. Applying the test to the Brodeur et al. (2020) meta-data, we find that
the t-curves for RCTs, IVs, and DIDs are more distorted than could arise by
chance. But an Edgeworth Expansion reveals that these distortions are small
enough to be plausibly explained by the only approximate normality of the
individual t-scores. The detection of selective reporting in this meta-sample
is therefore more fragile than previously known.

</details>


### [131] [Daily Fluctuations in Weather and Economic Growth at the Subnational Level: Evidence from Thailand](https://arxiv.org/abs/2506.20105)
*Sarun Kamolthip*

Main category: econ.EM

TL;DR: 研究发现泰国气温波动与经济增长呈倒U型关系，升温对农业部门增长有负面影响，预计气候变化将显著影响泰国未来经济。


<details>
  <summary>Details</summary>
Motivation: 探究气温波动对泰国经济产出的历史影响，为气候变化的经济效应提供实证依据。

Method: 分析省级人均产出增长率与气温变化的关系，使用不同函数形式估计边际效应，并基于RCP4.5和RCP8.5情景进行省级预测。

Result: 升温对人均产出增长率有显著负面影响（-1.248至-3.799个百分点），仅影响农业部门。预计2050年半数泰国人口受气候变化影响，2090年62%-86%人口将更贫困。

Conclusion: 气候变化对泰国经济增长有显著负面影响，但预测结果需谨慎解读，尤其是未考虑滞后效应的模型。

Abstract: This study investigates the historical relationship between temperature
fluctuations and Thailand's aggregate economic output. The results show
inverted-U shape temperature effects on the annual growth rate of provincial
output per capita. The estimated marginal change in the annual growth rate of
provincial output per capita of a 1 degree Celsius temperature increase is
between -1.248 and -3.799 percentage points, depending on the functional form
used. I find that the warming temperature may affect the growth of economic
output rather than its level. The results also suggest that temperature changes
affect output growth only in the agricultural sector, but not in the industrial
and service sectors. Province-level projections under both the RCP4.5 and
RCP8.5 emission scenarios show that climate change will affect half of the Thai
population by 2050. Projections show that climate change will make 62% (RCP4.5)
and 86% (RCP8.5) of the Thai population poorer in per-capita terms than they
would be in the absence of climate change in 2090. Nonetheless, these
projections are sensitive to potential biases in future climate projections,
particularly in models that do not account for lagged effects. All projections
should be interpreted with caution.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [132] [An experiment in price perception error](https://arxiv.org/abs/2506.19953)
*Shawn Berry*

Main category: econ.GN

TL;DR: 研究探讨了消费者价格感知的多维性，分析了态度和人口因素对价格猜测准确性的影响，发现品牌忠诚度、收入等因素显著影响价格感知误差。


<details>
  <summary>Details</summary>
Motivation: 价格感知是消费者决策的重要但未被充分理解的维度，研究旨在揭示影响价格感知误差的因素。

Method: 通过ANOVA和潜在变量分析，对351名受访者对13种产品和服务价格猜测的准确性进行研究。

Result: 品牌忠诚度、收入等显著影响价格感知误差；潜在变量分析显示人口统计、决策和价格敏感性因素影响最大。

Conclusion: 价格感知误差受多种因素影响，研究为理解消费者价格感知提供了新视角。

Abstract: The process of consumer decision-making is multidimensional, and price
perception is a very important but still not well-understood dimension for both
marketers and consumers. Although heuristics or mental shortcuts are seen as
biased and can cause decision errors, consumers tend to use price knowledge
heuristics for purchase decisions, sometimes relying on old information. This
study examined the effects of individual attitudinal and demographic factors on
the ability of 351 respondents to correctly guess the prices of 13 products and
services using ANOVA and the development of a seven-factor price perception
model using latent variable analysis. While most respondents preferred to
either research prices first or compare prices to a similar product, they
either systematically underestimated or overestimated the prices of the
products. In the ANOVA, brand loyalty, importance of substitute products and
knockoff products, how financially well-off the respondent household was
growing up, product quality importance, haggling tendency, and level of income
were statistically significant predictors of price perception error. Latent
variable analysis revealed that demographics, decision making, and price
sensitivity factors had the greatest influence on price perception error. Level
of education and income, frequency of regret, coupon importance, and how
respondents chose to save or spend money were significant latent variables.

</details>


### [133] [Exiting National Anti-Poverty Campaign, Social Support, and Improved Mental Health](https://arxiv.org/abs/2506.20292)
*Zhengwen Liu,Castiel Chen Zhuang,Yibo Wu*

Main category: econ.GN

TL;DR: 研究中国退出国家扶贫运动对心理和社会的影响，发现退出后心理健康改善，社会与家庭联系增强，收入未显著变化。


<details>
  <summary>Details</summary>
Motivation: 探讨政策退出对心理和社会关系的影响，为政策设计提供参考。

Method: 利用回归断点设计，分析中国扶贫政策退出的自然实验数据。

Result: 退出扶贫运动后，心理健康提升，社会与家庭支持增强，收入未显著变化。

Conclusion: 政策退出需注重维持社区和家庭支持系统，以保障心理和社会福祉。

Abstract: We study the psychological and social impacts of exiting a national
anti-poverty campaign, leveraging China's phase-out of its national poverty
assistance as a natural experiment. Using a regression discontinuity design, we
find that exiting the national campaign improves mental wellbeing. These
improvements are accompanied by stronger social and family ties -- such as
greater perceived support and communication, while income and material
conditions remain largely unchanged. Our findings offer insights into the
design of policy exits and underscore the importance of incorporating measures
that sustain community- and family-based support systems when implementing or
ending assistance programs.

</details>


### [134] [Cost-benefit analysis of an AI-driven operational digital platform for integrated electric mobility, renewable energy, and grid management](https://arxiv.org/abs/2506.20631)
*Arega Getaneh Abate,Xiaobing Zhang,Xiufeng Liu,Dogan Keles*

Main category: econ.GN

TL;DR: 本文提出了一种基于AI驱动的数字平台（ODP），用于整合电动交通和可再生能源与电网，并通过成本效益分析（CBA）评估其经济、可靠性和环境效益。


<details>
  <summary>Details</summary>
Motivation: 整合电动交通和可再生能源是实现脱碳、效率和电网稳定的关键，但需解决数字平台开发和成本效益研究的挑战。

Method: 采用七步CBA框架，量化AI驱动的ODP的经济、可靠性和环境效益，并与资本和运营支出对比。

Result: ODP显著提升了能源效率、电网可靠性和环境可持续性，具体表现为市场套利、预测能力和运营效率的量化改进。

Conclusion: AI驱动的ODP在跨部门优化中表现出显著的成本效益，为实现脱碳和电网稳定提供了可行解决方案。

Abstract: Integrating electric mobility (electric vehicles (EVs), electric trucks
(ETs)) and renewable energy sources (RES) with the power grid is paramount for
achieving decarbonization, efficiency, and stability. Given the rapid growth of
decentralized technologies and their critical role in decarbonization, two
critical challenges emerge: first, the development of a digital platform for
operational coordination; and second, rigorous research into their cost-benefit
profile. This paper addresses this by presenting a comprehensive cost-benefit
analysis (CBA) of an AI-driven operational digital platform (ODP) designed for
holistic, cross-sectoral optimization. The ODP aims to enhance energy
efficiency, grid reliability, and environmental sustainability. A seven-step
CBA framework, aligned with EU guidelines, quantifies economic, reliability,
and environmental benefits against capital and operational expenditures,
explicitly linking benefit magnitude to AI-driven ODP and optimization
efficiencies, such as quantified improvements in market arbitrage from ODP,
enabled forecasting, and enhanced operational efficiencies across various
services.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [135] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 本文提出了一种基于时空点过程的概率模型，用于更全面地模拟阅读行为，包括注视点的位置、时间以及持续时间，并通过Hawkes过程模拟眼跳行为。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常基于聚合的眼动追踪数据，忽略了阅读过程中的时空动态性，因此需要更通用的模型来捕捉这些细节。

Method: 使用标记的时空点过程模型，Hawkes过程模拟眼跳，注视持续时间通过时间卷积的预测变量建模。

Result: Hawkes过程模型在模拟人类眼跳行为上优于基线模型，但上下文意外性对注视持续时间的预测效果有限。

Conclusion: 该模型能更好地捕捉阅读的时空动态性，但意外性理论在解释精细眼动行为上存在局限性。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [136] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE是一个新的多模态基准测试，专注于农业领域的专家级推理和决策，结合自然用户查询、专家回答和图像上下文，用于评估模型在真实知识密集型领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常依赖明确输入和封闭分类法，而MIRAGE旨在解决开放世界场景中模型推理、澄清策略和长文本生成的挑战。

Method: 基于35,000多组真实用户-专家交互数据，通过多步骤流程构建，涵盖作物健康、害虫诊断和管理场景，包含7,000多种生物实体。

Result: MIRAGE成为视觉语言模型中分类多样性最高的基准之一，支持开放世界场景下的推理和交互。

Conclusion: MIRAGE为多模态模型在真实知识密集型领域的评估提供了高保真基准，填补了现有研究的空白。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>
