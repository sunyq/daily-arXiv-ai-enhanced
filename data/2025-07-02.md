<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 99]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]
- [econ.EM](#econ.EM) [Total: 5]
- [econ.GN](#econ.GN) [Total: 3]
- [econ.TH](#econ.TH) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI是一个无需训练的GUI自然语言查询框架，通过动态视觉定位和模态感知优化解决GUI中的视觉元素多样性和语言歧义问题。


<details>
  <summary>Details</summary>
Motivation: GUI中的视觉元素多样性和语言歧义使得自然语言查询具有挑战性。

Method: 将GUI分为文本和图标元素，利用通用视觉语言模型独立处理，并通过动态聚焦和分层细化优化结果。

Result: 在标准基准测试中表现优于基线方法，证明了模态分离和区域聚焦推理的有效性。

Conclusion: DiMo-GUI通过动态和模态感知策略，无需额外训练即可提升GUI定位性能。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [2] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: TalentMine是一个基于LLM的框架，通过语义增强的表格表示解决传统表格提取方法在语义关系理解上的不足，显著提升了人才管理文档的查询准确性。


<details>
  <summary>Details</summary>
Motivation: 传统表格提取方法在语义理解上表现不佳，导致信息检索和决策支持系统失效，特别是在复杂的表格数据中。

Method: TalentMine采用多模态推理方法，将提取的表格转换为语义丰富的表示，保留结构和语义信息。

Result: 实验表明，TalentMine在查询任务中达到100%准确率，远超AWS Textract的0%和40%。

Conclusion: TalentMine通过语义增强的表格表示，显著提升了人才管理系统的性能，为相关应用提供了高效解决方案。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [3] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: 论文提出了一种基于FAIR数据基础设施的分布式自驱动实验室（SDL）框架，通过在线模拟和机器学习加速科学和工程中的优化任务。


<details>
  <summary>Details</summary>
Motivation: 通过整合机器学习和FAIR数据基础设施，促进自驱动实验室间的协作，提高实验数据的可重用性和优化效率。

Method: 使用nanoHUB服务构建分布式SDL框架，支持数据共享、自动处理和主动学习优化。

Result: 实现了基于FAIR数据的协作优化平台，适用于低成本实验（如食品染料配方优化），并展示了工具的通用性。

Conclusion: 该框架为科学和工程中的优化问题提供了可扩展的解决方案，结合了FAIR数据和机器学习模型的优势。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [4] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: 本文提出了一种新型的自解释零样本人类活动识别网络（SEZ-HARN），能够识别未在训练中遇到的活动，并通过骨架视频解释其决策过程，同时保持竞争性的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本人类活动识别（ZS-HAR）模型缺乏透明性的问题，同时克服数据限制。

Method: 开发了SEZ-HARN模型，结合零样本学习和自解释能力，通过骨架视频提供决策解释。

Result: 在四个基准数据集上测试，SEZ-HARN的零样本预测准确率接近最佳黑盒模型（PAMAP2上相差3%），同时提供可理解的解释。

Conclusion: SEZ-HARN在保持高准确率的同时，显著提升了模型的透明性和可解释性。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [5] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: AdvDistill提出了一种基于奖励的数据集蒸馏框架，通过多生成响应和规则验证器分配奖励，显著提升了小语言模型在数学和复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏技术通常局限于学生模型复制教师模型的分布内响应，限制了其泛化能力，尤其在推理任务中表现不足且计算成本高。

Method: AdvDistill利用教师模型对每个提示生成多个响应，并通过基于规则的验证器分配奖励，将这些奖励作为训练学生模型的权重。

Result: 该方法显著提升了学生模型在数学和复杂推理任务中的性能。

Conclusion: 研究表明，在数据集蒸馏过程中引入奖励机制是有效的，能够提升模型的泛化能力和推理性能。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [6] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: 论文提出VoyagerVision，一种多模态模型，通过视觉反馈在Minecraft中创建结构，扩展了开放性的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过视觉输入增强模型对空间环境的理解，从而扩展其任务执行能力和开放性潜力。

Method: 提出VoyagerVision模型，利用截图作为视觉反馈，在Minecraft中创建结构。

Result: VoyagerVision平均在50次迭代中创建2.75个独特结构，并在平坦世界中成功完成一半的构建任务。

Conclusion: 视觉输入显著提升了模型的任务执行能力，为开放性研究提供了新方向。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [7] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 论文提出了一种名为“逆向推理”的新范式，通过SAGE-nano模型实现LLM的自我反思和解释，显著提升了推理透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在Chain-of-Thought提示下的决策过程不透明问题，增强模型的可解释性和安全性。

Method: 采用逆向推理范式，通过元认知结构和注意力机制反向分析推理链，生成解释。

Result: SAGE-nano在AQUA-RAT等测试中达到74.6%的推理准确率和92.1%的人类偏好评分，性能接近Claude-3.5和GPT-4o。

Conclusion: 逆向推理为透明AI系统开辟了新途径，填补了AI安全、教育和科学发现中的关键空白。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [8] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的自动化方法，从黑盒遗留系统中提取可解释的决策逻辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如行为克隆）仅复制输入输出行为，无法捕捉底层意图，而遗留系统缺乏文档和理解。

Method: 使用强化学习代理探索输入空间，识别关键决策边界，并通过K-Means聚类和决策树提取可读规则。

Result: 在三种不同复杂度的遗留系统上验证，提取的规则准确反映了核心逻辑。

Conclusion: 该方法为遗留系统迁移中的规范和测试用例生成提供了有前景的基础。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [9] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: 研究表明，ChatGPT等生成式AI工具可能降低学生在学术写作任务中的认知投入，导致认知卸载。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI工具（如ChatGPT）对学生认知投入的影响，以应对教育中对AI可能削弱深度思考和主动学习的担忧。

Method: 采用实验设计，随机分配学生至AI辅助（ChatGPT）或非辅助（对照组）条件，完成议论文写作任务，并使用认知投入量表（CES-AI）评估。

Result: ChatGPT组的认知投入分数显著低于对照组，表明AI辅助可能导致认知卸载。

Conclusion: 研究呼吁开发教学策略，促进学生与AI生成内容的主动反思性互动，以避免损害自主学习和深度认知投入。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [10] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: xHAIM是一个基于生成式AI的框架，通过任务相关数据识别、患者摘要生成、预测建模和临床解释，提升了HAIM框架的预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决HAIM框架在任务无关性和缺乏可解释性方面的局限性。

Method: xHAIM通过四个步骤实现：任务相关数据识别、患者摘要生成、预测建模和临床解释。

Result: 在HAIM-MIMIC-MM数据集上，xHAIM将平均AUC从79.9%提升至90.3%。

Conclusion: xHAIM将AI从黑盒预测器转变为可解释的决策支持系统，增强了临床实用性。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [11] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: 综述了机器学习在解决NP难组合优化问题（如TSP和VRP）中的应用进展，提出了一种分类方法，并探讨了与传统运筹学方法的结合。


<details>
  <summary>Details</summary>
Motivation: 由于NP难问题的复杂性，传统方法难以高效求解，机器学习为这些问题的解决提供了新的可能性。

Method: 提出了一种分类法，将基于ML的路由方法分为构造型和改进型，并探讨了其适用性。

Result: 综述了ML在路由问题中的应用，为未来研究提供了结构化框架。

Conclusion: ML与传统运筹学方法的结合为解决新兴VRP变体提供了新的研究方向。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [12] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO框架通过自回归搜索训练语言模型，利用自我反思、回溯和探索，显著提升了非推理模型的数学问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 提升非推理模型（如Llama 3）的推理能力，通过结构化搜索行为训练增强其性能。

Method: 使用蒙特卡洛树搜索（MCTS）生成合成数据集，将搜索轨迹转化为自然语言链式思考，并通过强化学习（RL）进一步优化。

Result: 在MATH-500、AMC 2023和AIME 2024上分别实现16.0%、26.9%和20.0%的性能提升，尤其在需要迭代修正的难题上表现突出。

Conclusion: 搜索启发的训练方法为开源大语言模型提供了增强推理能力的有效途径。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [13] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 研究发现，数学推理能力强的LLMs在其他领域表现不佳，RL调优模型比SFT调优模型更具泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探讨数学推理能力的提升是否反映更广泛的解决问题的能力，还是仅仅是过度拟合。

Method: 评估20多个开放权重的推理调优模型，涵盖数学、科学QA、代理规划、编码和指令遵循任务，并对Qwen3-14B模型进行控制实验。

Result: 大多数数学表现好的模型在其他领域表现不佳；RL调优模型泛化能力强，SFT调优模型易遗忘通用能力。

Conclusion: 需重新思考后训练方法，减少对SFT蒸馏数据的依赖，以提升推理模型的泛化能力。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [14] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种改进的局部搜索方法（2d-LS）用于SMT-NRA，结合了MCSAT框架和样本单元投影算子，显著提升了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 提升SMT-NRA问题的局部搜索效率，通过引入新的操作和框架优化搜索过程。

Method: 1. 提出2d-cell-jump操作；2. 扩展局部搜索框架2d-LS，整合MCSAT；3. 实现样本单元投影算子；4. 设计混合框架结合MCSAT、2d-LS和OpenCAD。

Result: 实验结果显示局部搜索性能显著提升。

Conclusion: 提出的方法有效提升了SMT-NRA问题的搜索效率，证明了其优越性。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [15] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: 研究探讨了通过强化学习（RL）提升大语言模型（LLMs）在象棋中的战略推理能力，发现基于知识蒸馏的密集奖励优于稀疏奖励，但模型表现仍远低于专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否可以通过RL在象棋中发展战略推理能力，填补相关领域的研究空白。

Method: 利用预训练的象棋动作价值网络为LLM的输出动作质量提供密集奖励（知识蒸馏），并与稀疏二进制奖励进行对比。

Result: 密集奖励通常优于稀疏奖励，但所有模型表现均远低于专家水平。

Conclusion: 预训练模型对象棋的内部理解不足可能是性能瓶颈，仅靠RL难以完全克服。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [16] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 提出了一种改进的数值算法，用于解决基于非光滑优化、二次规划和迭代过程的极小极大问题，并提供了收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 极小极大问题在鲁棒优化和不平衡学习等领域有广泛应用，但现有算法在收敛性和效率上存在不足。

Method: 结合非光滑优化、二次规划和迭代过程，提出改进算法，并证明其在梯度连续性和有界性等条件下的收敛性。

Result: 算法在理论上是收敛的，适用于多种应用场景。

Conclusion: 该算法为极小极大问题提供了一种高效且可靠的解决方案，具有广泛的应用潜力。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [17] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 论文探讨了多模态基础模型在智能代理系统中的安全风险，提出了一种结合行为序列信息的风险识别机制和基于大语言模型的自动化评估方案，初步验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态基础模型在智能代理系统中的广泛应用，系统面临潜在的越狱风险，现有安全措施在复杂交互中仍有局限性，缺乏高效的风险评估方法。

Method: 通过结合行为序列信息构建风险识别机制，并设计基于大语言模型的自动化辅助评估方案。

Result: 初步验证表明，该方法能提高风险行为识别能力，降低代理被越狱的概率。

Conclusion: 该研究为多模态智能代理系统的安全风险建模与防护提供了有价值的参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [18] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: 本文探讨了机器是否能够像人类一样思考、推理和行动，分析了当前AI模型的局限性，并提出了跨学科的AGI发展框架，强调模块化推理、记忆和多智能体协调的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索机器实现通用智能的可能性，并解决当前AI模型在推理和行动上的局限性。

Method: 通过跨学科综合研究，结合人工智能、认知神经科学、心理学、生成模型和智能体系统，分析通用智能的架构和认知基础。

Result: 提出了Agentic RAG框架和多种泛化策略，强调记忆与推理的整合是实现真正智能的关键。

Conclusion: 尽管在统计学习和目标导向认知之间取得进展，但AGI的发展仍面临科学、技术和伦理挑战。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [19] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 论文提出了一种名为CIP的新技术，利用因果影响图（CIDs）来识别和减轻自主代理决策中的风险，从而提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的自主代理在辅助任务中展现潜力，确保其行为安全可靠以防止意外后果变得至关重要。

Method: 方法包括三个步骤：1）基于任务规范初始化CID以描述决策过程；2）使用CID指导代理与环境交互；3）根据观察到的行为和结果迭代优化CID。

Result: 实验结果表明，该方法在代码执行和移动设备控制任务中显著提升了安全性。

Conclusion: CIP通过结构化因果关系表示，有效增强了自主代理的安全性。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文研究了文本和多模态LLM在表格理解任务中的表现，发现其在科学表格处理中存在挑战。


<details>
  <summary>Details</summary>
Motivation: 表格在多个领域广泛使用，但LLM处理表格数据的效率尚未充分研究。

Method: 通过跨领域和跨模态评估，比较LLM在不同表格类型（科学与非科学、图像与文本）中的表现，并进行可解释性分析。

Result: LLM在表格模态间表现稳健，但在科学表格处理中存在显著困难。

Conclusion: 提出了TableEval基准，为未来研究提供了工具，并指出LLM在科学表格处理上的局限性。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [21] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: 提示（prompting）是研究和控制大型语言模型的主要方法，也是其最强大的工具之一。然而，它常被视为“炼金术”而非科学。本文认为应将提示视为行为科学的一部分，而非权宜之计。


<details>
  <summary>Details</summary>
Motivation: 探讨提示在大型语言模型研究中的科学地位，反驳将其视为“炼金术”的观点。

Method: 通过将大型语言模型视为一种复杂且不透明的生物体，将提示类比为行为科学的研究方法。

Result: 提示是大型语言模型科学中的关键组成部分，而非次要工具。

Conclusion: 提示应被视为一门科学，是理解和控制大型语言模型的核心方法。

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [22] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 论文提出了一种名为LineRetriever的新方法，通过语言模型识别和检索与未来导航步骤最相关的观察行，解决了网页导航任务中上下文过长的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法（如自下而上的截断或基于嵌入的检索）在网页导航任务中会丢失关键信息，尤其是影响自适应规划的状态和动作历史。

Method: 提出LineRetriever方法，利用语言模型识别并检索对预测未来动作最有帮助的观察行，而不仅仅是语义相似性。

Result: 实验表明，LineRetriever能在保持性能的同时减少每一步的观察内容大小。

Conclusion: LineRetriever优化了检索方法，特别适用于自适应规划的网页导航任务。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [23] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段方法，利用LLM生成推理增强文本分类性能，实验显示准确率提升8.7%。


<details>
  <summary>Details</summary>
Motivation: 标准分类模型缺乏显式推理，限制了性能、鲁棒性和可解释性。

Method: 1. 微调Llama-3.2-1B-Instruct生成推理；2. 用推理增强下游生成模型的训练数据。

Result: 生成模型（输出推理和情感）比基线模型（仅输出情感）准确率提高8.7%。

Conclusion: LLM生成的推理能丰富训练数据，提升下游任务性能并提供显式解释。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [24] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: 论文探讨了LLM在风格翻译中的失败，并提出RASTA方法以改善文化差异导致的风格对齐问题。


<details>
  <summary>Details</summary>
Motivation: 文化差异常导致说话者意图与听者理解的风格不一致，例如礼貌在翻译中丢失。

Method: 提出RASTA方法，利用学习的风格概念改进LLM翻译，以更好地传达文化沟通规范。

Result: LLM在风格翻译中存在偏向中立和在非西方语言中表现较差的问题。

Conclusion: RASTA方法有效改善了LLM在风格翻译中的对齐问题。

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [25] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 研究发现，即使经过指令微调和对齐，语言模型（LMs）仍可通过线性探针解码被拒绝的有害信息，表明这些信息在表示空间中未被消除或重定位，仅被直接表达抑制。


<details>
  <summary>Details</summary>
Motivation: 探讨指令微调和对齐后的语言模型是否真正消除了有害信息，以及这些信息是否仍可通过线性探针解码。

Method: 使用线性探针训练在LM隐藏状态上，研究被拒绝信息是否可解码，并测试探针在基础模型和指令微调模型间的迁移能力。

Result: 线性探针可解码大量被拒绝信息（如国家平均IQ，Pearson相关性超过0.8），且基础模型的探针可迁移至指令微调模型，表明有害信息在内部表示中持续存在。

Conclusion: 指令微调并未完全消除或重定位有害信息，仅抑制其直接表达，这些信息仍可通过线性探针访问，并间接影响下游行为。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [26] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: 论文提出了一种基于数学模型的形态学-句法接口理论，结合了Merge和强极简论题，通过代数操作和形态学树结构描述形态句法树的形成。


<details>
  <summary>Details</summary>
Motivation: 探索形态学和句法之间的接口问题，特别是在强极简论题的框架下，如何用数学模型描述两者的交互。

Method: 使用形态学树的代数结构（magma）和操作数（operad）对应关系，结合形态学余积分解，构建形态句法树。

Result: 提出了一个数学模型，能够灵活调整形态学和句法之间的边界，并重新解释了分布式形态学中的某些操作。

Conclusion: 该模型为形态学-句法接口提供了数学基础，支持强极简论题，并展示了形态学和句法在结构形成中的动态交互。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [27] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: 研究发现，非英语语言在推理任务中更节省token且保持准确性，支持多语言推理的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨英语是否是最高效的推理语言，并评估多语言模型在非英语语言中的表现。

Method: 评估三种开源语言推理模型（DeepSeek R1、Qwen 2.5、Qwen 3）在四种数学数据集和七种语言中的表现。

Result: 非英语语言推理减少token使用且保持准确性，且效果不受翻译影响。

Conclusion: 多语言推理具有潜力，强调多语言基础的重要性。

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [28] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 论文研究了不同微调方法对隐私风险的影响，发现基于提示的微调在性能相当的同时，比基于参数的微调更隐私安全。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型能力的提升，微调方法多样化，但其隐私风险（如记忆化）未得到足够关注。

Method: 通过成员推理攻击（MIAs）评估不同微调方法对记忆化的影响。

Result: 基于提示的微调性能与基于参数的微调相当，但对MIAs的脆弱性更低，且不受模型规模影响。

Conclusion: 基于参数的微调更容易泄露隐私信息，而基于提示的微调是更隐私安全的选择。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [29] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 论文探讨了低资源语言（特别是撒哈拉以南非洲语言）在NLP中的挑战，提出通过高质量语料库和多语言预训练模型提升语义表示，并开发了标注数据集以支持研究。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在NLP中的代表性不足问题，尤其是撒哈拉以南非洲语言，因缺乏标注数据和高质量语料而难以评估模型性能。

Method: 分析公开语料库的噪声，构建高质量语料库；研究多语言预训练模型在低资源语言中的适应性；开发21种非洲语言的标注数据集。

Result: 研究表明语义表示质量依赖数据质量而非数量；多语言预训练模型在低资源语言中表现潜力；标注数据集支持了命名实体识别和机器翻译任务。

Conclusion: 高质量数据和多语言预训练模型是提升低资源语言NLP性能的关键，非洲语言的标注数据集填补了研究空白。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [30] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 语言模型在生成平衡括号等简单语法任务中表现不佳，研究发现错误源于不可靠组件（如注意力头和FF神经元）的干扰。提出RASteer方法，通过增强可靠组件的作用显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在简单语法任务中持续出错的原因，并提出解决方案以提升模型表现。

Method: 分析模型组件（注意力头和FF神经元）的作用，发现可靠与不可靠组件的差异，提出RASteer方法增强可靠组件。

Result: RASteer将平衡括号任务的准确率从0%提升至近100%，并在算术推理任务中实现约20%的性能提升。

Conclusion: 通过识别和增强可靠组件，RASteer能有效提升语言模型在特定任务中的表现，且不影响其通用能力。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [31] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: COLDSELECT是一种联合选择verbalizer和实例的方法，通过建模数据多样性，减少不确定性并提高泛化能力，在冷启动场景中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法对模板、verbalizer和少样本实例选择敏感，尤其在无标签数据的冷启动场景中。现有研究忽视了实例与verbalizer之间的依赖关系。

Method: COLDSELECT将PLM词汇和$h_{[MASK]}$嵌入映射到共享空间，应用降维和聚类，以最小化不确定性和最大化多样性为目标进行选择。

Result: 在八个基准测试中，COLDSELECT在减少不确定性和增强泛化能力方面表现优异，优于基线方法。

Conclusion: COLDSELECT通过联合优化verbalizer和实例选择，有效解决了冷启动场景中的问题，提升了模型性能。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [32] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 论文提出了一种结合问题分解的RAG流程，通过分解多跳问题为子问题并重新排序检索结果，显著提升了检索和答案生成的性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准RAG在多跳问题中因信息分散而难以检索足够证据的挑战。

Method: 1. 使用LLM分解原始问题为子问题；2. 为每个子问题检索相关段落；3. 合并并重新排序候选段落以提高覆盖率和精确度。

Result: 在MultiHop-RAG和HotpotQA上，检索性能（MRR@10）提升36.7%，答案准确率（F1）提升11.6%。

Conclusion: 问题分解与重新排序结合是一种无需额外训练或索引的实用改进方法，显著提升了多跳问题的处理能力。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [33] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: 论文探讨了格里高利圣歌旋律的分段理论（centonisation），通过无监督分层Pitman-Yor语言模型寻找最优分段，发现其在调式分类中表现优异，但结果与传统centonisation理论不符。


<details>
  <summary>Details</summary>
Motivation: 研究格里高利圣歌旋律的分段理论（centonisation），并探索其与记忆效率和调式分类的关系。

Method: 使用无监督分层Pitman-Yor语言模型对圣歌旋律进行最优分段。

Result: 最优分段在调式分类中表现优异，但与传统centonisation理论不符；旋律的开头和结尾部分更具公式化特征。

Conclusion: 尽管最优分段在调式分类中有效，但它并未支持传统的centonisation理论，表明记忆效率与调式分类存在关联。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [34] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: CAPITAL是一种因果提示框架，通过将前门调整融入思维链推理，提高了隐式情感分析的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于提示的大语言模型在隐式情感分析中依赖多数投票而忽略因果有效性的问题。

Method: CAPITAL将总体因果效应分解为输入提示对推理链的影响和推理链对最终输出的影响，利用编码器聚类和NWGM近似进行估计，并通过对比学习对齐编码器表示与LLM推理空间。

Result: 在基准数据集和三种LLM上，CAPITAL在准确性和鲁棒性上均优于现有方法，尤其在对抗条件下表现突出。

Conclusion: CAPITAL为将因果推理融入LLM提示提供了原则性方法，并展示了其在偏注意识情感推理中的优势。

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [35] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 通过简单监督提升语言模型与多样人群的对齐能力，提供实用指南和开源基准。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同人群对主观问题的回答具有重要价值。

Method: 使用相对简单的监督方法，评估多种语言模型和提示策略。

Result: 显著提高了语言模型与多样人群的对齐能力，并提供了具体群体的对齐差异。

Conclusion: 方法简单通用，易于采用，并为未来研究提供了有用的基准。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [36] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 研究发现，开放的LLM基准测试（如HELM）存在漏洞，通过微调小型模型在公开测试集上“作弊”可以取得高分，但实际泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 揭示开放基准测试的潜在问题，强调其可能误导模型评估。

Method: 通过微调BART、T5和GPT-2的小型变体在公开测试集上构造“作弊”模型。

Result: 这些模型在HELM基准上排名靠前，但泛化能力差，实用性有限。

Conclusion: 需结合私有或动态基准测试，并重新评估当前基准测试实践，以确保评估的可靠性。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [37] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 论文提出了一种模块化广告管理流程，用于RAG对话系统，包括广告重写器和分类器，通过合成数据训练分类器，优化广告插入策略。


<details>
  <summary>Details</summary>
Motivation: 生成式搜索系统模糊了信息与广告的界限，影响透明度和信任，需解决广告插入的挑战。

Method: 使用合成数据训练广告分类器，指导广告重写器的微调和最佳候选选择策略。

Result: 分类器检测性能强，优化策略显著提升广告隐蔽性。

Conclusion: 研究为广告感知生成系统和分类器提供了对抗性共进化框架。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [38] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Nirantar是一个用于评估多语言和多领域ASR中持续学习的框架，基于真实世界数据，涵盖22种语言和208个印度地区，支持多种增量学习场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有持续学习研究依赖模拟数据的局限性，提供真实、动态的语言和领域变化数据。

Method: 利用增量收集的3250小时语音数据，包括1720小时新数据，评估语言增量、领域增量及混合增量学习场景。

Result: 现有方法在Nirantar框架下表现不一致，表明需要更鲁棒的持续学习策略。

Conclusion: Nirantar为持续学习研究提供了更真实的测试平台，并揭示了现有方法的不足。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [39] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 提出了一种基于胶囊网络的用户语义意图建模算法，通过动态路由机制和卷积特征提取模块提升意图识别的准确性和层次关系捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 解决人机交互中意图识别精度不足的问题。

Method: 使用向量化胶囊结构和动态路由机制，结合卷积特征提取模块和基于边界的损失函数。

Result: 在公开数据集上表现优于传统方法和深度学习结构，验证了模型的稳定性和有效性。

Conclusion: 提出了一种新的结构化建模方法，提升了复杂语义条件下的意图识别能力。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [40] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 论文探讨了高级计算算法在理论研究中的应用及其方法学挑战，提出了确保主题建模研究严谨性的指南。


<details>
  <summary>Details</summary>
Motivation: 由于高级计算算法的不透明性和应用中的透明度不足，研究信任度可能受损，因此需要探讨方法学严谨性。

Method: 通过结构主题建模算法的应用示例和一套指南，讨论如何确保主题建模研究的严谨性。

Result: 提出的指南不仅适用于主题建模算法，也可通过调整应用于其他算法，对新手研究者、编辑和审稿人有帮助。

Conclusion: 论文为主题建模文献和方法学严谨性讨论做出了贡献。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [41] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 论文提出了一种多语言幻觉识别系统，结合检索式事实验证和BERT模型，在SemEval-2025任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的幻觉问题阻碍了其可信度和广泛应用，且现有研究多集中于英语数据，忽略了多语言场景。

Method: 采用两部分管道：基于Wikipedia的检索式事实验证和针对常见幻觉模式微调的BERT系统。

Result: 系统在八种语言中进入前10名，支持超过任务涵盖的14种语言。

Conclusion: 多语言幻觉识别器有助于提升LLM输出的质量和实用性。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [42] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: 提出了一种结合知识转移模块和参数高效微调策略的统一框架，以提升大语言模型在低资源语言场景中的迁移和适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言场景中迁移和适应能力有限的问题。

Method: 引入知识对齐损失和软提示调优，结合轻量级适应模块、冻结策略和提示注入，以最小标注实现高效迁移。

Result: 在跨语言任务（如MLQA、XQuAD和PAWS-X）中表现优于现有方法，尤其在数据稀缺条件下优势显著。

Conclusion: 该方法具有强通用性和可扩展性，既增强了任务适应性，又保留了大语言模型的通用能力。

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [43] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: MoR框架通过嵌入多样推理策略，使LLMs无需手动设计提示即可自适应任务，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs依赖任务特定提示，限制了适应性和效率。

Method: MoR分为两阶段：Thought Generation生成推理链模板，SFT Dataset Construction用模板和基准数据集进行监督微调。

Result: MoR150在CoT提示下性能提升2.2%，相比基线提升13.5%。

Conclusion: MoR提供了一种通用解决方案，适用于多样化任务的鲁棒推理。

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [44] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: SAFER框架通过稀疏自编码器（SAEs）解释和改进奖励模型，揭示人类可理解的特征，量化特征重要性，并设计数据毒化和去噪策略，提升LLM安全对齐。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在RLHF中核心但不透明，需改进以增强LLM与人类价值观的对齐。

Method: 利用稀疏自编码器（SAEs）分析奖励模型激活，设计数据毒化和去噪策略。

Result: SAFER能精确调整安全对齐，不影响通用聊天性能。

Conclusion: SAFER为高风险LLM对齐任务提供了解释、审计和改进奖励模型的方法。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [45] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: 研究发现，视觉语言模型（VLMs）在训练语言不同（如日语和英语）时，会表现出与人类文化认知相似的注意力模式差异。


<details>
  <summary>Details</summary>
Motivation: 探讨不同文化背景（东亚与西方）对视觉信息处理的影响是否也会体现在VLMs中。

Method: 通过比较分析图像描述，研究VLMs是否表现出整体性与分析性倾向的差异。

Result: VLMs不仅内化了语言结构特性，还再现了训练数据中的文化行为，表明文化认知可能隐式影响模型输出。

Conclusion: 文化认知可能通过训练数据影响VLMs的输出，使其表现出与人类相似的文化行为。

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [46] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）从时间序列数据生成财务报告的潜力，提出了一个包含提示工程、模型选择和评估的框架，并引入自动化高亮系统分类信息。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在财务报告生成中的应用潜力，评估其事实基础和推理能力。

Method: 提出框架，结合提示工程、模型选择和评估，引入自动化高亮系统分类信息。

Result: 实验表明，LLMs能生成连贯且信息丰富的财务报告。

Conclusion: LLMs在财务报告生成中具有潜力，框架和评估方法有助于提升其应用效果。

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [47] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: LitBench是一个用于评估创意写作的标准基准和数据集，通过人类偏好标签训练奖励模型，发现Claude-3.7-Sonnet是最强的零样本评委，奖励模型表现优于现成评委。


<details>
  <summary>Details</summary>
Motivation: 创意写作缺乏明确的评估标准，现成语言模型作为评委的可靠性存疑，因此需要一种可靠的自动化评估方法。

Method: 引入LitBench数据集，训练Bradley Terry和生成奖励模型，并进行在线人类研究验证。

Result: Claude-3.7-Sonnet与人类偏好一致性达73%，奖励模型准确率达78%，优于现成评委。

Conclusion: LitBench为创意写作系统提供了可靠的自动化评估和优化资源。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [48] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 本文研究了一种功能性编程方法用于自然语言语义，通过基于类别的类型和效果系统及图解演算，提高了传统指称风格的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统指称风格在表达自然语言语义时存在局限性，需要更高效和表达性更强的方法。

Method: 提出基于类别的类型和效果系统，并构建图解演算模型以解析和处理效果。

Result: 该方法能高效计算句子的指称。

Conclusion: 功能性编程方法显著提升了自然语言语义的表达能力和计算效率。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [49] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: 本文回顾了GenAI在科学计量学中的应用，并探讨了其对领域的广泛影响。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI在科学计量学中的潜力及其对人类推理的模仿能力，以及对科学测量文本特征的可能影响。

Method: 通过介绍GenAI的生成和概率特性，结合其在科学计量学中的实验应用（如主题标注、引文分析等），进行批判性分析。

Result: GenAI在语言生成任务中表现良好，但在需要稳定语义或领域知识的任务中存在局限。建议系统比较不同模型在特定任务中的表现。

Conclusion: 需通过实证研究和理论反思来应对GenAI对科学知识生产模式的潜在影响。

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [50] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 研究发现，多智能体LLM系统在群体讨论中表现出类似人类的功利主义倾向，但其驱动机制与人类不同。


<details>
  <summary>Details</summary>
Motivation: 探讨多智能体LLM系统在道德判断中的集体行为，并与人类群体行为进行对比。

Method: 测试六种模型在独立和群体讨论条件下的道德困境表现。

Result: LLM群体更倾向于接受道德违规行为，但驱动机制与人类不同。

Conclusion: LLM集体行为的表面模仿人类，但内在机制不同，对AI对齐和多智能体设计有启示。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [51] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 论文提出了一种可扩展的人工评估协议及其自动化近似方法，用于评估主题模型和文档聚类，解决了传统方法依赖专家标签或自动化指标与人类偏好不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法要么依赖难以扩展的专家标签，要么使用与人类偏好不一致的自动化指标，无法满足实际需求。

Method: 设计了一种评估协议，通过人工标注或基于LLM的代理推断类别并应用于其他文档，收集了大量标注数据验证自动化代理的效果。

Result: 研究发现，最佳的LLM代理在统计上与人类标注者无显著差异，可作为自动化评估的合理替代。

Conclusion: 提出的协议和自动化代理为实际应用中的主题模型和文档聚类评估提供了高效且可靠的解决方案。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [52] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 论文探讨了使用文体测量学区分大型语言模型（LLM）与人类生成文本的方法，涉及模型归属、知识产权和AI伦理问题。通过构建基准数据集并应用树模型分类，结果显示在多类和二分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决LLM生成文本的归属问题及其对知识产权和AI伦理的影响，探索文体测量学在此领域的适用性。

Method: 方法包括构建基于Wikipedia的基准数据集，涵盖人类和多种LLM生成的文本，使用树模型（如LightGBM）结合文体特征（StyloMetrix和n-gram）进行分类。

Result: 结果显示多分类任务中Matthews相关系数达0.87，二分类准确率在0.79至1之间，Wikipedia与GPT-4的平衡数据集准确率高达0.98。

Conclusion: 结论表明，对于特定文本类型，文体测量学可以有效区分机器与人类生成文本，尤其在LLM日益复杂的背景下具有重要意义。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [53] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: TransLaw是一个多智能体框架，用于翻译香港法律判决，通过三个专门代理（翻译、注释、校对）协作，提高翻译准确性，并显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在法律翻译中的潜力，解决法律术语复杂、文化差异和语言结构严格等挑战。

Method: 采用三个代理（Translator、Annotator、Proofreader）协作的框架，支持自定义LLM配置。

Result: TransLaw在语义准确性、结构连贯性和风格忠实度上超越GPT-4o，但在复杂术语和风格自然度上仍不及人类专家。

Conclusion: TransLaw展示了LLM在法律翻译中的潜力，但仍需改进以完全替代人类专家。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [54] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 研究发现，数学问题的呈现方式带有文化背景，语言模型在文化适应版本上的表现普遍不如原版美国数据集，但具备推理能力的模型表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 数学问题常被认为文化中立，但实际呈现方式隐含文化背景。现有基准（如GSM8K）以西方文化为主，缺乏多样性。

Method: 通过提示转换和人工验证，为非洲、印度、中国、韩国和日本创建文化适应版本的GSM8K测试集，并评估六种不同规模的LLM在五种提示策略下的表现。

Result: 模型在原版美国数据集上表现最佳，文化适应版本表现较差，但具备推理能力的模型对文化差异更具鲁棒性。

Conclusion: 深层推理能力有助于缩小数学任务中文化呈现差异带来的性能差距。

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [55] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 研究发现，下游扩展定律仅在少数情况下（39%）符合线性趋势，实验设置的微小变化可能完全改变扩展趋势。


<details>
  <summary>Details</summary>
Motivation: 探讨下游扩展定律是否能预测任务性能，以及其适用条件和局限性。

Method: 对现有下游扩展定律数据进行元分析，评估线性扩展趋势的普遍性和影响因素。

Result: 仅39%的情况下符合线性扩展趋势，实验设置的微小变化可能显著改变扩展行为。

Conclusion: 需进一步研究扩展定律的成功条件，并关注偏离线性趋势的情况。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [56] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 论文介绍了MemeCMD，一个自动生成的中文多轮对话数据集，结合了上下文检索的表情包，解决了现有对话数据集缺乏多模态表达的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集多为纯文本或手动标注，缺乏多模态交互的表达力和上下文细微差别。

Method: 结合大规模MLLM标注的表情包库和双代理自动生成的对话，引入检索框架和自适应阈值确保表情包的上下文相关性和自然间隔使用。

Result: 实验表明，该方法能生成上下文恰当且多样化的表情包对话，为多模态对话AI提供了可扩展且隐私保护的资源。

Conclusion: MemeCMD为多模态对话研究提供了有效的数据集和方法支持。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [57] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: 论文探讨了利用计算系统发育方法处理同源词数据的挑战，指出当前缺乏自动生成大规模同源词数据集的可行方法，并验证了从BabelNet提取的数据集在系统发育推断中的不一致性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决计算系统发育方法在同源词数据应用中的瓶颈，即缺乏大规模数据集。

Method: 通过从BabelNet自动提取数据集，并分析其系统发育推断结果与标准黄金树的差异。

Result: 结果显示，从BabelNet提取的数据集生成的系统发育树与标准黄金树不一致，且其他多语言资源也难以提供更合适的特征矩阵。

Conclusion: 结论指出，当前无法为同源词数据生成足够大的数据集，限制了计算系统发育方法在历史语言学中的应用。

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [58] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 论文探讨了LLMs的道德自我修正能力，发现其存在表面性和诊断不足的悖论，并提出基于启发式数据集的改进方案。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决LLMs在道德自我修正中的表面性和诊断不足问题。

Method: 分析微调语料库中的话语结构，揭示启发式捷径的存在。

Result: 发现道德自我修正依赖启发式捷径，导致修正与诊断能力不一致。

Conclusion: 提出利用启发式数据集改进道德自我修正，并指出其泛化挑战。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [59] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 论文探讨了CLM和MLM在文本表示任务中的表现差异，提出了一种结合两者的双阶段训练策略，并在固定计算预算下实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究CLM和MLM在文本表示任务中的表现差异，明确其优势是否源于目标函数本身还是其他因素。

Method: 通过大规模对照实验，训练30个模型（参数从2.1亿到10亿），并进行超过15,000次微调和评估。

Result: MLM在文本表示任务中表现更好，但CLM模型更具数据效率和微调稳定性；双阶段训练策略（先CLM后MLM）在固定计算预算下表现最优。

Conclusion: 双阶段训练策略结合CLM和MLM的优势，尤其适用于从现有LLM生态系统初始化模型，显著降低训练成本。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [60] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: La Leaderboard是首个开源排行榜，用于评估西班牙和拉丁美洲语言及方言的生成型大语言模型（LLMs），旨在推动西班牙语社区的LLM发展。


<details>
  <summary>Details</summary>
Motivation: 激励开发能够代表西班牙语社区语言和文化多样性的LLMs。

Method: 结合66个数据集（包括巴斯克语、加泰罗尼亚语、加利西亚语及西班牙语方言），评估50个模型，并提供方法论指导。

Result: 展示了50个模型的评估结果，并提倡减少few-shot示例以降低环境影响。

Conclusion: La Leaderboard为西班牙语社区的LLM开发提供了评估标准，并鼓励其他语言的社区驱动排行榜发展。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [61] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena是一个开放协作的平台，用于评估基础模型在科学文献任务上的表现，通过社区投票和集体智慧驱动模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 传统科学文献理解与合成的基准测试缺乏社区参与，SciArena旨在通过社区投票和集体智慧提供更开放的评估方式。

Method: 采用类似Chatbot Arena的社区投票方法，支持23个开源和专有基础模型，收集了13,000多份研究者投票。

Result: 数据显示问题多样且符合实际文献需求，研究者评估具有一致性和高标注者间一致性。发布了SciArena-Eval基准测试，用于自动化评估系统的研究。

Conclusion: SciArena为科学文献任务提供了社区驱动的评估平台，但自动化评估方法仍需改进。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [62] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 提出了一种名为“moment sampling”的新方法，通过文本到视频时刻检索模型指导帧采样，提升长视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频中表现不佳，帧子采样常丢失关键帧或引入冗余信息，影响问答准确性。

Method: 使用轻量级时刻检索模型优先选择与问题最相关的帧。

Result: 在四个长视频问答数据集和四种先进视频大语言模型上验证了方法的有效性。

Conclusion: moment sampling显著提升了长视频问答的性能，是一种模型无关的优化方法。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [63] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: ER-EMU算法通过自适应经验回放和域距离度量选择历史数据，解决边缘模型在动态交通环境中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 动态交通环境中的周期性变化导致模型在适应新数据分布时遗忘旧知识，现有方法无法有效利用历史数据。

Method: 提出ER-EMU算法，结合FIFO经验缓冲区和DDM-ES算法，使用MK-MMD度量域距离，优先选择与当前域差异大的历史数据。

Result: 在Bellevue交通视频数据集上，ER-EMU显著提升了多种云边协同目标检测框架的性能。

Conclusion: ER-EMU通过优化历史数据选择和多样性训练，有效缓解灾难性遗忘，提升模型适应性。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [64] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一种多模态对比学习框架，通过将MRI图像与DICOM元数据对齐，学习对比感知表示，无需依赖人工标注。


<details>
  <summary>Details</summary>
Motivation: MRI扫描的对比度信息通常不完整或缺失，导致图像解释和检索困难，影响临床应用。

Method: 提出MR-CLIP框架，利用对比学习将MRI图像与DICOM元数据对齐，学习对比感知表示。

Result: 在跨模态检索和对比分类任务中表现优异，展示了其可扩展性和临床应用潜力。

Conclusion: MR-CLIP为解决MRI对比度信息缺失问题提供了有效方案，具有广泛的应用前景。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [65] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: 论文提出了三种方法来检测全切片图像（WSI）中的伪影，并比较了它们的性能。


<details>
  <summary>Details</summary>
Motivation: WSI在癌症诊断中广泛应用，但伪影会影响图像分析质量，因此需要有效的检测方法。

Method: 提出了三种方法：基于基础模型的FMA、基于深度学习的DLA和基于知识的KBA，用于检测六种常见伪影。

Result: FMA性能最佳（AUROC: 0.995），优于DLA（0.977）和KBA（0.940）。

Conclusion: FMA是检测WSI伪影的最有效方法，并开发了质量报告工具以辅助诊断。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [66] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文探讨了多模态大语言模型（MLLMs）在复杂任务中的表现，发现其在某些场景（如CaughtCheating）中表现极差，揭示了其局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多数基准测试中表现优异，但在某些复杂任务（如侦探推理）中表现不足，需要更挑战性的测试任务。

Method: 通过设计CaughtCheating任务（基于社交媒体中检测伴侣可疑行为的场景），对MLLMs进行实验和分析。

Result: 发现MLLMs在CaughtCheating任务中表现接近零，揭示了其在视觉感知和推理能力上的不足。

Conclusion: CaughtCheating任务为MLLMs提供了挑战性测试，成功解决此类任务将推动其达到人类侦探水平的感知和推理能力。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [67] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积（AFSD）过程中的完整性。采用粒子群优化（PSO）确定最佳分割阈值，结合梯度分析和距离变换生成注意力加权可视化，揭示传统成像难以观察到的缺陷和材料过渡区。


<details>
  <summary>Details</summary>
Motivation: 传统成像方法难以检测AFSD过程中的缺陷和材料过渡区，因此需要一种更精确的自动分割和可视化方法。

Method: 结合粒子群优化（PSO）和梯度分析、距离变换，生成注意力加权可视化，并通过多通道技术量化界面质量。

Result: PSO自动确定了最佳阈值（156-173），多通道可视化技术成功揭示了不完全结合和不均匀区域，提供了定量指标。

Conclusion: 该方法能有效识别AFSD中的缺陷和不均匀性，为增材制造组件的质量评估和工艺优化提供了新工具。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [68] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: AdaDeDup是一种混合框架，结合密度剪枝和模型反馈，自适应地剪枝冗余数据，显著提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集的冗余和计算负担挑战机器学习模型训练，现有方法存在任务无关性或计算成本高的问题。

Method: AdaDeDup通过密度剪枝和代理模型反馈，自适应调整剪枝阈值，保留关键数据并减少冗余。

Result: 在多个大规模目标检测基准测试中，AdaDeDup显著优于基线方法，减少性能下降54%以上，剪枝20%数据后仍接近原始性能。

Conclusion: AdaDeDup有效提升大规模模型训练的数据效率，代码已开源。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [69] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: VSF--Med是一个端到端的医疗视觉语言模型（VLM）漏洞评分框架，包含文本提示攻击模板、视觉扰动和八维评分标准，用于评估医疗VLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在临床环境中的安全性评估不足，VSF--Med旨在填补这一空白，提供系统化的安全评估工具。

Method: VSF--Med结合文本攻击模板、视觉扰动和LLM评分标准，生成30,000多种对抗性变体，并通过z-score归一化计算综合风险指标。

Result: 分析显示，主流VLM在攻击持续性、提示注入有效性和安全绕过成功率上均存在显著漏洞，Llama-3.2-11B-Vision-Instruct和GPT-4o表现尤为突出。

Conclusion: VSF--Med为医疗VLM的安全性提供了可复现的基准测试工具，揭示了当前模型的潜在风险。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [70] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA是一个多模态学习框架，通过文本对齐统一视觉和听觉输入，解决了语义对齐、时间同步、多尺度表示和稀疏信息检索等挑战，显著提升了长视频问答和跨模态理解的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法通常将不同模态分开处理，导致表示和推理的不一致性，需要一种统一的方法来优化多模态数据的处理。

Method: MANTA通过信息论优化实现跨模态语义对齐，自适应时间同步，分层内容表示和上下文感知检索，并采用数学框架证明其在令牌约束下的最优性。

Result: 在长视频问答任务中，MANTA将最先进模型的准确率提高了22.6%，在超过30分钟的视频中提升27.3%，同时在时间推理和跨模态理解任务中分别提升23.8%和25.1%。

Conclusion: MANTA通过结构化文本统一多模态表示，为多模态学习提供了新的理论基础和技术支持。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [71] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLOv7和YOLOv8的迁移学习系统，用于自动化检测植物病害，结果显示YOLOv8性能优越，适用于现代农业。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农业造成重大影响，早期检测至关重要，技术发展为自动化监测提供了可能。

Method: 使用YOLOv7和YOLOv8模型，通过微调植物叶片图像数据集，检测细菌、真菌和病毒病害。

Result: 模型性能指标优异，mAP为91.05，F1-score为89.40，YOLOv8表现最佳。

Conclusion: 该系统为植物病害早期检测提供了可扩展的自动化解决方案，有助于提高作物产量和可持续农业。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [72] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的图像增强方法，用于改善自动驾驶车辆在雪地环境中的感知性能。


<details>
  <summary>Details</summary>
Motivation: 基于学习的感知算法在分布外和代表性不足的环境中表现不佳，尤其是户外机器人因动态光照、季节和天气变化导致训练数据不足。

Method: 利用扩散模型进行图像增强，结合开放词汇语义分割模型过滤幻觉候选图像，以更接近部署环境的训练数据。

Result: 该方法能够控制训练数据中地面语义分布，并针对部署环境微调模型。

Conclusion: 扩散图像增强可扩展至其他环境（如沙地和火山地形），提升感知算法的适应性。

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [73] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: FreeLong和FreeLong++是无需训练的长视频生成框架，通过平衡频率分布和多分支架构提升视频的时序一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有短视频生成模型在生成长视频时存在时序一致性和视觉保真度下降的问题，尤其是高频失真现象。

Method: FreeLong通过全局低频和局部高频特征融合；FreeLong++扩展为多分支架构，实现多频段融合。

Result: 在无需额外训练的情况下，显著提升长视频生成质量（如4倍和8倍长度），支持多提示生成和可控视频生成。

Conclusion: FreeLong++为长视频生成提供了一种高效且灵活的解决方案，兼容现有模型并显著优于先前方法。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [74] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: SelvaBox是一个用于热带树冠检测的最大开放数据集，包含83,000多个标记树冠，显著提升了检测精度和零样本性能。


<details>
  <summary>Details</summary>
Motivation: 热带树冠检测对研究复杂生态系统至关重要，但现有数据集稀缺，限制了模型开发。

Method: 引入SelvaBox数据集，结合高分辨率无人机图像和多分辨率管道进行训练和评估。

Result: 高分辨率输入提升检测精度；SelvaBox训练的模型在零样本检测中表现优异。

Conclusion: SelvaBox为热带树冠检测提供了强大工具，公开数据集和代码促进进一步研究。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [75] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络（GNN）和主成分分析（PCA）的深度学习架构，用于从LiDAR 3D点云数据中识别植物组件，准确率超过80%。


<details>
  <summary>Details</summary>
Motivation: 传统方法在3D数据处理和植物组件识别上存在不足，需要更高效的方法。

Method: 结合GNN、PCA和KNN构建图结构，利用Edge-Conv和GAT层增强特征并分类植物组件。

Result: 模型在IoU平均准确率上超过80%，优于现有方法。

Conclusion: 图神经网络显著提升了植物组件的识别精度。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [76] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: 论文提出FiboSB数据集，用于6D姿态估计在协作任务中的挑战，并通过改进YOLO11-x算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以准确捕捉学生与物理对象的交互，6D姿态估计可解决这一问题。

Method: 引入FiboSB数据集，评估四种6D姿态估计算法，并改进YOLO11-x。

Result: 改进后的YOLO11-x在FiboSB上达到mAP_50为0.898。

Conclusion: FiboSB数据集和改进算法为协作场景中的6D姿态估计奠定了基础。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [77] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: VOCAL是一种基于对比学习的视觉里程计框架，通过贝叶斯推理和表示学习提升特征可解释性和多模态数据兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型视觉里程计方法依赖刚性几何假设，缺乏理论基础和可解释性。

Method: 将视觉里程计重新定义为标签排序问题，结合贝叶斯推理和表示学习，组织视觉特征以反映相机状态。

Result: 在KITTI数据集上验证了VOCAL的优越可解释性和灵活性。

Conclusion: VOCAL推动了视觉里程计向更通用和可解释的空间智能发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [78] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 提出了一种基于轻量级DNN的实时手语识别框架，解决了数据稀缺、高计算成本和帧率差异等问题，在边缘设备上实现低延迟高精度分类。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中的数据稀缺、高计算成本和训练与推理环境帧率差异等关键挑战。

Method: 通过编码手语特定参数（如手形、手掌方向、动作和位置）为向量化输入，并利用MediaPipe提取关键点，设计优化的DNN架构（<10MB）。

Result: 在343个手语分类中实现92%的准确率，延迟低于10ms，并集成到'slait ai'应用中。

Conclusion: 该框架在边缘设备上高效运行，为实时手语识别提供了可行解决方案。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [79] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为GazeTarget360的系统，用于从图像中估计360度视线目标，解决了现有方法在背景信息吸收和视线目标预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 在真实人机交互中，机器人理解人类视线目标是关键，但现有方法（如OpenFace）无法有效预测视线目标，尤其是在视线离开相机时。

Method: GazeTarget360整合了条件推理引擎，包括眼接触检测器、预训练视觉编码器和多尺度融合解码器。

Result: 交叉验证表明，GazeTarget360能在未见场景中准确预测视线目标，是首个高效且可部署的系统。

Conclusion: GazeTarget360为视线目标预测提供了创新解决方案，代码已开源。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [80] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: VirtualFencer系统通过无监督学习从视频中提取3D击剑动作和策略，并生成逼真的击剑行为。


<details>
  <summary>Details</summary>
Motivation: 击剑动作多样且受策略驱动，需要数据驱动建模来捕捉其复杂性。

Method: 提出VirtualFencer系统，从视频中无监督提取3D动作和策略，并生成行为。

Result: 系统能自我对抗、与真实击剑手动作对抗，并与专业击剑手互动。

Conclusion: VirtualFencer展示了数据驱动建模在击剑运动中的潜力。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [81] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: 提出了一种高效机器学习流程，用于解决度假租赁平台中房间场景发现和分组问题，以及识别卧室床型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 度假租赁平台中大量未分类的房产图片给旅行者理解空间布局带来挑战，需要一种高效解决方案。

Method: 结合监督学习的房间类型检测、重叠检测模型和聚类算法，并利用多模态大语言模型映射卧室床型。

Result: 整体流程表现优异，显著优于对比学习和预训练嵌入聚类等现有方法。

Conclusion: 提出的方法在实时和数据稀缺环境下高效，为旅行者提供了更好的空间布局理解。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [82] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: 提出了一种自监督管道，通过合成X射线视图自动生成多对多对应矩阵，无需手动标注，提升了多视图骨折检测的性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在多视图X射线分析中难以建立稳健的对应关系，这对临床诊断至关重要。

Method: 利用数字重建放射影像（DRR）从无标注CT体积自动生成合成X射线视图，并通过基于Transformer的训练预测多视图对应关系。

Result: 在合成和真实X射线数据集上的评估表明，引入对应关系提升了多视图骨折分类的性能。

Conclusion: 该方法为多视图X射线分析提供了一种有效的自监督预训练策略，显著提升了骨折检测的准确性。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [83] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 提出一种多保真度模型融合策略，用于减少多实例学习（MIL）方法在数字病理学中的性能波动。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全切片图像（WSIs）因高分辨率和尺寸大，难以直接应用深度学习模型。MIL方法虽有效，但性能波动大，影响可靠比较。

Method: 通过训练多个模型并基于验证分数平均最稳定和有前景的模型，减少性能波动。适用于任何现有MIL模型。

Result: 在2个数据集、3种初始化策略和5种MIL方法上验证，共2000多次实验，显著降低性能波动。

Conclusion: 该方法简化超参数调优，提高可重复性，同时保持计算效率。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [84] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: SR-LoRA利用预训练权重矩阵的稳定秩作为层间秩分配的先验，提升低秩自适应（LoRA）在领域差距大的任务中的适应性，无需额外搜索成本。


<details>
  <summary>Details</summary>
Motivation: 固定低秩结构的LoRA在领域差距大的任务中适应性不足，现有方法依赖计算密集型技术。

Method: 提出SR-LoRA框架，利用稳定秩指导层间秩分配，高效提升适应性。

Result: 在领域差距大的少样本任务中，SR-LoRA性能优于现有自适应LoRA方法。

Conclusion: SR-LoRA在性能和效率之间实现了更优的平衡。

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [85] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: MammoTracker是一个基于掩模引导的病灶追踪框架，用于自动化乳腺X光片中病灶的定位，通过全局搜索、局部搜索和分数细化模块实现粗到细的策略。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光片中病灶的准确追踪对乳腺癌进展监测和早期诊断至关重要，但目前自动化病灶对应仍是一个挑战。

Method: 提出MammoTracker框架，采用全局搜索、局部搜索和分数细化三模块的粗到细策略，并引入新数据集支持训练与评估。

Result: 实验结果显示，MammoTracker的平均重叠率为0.455，准确率为0.509，优于基线模型8%。

Conclusion: MammoTracker在病灶追踪方面表现出色，有望提升计算机辅助诊断系统的病灶进展分析能力。

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [86] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: 探索文本到视频模型是否可作为交互式世界模拟器，通过单张场景图像预测人类行为，无需显式条件。


<details>
  <summary>Details</summary>
Motivation: 研究视频生成模型是否具备感知场景功能（affordance perception）的潜力，以预测人类与环境交互。

Method: 基于场景图像和动作提示，微调模型以插入人物，确保行为、外观、协调性和场景功能的连贯性。

Result: 通过跨注意力热图分析，发现预训练视频模型具备无需标注数据的内在功能感知能力。

Conclusion: 视频生成模型可被重新用作交互式世界模拟器，具备感知场景功能的潜力。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [87] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: MOVi-MC-AC是一个新的多摄像头视频数据集，用于模态分割和内容完成，提供多视角对象跟踪和模态内容标签。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多摄像头视角和模态内容标签，限制了模态分割和内容完成的研究。

Method: 通过模拟多摄像头视频场景，提供一致的对象ID和模态内容标签。

Result: 数据集包含580万个对象实例，是模态分割领域最大的数据集，并首次提供真实模态内容标签。

Conclusion: MOVi-MC-AC填补了多摄像头视角和模态内容标签的空白，推动了计算机视觉领域的研究。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [88] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: 该研究提出了CGEarthEye框架，专为吉林一号卫星设计，包含五个不同参数规模的骨干网络，总参数达21亿。通过构建全球覆盖的多时相自监督学习数据集JLSSD，并结合多种对比策略进行预训练，CGEarthEye在10个基准数据集上表现出色，具备卓越的特征可视化、模型收敛性和参数效率。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率光学遥感影像获取渠道有限，限制了高分辨率遥感视觉基础模型（RSVFM）的发展。吉林一号卫星作为全球最大的亚米级商业遥感卫星星座，拥有丰富的亚米级影像资源，研究旨在利用这些资源推动RSVFM的进步。

Method: 提出CGEarthEye框架，包含五个不同参数规模的骨干网络；构建JLSSD数据集，采用多级表示聚类和采样策略；结合季节性对比、增强对比和掩码补丁标记对比策略进行预训练。

Result: 在10个基准数据集上，CGEarthEye实现了最先进的性能，并在特征可视化、模型收敛性、参数效率和实际应用中表现出色。

Conclusion: CGEarthEye的卓越表征能力有望推动吉林一号数据在传统地球观测应用中的更广泛和高效使用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [89] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: 提出了一种改进3D高斯泼溅（3DGS）的方法，解决了初始化、优化和密度控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 3DGS因其显式3D高斯表示而流行，但其依赖准确初始化，且在优化无序高斯分布为有序表面时存在困难，缺乏自适应密度控制机制。

Method: 1. 几何引导的初始化预测高斯参数；2. 表面对齐的优化策略；3. 动态自适应密度控制机制。

Result: 实现了高保真实时渲染，视觉质量显著提升，在复杂场景中表现优异。

Conclusion: 该方法在实时渲染高保真图像方面表现优于或与现有最佳方法相当。

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [90] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进U-net结构的签名手写去噪模型，通过离散小波变换和PCA变换增强去噪能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 手写签名在身份识别中具有重要作用，但历史样本常含干扰信息，给识别带来挑战。

Method: 采用改进的U-net结构，结合离散小波变换和PCA变换，提升去噪效果。

Result: 模型去噪效果显著优于传统方法，提高了签名图像的清晰度和可读性。

Conclusion: 该模型为签名分析和识别提供了更可靠的技术支持。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [91] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: 论文提出了一种名为ATLI的新方法，通过自适应选择有效的top-k logits并结合最大logit，显著提高了OOD检测的性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络对分布外（OOD）样本的预测往往过于自信，因此OOD检测对提升机器学习安全性至关重要。

Method: 提出ATLI方法，自适应确定模型特定的top-k logits，并将其与最大logit结合。

Result: 在ImageNet-1K基准测试中，ATLI将FPR95降低了6.73%（相比MaxLogit），并比其他先进方法进一步降低了2.67%。

Conclusion: ATLI方法在OOD检测中表现出色，显著优于现有方法。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [92] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: 提出了一种名为PlantSegNeRF的新方法，通过多视角RGB图像序列直接生成高精度植物器官点云，显著提升了分割精度和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云器官分割技术在分辨率、精度和跨物种通用性方面存在局限，需要一种更高效、高精度的解决方案。

Method: PlantSegNeRF结合2D实例分割、实例匹配模块和实例NeRF，从多视角图像生成包含颜色、密度、语义和实例信息的隐式场景，最终转换为高精度点云。

Result: 在语义分割任务中，PlantSegNeRF在精度、召回率、F1分数和IoU上平均提升16.1%-24.2%；在实例分割任务中，mPrec、mRec、mCov和mWCov平均提升11.7%-38.2%。

Conclusion: PlantSegNeRF为植物器官表型分析提供了高通量方法，并为植物科学大规模模型开发提供了高质量3D数据。

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [93] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: 提出一种高效且可扩展的数据集合成方法，解决大光圈相机因浅景深导致的模糊问题，无需依赖真实数据微调。


<details>
  <summary>Details</summary>
Motivation: 大光圈相机因浅景深导致图像模糊，固定对焦相机（如智能眼镜）难以添加自动对焦机制，现有深度学习模型因光学像差和散焦特性不匹配而表现不佳。

Method: 同时建模深度相关散焦和空间变化光学像差，解决计算复杂性和高质量RGB-D数据集稀缺问题。

Result: 实验表明，用低分辨率合成图像训练的网络能有效泛化到高分辨率（12MP）真实场景图像。

Conclusion: 提出的方法能高效合成数据集，解决实际应用中的模糊问题，且无需真实数据微调。

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [94] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: 提出了一种可自定义ROI的深度图像压缩方法，支持用户通过文本定义ROI并调整ROI与非ROI的质量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有ROI图像压缩方案固定ROI且无法平衡ROI与非ROI的质量，无法满足多样化用户需求。

Method: 开发了文本控制掩码获取模块（TMA）、自定义值分配机制（CVA）和潜在掩码注意力模块（LMA）。

Result: 实验证明该方法有效支持ROI自定义及质量权衡管理。

Conclusion: 该方法为ROI图像压缩提供了灵活性和用户定制能力。

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [95] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: MedDiff-FT是一种可控的医学图像生成方法，通过微调扩散基础模型，以数据高效的方式生成具有结构依赖性和领域特异性的医学图像。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中高质量训练数据稀缺的问题，以及现有扩散模型在医学成像中的局限性。

Method: 使用动态自适应引导掩码和轻量级随机掩码生成器，结合自动质量评估协议和掩码腐蚀技术。

Result: 在五个医学分割数据集上，MedDiff-FT的合成图像-掩码对将SOTA方法的Dice分数平均提高了1%。

Conclusion: MedDiff-FT在生成质量、多样性和计算效率之间取得了平衡，为医学数据增强提供了实用解决方案。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [96] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: 提出了一种名为Lift to Match (L2M)的两阶段框架，通过将2D图像提升到3D空间，利用大规模单视图图像实现鲁棒特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有特征匹配方法依赖稀缺且干净的多视图图像，限制了其在多样化场景中的泛化能力。传统特征编码器基于单视图2D图像训练，难以捕捉3D感知对应关系。

Method: 第一阶段通过多视图图像合成和3D特征高斯表示学习3D感知特征编码器；第二阶段利用新视图渲染策略和大规模合成数据学习特征解码器。

Result: 实验表明，该方法在零样本评估基准上表现出卓越的泛化能力。

Conclusion: L2M框架通过3D感知特征编码和合成数据增强，实现了跨领域的鲁棒特征匹配。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [97] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 提出了一种名为MIV-head的新方法，用于跨域少样本学习，无需微调主干网络，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在无法微调主干网络的情况下，处理低质量静态嵌入的少样本学习问题。

Method: 将少样本分类表示为多实例验证任务，设计MIV-head，无需微调主干网络，计算高效。

Result: 在跨域少样本图像分类任务中，MIV-head性能优于现有方法，且适应成本更低。

Conclusion: MIV-head是一种高效且性能优越的少样本学习方法，适用于实际应用场景。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [98] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: DiGA3D是一个统一的3D修复管道，通过扩散模型在粗到细的方式中传播一致的外观和几何信息，解决了多视图修复中的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D修复方法在处理远离参考视图的视角、多视图图像的外观不一致以及几何变化时的几何不一致性方面存在挑战。

Method: DiGA3D采用多参考视图选择策略、注意力特征传播机制（AFP）和纹理-几何分数蒸馏采样（TG-SDS）损失来提高一致性和性能。

Result: 实验表明，DiGA3D在多种3D修复任务中表现优异。

Conclusion: DiGA3D通过创新的策略和机制，显著提升了3D修复的一致性和效果。

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [99] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合频域分析的手写数学表达式识别方法（MFH），利用离散余弦变换（DCT）提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）因复杂的公式结构和字符布局而面临挑战，频域信息可能提供结构分析辅助。

Method: 提出MFH方法，将频域分析（DCT）与HMER结合，增强模型对公式结构的理解。

Result: 在CROHME 2014/2016/2019测试集上，MFH-CoMER分别达到61.66%、62.07%、63.72%的准确率。

Conclusion: 频域信息能显著提升HMER性能，MFH方法有效且具有潜力。

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [100] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: Latent-PMRF通过将PMRF方法改进到VAE的潜在空间中，以更好地与人类感知对齐，显著提升了盲脸恢复的性能和效率。


<details>
  <summary>Details</summary>
Motivation: PMRF方法在像素空间中建模，限制了其与人类感知的对齐能力，因此需要改进以更好地平衡感知质量和保真度。

Method: 提出Latent-PMRF，将PMRF重新定义在VAE的潜在空间中，利用潜在表示的最小失真估计来优化PD权衡。

Result: 实验表明，Latent-PMRF在盲脸恢复中优于现有方法，实现了5.79倍的FID加速。

Conclusion: Latent-PMRF通过潜在空间建模显著提升了PD权衡和收敛效率，为图像恢复提供了更优方案。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [101] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: 提出了一种名为ATSTrack的视觉语言跟踪方法，通过对齐视觉和语言输入的时间和空间尺度差异，提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言跟踪中，视觉输入与语言描述之间的不对齐问题阻碍了现有方法的性能，尤其是时间和空间尺度的差异未被充分探索。

Method: 将语言描述分解为具有不同属性的短语，并根据其与视觉输入的时间和空间对应关系进行细粒度特征修改；引入视觉语言令牌以利用前一帧的语言信息指导视觉特征提取。

Result: 实验表明，ATSTrack的性能与现有方法相当。

Conclusion: 通过时间和空间尺度的对齐，ATSTrack有效解决了视觉语言跟踪中的不对齐问题。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [102] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: MS-TTA是一种无需训练的测试时适应方法，通过kNN Mean-Shift增强CLIP的特征表示，提升分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖高置信度样本，忽略了低置信度样本的潜力，导致对分布偏移的适应能力不足。

Method: 提出MS-TTA，利用单步kNN Mean-Shift优化所有测试样本的特征表示，增强特征紧凑性和类别可分性。

Result: 在OOD和跨数据集基准测试中，MS-TTA表现优于现有方法，实现稳定适应。

Conclusion: MS-TTA无需额外训练即可显著提升CLIP在分布偏移下的性能。

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [103] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: 论文提出Bisecle方法，通过多方向监督模块和对比提示学习方案，解决视频语言持续学习中的灾难性遗忘和更新冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频是持续演化的数据流，现有视觉语言模型在持续学习中面临灾难性遗忘和更新冲突的挑战。

Method: 受海马体快速绑定和模式分离机制启发，设计多方向监督模块和对比提示学习方案。

Result: 在多个VideoQA基准测试中，Bisecle有效减轻遗忘并提升跨任务泛化能力。

Conclusion: Bisecle通过模仿海马体机制，实现了视频语言持续学习的鲁棒性和高效性。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [104] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出了一种基于自回归的帧级框架ARIG，用于实时生成更真实的交互式头部运动。


<details>
  <summary>Details</summary>
Motivation: 面对面对话是常见的人类活动，但现有方法在实时性和真实性上存在局限。

Method: 采用非向量量化的自回归过程建模运动预测，结合扩散过程表示运动分布，并强调交互行为理解和对话状态理解。

Result: 实验验证了模型的有效性。

Conclusion: ARIG框架在实时性和交互真实性上表现优异。

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [105] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: 提出了一种名为ADAptation的无监督主动学习框架，用于解决深度学习模型在分布偏移下的性能下降问题，通过扩散模型和双评分机制提升样本选择效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在训练和测试域分布偏移时的性能下降问题，同时减少标注成本。

Method: 利用扩散模型进行分布对齐，结合超球约束对比学习网络和双评分机制（不确定性和代表性）选择样本。

Result: 在四个乳腺超声数据集上验证，性能优于现有主动学习方法。

Conclusion: ADAptation框架在临床领域适应中表现出高效性和泛化能力。

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [106] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: 该论文提出了LMM-JND概念，用于量化大型多模态模型（LMMs）的视觉盲区，并构建了VPA-JND数据集，揭示了当前LMMs在视觉任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 研究LMMs在视觉感知任务中的缺陷，以解决潜在的安全问题和响应效率不足。

Method: 提出LMM-JND概念及其确定流程，构建包含21.5k参考图像和489k刺激的大规模数据集VPA-JND，分析多种LMM家族的视觉表现。

Result: 发现当前先进LMMs（如GPT-4o和InternVL2.5系列）在基本视觉比较任务中表现显著低于人类水平。

Conclusion: LMM-JND为研究LMMs提供了新视角，其可预测性对安全性至关重要。

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [107] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 论文提出了一种新的形状评估方法（CSS），用于衡量模型对全局配置形状的敏感性，发现高CSS模型依赖长程交互，并展示了形状和纹理的整合对稳健视觉系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型主要依赖局部纹理线索，忽略了形状配置的重要性，导致特征脆弱且非组合性。研究旨在评估模型对形状配置的绝对能力，而非与纹理对立的相对能力。

Method: 提出Configural Shape Score (CSS)，通过Object-Anagram对测量模型对全局形状配置的敏感性，并分析86种模型的性能。

Result: 发现高CSS模型（如DINOv2、SigLIP2、EVA-CLIP）依赖长程交互，表现出局部到全局编码的过渡，且CSS能预测其他形状依赖任务的表现。

Conclusion: 真正稳健且类人的视觉系统需整合局部纹理和全局形状配置，而非在二者间做人为选择。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [108] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: Laplace-Mamba框架结合Laplace频率先验与混合Mamba-CNN架构，通过双并行路径分别处理高低频成分，显著提升图像去雾效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于SSM的方法在重建局部结构和高维数据处理上表现不足，导致细粒度图像特征恢复不理想。

Method: 提出Laplace-Mamba框架，利用Laplace分解将图像分为低频和高频成分，分别通过SSM和CNN处理，结合Nyquist理论优化计算效率。

Result: 在多个基准测试中，Laplace-Mamba在恢复质量和效率上均优于现有方法。

Conclusion: Laplace-Mamba通过结合Laplace分解与混合架构，有效解决了图像去雾中的局部结构和高维数据问题。

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [109] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: Box-QAymo是一个用于评估和微调视觉语言模型（VLMs）的数据集和基准，专注于用户指定对象的空间和时间推理。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在真实场景中难以捕捉用户意图，现有数据集局限于全场景描述或路径预测，无法评估VLMs对局部用户驱动查询的响应能力。

Method: 提出Box-QAymo数据集，用户通过绘制边界框表达意图，支持层次化评估协议，包括属性预测、运动理解和时空推理。

Result: 当前VLMs在感知问题上的表现显著受限，揭示了与现实世界性能的差距。

Conclusion: 该工作为开发更鲁棒和可解释的自动驾驶系统奠定了基础，支持真实条件下的有效用户沟通。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [110] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: ExPaMoE提出了一种基于可扩展并行混合专家架构的新框架，用于解决持续测试时间适应中的特征纠缠和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖共享模型参数，易受大或非平稳域偏移的影响，导致特征纠缠和灾难性遗忘。

Method: ExPaMoE通过双分支专家设计和基于频率域线索的实时分布变化检测器（SODD），动态扩展专家池，解耦领域通用和领域特定知识。

Result: 在多个标准基准测试中，ExPaMoE表现优异，展示了强大的鲁棒性、可扩展性和抗遗忘能力。

Conclusion: ExPaMoE为CTTA提供了一种高效且可扩展的解决方案，适用于复杂域演化的长期适应。

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [111] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: 本文研究了CLIP图像编码器中注意力头的作用，提出了一种名为注意力消融技术（AAT）的方法，通过抑制特定头的贡献来提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在多种应用中表现稳健，但某些注意力头可能对最终表示产生负面影响，消融这些头可能提升性能。

Method: 提出AAT方法，通过操纵注意力权重抑制特定头的贡献，并针对不同场景整合两种策略。

Result: 实验显示AAT在多个领域提升下游任务性能，跨模态检索召回率最高提升11.1%。

Conclusion: AAT能有效优化大规模视觉语言模型，且几乎不增加推理成本。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [112] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: LLaVA-SP通过添加六个空间视觉标记增强视觉表示，提出新型投影器和两种变体，显著提升多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: CLIP-ViT在捕捉局部关系时表现不佳，影响MLLMs的细节理解能力，需改进视觉表示。

Method: 提出LLaVA-SP，添加空间视觉标记，使用卷积核和交叉注意力机制，开发两种变体（Cropping和Pooling）。

Result: 实验表明LLaVA-SP在多模态基准测试中显著优于LLaVA-1.5，推理延迟几乎相同。

Conclusion: LLaVA-SP通过简单改进显著提升视觉表示和任务性能，代码和模型已开源。

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [113] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: 提出了一种名为SCING的简单有效框架，通过选择性视觉提示融合和扰动驱动一致性对齐，提升跨模态对齐和鲁棒性，避免了复杂适配器设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂适配器或模态特定调优，忽略了跨模态交互，导致高计算成本或对齐效果不佳。

Method: 提出选择性视觉提示融合（SVIP）和扰动驱动一致性对齐（PDCA），动态注入视觉特征并增强特征对齐鲁棒性。

Result: 在多个基准测试中表现优异，实现了性能与计算开销的最佳平衡。

Conclusion: SCING框架简化了设计，提升了跨模态对齐效果，具有高效推理优势。

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [114] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: ReStraV利用神经表示几何学区分AI生成视频与真实视频，通过分析时间曲率和逐步距离差异，实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致合成视频高度逼真，现有检测方法难以泛化且难以捕捉时间不一致性。

Method: 基于“感知拉直”假设，使用预训练的自监督视觉变换器（DINOv2）量化视频的神经表示域中的时间曲率和逐步距离，并训练分类器。

Result: 在VidProM基准测试中，轻量级分类器达到97.17%准确率和98.63% AUROC，显著优于现有方法。

Conclusion: ReStraV为AI生成视频检测提供了一种低成本、高效的解决方案，并揭示了神经表示几何学的新应用。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [115] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: ONLY是一种无需训练的解码方法，通过单次查询和单层干预减少大型视觉语言模型（LVLM）的幻觉问题，实现高效实时部署。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态任务中表现优异，但存在幻觉问题，影响实际应用可靠性。现有方法需要多次查询，效率低下。

Method: 提出ONLY方法，通过文本到视觉熵比选择性放大关键文本信息，仅需单次查询和单层干预。

Result: 实验表明，ONLY在多个基准测试中优于现有方法，且实现简单、计算成本低。

Conclusion: ONLY为LVLM的实时部署提供了高效解决方案，显著减少幻觉问题。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [116] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: TopoNet是一种用于腹腔镜肝脏标志物检测的新型拓扑约束学习框架，通过结合RGB-D特征和拓扑约束损失，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 肝脏标志物在腹腔镜手术中为外科医生提供关键解剖引导，但标志物的管状结构和术中动态变形增加了自动检测的难度。

Method: 采用snake-CNN双路径编码器捕捉RGB纹理和深度拓扑结构，提出边界感知拓扑融合模块（BTF）和拓扑约束损失函数。

Result: 在L3D和P2ILF数据集上的实验表明，TopoNet在精度和计算复杂度上表现优异。

Conclusion: TopoNet具有临床应用的潜力，代码已开源。

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [117] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: TopoStreamer提出了一种端到端的时序感知模型，用于车道段拓扑推理，通过流式属性约束、动态车道边界位置编码和车道段去噪，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在位置嵌入和时序多属性学习上的局限性阻碍了道路网络重建的准确性，因此需要改进。

Method: TopoStreamer引入了流式属性约束、动态车道边界位置编码和车道段去噪三项关键技术。

Result: 在OpenLane-V2数据集上，TopoStreamer在车道段感知和中心线感知任务中分别提升了3.4% mAP和2.1% OLS。

Conclusion: TopoStreamer通过改进时序一致性和位置信息学习，显著提升了车道段拓扑推理的准确性。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [118] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: LOD-GS提出了一种动态预测3D高斯原语最优滤波强度的框架，解决了3D高斯渲染中的锯齿问题，并通过新数据集验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对采样率不敏感，导致渲染质量不足或过度平滑，LOD-GS旨在解决这一问题。

Method: 引入一组基函数，以采样率为输入建模外观变化，实现采样率敏感滤波，并与3D高斯联合优化。

Result: 在公开数据集和新数据集上，LOD-GS实现了SOTA渲染质量，有效消除锯齿。

Conclusion: LOD-GS通过动态滤波和全面评估，显著提升了3D高斯渲染的质量和效率。

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [119] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种针对个性化模型的无害所有权验证方法，通过解耦相似特征来防御模型窃取攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对微调模型效果不佳或引入额外风险，需开发更安全有效的验证方法。

Method: 分三阶段：创建保留共同特征的影子模型，训练元分类器识别窃取模型，通过假设检验验证所有权。

Result: 在基准数据集上的实验验证了该方法能有效检测多种模型窃取类型。

Conclusion: 该方法为个性化模型提供了安全、高效的所有权验证方案。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [120] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种原型引导的特征对齐范式（PGFA），用于零样本骨架动作识别，通过端到端跨模态对比训练框架和原型引导的文本特征对齐策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在骨架特征判别性和骨架-文本对齐偏差方面的不足，提升零样本骨架动作识别的效果。

Method: 提出PGFA方法，包括端到端跨模态对比训练框架和原型引导的文本特征对齐策略。

Result: 在NTU-60、NTU-120和PKU-MMD数据集上，PGFA比SMIE方法分别提升了22.96%、12.53%和18.54%的准确率。

Conclusion: PGFA通过改进骨架特征判别性和对齐策略，显著提升了零样本骨架动作识别的性能。

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [121] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文综述了OOD检测在可信赖AI中的重要性，涵盖用例、数据集、评估指标、方法比较及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中未见的对象在现实世界中的识别问题，提升AI系统的可靠性和安全性。

Method: 通过比较OOD检测方法，分析模型结构、不确定性指标和分布距离分类，并结合不确定性校准技术。

Result: 提供了理论和实践见解，展示了3D视觉集成等新兴研究方向。

Conclusion: OOD检测是开发可靠AI系统的关键，未来研究应关注对抗性鲁棒性和失败识别。

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [122] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出一种基于预训练潜在扩散模型的无数据集统一图像恢复方法，通过循环后验采样和多模态理解模型提供语义先验，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么针对特定任务设计，泛化性差，要么依赖配对数据集，受限于闭集约束。

Method: 利用预训练潜在扩散模型，结合多模态理解模型提供语义先验，通过轻量级模块对齐退化输入，并采用循环细化进行后验采样。

Result: 实验证明该方法在性能和鲁棒性上优于现有技术。

Conclusion: 该方法为无数据集统一图像恢复提供了有效且鲁棒的解决方案。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [123] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: 论文提出Sim-MPNet用于医学图像分割，结合动态记忆权重损失注意力和双相似性全局内部增强模块，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 受猕猴初级视觉皮层中“祖母细胞”启发，探索其在医学图像分割中的应用价值。

Method: 设计Sim-MPNet，包括动态记忆权重损失注意力（DMW-LA）和双相似性全局内部增强模块（DS-GIM）。

Result: 在四个公开数据集上表现优于现有方法。

Conclusion: Sim-MPNet通过动态记忆和相似性增强，有效提升医学图像分割性能。

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [124] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的上下文感知学术情感识别方法（CLIP-CAER），并发布了首个涵盖多样化自然学习场景的学术情感数据集RAER。


<details>
  <summary>Details</summary>
Motivation: 学术情感分析对评估学生学习状态至关重要，但现有研究多集中于基础情感识别，缺乏针对学术情感的数据集和方法。

Method: 通过RAER数据集和CLIP-CAER方法，结合面部表情和上下文线索（如学习场景）进行学术情感识别。

Result: 实验表明，CLIP-CAER在学术情感识别上显著优于现有基于视频的面部表情识别方法。

Conclusion: 上下文信息对学术情感识别至关重要，CLIP-CAER为学术情感分析提供了有效工具。

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [125] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: CAVALRY-V是一种针对视频多模态大语言模型（V-MLLMs）的对抗攻击框架，通过双目标语义-视觉损失函数和高效的两阶段生成器，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 探索V-MLLMs在对抗攻击中的脆弱性，解决跨模态推理、时间依赖和计算限制的挑战。

Method: 提出双目标语义-视觉损失函数和两阶段生成器框架，结合大规模预训练和微调。

Result: 在多个基准测试中，CAVALRY-V平均提升22.8%的攻击效果，并在图像理解任务中提升34.4%。

Conclusion: CAVALRY-V为多模态系统的对抗研究提供了基础性方法。

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [126] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: 研究通过分析卡车CAN总线数据，评估三种分类器（ANN、RF、SVM）用于超车检测，发现多车数据训练和分数级融合策略能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 卡车安全超车对防止事故和交通效率至关重要，需通过ADAS系统准确预测超车行为。

Method: 使用Volvo提供的五辆卡车CAN总线数据，评估ANN、RF和SVM三种分类器，分析不同预处理配置对性能的影响。

Result: 多车数据训练提升泛化能力，分数级融合策略在多数情况下表现最佳，最终实现TNR=93%和TPR=86.5%。

Conclusion: 数据多样性和多车训练对超车检测至关重要，分数级融合策略显著提升分类性能。

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [127] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: World4Drive是一个端到端自动驾驶框架，通过自监督学习构建驾驶世界模型，无需感知标注即可生成多模态规划轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决依赖昂贵感知监督提取场景信息的挑战，实现无感知标注的端到端规划。

Method: 利用视觉基础模型提取场景特征，生成多模态轨迹，并通过世界模型选择器评估最佳轨迹。

Result: 在nuScenes和NavSim基准测试中表现优异，L2误差降低18.1%，碰撞率减少46.7%，训练收敛速度快3.75倍。

Conclusion: World4Drive展示了无感知标注的端到端自动驾驶的可行性，性能显著提升。

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [128] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 论文提出DeSimPL方法，通过减少训练中的简单样本比例，提升自标记检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 解决自标记检测器因简单样本比例过高而性能不及领域对齐方法的问题。

Method: 提出DeSimPL方法，包括实例级记忆库、伪标签更新策略、对抗样本和自适应加权损失。

Result: 实验表明DeSimPL显著提升性能，并在四个基准测试中验证了其有效性。

Conclusion: DeSimPL成功解决了简单样本偏差问题，为自标记检测器提供了性能提升的途径。

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [129] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: UMDATrack提出了一种统一域适应框架，能够在多种恶劣天气条件下保持高质量的目标状态预测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉目标跟踪方法在白天数据表现良好，但在恶劣天气条件下性能显著下降。

Method: 使用可控场景生成器合成少量未标记视频，设计域定制适配器（DCA）和目标感知置信度对齐模块（TCA）。

Result: UMDATrack在实验中显著超越现有先进视觉跟踪器，达到新的最佳性能。

Conclusion: UMDATrack通过统一域适应框架有效解决了恶劣天气条件下的目标跟踪问题。

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [130] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

TL;DR: 该论文提出了一种利用单张术中图像和术前数据高效训练NeRF的方法，解决了手术场景中多视图数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: NeRF在3D重建和视图合成中表现优异，但在手术中因数据有限而难以应用。本文旨在通过单张术中图像和术前数据实现高效训练。

Method: 利用术前MRI数据定义训练集，通过神经风格迁移将术中图像外观转移到训练集，结合WTC2和STROTSS避免过度风格化。

Result: 在四个神经外科案例中验证，与真实手术显微镜图像训练的NeRF模型相比，重建保真度和风格对齐表现优异。

Conclusion: 该方法证明了单图像NeRF训练在手术中的可行性，克服了传统多视图方法的限制。

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


### [131] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: 提出了一种基于低细节层次（LoD）城市模型的空中视觉定位新方法LoD-Loc v2，通过粗到细的策略实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖高LoD模型，但实际可用的多为低LoD模型，限制了无人机全球定位的潜力。

Method: 采用粗到细策略，包括建筑轮廓分割、粗姿态选择和细粒度粒子滤波优化。

Result: LoD-Loc v2首次实现低LoD模型定位，精度优于现有方法，并扩大了收敛范围。

Conclusion: 该方法显著提升了定位能力，适用于更广泛的场景，并发布了相关数据集推动研究。

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [132] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的全身抓握框架，解决了姿势生成和运动填充问题，实现了真实稳定的物体交互。


<details>
  <summary>Details</summary>
Motivation: 解决全身抓握中姿势生成和运动连续性的挑战，并克服手-物体交互数据稀缺的问题。

Method: 采用三阶段流程：抓握姿势生成、时间填充和LiftUp Transformer，并通过广义预训练提高数据效率。

Result: 在GRAB数据集上实验表明，该方法在连贯性、稳定性和视觉真实性上优于现有基线。

Conclusion: 模块化设计易于适配其他人体运动应用，具有广泛适用性。

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [133] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

TL;DR: GLM-4.1V-Thinking是一个视觉语言模型，通过强化学习和课程采样提升多模态推理能力，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动通用多模态推理技术的发展，提升模型在复杂任务中的表现。

Method: 大规模预训练构建视觉基础模型，结合强化学习与课程采样（RLCS）优化性能。

Result: 在28个公共基准测试中表现优异，优于同类模型，甚至与更大模型或闭源模型（如GPT-4o）竞争。

Conclusion: GLM-4.1V-Thinking展示了强大的多模态推理能力，为研究领域提供了开源资源。

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [134] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

TL;DR: CageAttack提出了一种基于笼形变形的框架，用于生成自然的对抗性点云，平衡了可转移性、不可防御性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性点云攻击方法受限于几何约束，导致可转移性和不可防御性不足，而现有非结构化方法可能导致不自然的变形。

Method: CageAttack通过构建目标对象的笼形结构，对笼顶点施加扰动，实现平滑自然的点云变形。

Result: 在七个3D深度神经网络分类器和三个数据集上的实验表明，CageAttack在可转移性、不可防御性和合理性方面优于现有方法。

Conclusion: CageAttack提供了一种有效的对抗性点云生成方法，平衡了多个关键性能指标。

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [135] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

TL;DR: 论文分析了线性注意力（Linear Attention）性能下降的原因，并提出了一种改进方法MALA，通过引入查询（Query）的幅度信息，使其性能接近标准Softmax Attention。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然复杂度低，但性能显著低于Softmax Attention，原因是其忽略了查询的幅度信息，导致注意力分布无法动态适应。

Method: 提出MALA方法，通过修改线性注意力的计算方式，引入查询的幅度信息，使其注意力分布更接近Softmax Attention。

Result: MALA在图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成等任务中表现优异。

Conclusion: MALA通过引入查询的幅度信息，显著提升了线性注意力的性能，同时保持了其低复杂度的优势。

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [136] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

TL;DR: BEV-VAE提出了一种基于3D建模的多视角图像生成方法，通过统一的BEV潜在空间和潜在扩散变换器实现一致且可控的视图合成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多视角图像生成需要跨视图的3D场景一致性，现有方法缺乏显式3D建模，因此提出结构化表示的重要性。

Method: BEV-VAE训练多视角图像变分自编码器以构建统一的BEV潜在空间，并使用潜在扩散变换器生成场景。

Result: 在nuScenes和Argoverse 2数据集上，BEV-VAE在3D一致性重建和生成方面表现优异。

Conclusion: BEV-VAE通过结构化表示实现了多视角图像的高效生成，支持任意视角和3D布局输入。

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [137] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

TL;DR: UPRE框架通过联合优化文本提示和视觉表示，解决了零样本域适应中的任务与视觉语言模型不对齐问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 零样本域适应（ZSDA）因目标域缺乏图像而具有挑战性，现有方法主要关注域分布偏移，忽视了检测任务与视觉语言模型（VLMs）之间的不对齐问题。

Method: 提出UPRE框架，结合多视角域提示和视觉表示增强模块，并采用多级增强策略（如相对域距离和正负分离）。

Result: 在九个基准数据集上的实验表明，UPRE在ZSDA检测场景中表现优异。

Conclusion: UPRE通过优化提示和视觉表示，有效解决了任务与模型不对齐问题，为ZSDA提供了新思路。

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [138] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种基于提升方案的双正交可调小波单元，放宽了正交性和滤波器长度限制，提升了CNN中的卷积、池化和下采样操作，显著提高了图像分类和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统小波单元的正交性和滤波器长度限制限制了灵活性，影响了CNN的性能。

Method: 通过提升方案构建双正交可调小波单元，放宽正交性和滤波器长度约束，优化卷积、池化和下采样操作。

Result: 在ResNet-18中，CIFAR-10分类准确率提升2.12%，DTD提升9.73%；在MVTec异常检测中表现优于现有方法。

Conclusion: 该方法在图像分类和异常检测任务中表现出色，验证了其灵活性和有效性。

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [139] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于强化学习的后训练策略，用于提升多模态大语言模型在多图像任务中的推理性能，通过合成高质量思维链数据和监督微调，结合规则强化学习，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在单图像场景中表现优异，但在多图像组合和多模态指令的复杂任务中性能下降，需要改进其跨图像推理和泛化能力。

Method: 采用强化学习后训练策略，包括合成思维链数据初始化、监督微调（LoRA），并通过拒绝采样和规则强化学习优化推理路径。

Result: 在MIG-Bench上提升9.04%，在多个域外推理基准上提升4.98%，在BLINK和MMIU子集上分别提升3.1%和2.4%。

Conclusion: 该方法有效提升了多模态大语言模型在多图像任务中的推理和泛化能力，具有实际应用潜力。

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [140] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

TL;DR: 提出了一种多模态图卷积网络（MMGCN），通过整合低帧率视觉数据和高帧率运动数据，减少动作分割中的过分割错误，提升时间一致性。


<details>
  <summary>Details</summary>
Motivation: 在协作机器人场景中，准确的时间动作分割对理解子活动标签及其时间结构至关重要，但现有方法因噪声问题容易导致过分割错误。

Method: 采用多模态图卷积网络（MMGCN），结合正弦编码策略、时间图融合模块和数据增强技术SmoothLabelMix，提升动作分割的鲁棒性和时间一致性。

Result: 在Bimanual Actions Dataset上表现优异，F1@10为94.5%，F1@25为92.8%，优于现有方法。

Conclusion: MMGCN通过多模态数据融合和增强技术，显著提升了动作分割的准确性和时间一致性。

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [141] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

TL;DR: LUViT通过联合预训练策略解决LLM与ViT的模态不匹配问题，显著提升视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM与ViT模态不匹配导致的直接融合效果不佳问题。

Method: 采用MAE预训练ViT，同时用LoRA层训练LLM块，实现联合优化。

Result: LUViT在多种下游视觉任务中表现显著提升。

Conclusion: LUViT为利用LLM知识进行视觉理解提供了更高效的方法。

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [142] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种开放世界动作分割框架，通过增强的金字塔图卷积网络、Mixup训练和时序聚类损失，显著提升了开放集分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在封闭世界动作分割表现良好，但难以泛化到开放世界场景，需要无需人工标注的模型来处理新动作。

Method: 提出EPGCN网络、Mixup训练合成分布外数据、时序聚类损失优化动作分组。

Result: 在Bimanual Actions和H2O数据集上，开放集分割性能提升16.9%，分布外检测性能提升34.6%。

Conclusion: 框架有效解决了开放世界动作分割问题，各组件通过消融研究验证了其贡献。

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [143] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

TL;DR: OptiPrune结合了分布感知的初始噪声优化和基于相似性的token剪枝，提升了文本到图像扩散模型的语义对齐和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在语义对齐和计算效率之间的权衡问题。

Method: 1. 分布感知噪声优化模块；2. 硬件高效的token剪枝策略。

Result: 在基准数据集上实现了最先进的提示-图像一致性，显著降低了计算成本。

Conclusion: OptiPrune在保持语义对齐的同时提升了效率，适用于资源受限的硬件。

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [144] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

TL;DR: 本文提出了一种基于TensorFlow的实时逆向运动学（IK）求解器，用于生成逼真的人体运动，解决了多约束问题中的误差累积和关节限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 实时生成逼真的虚拟人体运动在计算机图形学、虚拟环境、机器人和生物力学等领域具有重要意义。

Method: 利用TensorFlow的自动微分和即时编译技术，将正向和逆向运动学视为可微分操作，处理高自由度的人体骨骼模型。

Result: 实验表明，该求解器在SMPLX模型上表现优于现有方法，具有快速收敛、低计算开销和高成功率。

Conclusion: 该方法为实时人体运动生成提供了高效且逼真的解决方案。

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


### [145] [TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency](https://arxiv.org/abs/2507.00802)
*Minye Shao,Xingyu Miao,Haoran Duan,Zeyu Wang,Jingkun Chen,Yawen Huang,Xian Wu,Jingjing Deng,Yang Long,Yefeng Zheng*

Main category: cs.CV

TL;DR: TRACE是一个基于2D多模态条件扩散的框架，用于生成具有时空对齐的3D医学图像，解决了现有方法在解剖保真度、轴向长度和计算成本上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学图像生成方法存在解剖保真度低、轴向长度受限和计算成本高的问题，限制了其在资源有限地区的应用。

Method: TRACE通过将2D切片建模为视频帧对，结合分割先验和放射学报告实现解剖对齐，并利用光流保持时间一致性。推理时采用重叠帧策略生成灵活长度的3D序列。

Result: 实验表明，TRACE在计算效率和保持解剖保真度与时空一致性之间取得了平衡。

Conclusion: TRACE为3D医学图像生成提供了一种高效且可靠的解决方案，适用于临床实践。

Abstract: 3D medical image generation is essential for data augmentation and patient
privacy, calling for reliable and efficient models suited for clinical
practice. However, current methods suffer from limited anatomical fidelity,
restricted axial length, and substantial computational cost, placing them
beyond reach for regions with limited resources and infrastructure. We
introduce TRACE, a framework that generates 3D medical images with
spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.
TRACE models sequential 2D slices as video frame pairs, combining segmentation
priors and radiology reports for anatomical alignment, incorporating optical
flow to sustain temporal coherence. During inference, an overlapping-frame
strategy links frame pairs into a flexible length sequence, reconstructed into
a spatiotemporally and anatomically aligned 3D volume. Experimental results
demonstrate that TRACE effectively balances computational efficiency with
preserving anatomical fidelity and spatiotemporal consistency. Code is
available at: https://github.com/VinyehShaw/TRACE.

</details>


### [146] [Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data](https://arxiv.org/abs/2507.00822)
*Yasser El Jarida,Youssef Iraqi,Loubna Mekouar*

Main category: cs.CV

TL;DR: 使用CNN和Blender生成的合成粒子图像实现实时PSD测量，EfficientNet-B0表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统PSD测量方法耗时且受限，需要自动化解决方案。

Method: 利用Blender生成合成粒子图像训练CNN（ResNet-50、InceptionV3、EfficientNet-B0）预测PSD参数。

Result: EfficientNet-B0在计算效率和准确性上表现最佳，适合工业实时部署。

Conclusion: 合成数据训练CNN有效，为工业PSD监测提供自动化潜力。

Abstract: Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present a CNN-based methodology
trained on realistic synthetic particle imagery generated using Blender's
advanced rendering capabilities. Synthetic data sets using this method can
replicate various industrial scenarios by systematically varying particle
shapes, textures, lighting, and spatial arrangements that closely resemble the
actual configurations. We evaluated three CNN-based architectures, ResNet-50,
InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,
d50, d90). Results demonstrated comparable accuracy across models, with
EfficientNet-B0 achieving the best computational efficiency suitable for
real-time industrial deployment. This approach shows the effectiveness of
realistic synthetic data for robust CNN training, which offers significant
potential for automated industrial PSD monitoring. The code is released at :
https://github.com/YasserElj/Synthetic-Granular-Gen

</details>


### [147] [High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery](https://arxiv.org/abs/2507.00825)
*Hongxing Peng,Lide Chen,Hui Zhu,Yan Chen*

Main category: cs.CV

TL;DR: HEGS-DETR是一种针对无人机目标检测的改进Transformer框架，通过高频增强语义网络、高效小目标金字塔和选择性查询重收集模块，显著提升了小目标和密集场景的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临小目标、高密度分布和复杂背景的挑战，现有算法依赖手工组件且泛化能力有限，需要针对性解决方案。

Method: 提出HEGS-DETR框架，包括高频增强语义网络（HFESNet）、高效小目标金字塔（ESOP）、选择性查询重收集（SQR）和几何感知位置编码（GAPE）。

Result: 在VisDrone数据集上，HEGS-DETR比基线模型AP$_{50}$提升5.1%，AP提升3.8%，同时保持实时速度和减少参数。

Conclusion: HEGS-DETR有效解决了无人机目标检测的挑战，显著提升了性能，适用于复杂场景。

Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial
challenges, including small target sizes, high-density distributions, and
cluttered backgrounds in UAV imagery. Current algorithms often depend on
hand-crafted components like anchor boxes, which demand fine-tuning and exhibit
limited generalization, and Non-Maximum Suppression (NMS), which is
threshold-sensitive and prone to misclassifying dense objects. These generic
architectures thus struggle to adapt to aerial imaging characteristics,
resulting in performance limitations. Moreover, emerging end-to-end frameworks
have yet to effectively mitigate these aerial-specific challenges.To address
these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time
Detection Transformer framework tailored for UAVs. First, we introduce the
High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.
HFESNet preserves critical high-frequency spatial details to extract robust
semantic features, thereby improving discriminative capability for small and
occluded targets in complex backgrounds. Second, our Efficient Small Object
Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with
minimal computational overhead, significantly boosting small object detection.
Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware
Positional Encoding (GAPE) modules enhance the detector's decoder stability and
localization accuracy, effectively optimizing bounding boxes and providing
explicit spatial priors for dense scenes. Experiments on the VisDrone dataset
demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase
over the baseline, while maintaining real-time speed and reducing parameter
count by 4M.

</details>


### [148] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlík,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajová*

Main category: cs.CV

TL;DR: 论文探讨了在降水临近预报中，利用Echo Top Height（ETH）作为深度学习模型的辅助输入变量，以提升预报技能。虽然ETH在低雨强下表现良好，但在高雨强下效果不一致且可能增加误差。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对天气敏感行业至关重要，但现有深度学习模型多忽略3D雷达数据的垂直信息。研究旨在探索ETH作为辅助变量的潜力。

Method: 使用3D U-Net模型，将雷达反射率和ETH作为独立输入通道，分析ETH与降雨强度的关系。

Result: ETH在低雨强下能提升预报技能，但在高雨强下效果不稳定且可能导致低估降水强度。

Conclusion: ETH作为辅助变量在临近预报中有潜在价值，但需进一步研究其在高雨强下的表现和优化方法。

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [149] [UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection](https://arxiv.org/abs/2507.00849)
*Wei Li,Jiaman Tang,Yang Li,Beihao Xia,Ligang Tan,Hongmao Qin*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba架构的多模态无人机目标检测框架UAVD-Mamba，通过改进几何适应性和多模态特征互补性，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测在交通管理、农业等领域有广泛应用，但面临遮挡、小目标和形状不规则等挑战，需要一种鲁棒且高效的多模态检测方法。

Method: 设计了Deformable Token Mamba Block（DTMB）提升几何适应性，并分别处理RGB和红外模态；通过堆叠DTMB实现多尺度特征提取；改进了YOLOv11的SPPF和C3K2模块。

Result: 在DroneVehicle数据集上，mAP指标比基线OAFA方法提高了3.6%。

Conclusion: UAVD-Mamba通过多模态融合和多尺度特征提取，显著提升了无人机目标检测的性能。

Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in
traffic management, agriculture, emergency rescue, etc. However, it faces
significant challenges, including occlusions, small object sizes, and irregular
shapes. These challenges highlight the necessity for a robust and efficient
multimodal UAV object detection method. Mamba has demonstrated considerable
potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a
multimodal UAV object detection framework based on Mamba architectures. To
improve geometric adaptability, we propose the Deformable Token Mamba Block
(DTMB) to generate deformable tokens by incorporating adaptive patches from
deformable convolutions alongside normal patches from normal convolutions,
which serve as the inputs to the Mamba Block. To optimize the multimodal
feature complementarity, we design two separate DTMBs for the RGB and infrared
(IR) modalities, with the outputs from both DTMBs integrated into the Mamba
Block for feature extraction and into the Fusion Mamba Block for feature
fusion. Additionally, to improve multiscale object detection, especially for
small objects, we stack four DTMBs at different scales to produce multiscale
feature representations, which are then sent to the Detection Neck for Mamba
(DNM). The DNM module, inspired by the YOLO series, includes modifications to
the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In
particular, we employ cross-enhanced spatial attention before the DTMB and
cross-channel attention after the Fusion Mamba Block to extract more
discriminative features. Experimental results on the DroneVehicle dataset show
that our method outperforms the baseline OAFA method by 3.6% in the mAP metric.
Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.

</details>


### [150] [Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting](https://arxiv.org/abs/2507.00852)
*Fatemeh Sadat Daneshmand*

Main category: cs.CV

TL;DR: 论文提出了一种基于Mask R-CNN的计算机视觉系统，使工业机器人能够在非结构化环境中检测和抓取笔零件，无需固定位置约束，并在不同光照条件下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中的柔性制造系统需要机器人能够在非结构化环境中处理物体，而无需严格的定位约束。

Method: 采用Mask R-CNN方法，在ZHAW的完整笔制造线上实现并评估，解决了无位置约束的物体检测、极端光照变化的鲁棒性以及低成本摄像头的可靠性能三大挑战。

Result: 系统在多样化光照条件下达到95%的检测准确率，消除了结构化零件放置的需求，减少了30%的安装时间，并显著提高了制造灵活性。

Conclusion: 通过四种不同光照场景的广泛测试验证了该方法的实用性，适用于实际工业部署。

Abstract: Flexible manufacturing systems in Industry 4.0 require robots capable of
handling objects in unstructured environments without rigid positioning
constraints. This paper presents a computer vision system that enables
industrial robots to detect and grasp pen components in arbitrary orientations
without requiring structured trays, while maintaining robust performance under
varying lighting conditions. We implement and evaluate a Mask R-CNN-based
approach on a complete pen manufacturing line at ZHAW, addressing three
critical challenges: object detection without positional constraints,
robustness to extreme lighting variations, and reliable performance with
cost-effective cameras. Our system achieves 95% detection accuracy across
diverse lighting conditions while eliminating the need for structured component
placement, demonstrating a 30% reduction in setup time and significant
improvement in manufacturing flexibility. The approach is validated through
extensive testing under four distinct lighting scenarios, showing practical
applicability for real-world industrial deployment.

</details>


### [151] [SafeMap: Robust HD Map Construction from Incomplete Observations](https://arxiv.org/abs/2507.00861)
*Xiaoshuai Hao,Lingdong Kong,Rong Yin,Pengwei Wang,Jing Zhang,Yunfeng Diao,Shu Zhao*

Main category: cs.CV

TL;DR: SafeMap是一个用于自动驾驶的高清地图构建框架，通过G-PVR和D-BEVC模块解决多视角数据缺失问题，显著提升鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角相机数据不完整时表现不佳，SafeMap旨在解决这一问题，确保高清地图构建的准确性。

Method: SafeMap整合了G-PVR模块（基于高斯分布动态优先处理信息区域）和D-BEVC模块（利用全景BEV特征校正不完整观测的BEV表示），实现端到端地图重建。

Result: 实验表明，SafeMap在完整和不完整场景下均显著优于现有方法，表现出更高的性能和可靠性。

Conclusion: SafeMap是一种易于实现且兼容现有系统的解决方案，为自动驾驶高清地图构建提供了鲁棒性增强的即插即用方案。

Abstract: Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge of view importance to dynamically prioritize
the most informative regions based on the relationships among available camera
views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV
representations derived from incomplete observations. Together, these
components facilitate the end-to-end map reconstruction and robust HD map
generation. SafeMap is easy to implement and integrates seamlessly into
existing systems, offering a plug-and-play solution for enhanced robustness.
Experimental results demonstrate that SafeMap significantly outperforms
previous methods in both complete and incomplete scenarios, highlighting its
superior performance and reliability.

</details>


### [152] [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/abs/2507.00868)
*Simon Reiß,Zdravko Marinov,Alexander Jaus,Constantin Seibold,M. Saquib Sarfraz,Erik Rodner,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 探索视觉上下文学习的潜力，使单一模型能处理多任务并在测试时适应新任务，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 目标是让模型适应任务序列而非单个任务，解决涉及多步骤的复杂任务，并灵活定义视觉流程。

Method: 研究视觉上下文学习架构的特性与限制，提出基于合成组合任务生成引擎的新训练方法，并探索掩码训练目标。

Result: 为多模态医疗任务序列提供了重要见解，同时指出了需解决的挑战。

Conclusion: 视觉上下文学习在多任务适应和复杂任务解决方面具有潜力，但仍需进一步研究。

Abstract: In this paper, we explore the potential of visual in-context learning to
enable a single model to handle multiple tasks and adapt to new tasks during
test time without re-training. Unlike previous approaches, our focus is on
training in-context learners to adapt to sequences of tasks, rather than
individual tasks. Our goal is to solve complex tasks that involve multiple
intermediate steps using a single model, allowing users to define entire vision
pipelines flexibly at test time. To achieve this, we first examine the
properties and limitations of visual in-context learning architectures, with a
particular focus on the role of codebooks. We then introduce a novel method for
training in-context learners using a synthetic compositional task generation
engine. This engine bootstraps task sequences from arbitrary segmentation
datasets, enabling the training of visual in-context learners for compositional
tasks. Additionally, we investigate different masking-based training objectives
to gather insights into how to train models better for solving complex,
compositional tasks. Our exploration not only provides important insights
especially for multi-modal medical task sequences but also highlights
challenges that need to be addressed.

</details>


### [153] [GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond](https://arxiv.org/abs/2507.00886)
*Anna-Maria Halacheva,Jan-Nico Zaech,Xi Wang,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯溅射的场景中心3D视觉语言模型（VLM），通过语言和任务感知的场景表示，解决了现有方法对对象检测器的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉语言模型（VLMs）依赖对象检测器，导致处理瓶颈和分类灵活性受限。

Method: 直接嵌入语言特征到3D场景表示中，通过任务和位置引导的双重稀疏化器生成紧凑的任务相关令牌。

Result: 在域外设置中，性能比现有3D VLM提升五倍。

Conclusion: 该方法首次利用高斯溅射技术，实现了高效的3D场景理解和语言对齐。

Abstract: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.

</details>


### [154] [Masks make discriminative models great again!](https://arxiv.org/abs/2507.00916)
*Tianshi Cao,Marie-Julie Rakotosaona,Ben Poole,Federico Tombari,Michael Niemeyer*

Main category: cs.CV

TL;DR: Image2GS提出了一种从单图像重建逼真3D场景的新方法，专注于图像到3D的转换问题，通过解耦可见区域重建和未可见区域补全任务，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决从单图像重建3D场景的挑战性问题，特别是图像到3D转换的确定性任务。

Method: 使用优化的3D高斯斑点生成的可见性掩码，在训练中排除不可见区域，专注于可见区域的重建。

Result: 在可见区域的重建质量显著优于基线方法，同时在完整场景评估中与现有最优方法竞争。

Conclusion: 研究表明，将图像到3D转换作为独立问题处理，并采用专门技术，能有效提升重建效果。

Abstract: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.

</details>


### [155] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: MVP是一个多模态视频预测框架，结合深度视频特征、用户元数据和上下文信息，用于社交媒体视频流行度预测，并在SMP Challenge 2025中获胜。


<details>
  <summary>Details</summary>
Motivation: 社交媒体视频的流行度预测对内容推荐、趋势检测和用户参与有重要价值。

Method: MVP整合预训练模型的深度视频特征、用户元数据和上下文信息，通过系统预处理（如对数变换和异常值去除）提升模型鲁棒性，并使用梯度提升回归模型捕捉多模态复杂模式。

Result: MVP在SMP Challenge 2025的Video Track官方评估中排名第一。

Conclusion: MVP是一种有效且可靠的多模态视频流行度预测方法。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [156] [RTMap: Real-Time Recursive Mapping with Change Detection and Localization](https://arxiv.org/abs/2507.00980)
*Yuheng Du,Sheng Yang,Lingxuan Wang,Zhenghua Hou,Chengying Cai,Zhitao Tan,Mingxia Chen,Shi-Sheng Huang,Qiang Li*

Main category: cs.CV

TL;DR: RTMap通过多轨迹众包高精地图，解决单次遍历方法的感知不准确、遮挡和多智能体观测融合问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线高精地图方法存在感知不准确、遮挡和无法融合多智能体观测的局限性。

Method: RTMap采用不确定性感知的位置建模、概率感知的定位和实时道路结构变化检测。

Result: 实验显示RTMap在众包地图质量和定位精度上表现优异，支持下游预测和规划模块。

Conclusion: RTMap能异步提升众包地图的准确性和新鲜度，代码将开源。

Abstract: While recent online HD mapping methods relieve burdened offline pipelines and
solve map freshness, they remain limited by perceptual inaccuracies, occlusion
in dense traffic, and an inability to fuse multi-agent observations. We propose
RTMap to enhance these single-traversal methods by persistently crowdsourcing a
multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap
simultaneously addresses three core challenges in an end-to-end fashion: (1)
Uncertainty-aware positional modeling for HD map elements, (2)
probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)
real-time detection for possible road structural changes. Experiments on
several public autonomous driving datasets demonstrate our solid performance on
both the prior-aided map quality and the localization accuracy, demonstrating
our effectiveness of robustly serving downstream prediction and planning
modules while gradually improving the accuracy and freshness of the
crowdsourced prior-map asynchronously. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RTMap (Camera ready version
incorporating reviewer suggestions will be updated soon).

</details>


### [157] [Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations](https://arxiv.org/abs/2507.00981)
*Jack Nugent,Siyang Wu,Zeyu Ma,Beining Han,Meenal Parakh,Abhishek Joshi,Lingjie Mei,Alexander Raistrick,Xinyuan Li,Jia Deng*

Main category: cs.CV

TL;DR: PDE（Procedural Depth Evaluation）是一个新的基准测试，用于系统评估单目深度估计模型的鲁棒性，通过程序生成3D场景测试多种扰动。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准测试仅评估准确性，缺乏对鲁棒性的全面评估，因此需要一种新方法来测试模型在多种扰动下的表现。

Method: 使用程序生成技术创建3D场景，测试模型对物体、相机、材质和光照变化的鲁棒性。

Result: 分析揭示了当前先进深度模型在特定扰动下的挑战性表现。

Conclusion: PDE为深度估计研究提供了新的鲁棒性评估工具，有助于未来研究的改进。

Abstract: Recent years have witnessed substantial progress on monocular depth
estimation, particularly as measured by the success of large models on standard
benchmarks. However, performance on standard benchmarks does not offer a
complete assessment, because most evaluate accuracy but not robustness. In this
work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which
enables systematic robustness evaluation. PDE uses procedural generation to
create 3D scenes that test robustness to various controlled perturbations,
including object, camera, material and lighting changes. Our analysis yields
interesting findings on what perturbations are challenging for state-of-the-art
depth models, which we hope will inform further research. Code and data are
available at https://github.com/princeton-vl/proc-depth-eval.

</details>


### [158] [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/abs/2507.00992)
*Yuanrui Wang,Cong Han,YafeiLi,Zhipeng Jin,Xiawei Li,SiNan Du,Wen Tao,Yi Yang,shuanglong li,Chun Yuan,Liu Lin*

Main category: cs.CV

TL;DR: 提出了一种基于分割引导的文本生成框架，通过像素级视觉文本掩码作为统一条件输入，解决了现有方法在字体风格和颜色保留上的问题，并在性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成方法在准确渲染视觉文本时存在字体模糊、语义漂移和风格控制有限的问题，且依赖预渲染字形图像导致模型复杂性和灵活性降低。

Method: 采用分割引导框架，包括微调的双语分割模型用于精确提取文本掩码，以及优化的扩散模型结合自适应字形条件和区域特定损失。

Result: 在AnyText基准测试中表现最佳，显著优于现有方法，并在新引入的GlyphMM和MiniText基准测试中大幅领先。

Conclusion: 该方法在文本生成中表现出色，尤其在复杂布局和小文本渲染方面，验证了其强大的泛化能力和部署准备性。

Abstract: Text-to-image generation has greatly advanced content creation, yet
accurately rendering visual text remains a key challenge due to blurred glyphs,
semantic drift, and limited style control. Existing methods often rely on
pre-rendered glyph images as conditions, but these struggle to retain original
font styles and color cues, necessitating complex multi-branch designs that
increase model overhead and reduce flexibility. To address these issues, we
propose a segmentation-guided framework that uses pixel-level visual text masks
-- rich in glyph shape, color, and spatial detail -- as unified conditional
inputs. Our method introduces two core components: (1) a fine-tuned bilingual
segmentation model for precise text mask extraction, and (2) a streamlined
diffusion model augmented with adaptive glyph conditioning and a
region-specific loss to preserve textual fidelity in both content and style.
Our approach achieves state-of-the-art performance on the AnyText benchmark,
significantly surpassing prior methods in both Chinese and English settings. To
enable more rigorous evaluation, we also introduce two new benchmarks:
GlyphMM-benchmark for testing layout and glyph consistency in complex
typesetting, and MiniText-benchmark for assessing generation quality in
small-scale text regions. Experimental results show that our model outperforms
existing methods by a large margin in both scenarios, particularly excelling at
small text rendering and complex layout preservation, validating its strong
generalization and deployment readiness.

</details>


### [159] [ShapeEmbed: a self-supervised learning framework for 2D contour quantification](https://arxiv.org/abs/2507.01009)
*Anna Foix Romero,Craig Russell,Alexander Krull,Virginie Uhlmann*

Main category: cs.CV

TL;DR: ShapeEmbed是一种自监督表示学习框架，用于编码2D图像中物体的轮廓，生成对平移、缩放、旋转、反射和点索引不变的形状描述符。


<details>
  <summary>Details</summary>
Motivation: 解决形状量化中提取的测量值对保持物体固有几何的变换（如大小、方向和位置变化）不变性的核心挑战。

Method: 使用欧几里得距离矩阵表示物体轮廓，通过自监督学习框架ShapeEmbed生成不变形状描述符。

Result: ShapeEmbed生成的描述符在自然和生物图像的形状分类任务中优于现有方法。

Conclusion: ShapeEmbed在生物成像应用中具有潜在的重要价值。

Abstract: The shape of objects is an important source of visual information in a wide
range of applications. One of the core challenges of shape quantification is to
ensure that the extracted measurements remain invariant to transformations that
preserve an object's intrinsic geometry, such as changing its size,
orientation, and position in the image. In this work, we introduce ShapeEmbed,
a self-supervised representation learning framework designed to encode the
contour of objects in 2D images, represented as a Euclidean distance matrix,
into a shape descriptor that is invariant to translation, scaling, rotation,
reflection, and point indexing. Our approach overcomes the limitations of
traditional shape descriptors while improving upon existing state-of-the-art
autoencoder-based approaches. We demonstrate that the descriptors learned by
our framework outperform their competitors in shape classification tasks on
natural and biological images. We envision our approach to be of particular
relevance to biological imaging applications.

</details>


### [160] [DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution](https://arxiv.org/abs/2507.01012)
*Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: DAM-VSR是一个基于外观和运动解耦的视频超分辨率框架，通过结合Stable Video Diffusion和ControlNet，解决了传统方法在时间一致性和细节生成上的不足。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视频超分辨率面临复杂且不可预测的退化问题，现有方法在时间一致性和细节生成上表现不佳。

Method: 提出DAM-VSR框架，将视频超分辨率分解为外观增强（通过参考图像超分辨率）和运动控制（通过视频ControlNet），并采用运动对齐的双向采样策略。

Result: DAM-VSR在真实世界数据和AIGC数据上实现了最先进的性能，展示了强大的细节生成能力。

Conclusion: DAM-VSR通过解耦外观和运动问题，有效提升了视频超分辨率的性能，尤其在时间一致性和细节生成方面表现突出。

Abstract: Real-world video super-resolution (VSR) presents significant challenges due
to complex and unpredictable degradations. Although some recent methods utilize
image diffusion models for VSR and have shown improved detail generation
capabilities, they still struggle to produce temporally consistent frames. We
attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address
this issue. However, due to the intrinsic image-animation characteristics of
SVD, it is challenging to generate fine details using only low-quality videos.
To tackle this problem, we propose DAM-VSR, an appearance and motion
disentanglement framework for VSR. This framework disentangles VSR into
appearance enhancement and motion control problems. Specifically, appearance
enhancement is achieved through reference image super-resolution, while motion
control is achieved through video ControlNet. This disentanglement fully
leverages the generative prior of video diffusion models and the detail
generation capabilities of image super-resolution models. Furthermore, equipped
with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can
conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art
performance on real-world data and AIGC data, demonstrating its powerful detail
generation capabilities.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [161] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

TL;DR: 该论文提出了一种新算法，用于计算数据感知Declare模型的最优对齐，结合A*搜索和SMT求解，支持更丰富的数据依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法仅支持控制流或有限的数据感知扩展，无法处理复杂数据依赖关系，限制了实际应用。

Method: 结合A*搜索和SMT求解，引入修复动作逐步解决约束冲突，高效探索搜索空间。

Result: 实验证明该方法在效率和表达能力上优于现有技术，支持更复杂的数据依赖。

Conclusion: 该方法为数据感知过程模型的对齐问题提供了高效且表达力强的解决方案，具有实际应用潜力。

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


### [162] [LIMAO: A Framework for Lifelong Modular Learned Query Optimization](https://arxiv.org/abs/2507.00188)
*Qihan Zhang,Shaolin Xie,Ibrahim Sabek*

Main category: cs.DB

TL;DR: LIMAO是一种终身学习的查询优化框架，通过模块化设计和注意力机制，有效解决了动态查询环境中的性能问题，显著提升了查询执行效率。


<details>
  <summary>Details</summary>
Motivation: 传统学习型查询优化器（LQO）在动态查询环境中表现不佳，且存在灾难性遗忘问题，限制了其实际应用。

Method: LIMAO采用模块化终身学习技术、基于注意力的神经网络架构和高效训练范式，保留旧知识并适应新环境。

Result: 实验显示，LIMAO将查询执行时间提升40%，执行时间方差降低60%，并在动态工作负载中表现稳定。

Conclusion: LIMAO显著提升了LQO的性能，解决了灾难性遗忘问题，适用于实际查询优化场景。

Abstract: Query optimizers are crucial for the performance of database systems.
Recently, many learned query optimizers (LQOs) have demonstrated significant
performance improvements over traditional optimizers. However, most of them
operate under a limited assumption: a static query environment. This limitation
prevents them from effectively handling complex, dynamic query environments in
real-world scenarios. Extensive retraining can lead to the well-known
catastrophic forgetting problem, which reduces the LQO generalizability over
time. In this paper, we address this limitation and introduce LIMAO (Lifelong
Modular Learned Query Optimizer), a framework for lifelong learning of plan
cost prediction that can be seamlessly integrated into existing LQOs. LIMAO
leverages a modular lifelong learning technique, an attention-based neural
network composition architecture, and an efficient training paradigm designed
to retain prior knowledge while continuously adapting to new environments. We
implement LIMAO in two LQOs, showing that our approach is agnostic to
underlying engines. Experimental results show that LIMAO significantly enhances
the performance of LQOs, achieving up to a 40% improvement in query execution
time and reducing the variance of execution time by up to 60% under dynamic
workloads. By leveraging a precise and self-consistent design, LIMAO
effectively mitigates catastrophic forgetting, ensuring stable and reliable
plan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup
on selected benchmarks, highlighting its practical advantages in real-world
query optimization.

</details>


### [163] [Meaningful Data Erasure in the Presence of Dependencies](https://arxiv.org/abs/2507.00343)
*Vishal Chakraborty,Youri Kaminsky,Sharad Mehrotra,Felix Naumann,Faisal Nawab,Primal Pappachan,Mohammad Sadoghi,Nalini Venkatasubramanian*

Main category: cs.DB

TL;DR: 论文提出了一种精确的数据删除定义，确保通过依赖关系推断删除数据的范围受到限制，并设计了低成本实现机制。


<details>
  <summary>Details</summary>
Motivation: GDPR等法规对数据删除的定义模糊，导致合规困难，尤其是在数据依赖关系复杂的数据库中。

Method: 正式定义了数据删除的精确概念，设计了低成本删除机制，并探索了批量删除和预计算数据保留时间的策略。

Result: 算法在真实和合成数据集上验证了实用性和可扩展性。

Conclusion: 提出的方法解决了数据删除的模糊性问题，同时兼顾了成本和效率。

Abstract: Data regulations like GDPR require systems to support data erasure but leave
the definition of "erasure" open to interpretation. This ambiguity makes
compliance challenging, especially in databases where data dependencies can
lead to erased data being inferred from remaining data. We formally define a
precise notion of data erasure that ensures any inference about deleted data,
through dependencies, remains bounded to what could have been inferred before
its insertion. We design erasure mechanisms that enforce this guarantee at
minimal cost. Additionally, we explore strategies to balance cost and
throughput, batch multiple erasures, and proactively compute data retention
times when possible. We demonstrate the practicality and scalability of our
algorithms using both real and synthetic datasets.

</details>


### [164] [Towards Robustness: A Critique of Current Vector Database Assessments](https://arxiv.org/abs/2507.00379)
*Zikai Wang,Qianxi Zhang,Baotong Lu,Qi Chen,Cheng Tan*

Main category: cs.DB

TL;DR: 论文指出平均召回率作为向量数据库评估的主要指标存在问题，提出新指标Robustness-δ@K以更全面评估性能。


<details>
  <summary>Details</summary>
Motivation: 平均召回率掩盖了查询间的性能差异，可能导致系统在困难查询上表现不佳，影响下游应用。

Method: 提出Robustness-δ@K指标，衡量查询中召回率超过阈值δ的比例，并将其集成到现有基准中。

Result: 主流向量索引在稳健性上存在显著差异，稳健性更高的索引在实际应用中表现更好。

Conclusion: Robustness-δ@K能更全面地评估向量数据库性能，为索引选择和优化提供指导。

Abstract: Vector databases are critical infrastructure in AI systems, and average
recall is the dominant metric for their evaluation. Both users and researchers
rely on it to choose and optimize their systems. We show that relying on
average recall is problematic. It hides variability across queries, allowing
systems with strong mean performance to underperform significantly on hard
queries. These tail cases confuse users and can lead to failure in downstream
applications such as RAG. We argue that robustness consistently achieving
acceptable recall across queries is crucial to vector database evaluation. We
propose Robustness-$\delta$@K, a new metric that captures the fraction of
queries with recall above a threshold $\delta$. This metric offers a deeper
view of recall distribution, helps vector index selection regarding application
needs, and guides the optimization of tail performance. We integrate
Robustness-$\delta$@K into existing benchmarks and evaluate mainstream vector
indexes, revealing significant robustness differences. More robust vector
indexes yield better application performance, even with the same average
recall. We also identify design factors that influence robustness, providing
guidance for improving real-world performance.

</details>


### [165] [Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition](https://arxiv.org/abs/2507.00427)
*Hao Wu,Changzheng Wei,Yanhao Wang,Li Lin,Yilong Leng,Shiyu He,Minghao Zhao,Hanghang Wu,Ying Yan,Aoying Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种针对图数据库的零知识可验证性方法，通过分解复杂查询为更细粒度的操作符，优化了ZKP电路的实现，显著提升了性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 研究图数据库的零知识可验证性，解决查询执行正确性证明的隐私保护问题，克服图查询复杂度高的挑战。

Method: 将图查询分解为细粒度操作符，设计基于PLONKish算术化的专用ZKP电路，重点优化图扩展操作。

Result: 实现了ZKGraph系统，性能显著优于传统方法，运行时间和内存消耗均有大幅改善。

Conclusion: 通过分解和优化ZKP电路，ZKGraph为图数据库提供了高效且隐私保护的查询验证方案。

Abstract: This paper investigates the feasibility of achieving zero-knowledge
verifiability for graph databases, enabling database owners to
cryptographically prove the query execution correctness without disclosing the
underlying data. Although similar capabilities have been explored for
relational databases, their implementation for graph databases presents unique
challenges. This is mainly attributed to the relatively large complexity of
queries in graph databases. When translating graph queries into arithmetic
circuits, the circuit scale can be too large to be practically evaluated. To
address this issue, we propose to break down graph queries into more
fine-grained, primitive operators, enabling a step-by-step evaluation through
smaller-scale circuits. Accordingly, the verification with ZKP circuits of
complex graph queries can be decomposed into a series of composable
cryptographic primitives, each designed to verify a fundamental structural
property such as path ordering or edge directionality. Especially, having
noticed that the graph expansion (i.e., traversing from nodes to their
neighbors along edges) operation serves as the backbone of graph query
evaluation, we design the expansion centric operator decomposition. In addition
to constructing circuits for the expansion primitives, we also design
specialized ZKP circuits for the various attributes that augment this
traversal. The circuits are meticulously designed to take advantage of PLONKish
arithmetization. By integrating these optimized circuits, we implement ZKGraph,
a system that provides verifiable query processing while preserving data
privacy. Performance evaluation indicates that ZKGraph significantly
outperforms naive in circuit implementations of graph operators, achieving
substantial improvements in both runtime and memory consumption.

</details>


### [166] [Towards Efficient Random-Order Enumeration for Join Queries](https://arxiv.org/abs/2507.00489)
*Pengyu Chen,Zizheng Guo,Jianwei Yang,Dongjing Miao*

Main category: cs.DB

TL;DR: 本文提出了一种高效的随机顺序枚举算法，用于连接查询，具有最坏情况下的时间复杂度保证，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在数据分析中，连接结果的随机顺序枚举是一个基础但耗时的过程，现有方法缺乏高效且具有最坏情况保证的算法。

Method: 开发了一种随机顺序枚举算法，复杂度为$O(\frac{\mathrm{AGM}(Q)}{|Res(Q)|}\log^2|Q|)$延迟和$O(\mathrm{AGM}(Q)\log|Q|)$总运行时间，无需特定查询预处理。

Result: 算法在理论上接近最优，实验结果显示其显著优于现有方法。

Conclusion: 该算法高效且灵活，适用于多种数据库索引，为连接查询的随机枚举提供了实用解决方案。

Abstract: In many data analysis pipelines, a basic and time-consuming process is to
produce join results and feed them into downstream tasks. Numerous enumeration
algorithms have been developed for this purpose. To be a statistically
meaningful representation of the whole join result, the result tuples are
required to be enumerated in uniformly random order. However, existing studies
lack an efficient random-order enumeration algorithm with a worst-case runtime
guarantee for (cyclic) join queries. In this paper, we study the problem of
enumerating the results of a join query in random order. We develop an
efficient random-order enumeration algorithm for join queries with no large
hidden constants in its complexity, achieving expected
$O(\frac{\mathrm{AGM}(Q)}{|Res(Q)|}\log^2|Q|)$ delay,
$O(\mathrm{AGM}(Q)\log|Q|)$ total running time after $O(|Q|\log|Q|)$-time index
construction, where $|Q|$ is the size of input, $\mathrm{AGM}(Q)$ is the AGM
bound, and $|Res(Q)|$ is the size of the join result. We prove that our
algorithm is near-optimal in the worst case, under the combinatorial $k$-clique
hypothesis. Our algorithm requires no query-specific preprocessing and can be
flexibly adapted to many common database indexes with only minor modifications.
We also devise two non-trivial techniques to speed up the enumeration, and
provide an experimental study on our enumeration algorithm along with the
speed-up techniques. The experimental results show that our algorithm, enhanced
with the proposed techniques, significantly outperforms existing
state-of-the-art methods.

</details>


### [167] [RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries](https://arxiv.org/abs/2507.00839)
*Chiyu Hao,Jixian Su,Shixuan Sun,Hao Zhang,Sen Gao,Jianwen Zhao,Chenyi Zhang,Jieru Zhao,Chen Chen,Minyi Guo*

Main category: cs.DB

TL;DR: RapidStore是一种高效的内存动态图存储系统，通过分离读写查询管理和版本数据与图数据，解决了现有方法在并发读写操作中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 动态图存储系统在实时应用中至关重要，但现有方法存在读写干扰、版本管理开销大以及性能不平衡等问题。

Method: 提出RapidStore，采用解耦系统设计，分离读写查询管理，并设计动态图存储与并发控制机制协作。

Result: 实验表明，RapidStore实现了快速、可扩展的并发图查询，平衡了插入、搜索和扫描性能。

Conclusion: RapidStore显著提升了动态图存储系统的效率，适用于读密集型工作负载。

Abstract: Dynamic graph storage systems are essential for real-time applications such
as social networks and recommendation, where graph data continuously evolves.
However, they face significant challenges in efficiently handling concurrent
read and write operations. We find that existing methods suffer from write
queries interfering with read efficiency, substantial time and space overhead
due to per-edge versioning, and an inability to balance performance, such as
slow searches under concurrent workloads. To address these issues, we propose
RapidStore, a holistic approach for efficient in-memory dynamic graph storage
designed for read-intensive workloads. Our key idea is to exploit the
characteristics of graph queries through a decoupled system design that
separates the management of read and write queries and decouples version data
from graph data. Particularly, we design an efficient dynamic graph store to
cooperate with the graph concurrency control mechanism. Experimental results
demonstrate that RapidStore enables fast and scalable concurrent graph queries,
effectively balancing the performance of inserts, searches, and scans, and
significantly improving efficiency in dynamic graph storage systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [168] [CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training](https://arxiv.org/abs/2507.00217)
*Tiancheng Chen,Ales Kubicek,Langwen Huang,Torsten Hoefler*

Main category: cs.DC

TL;DR: CrossPipe是一个优化跨数据中心大型语言模型训练的框架，通过减少网络延迟和带宽限制的影响，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型训练资源需求超过单个数据中心，跨数据中心策略变得至关重要。

Method: CrossPipe结合流水线并行和数据并行通信，采用求解器优化或快速贪心算法生成优化调度方案。

Result: 实验显示，CrossPipe在相同内存约束下减少训练时间达33.6%，在通信延迟下仍保持高效。

Conclusion: CrossPipe在资源受限环境中显著提升可扩展性和资源利用率。

Abstract: Training large language models (LLMs) now requires resources that exceed a
single datacenter, making cross-datacenter strategies increasingly crucial. We
present CrossPipe, a framework designed to optimize model training across
geographically distributed datacenters by explicitly modeling and mitigating
the impact of network latency and limited bandwidth. It enables unified
analysis and optimization incorporating both pipeline parallelism (PP) and
opportunities for overlapping data parallelism (DP) communication. CrossPipe
generates optimized pipeline schedules using either solver-based optimal or
fast near-optimal greedy algorithms, built upon a flexible execution engine
that separates scheduling logic from communication details. Our evaluation
shows that CrossPipe reduces training time by up to 33.6\% compared to
traditional pipeline schedules under identical memory constraints. When memory
constraints are relaxed, CrossPipe maintains strong performance despite
communication delays, approaching the efficiency of idealized schedules without
delays. CrossPipe offers improved scalability and resource utilization,
particularly in environments with high network latency or limited bandwidth.

</details>


### [169] [Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs](https://arxiv.org/abs/2507.00418)
*Mohammad Firas Sada,John J. Graham,Elham E Khoda,Mahidhar Tatineni,Dmitry Mishin,Rajesh K. Gupta,Rick Wagner,Larry Smarr,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: 对Qualcomm Cloud AI 100 Ultra（QAic）加速器在大型语言模型（LLM）推理中的能效和性能进行了基准测试，并与NVIDIA和AMD的GPU进行了对比。


<details>
  <summary>Details</summary>
Motivation: 评估QAic加速器在高性能计算（HPC）应用中的潜力，特别是在能效方面的表现。

Method: 使用vLLM框架对15个开源LLM（参数从1.17亿到900亿不等）进行推理测试，并与NVIDIA（A100、H200）和AMD（MI300A）GPU进行对比。

Result: QAic在大多数情况下表现出较高的能效（每瓦吞吐量），性能良好。

Conclusion: QAic加速器在HPC应用中具有潜力，尤其是在能效方面表现突出。

Abstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100
Ultra (QAic) accelerator for large language model (LLM) inference, evaluating
its energy efficiency (throughput per watt) and performance against leading
NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform
(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90
billion parameters, are served using the vLLM framework. The QAic inference
cards appears to be energy efficient and performs well in the energy efficiency
metric in most cases. The findings offer insights into the potential of the
Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications
within the National Research Platform (NRP).

</details>


### [170] [Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor](https://arxiv.org/abs/2507.00428)
*Mohammad Firas Sada,John J. Graham,Mahidhar Tatineni,Dmitry Mishin,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: 论文探讨了如何利用P4可编程FPGA SmartNICs实现低延迟的机器学习推理，以支持网络边缘的灵活ML模型部署。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在网络操作中的重要性增加，需要低延迟的ML推理能力，以支持QoS预测和网络安全异常检测等任务。

Method: 采用P4编程范式，将神经网络和回归模型的权重与偏置存储在控制平面表查找中，实现灵活的可编程性和高效的模型部署。

Result: 通过P4-programmable FPGA SmartNICs，能够在网络边缘实现高吞吐量、低延迟的ML推理，且支持动态重配置。

Conclusion: P4编程范式为网络边缘的ML模型部署提供了灵活且高效的解决方案，独立于核心基础设施。

Abstract: As machine learning (ML) applications become integral to modern network
operations, there is an increasing demand for network programmability that
enables low-latency ML inference for tasks such as Quality of Service (QoS)
prediction and anomaly detection in cybersecurity. ML models provide
adaptability through dynamic weight adjustments, making Programming
Protocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an
ideal platform for investigating In-Network Machine Learning (INML). These
devices offer high-throughput, low-latency packet processing and can be
dynamically reconfigured via the control plane, allowing for flexible
integration of ML models directly at the network edge. This paper explores the
application of the P4 programming paradigm to neural networks and regression
models, where weights and biases are stored in control plane table lookups.
This approach enables flexible programmability and efficient deployment of
retrainable ML models at the network edge, independent of core infrastructure
at the switch level.

</details>


### [171] [LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference](https://arxiv.org/abs/2507.00507)
*Chuhao Xu,Zijun Li,Quan Chen,Han Zhao,Minyi Guo*

Main category: cs.DC

TL;DR: LLM-Mesh是一种针对中小型LLM的无服务器推理方案，通过异构硬件弹性共享提升服务能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM需求的增长，现有解决方案通常独占GPU资源，而现代CPU架构内置加速器却未充分利用。

Method: LLM-Mesh通过细粒度计算资源分配、内存扩展机制和资源碎片化减少策略，实现异构硬件共享。

Result: 实验表明，LLM-Mesh通过共享提升服务能力44%-63%，结合CPU优化后可达91%-159%。

Conclusion: LLM-Mesh展示了异构硬件共享在LLM推理中的潜力，显著提升资源利用率。

Abstract: The rise of LLMs has driven demand for private serverless deployments,
characterized by moderate-scale models and infrequent requests. While existing
solutions follow exclusive GPU deployment, we take a step back to explore
modern platforms and find that: Emerging CPU architectures with built-in
accelerators are capable of serving LLMs but remain underutilized, and both
CPUs and GPUs can accommodate multiple LLMs simultaneously.
  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized
LLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh
tackles three fundamental challenges: (1) precise, fine-grained compute
resource allocation at token-level to handle fluctuating computational demands;
(2) a coordinated and forward-looking memory scaling mechanism to detect
out-of-memory hazards and reduce operational overhead; and (3) a dual approach
that reduces resource fragmentation through proactive preemption and reactive
bin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that
LLM-Meshimproves service capacity by 44% - 63% through sharing, while further
leveraging CPUs boosts this to 91% - 159%.

</details>


### [172] [Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling](https://arxiv.org/abs/2507.00550)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 提出一种基于多智能体系统的弹性云资源扩展优化方法，通过分布式决策与全局协作提升资源调度响应性和系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决云计算环境中资源快速变化和任务负载高度不确定性的挑战。

Method: 采用多智能体系统，结合分布式决策、协作价值函数和轻量级状态预测模型，利用集中训练分散执行的强化学习框架。

Result: 实验表明，该方法在资源利用率、SLA违规控制和调度延迟方面优于现有方法，表现出强适应性和智能调节能力。

Conclusion: 为复杂云平台中的弹性资源扩展问题提供了一种高效可靠的新方法。

Abstract: This paper addresses the challenges of rapid resource variation and highly
uncertain task loads in cloud computing environments. It proposes an
optimization method for elastic cloud resource scaling based on a multi-agent
system. The method deploys multiple autonomous agents to perceive resource
states in parallel and make local decisions. While maintaining the distributed
nature of the system, it introduces a collaborative value function to achieve
global coordination. This improves the responsiveness of resource scheduling
and enhances overall system performance. To strengthen system foresight, a
lightweight state prediction model is designed. It assists agents in
identifying future workload trends and optimizes the selection of scaling
actions. For policy training, the method adopts a centralized training and
decentralized execution reinforcement learning framework. This enables agents
to learn effectively and coordinate strategies under conditions of incomplete
information. The paper also constructs typical cloud scenarios, including
multi-tenancy and burst traffic, to evaluate the proposed method. The
evaluation focuses on resource isolation, service quality assurance, and
robustness. Experimental results show that the proposed multi-agent scaling
strategy outperforms existing methods in resource utilization, SLA violation
control, and scheduling latency. The results demonstrate strong adaptability
and intelligent regulation. This provides an efficient and reliable new
approach to solving the problem of elastic resource scaling in complex cloud
platforms.

</details>


### [173] [DynoStore: A wide-area distribution system for the management of data over heterogeneous storage](https://arxiv.org/abs/2507.00576)
*Dante D. Sanchez-Gallegos,J. L. Gonzalez-Compean,Maxime Gonthier,Valerie Hayot-Sasson,J. Gregory Pauloski,Haochen Pan,Kyle Chard,Jesus Carretero,Ian Foster*

Main category: cs.DC

TL;DR: DynoStore是一个跨异构存储系统的数据管理系统，通过数据容器抽象和负载均衡算法提升性能与容错能力。


<details>
  <summary>Details</summary>
Motivation: 分布式数据管理存在异构协议、认证模型不统一等问题，需要一种统一的协调框架。

Method: DynoStore采用数据容器抽象和负载均衡算法，构建广域存储网络，使用纠删码策略确保容错。

Result: 性能提升10%，优于集中式云系统，容错能力优于传统系统。

Conclusion: DynoStore在性能和容错方面表现优异，适用于地理分布式环境。

Abstract: Data distribution across different facilities offers benefits such as
enhanced resource utilization, increased resilience through replication, and
improved performance by processing data near its source. However, managing such
data is challenging due to heterogeneous access protocols, disparate
authentication models, and the lack of a unified coordination framework. This
paper presents DynoStore, a system that manages data across heterogeneous
storage systems. At the core of DynoStore are data containers, an abstraction
that provides standardized interfaces for seamless data management,
irrespective of the underlying storage systems. Multiple data container
connections create a cohesive wide-area storage network, ensuring resilience
using erasure coding policies. Furthermore, a load-balancing algorithm ensures
equitable and efficient utilization of storage resources. We evaluate DynoStore
using benchmarks and real-world case studies, including the management of
medical and satellite data across geographically distributed environments. Our
results demonstrate a 10\% performance improvement compared to centralized
cloud-hosted systems while maintaining competitive performance with
state-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits
superior fault tolerance, withstanding more failures than traditional systems.

</details>


### [174] [Accelerating Loading WebGraphs in ParaGrapher](https://arxiv.org/abs/2507.00716)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: ParaGrapher是一个图加载API和库，用于高效加载大规模压缩图。本文提出PG-Fuse和CompBin两种优化方法，分别解决存储利用率和解压缩带宽问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: ParaGrapher在高效加载压缩图方面存在存储利用率低和解压缩带宽不足的问题，限制了其性能。

Method: 引入PG-Fuse优化存储访问，通过增大请求块大小和缓存；提出CompBin改进压缩格式，支持直接访问邻居节点。

Result: 在12个真实和合成图上测试，PG-Fuse和CompBin分别实现7.6倍和21.8倍的加速。

Conclusion: PG-Fuse和CompBin有效解决了ParaGrapher的局限性，显著提升了图处理性能。

Abstract: ParaGrapher is a graph loading API and library that enables graph processing
frameworks to load large-scale compressed graphs with minimal overhead. This
capability accelerates the design and implementation of new high-performance
graph algorithms and their evaluation on a wide range of graphs and across
different frameworks. However, our previous study identified two major
limitations in ParaGrapher: inefficient utilization of high-bandwidth storage
and reduced decompression bandwidth due to increased compression ratios. To
address these limitations, we present two optimizations for ParaGrapher in this
paper. To improve storage utilization, particularly for high-bandwidth storage,
we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE
(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the
size of requested blocks, reducing the number of calls to the underlying
filesystem, and caching the received blocks in memory for future calls. To
improve the decompression bandwidth, we introduce CompBin, a compact binary
representation of the CSR format. CompBin facilitates direct accesses to
neighbors while preventing storage usage for unused bytes. Our evaluation on 12
real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse
and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.

</details>


### [175] [PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds](https://arxiv.org/abs/2507.00824)
*Matthieu Pigaglio,Onur Ascigil,Michał Król,Sergi Rene,Felix Lange,Kaleem Peeroo,Ramin Sadre,Vladimir Stankovic,Etienne Rivière*

Main category: cs.DC

TL;DR: PANDAS是一种将数据可用性采样（DAS）与以太坊Danksharding要求集成的实用方法，支持在4秒内完成层2数据传播和采样。


<details>
  <summary>Details</summary>
Motivation: 解决以太坊层2协议在全局广播数据时扩展性受限的问题，同时满足Danksharding对数据可用性采样的严格时间要求。

Method: PANDAS通过轻量级直接交换传播层2数据并采样其可用性，设计考虑了消息丢失、节点故障和无响应参与者。

Result: 在1000节点集群和20000节点的模拟中，PANDAS在4秒内实现了行星级延迟下的数据传播和采样。

Conclusion: PANDAS为以太坊Danksharding提供了一种无需修改共识和节点发现协议的实用DAS集成方案。

Abstract: Layer-2 protocols can assist Ethereum's limited throughput, but globally
broadcasting layer-2 data limits their scalability. The Danksharding evolution
of Ethereum aims to support the selective distribution of layer-2 data, whose
availability in the network is verified using randomized data availability
sampling (DAS). Integrating DAS into Ethereum's consensus process is
challenging, as pieces of layer-2 data must be disseminated and sampled within
four seconds of the beginning of each consensus slot. No existing solution can
support dissemination and sampling under such strict time bounds.
  We propose PANDAS, a practical approach to integrate DAS with Ethereum under
Danksharding's requirements without modifying its protocols for consensus and
node discovery. PANDAS disseminates layer-2 data and samples its availability
using lightweight, direct exchanges. Its design accounts for message loss, node
failures, and unresponsive participants while anticipating the need to scale
out the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node
cluster and simulations for up to 20,000 peers shows that it allows layer-2
data dissemination and sampling under planetary-scale latencies within the
4-second deadline.

</details>


### [176] [A New Family of Thread to Core Allocation Policies for an SMT ARM Processor](https://arxiv.org/abs/2507.00855)
*Marta Navarro,Josué Feliu,Salvador Petit,María E. Gómez,Julio Sahuquillo*

Main category: cs.DC

TL;DR: 本文提出了一种名为SYNPA的线程到核心（T2C）分配策略家族，通过改进性能堆栈构建方法（ISC堆栈）和性能预测模型，显著提升了ARM处理器上多线程应用的性能。


<details>
  <summary>Details</summary>
Motivation: 现代高性能服务器广泛使用SMT处理器，但多线程应用间的干扰问题限制了性能优化。本文旨在通过改进性能堆栈和预测模型，提升T2C分配策略的准确性。

Method: 1. 提出ISC堆栈方法，克服ARM PMU的限制；2. 使用ISC堆栈作为输入，构建性能预测模型；3. 提出SYNPA系列T2C分配策略。

Result: 实验表明，最佳变体SYNPA4的周转时间比Linux提升了38%，是现有ARM处理器策略的3倍。

Conclusion: 本文的方法不仅适用于ARM处理器，还可推广到其他SMT处理器，为性能分析师提供了构建高性能堆栈的实用指导。

Abstract: Modern high-performance servers commonly integrate Simultaneous
Multithreading (SMT) processors, which efficiently boosts throughput over
single-threaded cores. Optimizing performance in SMT processors faces
challenges due to the inter-application interference within each SMT core. To
mitigate the interference, thread-to-core (T2C) allocation policies play a
pivotal role. State-of-the-art T2C policies work in two steps: i) building a
per-application performance stack using performance counters and ii) building
performance prediction models to identify the best pairs of applications to run
on each core.
  This paper explores distinct ways to build the performance stack in ARM
processors and introduces the Instructions and Stalls Cycles (ISC) stack, a
novel approach to overcome ARM PMU limitations. The ISC stacks are used as
inputs for a performance prediction model to estimate the applications'
performance considering the inter-application interference. The accuracy of the
prediction model (second step) depends on the accuracy of the performance stack
(first step); thus, the higher the accuracy of the performance stack, the
higher the potential performance gains obtained by the T2C allocation policy.
  This paper presents SYNPA as a family of T2C allocation policies.
Experimental results show that $SYNPA4$, the best-performing SYNPA variant,
outperforms turnaround time by 38\% over Linux, which represents 3$\times$ the
gains achieved by the state-of-the-art policies for ARM processors.
Furthermore, the multiple discussions and refinements presented throughout this
paper can be applied to other SMT processors from distinct vendors and are
aimed at helping performance analysts build performance stacks for accurate
performance estimates in real processors.

</details>


### [177] [Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona](https://arxiv.org/abs/2507.00909)
*Philip Colangelo,Ayse K. Coskun,Jack Megrue,Ciaran Roberts,Shayan Sengupta,Varun Sivaram,Ethan Tiao,Aroon Vijaykar,Chris Williams,Daniel C. Wilson,Zack MacFarland,Daniel Dreiling,Nathan Morey,Anuja Ratnayake,Baskar Vairamohan*

Main category: cs.DC

TL;DR: 论文提出了一种名为Emerald Conductor的纯软件方法，将AI数据中心转变为灵活的电网资源，无需大规模基础设施改造即可高效利用现有电力系统。


<details>
  <summary>Details</summary>
Motivation: AI的快速发展导致电力需求激增，威胁电网可靠性，增加社区能源基础设施成本，并阻碍AI创新。

Method: 通过实时电网信号协调AI工作负载，无需硬件修改或储能，在凤凰城的一个256-GPU集群中进行了试验。

Result: 试验期间，在电网高峰时段实现了25%的集群功耗降低，同时保持AI服务质量。

Conclusion: 该方法将数据中心重新构想为增强电网可靠性、降低成本和加速AI发展的交互式电网资产。

Abstract: Artificial intelligence (AI) is fueling exponential electricity demand
growth, threatening grid reliability, raising prices for communities paying for
new energy infrastructure, and stunting AI innovation as data centers wait for
interconnection to constrained grids. This paper presents the first field
demonstration, in collaboration with major corporate partners, of a
software-only approach--Emerald Conductor--that transforms AI data centers into
flexible grid resources that can efficiently and immediately harness existing
power systems without massive infrastructure buildout. Conducted at a 256-GPU
cluster running representative AI workloads within a commercial, hyperscale
cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in
cluster power usage for three hours during peak grid events while maintaining
AI quality of service (QoS) guarantees. By orchestrating AI workloads based on
real-time grid signals without hardware modifications or energy storage, this
platform reimagines data centers as grid-interactive assets that enhance grid
reliability, advance affordability, and accelerate AI's development.

</details>


### [178] [How Fast Can Graph Computations Go on Fine-grained Parallel Architectures](https://arxiv.org/abs/2507.00949)
*Yuqing Wang,Charles Colley,Brian Wheatman,Jiya Su,David F. Gleich,Andrew A. Chien*

Main category: cs.DC

TL;DR: 论文探讨了在细粒度并行架构上实现高效图计算的潜力，通过设计UpDown架构，在PageRank和BFS基准测试中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大规模图计算问题日益重要，但传统并行架构支持不足，研究旨在探索细粒度架构的潜力。

Method: 设计了UpDown架构，针对细粒度并行性和真实图的不规则性优化，并通过PageRank和BFS基准测试评估性能。

Result: 在256节点模拟和16,384节点投影中，UpDown系统分别实现了637K GTEPS PR和989K GTEPS BFS，性能远超以往最佳结果。

Conclusion: UpDown架构通过代码设计显著提升了图计算性能，证明了细粒度并行架构的潜力。

Abstract: Large-scale graph problems are of critical and growing importance and
historically parallel architectures have provided little support. In the spirit
of co-design, we explore the question, How fast can graph computing go on a
fine-grained architecture? We explore the possibilities of an architecture
optimized for fine-grained parallelism, natural programming, and the
irregularity and skew found in real-world graphs. Using two graph benchmarks,
PageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph
architecture, UpDown, to explore what performance codesign can achieve. To
demonstrate programmability, we wrote five variants of these algorithms.
Simulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes
(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS
on RMAT, exceeding the best prior results by 5x and 100x respectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [179] [Plan-Based Scalable Online Virtual Network Embedding](https://arxiv.org/abs/2507.00237)
*Oleg Kolosov,David Breitgand,Dean H. Lorenz,Gala Yadgar*

Main category: cs.NI

TL;DR: 论文提出了一种名为OLIVE的在线算法，用于解决边缘计算中的虚拟网络嵌入（VNE）问题，显著提升了处理请求的速度和规模。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中虚拟化应用的部署需求高度不可预测且规模庞大，现有在线VNE解决方案无法高效处理。

Method: OLIVE算法通过离线计算的聚合需求嵌入作为计划，动态调整以处理实际请求。

Result: OLIVE处理请求的速度比现有最佳方案快两个数量级，适用于实际边缘环境。

Conclusion: OLIVE为在线VNE问题提供了高效且可扩展的解决方案，适用于大规模边缘计算场景。

Abstract: Network virtualization allows hosting applications with diverse computation
and communication requirements on shared edge infrastructure. Given a set of
requests for deploying virtualized applications, the edge provider has to
deploy a maximum number of them to the underlying physical network, subject to
capacity constraints. This challenge is known as the virtual network embedding
(VNE) problem: it models applications as virtual networks, where virtual nodes
represent functions and virtual links represent communication between the
virtual nodes.
  All variants of VNE are known to be strongly NP-hard. Because of its
centrality to network virtualization, VNE has been extensively studied. We
focus on the online variant of VNE, in which deployment requests are not known
in advance. This reflects the highly skewed and unpredictable demand intrinsic
to the edge. Unfortunately, existing solutions to online VNE do not scale well
with the number of requests per second and the physical topology size.
  We propose a novel approach in which our new online algorithm, OLIVE,
leverages a nearly optimal embedding for an aggregated expected demand. This
embedding is computed offline. It serves as a plan that OLIVE uses as a guide
for handling actual individual requests while dynamically compensating for
deviations from the plan. We demonstrate that our solution can handle a number
of requests per second greater by two orders of magnitude than the best results
reported in the literature. Thus, it is particularly suitable for realistic
edge environments.

</details>


### [180] [Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols](https://arxiv.org/abs/2507.00337)
*Yuxin Liu,Tianyang Zhang,Qiang Wu,Ju Ren,Kyle Jamieson,Yaxiong Xie*

Main category: cs.NI

TL;DR: 论文提出CellNinjia和Gandalf系统，通过实时监测RAN操作并补偿其引入的延迟，显著提升了延迟协议在蜂窝网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络中RAN缓冲区引入的延迟与拥塞无关，挑战了延迟协议的假设，导致性能下降。

Method: 开发CellNinjia提供RAN操作实时可见性，Gandalf利用此信息识别并补偿RAN操作引入的延迟。

Result: 在4G LTE和5G网络中，Gandalf使Copa和PCC Vivace性能分别提升7.49倍和9.53倍。

Conclusion: 通过系统化处理RAN延迟，延迟协议在蜂窝网络中可实现其全部潜力。

Abstract: Delay-based protocols rely on end-to-end delay measurements to detect network
congestion. However, in cellular networks, Radio Access Network (RAN) buffers
introduce significant delays unrelated to congestion, fundamentally challenging
these protocols' assumptions. We identify two major types of RAN buffers -
retransmission buffers and uplink scheduling buffers - that can introduce
delays comparable to congestion-induced delays, severely degrading protocol
performance. We present CellNinjia, a software-based system providing real-time
visibility into RAN operations, and Gandalf, which leverages this visibility to
systematically handle RAN-induced delays. Unlike existing approaches that treat
these delays as random noise, Gandalf identifies specific RAN operations and
compensates for their effects. Our evaluation in commercial 4G LTE and 5G
networks shows that Gandalf enables substantial performance improvements - up
to 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols'
core algorithms, demonstrating that delay-based protocols can realize their
full potential in cellular networks.

</details>


### [181] [Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols](https://arxiv.org/abs/2507.00623)
*Daniel Mejías,Inhar Yeregui,Roberto Viola,Miguel Fernández,Mario Montagud*

Main category: cs.NI

TL;DR: 论文探讨了如何通过远程渲染解决XR应用对高性能设备的需求，并分析了不同流媒体协议（如WebRTC、DASH和基于QUIC的新协议）在WiFi和5G下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着XR应用的复杂性增加，轻量级设备往往缺乏足够的处理能力和带宽，因此需要远程渲染技术来解决问题。

Method: 研究整合了GStreamer和FFmpeg多媒体框架与远程渲染引擎，比较了不同流媒体协议在WiFi和5G下的性能。

Result: 提供了一个先进的测试平台，支持XR领域的前沿研究。

Conclusion: 该解决方案为XR领域的远程渲染和流媒体协议研究提供了创新性的测试环境。

Abstract: The increasing complexity of Extended Reality (XR) applications demands
substantial processing power and high bandwidth communications, often
unavailable on lightweight devices. Remote rendering consists of offloading
processing tasks to a remote node with a powerful GPU, delivering the rendered
content to the end device. The delivery is usually performed through popular
streaming protocols such as Web Real-Time Communications (WebRTC), offering a
data channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH),
better suitable for scalability. Moreover, new streaming protocols based on
QUIC are emerging as potential replacements for WebRTC and DASH and offer
benefits like connection migration, stream multiplexing and multipath delivery.
This work describes the integration of the two most popular multimedia
frameworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote
Renderer, and analyzes their performance when offering different protocols for
delivering the rendered content to the end device over WIFI or 5G. This
solution constitutes a beyond state-of-the-art testbed to conduct cutting-edge
research in the XR field.

</details>


### [182] [Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration](https://arxiv.org/abs/2507.00672)
*Haoxiang Luo,Yinqiu Liu,Ruichen Zhang,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Zehui Xiong,Xianbin Wang,Xuemin Shen*

Main category: cs.NI

TL;DR: 该论文探讨了在边缘计算中集成多LLM（大语言模型）以提升复杂任务的性能和适应性，并讨论了相关技术和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在处理复杂动态任务和多模态数据时表现不足，多LLM协作可以解决这一问题。

Method: 通过动态编排、资源调度和跨领域知识转移等技术实现多LLM系统，并关注可信赖的多LLM架构。

Result: 多LLM系统能够提升任务性能和适应性，尤其在资源受限的边缘环境中。

Conclusion: 多LLM系统为边缘计算提供了新的解决方案，未来需关注资源效率、隐私和信任等问题。

Abstract: Edge computing enables real-time data processing closer to its source, thus
improving the latency and performance of edge-enabled AI applications. However,
traditional AI models often fall short when dealing with complex, dynamic tasks
that require advanced reasoning and multimodal data processing. This survey
explores the integration of multi-LLMs (Large Language Models) to address this
in edge computing, where multiple specialized LLMs collaborate to enhance task
performance and adaptability in resource-constrained environments. We review
the transition from conventional edge AI models to single LLM deployment and,
ultimately, to multi-LLM systems. The survey discusses enabling technologies
such as dynamic orchestration, resource scheduling, and cross-domain knowledge
transfer that are key for multi-LLM implementation. A central focus is on
trusted multi-LLM systems, ensuring robust decision-making in environments
where reliability and privacy are crucial. We also present multimodal multi-LLM
architectures, where multiple LLMs specialize in handling different data
modalities, such as text, images, and audio, by integrating their outputs for
comprehensive analysis. Finally, we highlight future directions, including
improving resource efficiency, trustworthy governance multi-LLM systems, while
addressing privacy, trust, and robustness concerns. This survey provides a
valuable reference for researchers and practitioners aiming to leverage
multi-LLM systems in edge computing applications.

</details>


### [183] [Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework](https://arxiv.org/abs/2507.00856)
*Beining Wu,Jun Huang,Qiang Duan,Liang Dong,Zhipeng Cai*

Main category: cs.NI

TL;DR: 本文提出了一种结合无线联邦学习（WFL）的车队协同（VP）系统性能优化框架RACE，通过联合优化信息时效性（AoI）和联邦学习模型漂移（FLMD），显著提升了动态环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，车队协同系统面临频繁的通信变化和资源限制，影响信息交换和模型同步，亟需一种兼顾时效性和准确性的解决方案。

Method: 提出两阶段资源感知控制框架（RACE）：第一阶段使用拉格朗日对偶分解进行资源配置，第二阶段采用多智能体深度强化学习进行车辆选择，结合多头自注意力与LSTM网络捕捉时空相关性。

Result: 实验表明，RACE在AI4MARS数据集上优化AoI达45%，加速学习收敛，并更适应动态环境。

Conclusion: RACE框架有效解决了动态VP环境中的资源与模型同步问题，为WFL在VP中的应用提供了实用方案。

Abstract: This paper aims to enhance the performance of Vehicular Platooning (VP)
systems integrated with Wireless Federated Learning (WFL). In highly dynamic
environments, vehicular platoons experience frequent communication changes and
resource constraints, which significantly affect information exchange and
learning model synchronization. To address these challenges, we first formulate
WFL in VP as a joint optimization problem that simultaneously considers Age of
Information (AoI) and Federated Learning Model Drift (FLMD) to ensure timely
and accurate control. Through theoretical analysis, we examine the impact of
FLMD on convergence performance and develop a two-stage Resource-Aware Control
framework (RACE). The first stage employs a Lagrangian dual decomposition
method for resource configuration, while the second stage implements a
multi-agent deep reinforcement learning approach for vehicle selection. The
approach integrates Multi-Head Self-Attention and Long Short-Term Memory
networks to capture spatiotemporal correlations in communication states.
Experimental results demonstrate that, compared to baseline methods, the
proposed framework improves AoI optimization by up to 45%, accelerates learning
convergence, and adapts more effectively to dynamic VP environments on the
AI4MARS dataset.

</details>


### [184] [QUIC Delay Control: an implementation of congestion and delay control](https://arxiv.org/abs/2507.00896)
*Saverio Mascolo,Andrea Vittorio Balillo,Gioacchino Manfredi,Davide D'Agostino,Luca De Cicco*

Main category: cs.NI

TL;DR: QUIC-DC是一种新的拥塞和延迟控制算法，通过估计单向排队延迟来提前应对拥塞，显著减少数据包丢失和端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 解决实时应用中拥塞和排队延迟的问题，提升网络性能和用户体验。

Method: 结合TCP Westwood+算法，估计单向排队延迟并触发早期拥塞反应。

Result: 在仿真和实际网络中，QUIC-DC显著减少数据包丢失和延迟，同时保持网络利用率。

Conclusion: QUIC-DC适用于实时应用，能有效提升网络性能。

Abstract: A new congestion and delay control algorithm named QUIC Delay Control
(QUIC-DC) is proposed for controlling not only congestion but also the queuing
delay encountered along the forward communication path. The core idea is to
estimate the one-way queuing delay of a connection to trigger an early reaction
to congestion. This idea, along with the TCP Westwood+ congestion control
algorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2,
NewReno, Westwood+. The results obtained in both emulated and real network
connections show that QUIC-DC can significantly reduce packet losses along with
end-to-end communication delays, while preserving network utilization, features
that are both very useful for real-time applications.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [185] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: 提出了一种名为“不连贯性”的度量方法，用于在没有正确实现（oracle）的情况下量化LLM生成代码的错误概率。实验表明，该方法能高效识别约三分之二的错误程序，且无假阳性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码可能存在语法正确但事实错误的问题，需一种无需oracle的错误量化方法。

Method: 提出“不连贯性”度量，作为错误概率的下界，可高效估计。

Result: 实验显示，该方法能自动识别约三分之二的错误程序，且与oracle评估结果高度一致。

Conclusion: 不连贯性评估可替代oracle评估，且与LLM排名结果高度一致。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


### [186] [Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains](https://arxiv.org/abs/2507.00264)
*Isabella Basso do Amaral,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.PL

TL;DR: 比较研究评估了PyO3、ctypes和cffi在性能和易用性上的表现，发现PyO3结合Rust工具链能实现高性能且无需担心API兼容性。


<details>
  <summary>Details</summary>
Motivation: Python因其语法和科学库而受欢迎，但解释器速度慢，优化关键部分需要复杂的跨语言交互知识，手动实现繁琐。

Method: 通过比较PyO3、ctypes和cffi的性能和易用性，评估Rust工具链在Python中的表现。

Result: PyO3结合Rust工具链能实现高性能，且无需担心API兼容性问题。

Conclusion: PyO3为Python优化提供了一种高效且易用的解决方案。

Abstract: The Python programming language is best known for its syntax and scientific
libraries, but it is also notorious for its slow interpreter. Optimizing
critical sections in Python entails special knowledge of the binary
interactions between programming languages, and can be cumbersome to interface
manually, with implementers often resorting to convoluted third-party
libraries. This comparative study evaluates the performance and ease of use of
the PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using
Rust tooling developed for Python, we can achieve state-of-the-art performance
with no concern for API compatibility.

</details>


### [187] [Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?](https://arxiv.org/abs/2507.00488)
*Lloyd Allison*

Main category: cs.PL

TL;DR: 论文探讨了编程语言中函数分类不足的问题，提出了基于面向对象思想的函数分类方法，并在流行语言中进行了实验。


<details>
  <summary>Details</summary>
Motivation: 数学中的函数有明确的分类，而编程语言中的函数缺乏类似的结构化分类，尤其是在面向对象语言中缺少对函数的类化与子类化。

Method: 提出了一些具有特定属性的函数类，并在流行的编程语言中实现这些类，进行实验验证。

Result: 实验展示了在面向对象语言中实现函数分类的可行性，揭示了当前语言设计中的不足。

Conclusion: 编程语言设计可以借鉴数学中函数的分类方法，通过面向对象的方式丰富函数的功能和表达力。

Abstract: Compared to functions in mathematics, functions in programming languages seem
to be under classified. Functional programming languages based on the lambda
calculus famously treat functions as first-class values. Object-oriented
languages have adopted ``lambdas'', notably for call-back routines in
event-based programming. Typically a programming language has functions, a
function has a type, and some functions act on other functions and/or return
functions but there is generally a lack of (i) ``class Function'' in the OO
sense of the word class and particularly (ii) subclasses of Function for
functions having specific properties. Some such classes are presented here and
programmed in some popular programming languages as an experimental
investigation into OO languages missing this opportunity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [188] [VTS-Guided AI Interaction Workflow for Business Insights](https://arxiv.org/abs/2507.00347)
*Sun Ding,Ude Enebeli,Atilhan,Manay,Ryan Pua,Kamal Kotak*

Main category: cs.SE

TL;DR: VTS-AI 是一个结合视觉思维策略的 AI 系统，用于从非结构化报告中提取商业洞察，速度快且结果丰富。


<details>
  <summary>Details</summary>
Motivation: 现代企业面临大量非结构化报告，传统方法效率低且不敏捷，VTS-AI 旨在填补这一空白。

Method: 系统分为三层（微、中、宏观），通过标记问题、链接来源并生成可搜索的 YAML 文件，提取文本、表格和图像中的洞察。

Result: 在测试中，VTS-AI 速度与 ChatGPT 相当，但提供更丰富的结果（如页面位置、引用、严重性评分和因果链接）。

Conclusion: VTS-AI 有望成为快速商业分析的生产级工具，未来将扩展金融模型和风险层功能。

Abstract: Modern firms face a flood of dense, unstructured reports. Turning these
documents into usable insights takes heavy effort and is far from agile when
quick answers are needed. VTS-AI tackles this gap. It integrates Visual
Thinking Strategies, which emphasize evidence-based observation, linking, and
thinking, into AI agents, so the agents can extract business insights from
unstructured text, tables, and images at scale. The system works in three tiers
(micro, meso, macro). It tags issues, links them to source pages, and rolls
them into clear action levers stored in a searchable YAML file. In tests on an
18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt
yet produced richer findings: page locations, verbatim excerpts, severity
scores, and causal links. Analysts can accept or adjust these outputs in the
same IDE, keeping human judgment in the loop. Early results show VTS-AI spots
the direction of key metrics and flags where deeper number-crunching is needed.
Next steps include mapping narrative tags to financial ratios, adding
finance-tuned language models through a Model-Context Protocol, and building a
Risk & Safety Layer to stress-test models and secure data. These upgrades aim
to make VTS-AI a production-ready, audit-friendly tool for rapid business
analysis.

</details>


### [189] [An AST-guided LLM Approach for SVRF Code Synthesis](https://arxiv.org/abs/2507.00352)
*Abanoub E. Abdelmalak,Mohamed A. Elsayed,David Abercrombie,Ilhami Torunoglu*

Main category: cs.SE

TL;DR: 论文提出了一种结合AST嵌入和RAG的新方法，用于提升SVRF代码合成的准确性和效率，在740条DRC规则测试中表现优于传统方法40%。


<details>
  <summary>Details</summary>
Motivation: 随着半导体技术节点的进步，传统SVRF开发方法因复杂设计规则而失效，亟需新方法填补技术空白。

Method: 采用AST嵌入进行结构验证，结合RAG注入领域知识，提出SVRF专用评分框架。

Result: 在740条DRC规则测试中，代码生成准确性提升40%。

Conclusion: 该方法优化了SVRF开发流程，提升了生产效率和代码质量。

Abstract: Standard Verification Rule Format (SVRF) is essential for semiconductor
applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and
Optical Proximity Correction (OPC) and it faces challenges as advancing nodes
create complex design rules that renders traditional SVRF development
ineffective and highlight an expertise gap. This paper introduces a novel
methodology integrating Abstract Syntax Tree (AST) embedding and
Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring
semantic accuracy and error minimization through structural validation with
domain-specific insights for precise code generation.
  We evaluate different T5-based models and propose an innovative SVRF-specific
scoring framework that complements standard metrics like BLEU and ROUGE-L. In
our approach, AST provides rigorous structural validation, while RAG infuses
relevant domain knowledge, effectively enhancing the code generation workflow.
  Testing on a comprehensive benchmark of 740 DRC rule implementations, our
methodology demonstrates up to a 40\% improvement in code generation accuracy
compared to basic text-based fine-tuning process. This fusion of industry
expertise with advanced coding strategies not only optimizes SVRF development
under limited dataset constraints but also creates a more intuitive and
efficient coding environment. Consequently, users can rapidly iterate through
design cycles, reduce manual error correction, and significantly improve
overall productivity.

</details>


### [190] [iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing](https://arxiv.org/abs/2507.00378)
*Xikai Sun,Fan Dang,Kebin Liu,Xin Miao,Zihao Yang,Haimo Lu,Yawen Zheng,Yunhao Liu*

Main category: cs.SE

TL;DR: iPanda是一个利用大语言模型（LLMs）自动化协议一致性测试的端到端框架，显著提升了测试代码生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统协议一致性测试方法依赖人工创建测试用例和脚本，效率低下且耗时。LLMs的文本理解和代码生成能力为自动化提供了新机会。

Method: iPanda通过关键词方法生成测试用例，采用检索增强生成技术生成可执行测试代码，并通过迭代自校正机制优化代码质量。

Result: 实验表明，iPanda在测试代码生成成功率（Pass@1）上比纯LLM方法提升4.675至10.751倍。

Conclusion: iPanda框架为协议一致性测试提供了高效、自动化的解决方案，显著优于传统方法。

Abstract: Conformance testing is essential for ensuring that protocol implementations
comply with their specifications. However, traditional testing approaches
involve manually creating numerous test cases and scripts, making the process
labor-intensive and inefficient. Recently, Large Language Models (LLMs) have
demonstrated impressive text comprehension and code generation abilities,
providing promising opportunities for automation. In this paper, we propose
iPanda, the first end-to-end framework that leverages LLMs to automate protocol
conformance testing. Given a protocol specification document and its
implementation, iPanda first employs a keyword-based method to automatically
generate comprehensive test cases. Then, it utilizes a code-based
retrieval-augmented generation approach to effectively interpret the
implementation and produce executable test code. To further enhance code
quality, iPanda incorporates an iterative self-correction mechanism to refine
generated test scripts interactively. Finally, by executing and analyzing the
generated tests, iPanda systematically verifies compliance between
implementations and protocol specifications. Comprehensive experiments on
various protocols show that iPanda significantly outperforms pure LLM-based
approaches, improving the success rate (Pass@1) of test-code generation by
factors ranging from 4.675 times to 10.751 times.

</details>


### [191] [Recommending Variable Names for Extract Local Variable Refactorings](https://arxiv.org/abs/2507.00413)
*Taiming Wang,Hui Liu,Yuxia Zhang,Yanjie Jiang*

Main category: cs.SE

TL;DR: VarNamer是一种自动化方法，用于为提取局部变量重构推荐变量名，显著提高了与开发者手动命名的一致性，并在实际应用中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 现有IDE在提取局部变量重构时推荐的变量名与开发者手动命名差异较大，增加了开发者的重命名负担，VarNamer旨在解决这一问题。

Method: 通过大规模实证研究确定命名关键上下文，结合静态分析技术和数据挖掘技术，开发启发式规则推荐变量名。

Result: VarNamer在精确匹配率上显著优于Eclipse和IntelliJ IDEA，并在C++项目中表现出可比性能，用户研究表明其能加速重构并减少编辑。

Conclusion: VarNamer是一种高效且通用的变量命名推荐方法，已在Eclipse中集成并验证其实际价值。

Abstract: Extract local variable is one of the most popular refactorings, and most IDEs
and refactoring tools provide automated support for this refactoring. However,
we find approximately 70% of the names recommended by these IDEs are different
from what developers manually constructed, adding additional renaming burdens
to developers and providing limited assistance. In this paper, we introduce
VarNamer, an automated approach designed to recommend variable names for
extract local variable refactorings. Through a large-scale empirical study, we
identify key contexts that are useful for composing variable names. Leveraging
these insights, we developed a set of heuristic rules through program static
analysis techniques and employ data mining techniques to recommend variable
names effectively. Notably, some of our heuristic rules have been successfully
integrated into Eclipse, where they are now distributed with the latest
releases of the IDE. Evaluation demonstrates its superiority over
state-of-the-art IDEs. Specifically, VarNamer significantly increases the
chance of exact match by 52.6% compared to Eclipse and 40.7% compared to
IntelliJ IDEA. We also evaluated the proposed approach with real-world extract
local variable refactorings conducted in C++ projects, and the results suggest
that the approach can achieve comparable performance on programming languages
besides Java. It may suggest the generalizability of VarNamer. Finally, we
designed and conducted a user study and the results of the user study suggest
that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits
on the recommended variable names.

</details>


### [192] [Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development](https://arxiv.org/abs/2507.00421)
*Parthiv Katapara,Anand Sharma*

Main category: cs.SE

TL;DR: 本文综述了嵌入式系统中DevOps实践的适应情况，分析了工具、测试、自动化流水线和安全实践，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统的复杂性增加，硬件依赖和实时性要求使得DevOps实践在嵌入式环境中的适配成为研究重点。

Method: 通过分析20篇学术和工业文献，总结了嵌入式DevOps的工具、测试策略、流水线自动化和安全实践。

Result: 研究发现当前部署工作流和可观测性存在局限，提出了未来研究的路线图。

Conclusion: 本文为研究者和从业者提供了嵌入式DevOps的系统化视角，填补了文献碎片化的空白。

Abstract: The adoption of DevOps practices in embedded systems and firmware development
is emerging as a response to the growing complexity of modern
hardware--software co-designed products. Unlike cloud-native applications,
embedded systems introduce challenges such as hardware dependency, real-time
constraints, and safety-critical requirements. This literature review
synthesizes findings from 20 academic and industrial sources to examine how
DevOps principles--particularly continuous integration, continuous delivery,
and automated testing--are adapted to embedded contexts. We categorize efforts
across tooling, testing strategies, pipeline automation, and security
practices. The review highlights current limitations in deployment workflows
and observability, proposing a roadmap for future research. This work offers
researchers and practitioners a consolidated understanding of Embedded DevOps,
bridging fragmented literature with a structured perspective.

</details>


### [193] [The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach](https://arxiv.org/abs/2507.00481)
*Philipp M. Zähl,Sabine Theis,Martin R. Wolf*

Main category: cs.SE

TL;DR: 研究发现，开发者的HEXACO人格特质对软件团队的团队合作质量（TWQ）有显著影响，且人格因素比流程和工具更重要。初步数据（n=54）显示，人格特质组合、性别比例和年龄分布均影响TWQ。


<details>
  <summary>Details</summary>
Motivation: 认识到人格特质对软件开发团队合作的影响可能超过流程和工具，研究旨在量化HEXACO人格特质对TWQ的作用。

Method: 设计了一项研究，通过初步数据收集（n=54）分析HEXACO人格特质与TWQ的关系，并考察其他变量（如性别比例、年龄分布）的影响。

Result: 分析表明，多个HEXACO人格特质及其组合对TWQ有显著影响，性别比例和年龄分布也有作用。初步结果验证了研究设计的有效性。

Conclusion: 研究为IT组织改进团队合作提供了方向，并指出了进一步研究的途径。

Abstract: Although software engineering research has focused on optimizing processes
and technology, there is a growing recognition that human factors, particularly
teamwork, also significantly impact optimization. Recent research suggests that
developer personality has a strong influence on teamwork. In fact, personality
considerations may have a greater impact on software development than processes
and tools. This paper aims to design a study that measures the impact of HEXACO
personality traits on the Teamwork Quality (TWQ) of software teams. A
preliminary data collection (n=54) was conducted for this purpose. The analysis
showed that several personality traits, as well as their composition, had a
significant impact on TWQ. Additionally, other variables, such as the
proportion of women and age distribution, also affected TWQ. The study's
initial results demonstrate the usefulness and validity of the study design.
The results also suggest several opportunities to improve teamwork in IT
organizations and avenues for further research.

</details>


### [194] [Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey](https://arxiv.org/abs/2507.00496)
*Hongjing Guo,Chuanqi Tao,Zhiqiu Huang,Weiqin Zou*

Main category: cs.SE

TL;DR: 本文综述了深度学习中覆盖引导测试（CGT）的最新方法，包括测试覆盖率分析、测试输入生成和优化，并提出了分类法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在安全关键领域的应用增加，确保模型质量成为重要挑战。现有CGT研究分散，缺乏系统性总结。

Method: 通过全面回顾CGT方法，提出分类法，并分析评估实践，包括数据集、模型架构和评价维度。

Result: 总结了CGT的当前进展和趋势，并指出结构覆盖率与测试目标的关联性、方法通用性等挑战。

Conclusion: 本文为未来深度学习的质量保证研究提供了路线图，强调了标准化评估和工具支持的重要性。

Abstract: As Deep Learning (DL) models are increasingly applied in safety-critical
domains, ensuring their quality has emerged as a pressing challenge in modern
software engineering. Among emerging validation paradigms, coverage-guided
testing (CGT) has gained prominence as a systematic framework for identifying
erroneous or unexpected model behaviors. Despite growing research attention,
existing CGT studies remain methodologically fragmented, limiting the
understanding of current advances and emerging trends. This work addresses that
gap through a comprehensive review of state-of-the-art CGT methods for DL
models, including test coverage analysis, coverage-guided test input
generation, and coverage-guided test input optimization. This work provides
detailed taxonomies to organize these methods based on methodological
characteristics and application scenarios. We also investigate evaluation
practices adopted in existing studies, including the use of benchmark datasets,
model architectures, and evaluation aspects. Finally, open challenges and
future directions are highlighted in terms of the correlation between
structural coverage and testing objectives, method generalizability across
tasks and models, practical deployment concerns, and the need for standardized
evaluation and tool support. This work aims to provide a roadmap for future
academic research and engineering practice in DL model quality assurance.

</details>


### [195] [A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT](https://arxiv.org/abs/2507.00686)
*Ronny Seiger,Daniel Locher,Marco Kaufmann,Aaron F. Kurz*

Main category: cs.SE

TL;DR: 本文提出了一种名为Radiant的领域特定语言（DSL），用于将物联网（IoT）传感器数据抽象为业务流程级别的事件，并通过复杂事件处理（CEP）应用实时检测活动执行。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器数据过于细粒度，难以直接用于业务流程分析，需要一种方法将其抽象为更高层次的事件。

Method: 开发了Radiant DSL，用于指定传感器数据中的模式，并将其转换为CEP应用，以实现实时事件抽象。

Result: 在智能制造和智能医疗领域评估了CEP应用，结果显示其能有效监测活动执行，并为改进提供依据。

Conclusion: Radiant DSL和CEP应用为物联网传感器数据的业务流程分析提供了实用工具，帮助领域专家提升活动检测质量。

Abstract: Modern Internet of Things (IoT) systems are equipped with a plethora of
sensors providing real-time data about the current operations of their
components, which is crucial for the systems' internal control systems and
processes. However, these data are often too fine-grained to derive useful
insights into the execution of the larger processes an IoT system might be part
of. Process mining has developed advanced approaches for the analysis of
business processes that may also be used in the context of IoT. Bringing
process mining to IoT requires an event abstraction step to lift the low-level
sensor data to the business process level. In this work, we aim to empower
domain experts to perform this step using a newly developed domain-specific
language (DSL) called Radiant. Radiant supports the specification of patterns
within the sensor data that indicate the execution of higher level process
activities. These patterns are translated to complex event processing (CEP)
applications to be used for detecting activity executions at runtime. We
propose a corresponding software architecture for online event abstraction from
IoT sensor streams using the CEP applications. We evaluate these applications
to monitor activity executions using IoT sensors in smart manufacturing and
smart healthcare. The evaluation method and results inform the domain expert
about the quality of activity detections and potential for improvement.

</details>


### [196] [A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback](https://arxiv.org/abs/2507.00699)
*Guoliang Duan,Mingwei Liu,Yanlin Wang,Chong Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: MultiCodeIF是一个评估代码生成中指令遵循能力的基准，涵盖多维度约束，揭示LLMs在复杂指令下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注功能正确性，忽略了现实开发中的复杂约束，需更全面的评估框架。

Method: 提出MultiCodeIF基准，基于9类27种约束的自动化流程生成任务，评估LLMs在多轮反馈中的表现。

Result: Claude-3-7-Sonnet表现最佳（63.0%），但多层级约束下模型成功率降至18.8%；反馈可提升至83.4%。

Conclusion: MultiCodeIF为LLMs在真实代码生成场景中的评估提供了可扩展、反馈敏感的框架。

Abstract: Large language models (LLMs) have advanced significantly in code generation,
yet their ability to follow complex programming instructions with layered and
diverse constraints remains underexplored. Existing benchmarks often prioritize
functional correctness, overlooking the nuanced requirements found in
real-world development. We introduce MultiCodeIF, a comprehensive benchmark
designed to evaluate instruction-following in code generation across multiple
dimensions: constraint type, hierarchical levels, and iterative refinement.
Built upon a structured taxonomy of 9 categories and 27 constraint types,
MultiCodeIF enables granular assessment of both functional and non-functional
instruction adherence. Using an automated pipeline, ConstraGen, we synthesize
and evolve 2,021 code tasks sourced from 14 programming languages, supporting
multi-turn evaluation through feedback-driven task variants. Empirical
evaluation of six state-of-the-art LLMs uncovers substantial performance
disparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%
average constraint satisfaction, while smaller models like Qwen3-1.7B fall to
44.8%. Models perform well on explicit constraints, but struggle with implicit
or abstract constraints. Tasks with multiple hierarchical constraints
significantly reduce model success rates, from 54.5% in single-level to just
18.8% in multi-level scenarios. However, structured feedback enables
progressive improvement: average constraint satisfaction rises from 63.0% to
83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,
constraint-aware, and feedback-sensitive framework to benchmark LLMs under
realistic code generation scenarios, bridging the gap between synthetic
evaluations and real-world instruction complexity. The full benchmark dataset,
evaluation pipeline, and source code are available at
https://github.com/SYSUSELab/MultiCodeIF.

</details>


### [197] [Snaps: Bloated and Outdated?](https://arxiv.org/abs/2507.00786)
*Jukka Ruohonen,Qusai Ramadan*

Main category: cs.SE

TL;DR: Snap软件包系统在尺寸和更新频率上存在膨胀和滞后问题。


<details>
  <summary>Details</summary>
Motivation: 研究Snap软件包系统的实际表现，以验证对其的批评是否成立。

Method: 通过实证观察分析当前Snap软件包的尺寸和更新频率。

Result: 发现Snap软件包平均尺寸过大且更新频率滞后。

Conclusion: 该研究为软件包管理领域提供了实证依据，支持了对Snap的批评。

Abstract: Snap is an alternative software packaging system developed by Canonical and
provided by default in the Ubuntu Linux distribution. Given the heterogeneity
of various Linux distributions and their various releases, Snap allows an
interoperable delivery of software directly to users. However, concerns and
criticism have also been frequently expressed. Regarding this criticism, the
paper shows that currently distributed snap packages are indeed on average
bloated in terms of their sizes and outdated in terms updating frequencies.
With these empirical observations, this short paper contributes to the research
domain of software packaging, software packages, and package managers.

</details>


### [198] [Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability](https://arxiv.org/abs/2507.00788)
*Markus Borg,Dave Hewett,Nadim Hagatulah,Noric Couderc,Emma Söderberg,Donald Graham,Uttam Kini,Dave Farley*

Main category: cs.SE

TL;DR: 研究探讨AI助手对软件可维护性的影响，发现AI辅助开发能轻微提升后续代码演化的速度和代码健康度，但整体差异不显著。


<details>
  <summary>Details</summary>
Motivation: AI助手如GitHub Copilot和Cursor正在改变软件工程，但其对代码可维护性的影响尚不明确。

Method: 通过两阶段实验，151名参与者（95%为专业开发者）分别在有/无AI辅助下开发功能，随后由新参与者演化代码。

Result: AI辅助开发显著提升开发速度（中位数减少30.7%任务时间），但对可维护性影响较小，仅习惯性用户代码健康度显著提升。

Conclusion: AI助手能有效加速开发且未损害可维护性，未来研究需关注代码膨胀和认知债务风险。

Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming
software engineering. While several studies highlight productivity
improvements, their impact on maintainability requires further investigation.
[Objective] This study investigates whether co-development with AI assistants
affects software maintainability, specifically how easily other developers can
evolve the resulting source code. [Method] We conducted a two-phase controlled
experiment involving 151 participants, 95% of whom were professional
developers. In Phase 1, participants added a new feature to a Java web
application, with or without AI assistance. In Phase 2, a randomized controlled
trial, new participants evolved these solutions without AI assistance.
[Results] AI-assisted development in Phase 1 led to a modest speedup in
subsequent evolution and slightly higher average CodeHealth. Although neither
difference was significant overall, the increase in CodeHealth was
statistically significant when habitual AI users completed Phase 1. For Phase
1, we also observed a significant effect that corroborates previous
productivity findings: using an AI assistant yielded a 30.7% median decrease in
task completion time. Moreover, for habitual AI users, the mean speedup was
55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants
can effectively accelerate development. Moreover, we did not observe warning
signs of degraded code-level maintainability. We recommend that future research
focus on risks such as code bloat from excessive code generation and the
build-up of cognitive debt as developers invest less mental effort during
implementation.

</details>


### [199] [Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses](https://arxiv.org/abs/2507.00803)
*Gillian Daniel,Chris Hall,Per Hammer,Alec-Angus Macdonald,Hollie Marwick-Best,Emma McKenzie,George Popa,Derek Somerville,Tim Storer*

Main category: cs.SE

TL;DR: 论文探讨了行业从业者参与软件工程课程设计与交付的视角，填补了现有研究的空白，并提出了未来合作的建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注行业从业者在课程设计与交付中的视角，而他们的投入和支持对合作至关重要。

Method: 通过回顾性研究，收集并分析了从业者的观点，学术作者作为协调者。

Result: 研究揭示了从业者的动机、期望和经验，并提出了未来合作的建议。

Conclusion: 理解从业者的视角有助于形成可持续的行业-学术合作关系。

Abstract: Over more than two decades, The University of Glasgow has co-designed and
delivered numerous software engineering focused courses with industry partners,
covering both technical and discipline specific professional skills. Such
collaborations are not unique and many of the benefits are well recognised in
the literature. These include enhancing the real-world relevance of curricula,
developing student professional networks ahead of graduation and easing
recruitment opportunities for employers.
  However, there is relatively little scholarship on the perspectives of
industry practitioners who participate in course design and delivery. This gap
is significant, since the effort invested by practitioners is often substantial
and may require ongoing support from both the industry partner and academic
institution. Understanding the motivations, expectations and experiences of
practitioners who engage in course delivery can guide the formation of future
partnerships and ensure their long-term sustainability.
  We begin to address this gap by reporting on the outcomes of a retrospective
conducted amongst the practitioner coauthors of this paper, with the academic
coauthors acting as facilitators. All coauthors have participated in the recent
co-design and delivery of software engineering courses, but we choose to focus
explicitly on the perspectives of the practitioners. We report on the themes
that emerged from the discussions and our resulting recommendations for future
collaborations.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [200] [Extrapolation in Regression Discontinuity Design Using Comonotonicity](https://arxiv.org/abs/2507.00289)
*Ben Deaner,Soonwoo Kwon*

Main category: econ.EM

TL;DR: 提出了一种在具有多个协变量的尖锐回归不连续设计中推断因果效应的方法，适用于多观测变量或单一运行变量的情况。


<details>
  <summary>Details</summary>
Motivation: 解决在尖锐回归不连续设计中，如何从治疗与非治疗的边界外推因果效应的问题。

Method: 基于局部线性回归的估计方法，假设条件平均处理和非处理潜在结果是共单调的。

Result: 即使共单调性不成立，估计量仍为加权平均因果效应。

Conclusion: 方法成功应用于评估反事实强制暑期学校政策。

Abstract: We present a novel approach for extrapolating causal effects away from the
margin between treatment and non-treatment in sharp regression discontinuity
designs with multiple covariates. Our methods apply both to settings in which
treatment is a function of multiple observables and settings in which treatment
is determined based on a single running variable. Our key identifying
assumption is that conditional average treated and untreated potential outcomes
are comonotonic: covariate values associated with higher average untreated
potential outcomes are also associated with higher average treated potential
outcomes. We provide an estimation method based on local linear regression. Our
estimands are weighted average causal effects, even if comonotonicity fails. We
apply our methods to evaluate counterfactual mandatory summer school policies.

</details>


### [201] [Robust Inference when Nuisance Parameters may be Partially Identified with Applications to Synthetic Controls](https://arxiv.org/abs/2507.00307)
*Joseph Fry*

Main category: econ.EM

TL;DR: 提出一种新方法，在合成控制估计中实现渐近正态性，即使存在约束、高维或部分识别的干扰参数。


<details>
  <summary>Details</summary>
Motivation: 解决合成控制估计中干扰参数导致的渐近正态性失效问题。

Method: 通过正则化惩罚估计干扰参数，并使用正交化矩条件估计目标参数。

Result: 新方法在合成控制示例中验证了渐近正态性。

Conclusion: 该方法有效解决了干扰参数带来的渐近正态性问题。

Abstract: When conducting inference for the average treatment effect on the treated
with a Synthetic Control Estimator, the vector of control weights is a nuisance
parameter which is often constrained, high-dimensional, and may be only
partially identified even when the average treatment effect on the treated is
point-identified. All three of these features of a nuisance parameter can lead
to failure of asymptotic normality for the estimate of the parameter of
interest when using standard methods. I provide a new method yielding
asymptotic normality for an estimate of the parameter of interest, even when
all three of these complications are present. This is accomplished by first
estimating the nuisance parameter using a regularization penalty to achieve a
form of identification, and then estimating the parameter of interest using
moment conditions that have been orthogonalized with respect to the nuisance
parameter. I present high-level sufficient conditions for the estimator and
verify these conditions in an example involving Synthetic Controls.

</details>


### [202] [Plausible GMM: A Quasi-Bayesian Approach](https://arxiv.org/abs/2507.00555)
*Victor Chernozhukov,Christian B. Hansen,Lingwei Kong,Weining Wang*

Main category: econ.EM

TL;DR: 论文提出了一种准贝叶斯方法，用于处理经济学结构估计中矩条件可能不完全成立的情况。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决矩条件可能不完全成立的问题，通过引入先验分布建模对错误设定的信念。

Method: 采用准贝叶斯方法进行结构参数推断，允许矩条件存在一定程度的错误设定。

Result: 提供了准后验集中性结果，验证了准后验可用于获得近似最优的贝叶斯决策规则，并提供了频率覆盖结果。

Conclusion: 通过实证例子展示了该方法在放松矩条件严格成立要求时的有效性。

Abstract: Structural estimation in economics often makes use of models formulated in
terms of moment conditions. While these moment conditions are generally
well-motivated, it is often unknown whether the moment restrictions hold
exactly. We consider a framework where researchers model their belief about the
potential degree of misspecification via a prior distribution and adopt a
quasi-Bayesian approach for performing inference on structural parameters. We
provide quasi-posterior concentration results, verify that quasi-posteriors can
be used to obtain approximately optimal Bayesian decision rules under the
maintained prior structure over misspecification, and provide a form of
frequentist coverage results. We illustrate the approach through empirical
examples where we obtain informative inference for structural objects allowing
for substantial relaxations of the requirement that moment conditions hold
exactly.

</details>


### [203] [Comparing Misspecified Models with Big Data: A Variational Bayesian Perspective](https://arxiv.org/abs/2507.00763)
*Yong Li,Sushanta K. Mallick,Tao Zeng,Junxing Zhang*

Main category: econ.EM

TL;DR: 该论文研究了在大规模MIMO系统中数据检测的复杂性，提出基于变分贝叶斯（VB）的方法，并针对模型误设情况提出了两种新的信息准则，用于评估预测性能。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中的数据检测通常计算复杂度极高，现有算法在复杂性和性能之间存在权衡。VB方法在统计推断中广泛应用，但模型误设情况下的预测性能评估仍需改进。

Method: 研究基于变分后验分布的预测分布风险函数，提出两种新的信息准则，用于模型比较。通过KL散度衡量预测性能。

Result: 在特定条件下，提出的信息准则被证明是其风险函数的渐近无偏估计。数值模拟和实证应用验证了其有效性。

Conclusion: 该研究为大规模数据中模型误设情况下的预测性能评估提供了有效工具，提出的信息准则具有实际应用价值。

Abstract: Optimal data detection in massive multiple-input multiple-output (MIMO)
systems often requires prohibitively high computational complexity. A variety
of detection algorithms have been proposed in the literature, offering
different trade-offs between complexity and detection performance. In recent
years, Variational Bayes (VB) has emerged as a widely used method for
addressing statistical inference in the context of massive data. This study
focuses on misspecified models and examines the risk functions associated with
predictive distributions derived from variational posterior distributions.
These risk functions, defined as the expectation of the Kullback-Leibler (KL)
divergence between the true data-generating density and the variational
predictive distributions, provide a framework for assessing predictive
performance. We propose two novel information criteria for predictive model
comparison based on these risk functions. Under certain regularity conditions,
we demonstrate that the proposed information criteria are asymptotically
unbiased estimators of their respective risk functions. Through comprehensive
numerical simulations and empirical applications in economics and finance, we
demonstrate the effectiveness of these information criteria in comparing
misspecified models in the context of massive data.

</details>


### [204] [Randomization Inference with Sample Attrition](https://arxiv.org/abs/2507.00795)
*Xinran Li,Peizan Sheng,Zeyang Yu*

Main category: econ.EM

TL;DR: 提出了新的随机化推断方法，解决样本流失导致的严重尺寸失真问题，适用于多种缺失机制。


<details>
  <summary>Details</summary>
Motivation: 随机化推断在处理样本流失时存在尺寸失真问题，需要更稳健的方法。

Method: 构建有效p值，利用Fisher随机化测试的最坏情况p值，结合分布无关的检验统计量，并利用结构假设（如单调缺失）提高功效。

Result: 方法在模拟和实证应用中表现良好，适用于非尖锐零假设（如个体治疗效应的分位数）。

Conclusion: 新方法在多种缺失机制下有效，提高了随机化推断的稳健性和实用性。

Abstract: Although appealing, randomization inference for treatment effects can suffer
from severe size distortion due to sample attrition. We propose new,
computationally efficient methods for randomization inference that remain valid
under a range of potentially informative missingness mechanisms. We begin by
constructing valid p-values for testing sharp null hypotheses, using the
worst-case p-value from the Fisher randomization test over all possible
imputations of missing outcomes. Leveraging distribution-free test statistics,
this worst-case p-value admits a closed-form solution, connecting naturally to
bounds in the partial identification literature. Our test statistics
incorporate both potential outcomes and missingness indicators, allowing us to
exploit structural assumptions-such as monotone missingness-for increased
power. We further extend our framework to test non-sharp null hypotheses
concerning quantiles of individual treatment effects. The methods are
illustrated through simulations and an empirical application.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [205] [Unraveling Global Threads: Pandemic, Geopolitical Conflict, and Resilience in Fashion and Textile Supply Chain](https://arxiv.org/abs/2507.00207)
*Md. Al-Amin,Muneeb Tahir,Amit Talukder,Abdullah Al Mamun,Md Tanjim Hossain,Nigar Sultana*

Main category: econ.GN

TL;DR: 研究分析了COVID-19大流行及多场地区冲突对全球纺织和时尚供应链的影响，探讨了供应链中断的表现及恢复策略。


<details>
  <summary>Details</summary>
Motivation: 全球疫情和地区冲突对纺织和时尚供应链造成了显著破坏，研究旨在分析其影响并提出应对策略。

Method: 采用内容分析法，从Google Scholar、Summon数据库和NexisUni等来源筛选相关文献和新闻。

Result: 疫情导致订单取消、工厂关闭、工人失业，并加速了数字化和可持续发展；地区冲突的间接影响比直接军事行动更显著。

Conclusion: 研究强调了构建弹性供应链的重要性，并提出了恢复和强化供应链的策略。

Abstract: Several noteworthy scenarios emerged in the global textile and fashion supply
chains during and after the COVID-19 pandemic. The destabilizing influences of
a global pandemic and a geographically localized conflict are being acutely
noticed in the worldwide fashion and textile supply chains. This work examines
the impact of the COVID-19 pandemic, the Russo-Ukraine conflict,
Israel-Palestine conflict, and Indo-Pak conflict on supply chains within the
textile and fashion industry. This research employed a content analysis method
to identify relevant articles and news from sources such as Google Scholar, the
Summon database of North Carolina State University, and the scholarly news
portal NexisUni. The selected papers, news articles, and reports provide a
comprehensive overview of the fashion, textile, and apparel supply chain
disruptions caused by the pandemic and the war in Ukraine, accompanied by
discussions from common supply chain perspectives. Disruptions due to COVID-19
include international brands and retailers canceling orders, closures of stores
and factories in developing countries, layoffs, and furloughs of workers in
both retail stores and supplier factories, the increased prominence of online
and e-commerce businesses, the growing importance of automation and
digitalization in the fashion supply chain, considerations of sustainability,
and the need for a resilient supply chain system to facilitate post-pandemic
recovery. In the case of the Russo-Ukraine war, Israel-Palestine war, and
Indo-Pak war, the second-order effects of the conflict have had a more
significant impact on the textile supply chain than the direct military
operations themselves. In addition to these topics, the study delves into the
potential strategies for restoring and strengthening the fashion supply chain

</details>


### [206] [Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan](https://arxiv.org/abs/2507.00279)
*Xiao Hui Tai,Suraj R. Nair,Shikhar Mehra,Joshua E. Blumenstock*

Main category: econ.GN

TL;DR: 利用卫星影像和手机数据研究阿富汗季节性移民对鸦片收获的响应，发现高罂粟种植区吸引更多移民，短期暴力事件影响有限，但长期冲突模式（如塔利班控制）显著影响移民流动。


<details>
  <summary>Details</summary>
Motivation: 研究暴力与冲突如何影响季节性移民流动，填补冲突地区移民数据不足的空白。

Method: 结合卫星影像推断鸦片收获时间，利用全国手机记录分析移民流动，考察暴力事件和长期冲突的影响。

Result: 高罂粟种植区吸引更多移民；短期暴力事件影响小，长期冲突模式（如塔利班控制）显著影响移民。

Conclusion: 季节性移民对短期暴力事件具有韧性，但长期冲突模式会显著改变移民流动。

Abstract: Seasonal migration plays a critical role in stabilizing rural economies and
sustaining the livelihoods of agricultural households. Violence and civil
conflict have long been thought to disrupt these labor flows, but this
hypothesis has historically been hard to test given the lack of reliable data
on migration in conflict zones. Focusing on Afghanistan in the 8-year period
prior to the Taliban's takeover in 2021, we first demonstrate how satellite
imagery can be used to infer the timing of the opium harvest, which employs a
large number of seasonal workers in relatively well-paid jobs. We then use a
dataset of nationwide mobile phone records to characterize the migration
response to this harvest, and examine whether and how violence and civil
conflict disrupt this migration. We find that, on average, districts with high
levels of poppy cultivation receive significantly more seasonal migrants than
districts with no poppy cultivation. These labor flows are surprisingly
resilient to idiosyncratic violent events at the source or destination,
including extreme violence resulting in large numbers of fatalities. However,
seasonal migration is affected by longer-term patterns of conflict, such as the
extent of Taliban control in origin and destination locations.

</details>


### [207] [Factors Influencing Change Orders in Horizontal Construction Projects: A Comparative Analysis of Unit Price and Lump Sum Contracts](https://arxiv.org/abs/2507.00281)
*Mohamed Khalafalla,Tejal Mulay,Shonda L Bernadin*

Main category: econ.GN

TL;DR: 通过定量分析，研究发现项目规模、工期和工作类型对变更单频率有显著影响，离散选择模型能更准确地选择合同类型，从而减少变更单。


<details>
  <summary>Details</summary>
Motivation: 设计-招标-建造（DBB）项目变更单频率高，增加了成本和工期，研究旨在通过分析减少变更单。

Method: 利用佛罗里达交通部的历史投标数据，评估五种因素对变更单频率的影响，并使用离散选择模型比较单价和总价合同。

Result: 项目规模、工期和工作类型对变更单频率有显著影响，离散选择模型优于传统方法。

Conclusion: 通过优化合同类型选择，DBB项目可以减少变更单，提升效率。

Abstract: Change orders (COs) are a common occurrence in construction projects, leading
to increased costs and extended durations. Design-Bid-Build (DBB) projects,
favored by state transportation agencies (STAs), often experience a higher
frequency of COs compared to other project delivery methods. This study aims to
identify areas of improvement to reduce CO frequency in DBB projects through a
quantitative analysis. Historical bidding data from the Florida Department of
Transportation (FDOT) was utilized to evaluate five factors, contracting
technique, project location, type of work, project size, and duration, on
specific horizontal construction projects. Two DBB contracting techniques, Unit
Price (UP) and Lump Sum (LS), were evaluated using a discrete choice model. The
analysis of 581 UP and 189 LS projects revealed that project size, duration,
and type of work had a statistically significant influence on the frequency of
change orders at a 95% confidence level. The discrete choice model showed
significant improvement in identifying the appropriate contract type for a
specific project compared to traditional methods used by STAs. By evaluating
the contracting technique instead of project delivery methods for horizontal
construction projects, the use of DBB can be enhanced, leading to reduced
change orders for STAs.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [208] [Endogenous Network Structures with Precision and Dimension Choices](https://arxiv.org/abs/2507.00249)
*Nikhil Kumar*

Main category: econ.TH

TL;DR: 本文提出了一种社交学习模型，网络结构由信号精度和维度选择内生决定。研究发现，固定网络结构下，个体最优精度选择低于社会最优选择，动态网络结构下，理想网络应均衡分配影响力。


<details>
  <summary>Details</summary>
Motivation: 研究网络结构如何由信号精度和维度选择内生决定，以及个体与社会最优选择之间的差异。

Method: 通过模型分析固定和动态网络结构下，代理人的信号精度和维度选择如何影响网络结构和社会学习。

Result: 固定网络下，个体最优精度选择低于社会最优；动态网络下，理想网络应均衡分配影响力。

Conclusion: 网络结构的内生性对社交学习效率有重要影响，均衡分配影响力是理想状态。

Abstract: This paper presents a social learning model where the network structure is
endogenously determined by signal precision and dimension choices. Agents not
only choose the precision of their signals and what dimension of the state to
learn about, but these decisions directly determine the underlying network
structure on which social learning occurs. We show that under a fixed network
structure, the optimal precision choice is sublinear in the agent's stationary
influence in the network, and this individually optimal choice is worse than
the socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network
structure, we specify the network by defining a kernel distance between agents,
which then determines how much weight agents place on one another. Agents
choose dimensions to learn about such that their choice minimizes the squared
sum of influences of all agents: a network with equally distributed influence
across agents is ideal.

</details>


### [209] [Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context](https://arxiv.org/abs/2507.00288)
*Claire Li,David Freeborn*

Main category: econ.TH

TL;DR: 研究探讨AI驱动的数字创新如何重塑跨国治理中的组织问责制，结合TAM、ANT和制度理论分析AI技术的采纳及其对问责机制的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在审计和财务报告等领域的决策中日益重要，传统的问责机制（如控制、透明度和可审计性）受到挑战，需要研究如何适应跨国治理环境。

Method: 整合技术接受模型（TAM）、行动者网络理论（ANT）和制度理论，分析组织在跨国监管、伦理和文化压力下采纳AI技术的方式。

Result: 问责制是全球社会技术网络中共同构建的，受用户感知、治理逻辑和规范期望影响。研究提出两种组织策略：内部治理重构和外部行动者网络参与。

Conclusion: 通过内部治理调整和外部网络合作，可以促进会计领域负责任、合法且全球接受的AI技术采纳。

Abstract: This study explores how AI-powered digital innovations are reshaping
organisational accountability in a transnational governance context. As AI
systems increasingly mediate decision-making in domains such as auditing and
financial reporting, traditional mechanisms of accountability, based on
control, transparency, and auditability, are being destabilised. We integrate
the Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and
institutional theory to examine how organisations adopt AI technologies in
response to regulatory, ethical, and cultural pressures that transcend national
boundaries. We argue that accountability is co-constructed within global
socio-technical networks, shaped not only by user perceptions but also by
governance logics and normative expectations. Extending TAM, we incorporate
compliance and legitimacy as key factors in perceived usefulness and usability.
Drawing on ANT, we reconceptualise accountability as a relational and emergent
property of networked assemblages. We propose two organisational strategies
including internal governance reconfiguration and external actor-network
engagement to foster responsible, legitimate, and globally accepted AI adoption
in the accounting domain.

</details>


### [210] [Dynamic SINR-Guided Iterative Interference Cancellation for ODDM Systems in Doubly Dispersive Channels](https://arxiv.org/abs/2507.00397)
*Jiasong Han,Xuehan Wang,Jintao Wang*

Main category: econ.TH

TL;DR: 论文提出了一种动态SINR引导的低复杂度信号检测方法，用于正交延迟-多普勒分复用（ODDM）系统，通过迭代干扰消除提升通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 在高移动性环境中，ODDM调制面临由分数延迟和多普勒频移引起的信道扩展问题，导致信号检测复杂度高。

Method: 基于SINR理论分析，提出动态SINR引导方法，从最佳SINR的多载波符号开始迭代干扰消除，逐步初始化所有数据符号。

Result: 仿真实验表明，该方法在收敛性和误码性能上表现良好，且避免了全线性最小均方误差（LMMSE）初始化的高复杂度。

Conclusion: 动态SINR引导方法为ODDM系统提供了一种高效的低复杂度信号检测方案。

Abstract: Orthogonal delay-Doppler division multiplexing (ODDM) modulation has recently
gained significant attention as a promising candidate to promote the
communication reliability in high-mobility environments. Low complexity signal
detection is one of the most significant challenges for ODDM over general
physical channels, due to the large channel spreading caused by the fractional
delay and Doppler shifts. In this paper, we investigate the low-complexity data
detection for ODDM system by utilizing iterative interference cancellation.
Based on the theoretical analysis of signal to interference plus noise ratio
(SINR) during the iteration, a dynamic SINR-guided approach is proposed to
provide a better initialization result. Specifically, we analyze the SINR of
each time domain sample before initial estimate with consideration of off-grid
delay and Doppler shifts. The iteration is then started from the multi-carrier
symbol index which has the best SINR. The corresponding interference is then
eliminated for other time domain samples while the SINR for symbol awaiting
detection is also updated. Based on the updated SINR, the next multi-carrier
symbol index is selected for the same processing until all data symbols have
been initialized. Finally, we update the SINR synchronously until the end of
the initialization. Simulation experiments indicate that our proposed
algorithms demonstrate satisfying convergence and error performance while
avoiding the huge complexity introduced by full linear minimum mean squared
error (LMMSE) initialization.

</details>


### [211] [Local Strategy-Proofness and Dictatorship](https://arxiv.org/abs/2507.00913)
*Abinash Panda,Anup Pramanik,Ragini Saxena*

Main category: econ.TH

TL;DR: 论文研究了偏好域中一致且局部策略证明的社会选择函数（scf）满足独裁性的条件，提出了“与两个不同邻居相连”的条件，并证明了其必要性和部分充分性。


<details>
  <summary>Details</summary>
Motivation: 探索偏好域中一致且局部策略证明的scf满足独裁性的条件，填补相关理论空白。

Method: 提出“与两个不同邻居相连”的条件，分析其在一致和局部策略证明scf中的作用，并验证其必要性和部分充分性。

Result: 证明了该条件在一致和局部策略证明scf中的必要性，并在特定域中验证了其充分性。

Conclusion: 虽然完整刻画仍待解决，但论文在一致和策略证明条件下对独裁性的研究取得了重要进展。

Abstract: We investigate preference domains where every unanimous and locally
strategy-proof social choice function (scf) satisfies dictatorship. We identify
a condition on domains called connected with two distinct neighbours which is
necessary for unanimous and locally strategy-proof scfs to satisfy
dictatorship. Further, we show that this condition is sufficient within the
class of domains where every unanimous and locally strategy-proof scf satisfies
tops-onlyness. While a complete characterization remains open, we make
significant progress by showing that on connected with two distinct neighbours
domains, unanimity and strategy-proofness (a stronger requirement) guarantee
dictatorship.

</details>
