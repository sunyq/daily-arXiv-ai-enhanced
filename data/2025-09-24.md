<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.CV](#cs.CV) [Total: 123]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [econ.EM](#econ.EM) [Total: 2]
- [econ.GN](#econ.GN) [Total: 2]
- [econ.TH](#econ.TH) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 本文提出了一个成本效益分析框架，帮助企业决定何时本地部署开源LLM比商业订阅服务更经济可行。


<details>
  <summary>Details</summary>
Motivation: 企业面临使用商业LLM服务还是本地部署的选择，商业服务虽然方便但存在数据隐私、供应商锁定和长期成本问题，因此需要分析本地部署的经济可行性。

Method: 通过分析硬件需求、运营成本和开源模型性能基准，将本地部署总成本与主要云服务商的订阅费用进行比较。

Result: 研究提供了基于使用量和性能需求的盈亏平衡点估算。

Conclusion: 该框架为企业规划LLM战略提供了实用的决策工具。

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [2] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE是一个基于大语言模型的土壤湿度模式异常检测框架，利用ChatGPT-4.1实现零样本分析，无需特定任务标注或微调，在灌溉事件检测和异常分类方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度时间序列分析方法依赖基于阈值的规则或数据密集型机器学习模型，存在适应性和可解释性限制。需要开发更智能、可解释的解决方案来支持精准农业。

Method: SPADE将时间序列数据转换为文本表示，设计领域知识提示模板，利用ChatGPT-4.1的推理能力进行零样本分析，检测灌溉事件、估算净灌溉增益、检测分类异常并生成结构化报告。

Result: 在美国多个商业和实验农场的真实土壤湿度传感器数据上测试，SPADE在异常检测方面实现更高召回率和F1分数，准确分类异常类型；在灌溉事件检测方面达到高精度和召回率。

Conclusion: 该研究证明大语言模型作为精准农业可扩展、适应性工具的巨大潜力，能够整合定性知识和数据驱动推理，为土壤湿度监测和灌溉调度提供可操作见解。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [3] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 本文提出可解释不确定性估计（XUE）框架，将可解释性与不确定性量化相结合，以解决医疗AI中不确定性表达与临床推理脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统未能以符合临床推理的方式明确量化或传达不确定性，现有可解释AI（XAI）工作缺乏对预测置信度的捕捉，而不确定性估计（UE）技术又缺乏直观解释，这种脱节限制了AI在医学中的应用。

Method: 提出XUE框架，系统地将医学不确定性映射到AI不确定性概念，识别XUE实施的关键挑战，并规划技术方向包括多模态不确定性量化、模型无关可视化技术和不确定性感知决策支持系统。

Result: 分析了XUE的实现需求，提出了确保有效XUE实现的指导原则，强调了AI系统不仅需要生成可靠预测，还需以临床有意义的方式表达置信水平。

Conclusion: 这项工作通过桥接可解释性和不确定性，为开发可信赖的医疗AI做出贡献，为符合真实世界临床复杂性的AI系统铺平道路。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [4] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: HSGM是一种分层分段图记忆框架，通过将长文档分解为有意义的分段，构建局部语义图并提取摘要节点，显著降低了长文档语义解析的计算复杂度和内存需求。


<details>
  <summary>Details</summary>
Motivation: 长文档语义解析面临二次复杂度增长和内存需求激增的挑战，传统方法难以处理超长文本。

Method: 将输入文档分解为M个分段，为每个分段构建局部语义图，提取摘要节点形成全局图记忆，支持增量更新和分层查询处理。

Result: 在三个基准测试中，HSGM实现了2-4倍推理加速，峰值内存减少超过60%，同时保持基线准确率的95%以上。

Conclusion: HSGM为超长文本提供了可扩展、准确的语义建模方法，支持实时和资源受限的NLP应用。

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [5] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多智能体框架，通过单一自然语言提示自动化整个OpenFOAM工作流程，显著降低了CFD仿真的技术门槛。


<details>
  <summary>Details</summary>
Motivation: CFD仿真工具学习曲线陡峭且设置复杂，现有系统存在自动化程度不足的问题，需要开发能够端到端自动化仿真流程的解决方案。

Method: 采用多智能体架构，包括网格生成代理、配置生成代理等，使用Model Context Protocol暴露核心功能，通过层次化多索引RAG实现高精度配置生成。

Result: 在110个仿真任务基准测试中，Foam-Agent达到88.2%的成功率，显著优于现有框架（MetaOpenFOAM为55.5%）。

Conclusion: Foam-Agent有效降低了CFD的专业门槛，展示了专用多智能体系统在复杂科学计算民主化方面的潜力。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [6] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA是一个新颖的视觉记忆问答任务，Pensieve管道通过记忆增强、时空感知检索和多记忆问答微调，在QA准确率上比现有方法提升高达14%


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中基于视觉记忆的回忆问答任务面临的挑战，包括任务导向记忆创建、时空信息有效利用以及多记忆推理能力

Method: 提出Pensieve综合管道，包含记忆特定增强、时空感知多信号检索和多记忆问答微调三个核心组件

Result: 在构建的多模态基准测试中，Pensieve相比最先进方法在问答准确率上提升高达14%

Conclusion: Memory-QA任务具有现实意义，Pensieve管道有效解决了该任务的核心挑战，为视觉记忆问答领域提供了新的解决方案

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [7] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型在运筹学中的应用，将方法分为自动建模、辅助优化和直接求解三个方向，并总结了当前面临的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法依赖专家建模和手动参数调整，难以处理大规模、动态、多约束问题。LLMs通过语义理解、结构化生成和推理控制显示出解决这些局限性的潜力。

Method: 将LLMs与运筹学整合的方法分为三类：自动建模（将自然语言描述转换为数学模型或可执行代码）、辅助优化（生成启发式算法、演化算法）和直接求解优化任务。

Result: 综述了LLMs在运筹学中的最新进展，包括评估基准和领域特定应用，识别了语义到结构映射不稳定、研究进展碎片化、泛化能力有限和评估体系不足等关键问题。

Conclusion: 本文概述了推进LLMs在运筹学中作用的可能研究方向，为解决传统方法面临的挑战提供了新的技术路径。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [8] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 该论文提出了SAPA框架，使用大语言模型合成理论驱动的潜在态度来预测网约车出行选择，显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有网约车出行选择预测模型存在预测准确性有限的问题，主要原因是无法捕捉关键心理因素，且面临严重的类别不平衡问题（网约车出行仅占日常出行的很小比例）。

Method: SAPA框架采用分层方法：1）使用LLM从原始出行调查数据生成定性旅行者画像；2）训练倾向得分模型；3）LLM为理论驱动的潜在变量分配定量分数；4）最终分类器整合倾向得分、潜在变量分数和可观测出行属性进行预测。

Result: 在大规模多年出行调查上的实验表明，SAPA显著优于最先进的基线方法，在测试集上的PR-AUC指标提升了高达75.9%。

Conclusion: 该研究为准确预测网约车出行选择提供了强大工具，其方法可轻松迁移到各种应用中。

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [9] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: OBER是一个基于结果的教育推荐系统，通过将学习成果和评估项目嵌入数据模式，使任何算法都能根据其促进的掌握程度进行评估。


<details>
  <summary>Details</summary>
Motivation: 大多数教育推荐系统仅基于点击或评分相关性进行调优和判断，其真实的教学影响不明确。

Method: OBER使用简约的实体关系模型、日志驱动的掌握度公式和插件架构，在非正式学习领域集成到e-learning系统中，通过为期两周的随机分组测试评估了三种方法：固定专家路径、协同过滤和基于知识的过滤。

Result: 协同过滤最大化用户留存率，但固定路径实现了最高的掌握度。

Conclusion: OBER框架允许从业者在没有额外测试开销的情况下权衡相关性、参与度和结果掌握度，该框架与方法无关，可轻松扩展到未来的自适应或上下文感知推荐器。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [10] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 提出MMCD框架，通过多模态协作决策和跨模态知识蒸馏解决自动驾驶中传感器故障或连接车辆缺失时的鲁棒性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练和测试时所有数据模态和连接车辆都可用，这在实际中不现实，因为可能存在传感器故障或连接车辆缺失

Method: 提出MMCD框架，融合自车和协作车辆的多模态观测数据，采用教师-学生模型的跨模态知识蒸馏方法，教师模型使用多模态数据训练，学生模型设计为在模态减少时仍能有效工作

Result: 在连接自动驾驶和空地车辆协作实验中，该方法将驾驶安全性提高了20.7%，在潜在事故检测和安全驾驶决策方面优于现有最佳基线

Conclusion: MMCD框架能够有效提升自动驾驶系统在挑战性环境下的决策鲁棒性，特别是在数据模态不完整的情况下

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [11] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出了一种形式化方法来解释定量双极论证框架中推理变化的原因，通过追踪论证强度不一致性并识别其解释


<details>
  <summary>Details</summary>
Motivation: 在QBAF中推理时，当更新框架后重新得出结论时，论证强度排序可能出现不一致性，需要解释这些变化的原因

Method: 通过追踪论证强度不一致性，识别导致不一致的具体论证作为解释，定义了充分、必要和反事实解释，并提出了基于启发式的解释搜索方法

Result: 证明了强度不一致性解释存在的充要条件是更新导致强度不一致，并提供了相应的实现

Conclusion: 该方法能够有效解释QBAF中推理变化的原因，为理解论证框架的动态行为提供了理论支持

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [12] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 提出了Neural DNA（nDNA）作为语义基因型表示，通过潜在几何结构捕捉AI基础模型的内部认知身份，包括谱曲率、热力学长度和信念向量场三个维度。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试仅衡量模型行为，但模型的本质存在于其潜在几何结构中。需要一种能够捕捉模型内在认知身份的方法，类似于生物DNA编码遗传信息。

Method: nDNA从三个几何维度合成：谱曲率揭示跨层的概念流曲率；热力学长度量化语义转换所需的努力；信念向量场描述引导模型信念方向的语义扭转场。

Result: nDNA提供了一个稳定、坐标无关的神经DNA指纹，可用于追踪模型谱系、测量继承关系、检测漂移，并研究人工认知的演化。

Conclusion: 这项工作开启了神经基因组学新领域，将模型视为具有可追溯内部认知的数字语义有机体，为模型比较、风险诊断和治理提供了新方法。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [13] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 本文提出了相似性场理论，这是一个数学框架，用于形式化实体间相似性关系及其演化的原则。该理论定义了相似性场、系统演化、概念纤维和生成算子，并基于此形式化地定义了智能的概念。


<details>
  <summary>Details</summary>
Motivation: 作者认为持久化和转换相似性关系是任何可理解动态系统的结构基础，需要建立一个数学框架来形式化相似性值及其演化的原则。

Method: 定义了相似性场S: U×U→[0,1]，满足自反性但允许不对称性和非传递性；系统演化序列Zp=(Xp,S(p))；概念K诱导纤维Fα(K)；生成算子G。基于此框架形式化定义智能。

Result: 证明了两个定理：(i)不对称性阻碍相互包含；(ii)稳定性需要锚坐标或最终限制在f的水平集内。这些结果确保相似性场演化既受约束又可解释。

Conclusion: 相似性场理论为表征、比较和构建智能系统提供了基础语言，可用于解释大语言模型并将其作为社会认知的实验探针。

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [14] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer是一个用于预测个体健康风险的多模态AI框架，结合视觉和语言数据，在MIMIC-IV数据集上取得了0.90的AUROC。


<details>
  <summary>Details</summary>
Motivation: 随着慢性疾病负担增加和临床数据（医学影像、文本记录、可穿戴设备等）的多模态异质性，需要统一的多模态AI框架来主动预测个体健康风险。

Method: 提出VL-RiskFormer，一种分层堆叠的视觉语言多模态Transformer，具有LLM推理头。关键创新包括：跨模态预训练、时间融合块、疾病本体映射适配器。

Result: 在MIMIC-IV纵向队列中，VL-RiskFormer实现了平均AUROC 0.90，预期校准误差为2.7%。

Conclusion: VL-RiskFormer展示了在多模态临床数据上预测健康风险的有效性，为个性化医疗提供了有前景的解决方案。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [15] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind是一个混合架构，结合了探索链、知识图谱、检索增强生成和大语言模型，用于解决个性化菜谱推荐中的模糊意图、语义准确性和细节覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 个性化菜谱推荐面临处理模糊用户意图、确保语义准确性和提供足够细节覆盖的挑战。

Method: 提出ChefMind混合架构：探索链(CoE)将模糊查询细化为结构化条件，知识图谱(KG)提供语义推理和可解释性，检索增强生成(RAG)补充上下文烹饪细节，大语言模型(LLM)整合输出为连贯推荐。

Result: 在Xiachufang数据集和手动标注查询上的评估显示，ChefMind在准确性、相关性、完整性和清晰度方面优于仅使用LLM、KG或RAG的基线模型，平均得分8.7 vs 6.4-6.7，未处理查询降至1.6%。

Conclusion: ChefMind在个性化菜谱推荐中表现出优越性能和鲁棒性，能有效处理模糊需求。

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [16] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 本文提出了一种"N+1"GPT代理框架，用于机械工程问题的初步分析。该框架通过多个独立求解代理和比较代理来提高GPT在工程问题求解中的可靠性。


<details>
  <summary>Details</summary>
Motivation: GPT在机械工程分析中表现出不稳定性，成功概率仅为85%，这种不可靠性使其无法直接应用于教育或工程实践。需要一种机制来提高解决方案的可靠性。

Method: 采用"N+1"代理框架：首先启动N个独立的求解代理生成解决方案，然后通过比较代理对这些方案进行汇总、比较，并推荐最终解决方案。基于孔多塞陪审团定理，当每个求解代理的成功概率大于1/2且N足够大时，多数方案很可能对应正确解。

Result: 与商业多代理模型Grok Heavy相比，该框架在设计和性能上有相似之处，但更注重透明度和教学价值。比较代理还能整合次要方案的优点，特别是当这些方案代表不同的问题解释或数学解法时。

Conclusion: 该代理框架能够有效提高GPT在机械工程问题求解中的可靠性，通过多数决策机制和方案比较，为教育实践提供了更可靠的AI辅助工具。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [17] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: ComputerAgent是一个轻量级分层强化学习框架，用于桌面应用控制，通过两级选项过程（管理器和子策略）处理操作系统控制，在保持小模型规模（15M参数）的同时，性能可媲美200B参数的大型多模态语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLMs）的桌面应用控制方法存在推理延迟高、样本效率低、无法在设备上部署等问题，需要更实用的解决方案。

Method: 采用分层强化学习框架，包含三模态状态编码器（截图、任务ID、数值状态）、元动作与早停机制、紧凑视觉骨干网络和小型策略网络。

Result: 在135个真实桌面任务测试中，简单任务（<8步）成功率92.1%，困难任务（≥8步）成功率58.8%，模型大小减少4个数量级，推理时间减半。

Conclusion: 分层强化学习为计算机控制提供了一种实用、可扩展的替代方案，优于基于单体MLLM的自动化方法。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [18] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 论文指出当前医学基准测试存在严重问题，大型前沿模型虽然在基准测试中得分很高，但实际上缺乏真正的医学理解能力，存在脆弱性和捷径学习问题。


<details>
  <summary>Details</summary>
Motivation: 揭示当前医学AI基准测试的局限性，证明高分数并不等同于真实医疗场景中的可靠性和实用性，提醒业界不要被表面分数误导。

Method: 对6个旗舰模型在6个广泛使用的医学基准上进行压力测试，包括移除关键输入、改变提示词等方式，并通过临床医生指导的评估标准进行分析。

Result: 发现模型即使在没有关键信息的情况下也能猜对答案，对微小提示变化反应不稳定，会编造看似合理但有缺陷的推理过程。基准测试之间存在巨大差异，但被错误地等同对待。

Conclusion: 医学基准测试分数不能直接反映AI在真实医疗环境中的准备度。要建立医疗AI的信任，必须要求系统具备鲁棒性、合理推理能力和与真实医疗需求的匹配度，而不仅仅是排行榜上的胜利。

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [19] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 该论文研究了两种计算约束策略（推理长度约束和模型量化）对推理语言模型安全性能的影响，旨在在保证模型安全的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 测试时计算扩展虽然能通过生成长链思维序列提升推理模型性能，但显著增加了计算成本。本研究旨在探索在计算约束下如何平衡模型性能与安全性。

Method: 采用两种方法：1）基于长度控制策略优化的强化学习方法微调推理模型以满足用户定义的推理长度；2）应用量化技术，在用户定义的计算约束下最大化链式思维序列生成。

Result: 研究发现计算约束策略能够有效降低推理模型的计算需求，但需要仔细权衡计算效率与模型安全性之间的关系。

Conclusion: 计算约束策略是降低推理模型计算成本的有效方法，但在实际应用中需要谨慎处理计算效率与安全性能之间的平衡关系。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [20] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 该论文提出了Gödel测试，评估大型语言模型能否为简单未解数学猜想生成正确证明，并在组合优化领域测试了GPT-5的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿AI模型在中学和大学数学竞赛中表现优异，但能否解决更高级数学领域的新简单猜想仍不清楚，因此需要设计专门的测试方法。

Method: 使用五个组合优化猜想作为测试案例，为每个问题提供相关论文但不透露作者自己的猜想，然后详细评估GPT-5的推理过程。

Result: 在三个较简单问题上GPT-5几乎给出正确解，Problem 2中甚至推翻了作者的猜想并提供了有效解；但在需要跨论文综合的Problem 4上失败，Problem 5中算法正确但分析失败。

Conclusion: GPT-5在常规推理上有显著进步，偶尔展现原创性，但在跨论文综合方面仍有局限，可能代表前沿模型通过Gödel测试的早期步骤。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [21] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 该论文提出了首个基于美国海关裁决在线搜索系统的HTS编码分类基准，并展示了微调的Atlas模型在10位和6位分类上的显著性能提升，同时成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: HTS编码分类是全球贸易中的关键瓶颈，但机器学习社区对此关注不足。错误分类可能导致货物运输完全停止，主要邮政运营商因海关文件不完整而暂停向美国发货。

Method: 使用美国海关裁决在线搜索系统数据创建基准数据集，对领先的LLM进行评估，并微调Atlas模型进行HTS编码分类。

Result: 微调的Atlas模型在10位分类上达到40%的完全正确率，6位分类达到57.5%的正确率，比GPT-5-Thinking和Gemini-2.5-Pro-Thinking分别提升15和27.5个百分点，成本降低5-8倍。

Conclusion: Atlas模型为HTS分类设立了强基线，但该任务仍极具挑战性。通过发布数据集和模型，旨在将HTS分类定位为新的社区基准任务，促进检索、推理和对齐方面的未来研究。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [22] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC是一个专门评估大语言模型在函数调用中精确遵循格式指令能力的基准测试，填补了现有基准测试只关注参数正确性而忽略格式要求的空白。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准测试（如BFCL、tau^2-Bench、ACEBench）只评估参数正确性，但未测试模型是否遵循参数描述中嵌入的格式指令（如双引号、ISO日期格式等），这对于实际AI代理系统至关重要。

Method: 受IFEval启发，IFEval-FC在JSON模式描述中直接编码可验证的格式要求，包含750个测试用例，每个用例包含一个函数及其输入参数的嵌入格式要求，以及对应的用户查询，评估过程完全算法化。

Result: 结果显示，即使是GPT-5和Claude 4.1 Opus等最先进的专有模型也经常无法遵循基本的格式规则，这暴露了现实世界代理系统的实际局限性。

Conclusion: IFEval-FC强调了精确指令遵循在函数调用中的重要性，为评估和改进大语言模型的格式遵循能力提供了标准化工具，代码和数据已公开。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [23] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA是一个用于花剑比赛的AI裁判原型系统，结合姿态动作识别和规则推理来解决传统裁判的主观性、错误和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 击剑等体育运动的裁判面临主观判罚、人为错误、偏见以及在训练环境中可用性有限等挑战，需要自动化辅助系统。

Method: 系统从视频中提取2D关节位置，进行归一化处理，计算101维运动学特征，使用Transformer进行多标签动作和剑尖分类，并结合基于规则的推理模型进行优先权和得分判定。

Result: 在有限的手工标注数据上，5折交叉验证的平均macro-F1得分为0.549，优于TCN、BiLSTM和普通Transformer等多个基线模型。

Conclusion: 虽然尚未达到部署水平，但结果表明这是实现花剑比赛自动化裁判辅助的有前景路径，并为击剑领域的AI应用（如教练系统）开辟了新机会。

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [24] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: LLMZ+是一种基于提示白名单的防御机制，通过只允许上下文适当的消息与智能LLM交互来增强安全性，相比传统检测方法能更有效地防范越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 智能AI因其对数据源和API工具的特权访问而成为攻击者的高价值目标，其非确定性行为特性带来了重大的安全和信息安全风险。

Method: 采用提示白名单方法，只允许符合预定义用例和操作边界的上下文适当消息与智能LLM交互。

Result: 实证评估显示LLMZ+对常见越狱提示具有强韧性，在实验环境中假阳性和假阴性率均可降至0。

Conclusion: LLMZ+方法简化了安全框架，增强了长期韧性，减少了维持LLM信息安全所需的资源，同时不影响合法的业务通信。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [25] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: 提出一种结合LLM和外部符号方程求解器的新方法，通过分解问题生成方程、求解验证和迭代修正，显著提升数学应用题解决能力


<details>
  <summary>Details</summary>
Motivation: LLM在解决数学应用题时面临困难，需要结合推理和数学能力，现有方法仍有改进空间

Method: 先让LLM分解问题生成方程，用外部求解器得到答案，再让LLM估算答案进行验证，失败时采用迭代修正

Result: 在数值和代数应用题数据集上达到新的SOTA结果，平均提升近2%，在三角函数应用题上也取得满意结果

Conclusion: 该方法有效提升了LLM解决复杂数学问题的能力，并引入了两个新数据集推动LLM推理能力测试

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [26] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 本文提出了一个结合气候灾害数据和进化学习的地理空间多主体模型，用于评估气候风险对经济系统的影响，特别是通过供应链传导的级联风险。


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要建模空间异质性灾害与适应性经济系统之间的复杂相互作用，现有模型难以捕捉供应链传导的间接风险。

Method: 开发了基于Mesa的地理空间多主体模型，集成CLIMADA气候影响评估，引入进化学习机制让企业通过适应度选择和变异演化预算分配、定价、工资和风险适应策略。

Result: 使用RCP8.5情景下的河流洪水预测到2100年，显示进化适应使企业能够在数十年气候压力干扰后恢复到基线生产水平，但未直接暴露于洪水的企业仍通过供应链中断受到影响，世纪末商品平均价格比基线高5.6%。

Conclusion: 该开源框架为金融机构和公司提供了量化直接和级联气候风险的工具，同时评估成本效益适应的策略。

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [27] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG是一个低成本图检索增强生成框架，通过个性化PageRank在检索阶段构建信息图，仅用3%-11%的输出token就能达到主流图RAG方法80%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG系统忽视了LLM token使用的高成本，阻碍了大规模应用，需要开发低成本的图构建方法。

Method: 受HippoRAG启发，在检索阶段引入个性化PageRank(PPR)来构建信息图，显著降低token消耗。

Result: TERAG仅消耗3%-11%的输出token，就能达到广泛使用的图RAG方法至少80%的准确率。

Conclusion: TERAG证明了在显著降低计算成本的同时，仍能保持图RAG系统良好性能的可行性。

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [28] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 本文探讨了机器学习模型在航空系统中的安全合规性，提出了机器学习模型描述（MLMD）的概念，并细化了语义保持的关键概念，以确保模型的准确复制。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在航空系统中的应用增加，需要确保这些系统的安全操作并符合相关指导标准（如EASA概念论文和ED-324）。本文旨在澄清ML模型与其明确描述之间的区别，并确保模型在目标环境中保持训练性能。

Method: 通过定义机器学习模型描述（MLMD）和细化语义保持的概念，确保模型的准确复制。应用这些贡献到多个工业用例中，构建和比较多个目标模型。

Result: 通过工业用例的应用，验证了MLMD和语义保持概念的有效性，能够确保机器学习模型在目标环境中的安全性和性能一致性。

Conclusion: 本文提出的MLMD和语义保持概念为机器学习模型在航空系统中的安全合规性提供了重要基础，有助于确保模型在目标环境中的可靠操作。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [29] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型在医学领域的最新研究进展，包括训练技术、医疗应用、优势局限，并创新性地将医学LLMs分为三类，评估方法分为两类，提出解决现有挑战的方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，大型语言模型在医学领域展现出巨大应用潜力，需要系统梳理当前研究进展，为后续研究提供指导。

Method: 采用系统性文献综述方法，深入分析医学大模型的训练技术、医疗场景适应、相关应用及其优缺点。

Result: 提出了医学LLMs的三分类法和评估方法的两分类法，识别了现有挑战并提出了相应解决方案。

Conclusion: 通过系统综述，强调了开发医学LLMs的必要性，加深了对当前发展状态的理解，为后续研究提供了清晰指导。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [30] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 本文提出了自主数据代理（DataAgents）的概念，通过集成LLM推理与任务分解、行动推理和工具调用，能够自主处理数据任务，将复杂非结构化数据转化为可操作知识。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和复杂性增长，数据准备、转换和分析工作仍然劳动密集、重复且难以扩展。数据与AI之间的对齐至关重要，但数据通常没有以最优方式结构化以供AI利用。

Method: DataAgents整合LLM推理、任务分解、行动推理和工具调用，能够自主解释数据任务描述，将任务分解为子任务，推理行动，将行动转化为Python代码或工具调用，并执行操作。

Result: DataAgents能够处理数据收集、集成、预处理、选择、转换、重新加权、增强、重编程、修复和检索等任务，动态规划工作流程，调用强大工具，适应大规模多样化数据任务。

Conclusion: DataAgents代表了向自主数据到知识系统的范式转变，需要推进行动工作流优化、建立开放数据集和基准生态系统、保护隐私、平衡效率与可扩展性，并开发可信赖的DataAgent防护措施。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [31] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 提出了经验扩展框架，通过自主环境交互和协作经验共享实现LLM的持续进化，突破静态人类生成数据的限制


<details>
  <summary>Details</summary>
Motivation: 传统通过扩大模型规模、训练数据和计算能力的方法已接近饱和，人类生成文本资源枯竭，需要新的持续学习机制

Method: 经验扩展框架：捕获原始交互、提炼为紧凑可重用知识、定期优化存储内容以保持相关性和效率

Result: 在模拟真实场景中验证，包括泛化到未见相关任务、重复查询和知识库过饱和情况，经验扩展提高了准确性，维持了长期性能，并在新情境中保持优势

Conclusion: 结构化部署后学习能够超越静态人类生成数据的限制，为持续智能进步提供可扩展路径

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [32] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS是一个分布式目录服务，用于发现AI代理的能力、元数据和来源。它利用内容寻址存储、分层分类法和加密签名，在异构多代理系统中实现高效、可验证的多维发现。


<details>
  <summary>Details</summary>
Motivation: 为了解决异构多代理系统中代理能力发现和验证的挑战，需要一个能够支持高效、可验证且可扩展的目录服务。

Method: 基于Open Agentic Schema Framework构建，采用两级映射架构（通过Kademlia DHT实现），重用OCI/ORAS基础设施进行工件分发，集成Sigstore进行来源验证，支持基于模式的可扩展性。

Result: ADS提供了一个分布式目录服务，能够实现代理能力的高效发现、元数据管理和来源验证，支持新兴代理模式（如LLM提示代理、MCP服务器等）。

Conclusion: ADS为新兴代理注册和互操作性倡议提供了一个有前景的架构模型，具有安全性、性能和可扩展性优势。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [33] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER是一种基于模型检查的验证方法，用于验证LLM文本生成过程的概率计算树逻辑(PCTL)属性。该方法通过α-k有界文本生成来限制验证范围，聚焦于每个生成步骤中top-k令牌的α最大累积概率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本生成过程缺乏形式化验证方法，无法保证生成文本的一致性和可靠性。作者发现文本生成过程中通常只选择有限数量的令牌，这为形式化验证提供了可能性。

Method: 提出α-k有界文本生成方法，在每个生成步骤中只考虑累积概率超过阈值α的top-k令牌。LLMCHECKER利用模型检查技术来验证PCTL属性，支持多种文本量化评估方法。

Result: 该方法在多个LLM模型（包括Llama、Gemma、Mistral、Genstruct和BERT）上进行了验证，证明了其适用性。这是首次将PCTL模型检查应用于LLM文本生成过程的一致性验证。

Conclusion: LLMCHECKER为LLM文本生成过程提供了首个基于PCTL的形式化验证框架，能够有效验证文本质量和偏见等属性，为LLM的可靠应用提供了重要工具。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [34] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 提出一个模块化框架用于ICD-10-CM编码预测，通过原则性模型选择、冗余感知数据采样和结构化输入设计来解决LLM在医疗编码中的挑战。


<details>
  <summary>Details</summary>
Motivation: ICD编码对临床文档、计费和分析至关重要，但目前仍是劳动密集型且易出错的任务。LLM在自动化ICD编码方面有潜力，但在基础模型选择、输入情境化和训练数据冗余方面存在挑战。

Method: 采用LLM-as-judge评估协议和Plackett-Luce聚合来评估开源LLM对ICD-10-CM代码定义的理解；引入基于嵌入的相似性度量和冗余感知采样策略；利用台湾医院的结构化出院摘要评估情境效应。

Result: 实验表明，经过微调的选择基础模型在内外部评估中始终优于基线LLM；包含更多临床部分持续提高预测性能。

Conclusion: 该研究使用开源LLM建立了一个实用且原则性的ICD-10-CM编码预测方法，为自动化医疗编码系统的实际部署提供了可扩展的机构就绪解决方案。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [35] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种名为MAPO（Mixed Advantage Policy Optimization）的新策略，解决了GRPO中存在的优势函数反转和镜像问题，通过动态重加权优势函数来适应不同轨迹确定性的样本。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在优势函数分配上存在优势反转和优势镜像问题，阻碍了不同查询样本间的合理优势分配。

Method: 提出MAPO策略，引入优势百分比偏差来处理高确定性轨迹样本，并动态重加权优势函数以适应不同轨迹确定性的样本。

Result: 与相关最先进方法的比较以及不同优势变体的消融研究验证了该方法的有效性。

Conclusion: MAPO是一种简单但有效的GRPO策略，能够更好地处理优势函数分配问题，提升基础模型在推理任务上的性能。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [36] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 提出了ProfileBench基准和Conf-Profile框架，通过置信度驱动的两阶段方法解决用户画像中的标签稀缺和噪声问题，显著提升了LLM在用户画像任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 用户画像作为用户理解的核心技术面临缺乏全面基准的问题，且大规模真实标签收集困难，异构噪声用户信息会影响LLM的可靠性。

Method: 提出Conf-Profile框架：第一阶段利用高级LLM合成高质量标签，通过置信度加权投票和校准；第二阶段通过置信度引导的无监督强化学习增强推理能力。

Result: 实验结果表明Conf-Profile通过两阶段训练显著提升性能，在Qwen3-8B模型上F1分数提高了13.97。

Conclusion: 该方法有效解决了用户画像中的标签稀缺和可靠性问题，为标签无关的可靠用户画像提供了可行方案。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [37] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 本文提出了一个统一的LLM记忆操作定义和四部分分类法，建立了包含位置、持久性、写入/访问路径、可控性的记忆四元组，并通过三设置协议实现可比较的评估框架。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM记忆研究中的异构设置比较失真问题，建立可重现、可比较、可治理的记忆研究坐标系统。

Method: 采用四部分分类法（参数化、上下文、外部、程序性/情景性）和记忆四元组，通过三设置协议（仅参数化、离线检索、在线检索）进行分层评估。

Result: 构建了包含时间治理、泄漏审计、不确定性报告的综合评估框架，并提出了DMM Gov更新和遗忘协调机制。

Conclusion: 该框架为LLM记忆研究提供了可重现、可比较、可治理的坐标系统，并提出了四个可测试的命题来指导未来研究。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [38] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个5600亿参数的开源MoE推理模型，通过精心设计的训练流程（包括长链思维数据冷启动和大规模强化学习）实现高效推理能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够高效处理复杂推理任务的开源模型，特别是在STEM、代码和智能体推理等领域实现卓越性能。

Method: 采用长链思维数据冷启动训练策略，结合领域并行训练方案将不同领域的专家模型融合为单一帕累托最优模型，使用DORA系统进行大规模RL训练。

Result: 在复杂推理任务上达到开源模型的最先进性能，在AIME-25上智能体推理平均token消耗减少64.5%（从19,653降至6,965），且任务准确率不降低。

Conclusion: LongCat-Flash-Thinking展示了在推理系统和智能体AI研究方面的重大进展，模型已开源以促进进一步研究。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [39] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 本文系统研究了视觉语言模型中的视觉空间推理能力，提出了空间智能的三个能力层次，并创建了SIBench基准测试。实验发现当前模型在感知任务上表现良好，但在理解和规划任务上存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 视觉空间推理是人类核心认知能力，对推进具身智能和自主系统至关重要。尽管视觉语言模型取得进展，但实现人类水平的空间推理仍面临挑战，需要系统性的研究。

Method: 通过回顾现有方法（输入模态、模型架构、训练策略、推理机制），将空间智能分为三个能力层次，并构建包含20个开源数据集、23个任务设置的SIBench基准测试。

Result: 实验表明，最先进的视觉语言模型在基本感知任务上有竞争力，但在理解和规划任务上表现不佳，特别是在数值估计、多视图推理、时间动态和空间想象方面存在明显差距。

Conclusion: 实现空间智能仍面临重大挑战，本研究提供了系统性路线图和全面基准测试，以推动该领域未来研究。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [40] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了DEAL框架，将低秩适应（LoRA）与持续微调策略相结合，通过知识保留和自适应参数更新模块解决传统微调方法中的灾难性遗忘和数据效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法存在灾难性遗忘和次优数据效率的问题，限制了在实际应用中的适用性。需要一种能够在保持效率的同时提升任务性能的持续适应方法。

Method: DEAL框架整合了LoRA技术和持续微调策略，包含知识保留模块和自适应参数更新模块，在隐私保护设置下保持高效性。

Result: 在15个不同数据集上的实验表明，DEAL在任务准确性和资源效率方面均显著优于基线方法。

Conclusion: 该方法通过增强任务性能同时提高资源效率，展示了在LLMs中推进持续适应的潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [41] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本文首次对基于大语言模型（LLM）的智能体中的幻觉问题进行了全面调查，提出了新的分类法识别不同阶段的幻觉类型，深入分析了18种触发原因，并总结了幻觉缓解和检测方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM-based智能体在人类认知、推理和交互方面表现出强大能力，但它们仍然容易产生幻觉问题，这可能导致错误的任务执行并削弱系统可靠性。需要对这些幻觉问题进行深入理解和系统整合。

Method: 通过仔细分析智能体的完整工作流程，提出了新的分类法来识别不同阶段出现的幻觉类型；对18种触发原因进行了深入检查；通过大量现有研究的详细回顾，总结了幻觉缓解和检测方法。

Result: 提出了首个关于LLM-based智能体幻觉的全面调查，建立了系统的分类框架，识别了多种幻觉类型和触发机制，为后续研究提供了理论基础。

Conclusion: 这项调查旨在激发更多解决LLM-based智能体幻觉问题的努力，最终促进开发更健壮可靠的智能体系统，为未来研究指明了有前景的方向。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [42] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 研究大语言模型能否从数学可解释的推荐模型中生成有效的用户导向解释，通过用户研究评估不同解释策略的质量。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI研究多依赖自动评估指标，但这些指标往往无法捕捉用户的实际需求和感知，因此需要采用用户中心的方法来评估解释质量。

Method: 基于约束矩阵分解的推荐模型，通过精心设计的LLM提示将模型结构转化为自然语言解释，并对326名参与者进行用户研究，评估五种关键维度的解释质量。

Result: 所有解释类型都普遍受到好评，不同策略之间存在适度的统计差异，用户评论提供了超越定量结果的补充见解。

Conclusion: LLM能够从数学可解释的推荐模型生成有效的用户导向解释，用户中心评估方法为解释质量提供了更全面的理解。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [43] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文比较了四种剩余时间预测方法在物流公司出库仓库流程中的表现，发现深度学习模型精度最高，但浅层方法如传统提升技术也能达到竞争性精度且计算资源需求显著减少。


<details>
  <summary>Details</summary>
Motivation: 预测性过程监控旨在预测正在进行的流程执行的未来，其中一个常见预测目标是剩余时间。本文旨在在真实物流场景中比较不同剩余时间预测方法的性能。

Method: 在物流公司航空业务的出库仓库流程中，使用包含169,523条轨迹的新颖原始事件日志，比较了四种剩余时间预测方法，包括深度学习模型和浅层方法（如传统提升技术）。

Result: 深度学习模型实现了最高精度，但浅层方法如传统提升技术也能达到竞争性精度，且所需计算资源显著减少。

Conclusion: 虽然深度学习在剩余时间预测方面精度最高，但浅层方法在精度和计算效率之间提供了更好的平衡，在实际应用中可能更具实用性。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [44] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: 该论文提出了基于游戏研究和游戏AI的Landmarks、Monuments和Beacons三个嵌套概念，用于解决程序生成内容的算法评估问题，特别是针对复合制品的评估。


<details>
  <summary>Details</summary>
Motivation: 程序生成内容的算法评估难以找到与人类体验一致的指标，特别是对于复合制品。自动分解作为一种可能的解决方案需要满足一系列属性的概念。

Method: 从游戏研究和游戏AI研究中引入Landmarks、Monuments和Beacons三个嵌套概念，这些概念基于制品的可感知性、唤起性和行动召唤，采用玩家中心视角。这些术语具有游戏通用性，可跨类型使用。

Result: 这些实体可以使用当前研究和工业中使用的技术来发现和评估，为程序生成内容的完全自动化分解和显著子组件的评估开辟了道路。

Conclusion: 该方法旨在连接人文学科和技术游戏研究，实现更好的计算程序生成内容评估，虽然重点强调混合倡议PCG和组合PCG，但相信其应用范围更广。

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [45] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: 本文提出了一个使用可观测源作为辅助变量的因果表示学习框架，能够在已知潜在因果图的情况下识别潜在变量，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的因果表示学习方法通常需要已知辅助变量来实现可识别性，但这些方法限制辅助变量必须独立于混合函数。然而在实际中，系统驱动的潜在因素可能容易从数据中观测或提取，这有助于识别过程。

Method: 引入可观测源作为辅助变量的框架，使用保体积编码器识别潜在变量（达到子空间变换和排列的程度），并在有多个已知辅助变量时提供变量选择方案以最大化潜在因素的可恢复性。

Result: 实验在合成图和图像数据上验证了该框架的有效性，能够成功识别潜在变量。

Conclusion: 该框架扩展了当前方法的边界，通过利用可观测源作为辅助变量，为因果表示学习提供了新的可能性。

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [46] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC是一种基于代码驱动的规划方法，通过生成多样化高级规划程序并训练领域自适应批评器来减少LLM查询成本，同时提高长期奖励对齐的规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划方法依赖频繁查询进行迭代优化，导致高查询成本且难以实现长期奖励对齐。需要一种既能减少查询次数又能提升长期规划效果的方法。

Method: 使用LLM生成多样化高级规划程序，通过领域自适应批评器评估候选计划并选择最符合长期奖励的方案执行。

Result: 在ALFWorld、NetHack和StarCraft II Unit Building三个环境中，CoPiC相比AdaPlanner和Reflexion基线，平均成功率提升23.33%，查询成本降低91.27%。

Conclusion: CoPiC通过代码驱动规划和领域自适应批评器的结合，有效解决了LLM规划中的查询成本高和长期奖励对齐问题，在多个复杂环境中表现出优越性能。

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [47] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit是一个多智能体系统初始化方法，通过优化智能体团队结构来提升系统性能，结合自然语言到格式机制和帕累托平衡选择策略，在多个任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MAS初始化方法未能充分考虑后续阶段生成智能体的协作需求，需要更好的团队组合策略来提升系统效率和效果。

Method: AgentInit采用多轮智能体交互和反思，结合自然语言到格式机制确保一致性，应用帕累托原则进行平衡团队选择，同时考虑团队多样性和任务相关性。

Result: 实验表明AgentInit在多个框架和任务中持续优于最先进的初始化方法和预定义策略，性能提升分别达1.2和1.6倍，同时显著减少token消耗。

Conclusion: AgentInit具有良好的可迁移性和关键组件的有效性，证明其作为可靠MAS初始化方法的能力和适应性。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [48] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: 本文研究LLMs在阿拉伯世界的跨文化常识推理迁移，发现仅需12个特定文化示例就能平均提升10%性能，且来自印尼和美国的跨文化演示也能实现类似效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在西方中心偏见，在多元文化环境中效果有限。虽然已有文化对齐研究，但跨文化迁移潜力（利用一种文化的对齐来提升其他文化性能）仍未充分探索

Method: 使用覆盖13个阿拉伯国家的文化基础常识推理数据集，评估轻量级对齐方法（上下文学习、演示强化DITTO）以及监督微调和直接偏好优化基线

Result: 仅需12个特定文化示例就能在阿拉伯国家间平均提升10%性能；来自印尼和美国的跨文化演示在MCQ推理中能达到或超越文化内对齐效果

Conclusion: 高效的跨文化对齐是可行的，为LLMs适应低资源文化环境提供了有前景的方法

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [49] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种统一的多任务学习框架，通过动态提示调度机制解决大语言模型在多任务和跨域设置下的泛化限制问题。该方法使用提示池和任务感知调度策略，动态组合和对齐不同任务的提示，增强模型捕捉任务间语义差异的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法如SPoT依赖固定提示模板，在多任务和跨域场景下存在泛化限制。需要开发能够动态适应不同任务的统一学习框架，以提升模型的稳定性和迁移能力。

Method: 提出统一多任务学习框架，包含动态提示调度机制：1）使用提示池和任务感知调度策略动态组合提示；2）通过任务嵌入和门控机制精细控制提示信号；3）采用联合多任务学习优化目标，包含调度权重的自动学习策略。

Result: 敏感性实验验证了该方法在模型稳定性和迁移性方面的优势。实验结果显示，提示调度方法在一系列语言理解和知识推理任务上显著提升性能，证明了其在统一多任务建模和跨域适应中的有效性和适用性。

Conclusion: 提出的动态提示调度机制有效解决了多任务学习中的任务干扰和负迁移问题，为大规模语言模型在多任务和跨域场景下的应用提供了可行的解决方案。

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [50] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS是一个评估LLMs数学能力的基准测试，涵盖12个核心技能维度，分为三个领域：知识与理解、问题解决与沟通、元技能与创造力。通过按认知技能分类问题和设计隔离特定能力的任务，GAUSS构建了全面、细粒度且可解释的模型数学能力画像。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够全面评估LLMs数学能力多维度的基准测试，需要一种能够揭示模型底层数学智能结构的评估方法。

Method: 设计十二个核心技能维度的数学问题分类体系，通过隔离特定认知能力的任务设计来构建模型能力画像。

Result: 成功构建了GPT-5-thinking的技能画像，揭示了其优势、弱点以及与o4-mini-high的差异。

Conclusion: GAUSS基准测试提供了多维度的技能评估方法，能够更准确地反映LLMs的数学智能水平，为模型评估提供了新的视角。

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [51] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: 本文提出了一种基于Rubin因果模型的事件因果关系识别方法，通过将第一个事件视为治疗、第二个事件视为结果，利用合成控制方法生成虚拟双胞胎来估计因果效应，在COPES-hard基准上优于包括GPT-4在内的现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的事件因果关系识别方法主要依赖语言模式和多跳关系推理，容易因因果关系的非正式使用和虚假的图推理而导致错误识别。需要更可靠的方法来区分因果关系和相关关系。

Method: 采用Rubin因果模型框架，将时间上先发生的事件视为治疗，后发生的事件视为结果。由于无法在文本领域实际实施干预，使用合成控制方法从相关历史数据中生成虚拟双胞胎，结合文本嵌入合成和反演技术来估计因果效应。

Result: 该方法在因果关系基准COPES-hard上表现出色，识别效果优于包括GPT-4在内的现有方法，能够更稳健地识别事件间的因果关系。

Conclusion: 基于Rubin因果模型的合成控制方法为事件因果关系识别提供了更可靠的框架，通过概念化的干预和虚拟双胞胎生成，有效解决了文本领域因果推断的挑战，在准确性上超越了传统方法和大型语言模型。

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [52] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: ZERA是一种新颖的自动提示优化框架，通过联合优化系统提示和用户提示，使用结构化评分标准和低开销迭代，实现快速收敛到高质量提示。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示优化方法通常只关注用户提示，依赖非结构化反馈，需要大量样本和长迭代周期，导致成本高且脆弱。

Method: ZERA使用八个可泛化标准对提示进行评分，并基于这些结构化批评修订提示，通过最小化示例和短迭代周期实现快速收敛。

Result: 在五个LLM和九个多样化数据集上的实验结果显示，ZERA相比强基线方法取得了持续改进。消融研究验证了各组件对更有效提示构建的贡献。

Conclusion: ZERA框架通过结构化、低开销的提示优化方法，显著提高了LLM在各种任务上的性能，为自动提示优化提供了更高效可靠的解决方案。

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [53] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 该论文研究了外部信息对具有逐步思考能力的大型语言模型推理过程的因果影响，发现模型的思考模式是一把双刃剑：有帮助的信息能提高准确性，但误导性信息会导致性能灾难性下降，且思考过程会放大错误。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，LLMs经常被增强外部信息，这些信息可能是有帮助的、无关的甚至是误导性的。论文旨在系统性地测试模型对这些信息的鲁棒性。

Method: 引入SciAux数据集（基于ScienceQA），系统测试模型对帮助性、无关性和误导性信息的鲁棒性，分析模型思考过程对这些信息的响应。

Result: 研究发现模型的思考模式存在关键脆弱性：有帮助的上下文能提高准确性，但误导性信息会导致性能灾难性下降，且思考过程会放大错误程度。

Conclusion: 挑战不仅在于让模型"思考"，更重要的是赋予它们评估推理所依赖信息的批判能力。SciAux数据集可用于进一步研究。

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [54] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: 提出了一种过程监督的多智能体框架来优化检索增强生成（RAG）中检索器和生成器之间的协调问题，通过决策制定者和知识选择器两个轻量级智能体，结合过程级奖励和树状展开策略，实现了更高的准确性和更稳定的收敛。


<details>
  <summary>Details</summary>
Motivation: RAG系统中检索器和生成器独立开发导致交互不理想：检索器可能返回不相关或冗余文档，生成器未能充分利用检索到的证据。需要解决两者之间的协调问题。

Method: 使用过程监督的多智能体框架，包含决策制定者（决定何时继续检索或停止生成）和知识选择器（过滤检索文档）。采用LLM-as-a-Judge进行过程级奖励评估，树状展开策略探索多样化推理路径，使用PPO进行端到端训练。

Result: 在单跳和多跳问答基准测试中，该方法相比标准RAG基线实现了更高的准确性、更稳定的收敛性，并产生更可解释的推理轨迹。

Conclusion: 该框架是模块化且即插即用的，无需修改检索器或生成器，适用于实际RAG应用，有效解决了检索器与生成器之间的协调问题。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [55] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: 提出了一种新的情感识别和预测架构ERFC，用于预测对话中未来话语的情感，特别适用于呼叫中心等场景以提升客户体验。


<details>
  <summary>Details</summary>
Motivation: 在呼叫中心等场景中，客服人员需要及时了解客户情绪变化，通过保持中立和积极情绪来安抚客户。预测未来话语的情感有助于客服在合适时机提供正确解决方案。

Method: 提出ERFC架构，考虑多模态、情感的不同属性、上下文以及对话中说话者话语之间的相互依赖关系。

Result: 在IEMOCAP数据集上的密集实验证明了所提ERFC方法的可行性。

Conclusion: 该方法在呼叫中心等客户满意度至关重要的应用场景中具有巨大的商业价值。

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [56] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: 本文评估了八个开源LLM检测反犹内容的能力，提出了一种新的Guided-CoT提示方法，该方法能有效提升模型性能，并分析了LLM在仇恨内容检测中的差异和局限性。


<details>
  <summary>Details</summary>
Motivation: 仇恨内容检测是一个重要但具有挑战性的问题，需要自动化工具来适应社交媒体环境的不断变化。现有机器学习模型需要持续训练，而本研究探索LLM在此任务上的潜力。

Method: 使用八个开源LLM，采用上下文定义作为政策指南，探索多种提示技术，设计了新的Guided-CoT提示方法，并分析了模型生成的理性解释的语义差异。

Result: Guided-CoT方法在所有评估模型中都提高了性能，不受解码配置、模型大小或推理能力的影响。Llama 3.1 70B甚至超越了微调的GPT-3.5。同时发现了LLM之间的显著差异和矛盾行为。

Conclusion: 实验揭示了不同LLM在实用性、可解释性和可靠性方面的差异，强调了在仇恨内容检测任务中需要仔细评估和选择适当的模型和方法。

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [57] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出TEMPO算法，通过前缀树结构实现无critic模型的token级信用分配，解决强化学习在LLM推理任务中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 在数学和医疗QA等可验证奖励场景中，只有少数关键决策token影响最终结果，但现有方法如PPO训练复杂，GRPO忽略分支结构且信用分配粗糙。

Method: 提出P2T方法将多响应转换为前缀树计算非参数化前缀值，在此基础上开发TEMPO算法，结合GRPO的组相对信号和基于树的分支门控TD修正。

Result: 在Qwen3-1.7B/4B模型上，TEMPO在MATH、MedQA等基准测试中优于PPO和GRPO，并在分布外测试中表现更好，达到更高验证准确率。

Conclusion: TEMPO提供了一种简单有效的无critic token级信用分配方法，在可验证奖励场景下显著提升LLM推理性能。

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [58] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文探索了一种新范式：将LLM作为知识图谱推理路径的奖励模型，让模型学习判断候选路径是否能正确诊断患者症状，而不是直接将KG内容插入提示中。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过检索增强生成或微调来整合知识图谱，但缺乏结构化推理能力。本文受奖励训练和计算理论启发，希望实现更可靠的诊断推理。

Method: 系统评估了5种知识路径判断任务和8种训练范式，测试路径判断能力是否能泛化到下游诊断任务，包括诊断总结和医学问答。

Result: 实验显示特定奖励优化和蒸馏能带来强大的路径判断性能，但向下游任务的迁移性仍然较弱。

Conclusion: 这是对临床知识图谱进行"奖励模型风格"推理的首次系统评估，为基于奖励的监督如何影响医疗GenAI系统的诊断推理提供了见解。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [59] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SubSpec是一种无需训练的即插即用方法，通过生成低比特量化替代层来构建高度对齐的草稿模型，加速参数卸载，在有限VRAM下实现显著推理加速。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内存受限GPU上部署的挑战，现有压缩方法会降低质量，而参数卸载方法推理速度慢，需要一种既能保持质量又能加速推理的解决方案。

Method: 构建高度对齐的草稿模型，生成低比特量化替代层来替代卸载的目标LLM部分，共享剩余的GPU驻留层和KV-Cache，减少内存开销并增强对齐。

Result: 在8GB VRAM限制下，Qwen2.5 7B在MT-Bench上实现9.1倍加速；在24GB VRAM限制下，Qwen2.5 32B在流行生成基准测试中平均实现12.5倍加速。

Conclusion: SubSpec提供了一种无损且无需训练的方法，通过高度对齐的草稿模型实现显著的推理加速，有效解决了参数卸载的延迟问题。

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [60] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: Speech Vecalign是一种不依赖文本转录的并行语音文档对齐方法，通过单调对齐语音段嵌入，相比基线方法能产生更长的语音对齐且噪声更少。


<details>
  <summary>Details</summary>
Motivation: 现有的语音对齐方法如Global Mining和Local Mining存在对齐长度不足或噪声较多的问题，需要一种更鲁棒的语音对齐方法来提高语音到语音翻译的质量。

Method: Speech Vecalign采用单调对齐语音段嵌入的方法，不依赖文本转录，直接对语音信号进行处理。该方法在3000小时的未标记英语-德语并行语音文档上进行测试。

Result: 该方法产生了约1000小时的高质量对齐，相比Global Mining在英德和德英翻译任务上分别提升了0.37和0.18 ASR-BLEU分数，且使用比SpeechMatrix少8倍的原始语音文档就能达到或超过其性能。

Conclusion: Speech Vecalign是一种有效的语音对齐方法，能够在不依赖文本转录的情况下产生高质量的语音对齐，显著提升语音到语音翻译的性能。

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [61] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出了一种基于LLM的说话人日志校正系统，通过实时用户反馈来修正说话人归属错误，显著降低了说话人日志错误率


<details>
  <summary>Details</summary>
Motivation: 大多数自动语音处理系统以"开环"模式运行，缺乏用户反馈机制，而人机协作工作流可以显著提高准确性

Method: 系统执行流式ASR和说话人日志，使用LLM生成简洁摘要，接受用户简短语音反馈并立即整合。开发了SWM技术检测和分割多说话人段，以及基于用户校正的在线说话人注册

Result: 在AMI测试集上的LLM驱动模拟显示，系统显著降低了DER 9.92%和说话人混淆错误44.23%

Conclusion: 该系统通过人机协作显著提升了说话人日志的准确性，分析了不同设置下的校正效果

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [62] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: NormGenesis是一个多文化对话生成框架，通过Violation-to-Resolution对话类型和基于范例的迭代优化，提升跨语言对话系统的社会规范遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统缺乏对社会规范的建模能力，特别是在多语言环境下难以产生文化上可接受的回应。需要建立能够处理规范违反到修复过程的对话模型。

Method: 提出V2R对话类型建模规范违反到修复的过程；采用基于范例的迭代优化方法在对话生成早期引入语言、情感和社会文化对齐；构建包含10,800个多轮对话的数据集，标注规范遵循、说话者意图和情感响应。

Result: 人类和LLM评估显示，NormGenesis在优化质量、对话自然度和泛化性能上显著优于现有数据集；使用V2R增强数据训练的模型在伦理敏感场景中表现出更好的语用能力。

Conclusion: 该工作为文化自适应对话建模建立了新基准，提供了跨语言文化多样性的规范感知生成的可扩展方法。

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [63] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本文评估了大语言模型在生成波斯文学文本方面的创造力，通过构建包含20个主题的数据集，并采用托兰斯创造力测试的四个维度进行自动化评分，同时分析了模型对四种文学手法的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语文学创作，缺乏对非英语文学传统（如波斯文学）的系统评估，且缺乏标准化的创造力评估方法。

Method: 构建波斯文学数据集，采用托兰斯创造力测试的四个维度（原创性、流畅性、灵活性、精细性）进行LLM自动化评分，并验证其与人工评分的一致性，同时分析模型对四种文学手法的运用能力。

Result: LLM作为评分者与人工评分具有强一致性，模型在波斯文学文本生成方面展现出一定创造力，但也存在局限性。

Conclusion: LLMs在波斯文学创作中具有潜力但需要进一步改进，研究为跨语言文学创造力评估提供了标准化方法。

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [64] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究开发了一种使用语言建模和对话对齐(CA)评分来自动测量医患共享决策(SDM)的方法，通过深度学习模型和微调的BERT模型分析医患对话，发现CA评分与SDM结果显著相关。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏自动测量医患共享决策(SDM)的方法，而SDM对于实现以患者为中心的护理至关重要。

Method: 使用157个视频记录的医患对话，转录为42,559个句子，通过上下文-响应对和负采样训练深度学习模型和微调BERT模型，计算四种类型的CA评分，并评估其与SDM结果(决策冲突量表和OPTION12评分)的关联。

Result: 微调的BERTbase模型达到最高召回率0.640，DL模型和BERT模型生成的CA评分与OPTION12和决策冲突量表显著相关，模型大小不影响CA评分与SDM的关联。

Conclusion: 本研究引入了一种自动、可扩展的方法来通过可解释的CA评分测量医患对话中的SDM，具有大规模评估SDM策略的潜力。

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [65] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: CogniLoad是一个基于认知负荷理论的新型合成基准，通过独立可调参数来精确分析LLM的长上下文推理能力，揭示了任务长度是主要限制因素。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文推理基准往往模糊了关键因素（如任务复杂性、干扰信息等），需要更精确的失败分析工具。

Method: 基于认知负荷理论设计自然语言逻辑谜题，通过三个独立可调参数（内在难度d、干扰信号比ρ、任务长度N）来系统控制认知负荷维度。

Result: 评估22个最先进的推理LLM，发现任务长度是主要约束，模型对内在复杂性有不同容忍度，对干扰比呈现U型响应。

Conclusion: CogniLoad提供了一个可重复、可扩展且诊断性强的工具，可用于剖析LLM推理局限并指导未来模型开发。

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [66] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: LAWCAT是一种线性注意力框架，通过将预训练transformer的能力高效转移到线性注意力架构中，解决了transformer二次计算复杂度的问题，显著扩展了有效上下文窗口并减少了训练资源需求。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然性能优异，但其二次计算复杂度成为长上下文应用的瓶颈。现有线性复杂度替代方案训练成本高，需要一种能够高效利用预训练模型能力的方法。

Method: LAWCAT整合因果Conv1D层增强局部依赖建模，采用归一化门控线性注意力改善不同上下文长度的泛化能力，通过知识蒸馏将预训练transformer能力转移到线性架构。

Result: 在Mistral-7B上仅用1K长度序列蒸馏，就能在22K tokens内实现超过90%的passkey检索准确率；Llama3.2-1B LAWCAT变体在多个长上下文任务上表现优异，所需预训练token不到0.1%。

Conclusion: LAWCAT为高性能长上下文线性模型提供了高效路径，适合边缘部署，减少了对大量长序列训练数据和计算资源的依赖。

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [67] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: 本文通过大规模系统评估，研究了LLM在图数据上的表现，发现代码生成方法在图形任务中表现最佳，特别是在长文本或高度数图上优势明显，且所有交互策略在异质图上都有效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在文本丰富的图机器学习任务中应用日益广泛（如欺诈检测、推荐系统），但该领域缺乏对LLM与图数据交互能力的系统性理解，需要建立原则性的评估框架。

Method: 进行了大规模控制性评估，涵盖多个关键变量轴：LLM-图交互模式（提示、工具使用、代码生成）、数据集领域、结构机制、特征特性、模型配置等，并通过截断特征、删除边、移除标签等方法分析依赖关系。

Result: 1）代码生成方法在图形数据上表现最强，特别是在长文本或高度数图上优势显著；2）所有交互策略在异质图上都保持有效；3）代码生成能够灵活调整对结构、特征或标签的依赖，利用最信息丰富的输入类型。

Conclusion: 研究结果全面展示了当前LLM-图交互模式的优势和局限性，为未来方法提供了关键设计原则，特别强调了代码生成在图形推理任务中的有效性。

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [68] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种使用ByT5模型在阿拉伯诗歌中插入短语以符合特定韵律的方法，通过基于规则的音素-节拍转换和条件去噪目标训练，实现了高韵律对齐和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自动为阿拉伯诗歌插入符合特定韵律短语的方法，以支持古典阿拉伯诗歌创作的协同创作应用。

Method: 使用基于规则的grapheme-to-beat转换提取韵律，采用条件去噪目标微调ByT5模型，运用课程学习策略（先在通用阿拉伯数据集上预训练，再在诗歌数据集上微调），并探索从英语到阿拉伯语的跨语言迁移。

Result: 实验结果表明，所提出的模型在保持语义连贯性的同时实现了高韵律对齐。

Conclusion: 该模型在古典阿拉伯诗歌创作过程中具有协同创作应用的潜力。

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [69] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级框架来检测原始和经过提示修改的AI生成文本，通过分析文本内部结构而非词汇模式来解决现有检测器的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于词汇级别的AI文本检测器容易受到改写攻击，存在ChatGPT词汇模式偏见和训练数据偏见，在修改文本上性能下降，且通常需要大型模型或在线LLM交互。

Method: 使用预训练语言模型编码句子嵌入，通过注意力机制建模句子间关系，采用对比学习缓解自回归生成带来的嵌入偏见，并利用因果图和反事实方法从主题相关偏见中分离结构特征。

Result: 在两个精心策划的数据集（包括摘要比较和修订的生活FAQ）上的实验验证了该方法的有效性。

Conclusion: 该方法能够有效检测原始和修改后的AI生成文本，通过关注文本结构特征而非词汇模式，提供了更鲁棒的检测解决方案。

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [70] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: CCQA是一种针对小型语言模型的新型推理方法，通过循环一致性机制生成问题并评估相似度来选择最佳答案，在数学和常识推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的推理策略在大型语言模型上有效，但在小型模型上效果不佳。作者观察到传统方法在小模型上下文中的性能提升有限，因此需要专门针对小型模型的有效推理方法。

Method: 基于循环一致性原理，CCQA从每个推理路径和答案生成问题，评估其与原问题的相似度，选择相似度最高的候选解作为最终响应。使用专门的轻量级Flan-T5模型进行问题生成。

Result: 实验结果表明，CCQA在八个模型上持续优于现有最先进方法，在数学和常识推理基准测试中表现优异。

Conclusion: 该方法为小型语言模型的高效推理建立了新的实用基准，证明了循环一致性方法在小模型推理中的有效性。

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [71] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出一种基于先验的数据过滤方法，使用语料库级别的词频统计来估计标记先验，作为困惑度过滤的快速替代方案，无需模型推理即可实现高效数据筛选。


<details>
  <summary>Details</summary>
Motivation: 传统基于困惑度的数据过滤方法存在时间成本高和模型处理噪声或分布外样本时不可靠的问题，需要更高效可靠的替代方案。

Method: 基于语言学的词角色和词汇密度洞察，利用语料库级词频统计估计标记先验，通过标记先验的均值和标准差来过滤文档。

Result: 在20个下游基准测试中取得最高平均性能，同时将时间成本相比困惑度过滤降低了1000倍以上，并能适用于代码、数学等符号语言以及多语言语料库。

Conclusion: 提出的先验过滤方法简单但强大，为大规模语言模型预训练提供了高效可靠的数据选择方案。

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [72] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: TsqLoRA是一种新颖的参数高效微调方法，通过数据质量驱动选择和敏感度感知的低秩适应来提高微调效率


<details>
  <summary>Details</summary>
Motivation: 完全微调所有模型参数计算成本高且内存密集，现有参数高效微调方法忽视了不同模型层的敏感度差异和训练数据的重要性

Method: 包含两个主要组件：质量感知采样机制选择最有信息的训练数据，动态秩分配模块根据每层对参数更新的敏感度调整其秩

Result: 实验结果表明TsqLoRA在多种NLP任务上提高了微调效率，同时保持甚至提升了性能

Conclusion: TsqLoRA通过结合数据质量选择和敏感度感知的低秩适应，为资源受限环境下的模型微调提供了有效解决方案

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [73] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: UniECG是首个能够同时执行基于证据的心电图解释和文本条件心电图生成任务的统一模型，通过解耦的两阶段训练方法实现心电图与文本的双向转换。


<details>
  <summary>Details</summary>
Motivation: 现有的统一模型（如GPT-5）在视觉语言任务上取得了进展，但无法正确理解心电图信号并提供准确的医学诊断，也不能生成心电图信号。

Method: 采用解耦的两阶段训练方法：首先学习基于证据的解释技能（ECG-to-Text），然后通过潜在空间对齐注入心电图生成能力（Text-to-ECG）。

Result: UniECG能够根据用户输入自主选择解释或生成心电图，显著扩展了当前心电图模型的能力边界。

Conclusion: 该模型解决了现有统一模型在心电图理解与生成方面的局限性，为心电图分析提供了更全面的解决方案。

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [74] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 论文研究发现，LLM生成的计划在用户偏好和实际帮助性之间存在差距，用户偏好和模型偏好并不能准确预测哪些计划真正帮助用户完成任务，需要基于真实用户交互的反馈来对齐LLM的有用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法（如RLHF）基于用户偏好来训练或评估计划的有用性，但作者质疑这种偏好是否真正反映了计划对用户的帮助程度。

Method: 开发Planorama界面，让126名用户使用LLM计划回答300个多步骤问题，收集4388个计划执行和5584个比较数据，测量计划帮助性（QA成功率）和用户偏好，并在代理和奖励模型中重现设置。

Result: 1）用户/模型偏好和代理成功率不能准确预测计划的帮助性；2）这种差距不是用户特定偏好导致的；3）表面线索（如简洁性）与偏好强相关但不能预测帮助性。

Conclusion: 对齐有用的LLM需要基于真实用户交互的反馈，而不仅仅是看起来有用的偏好，作者讨论了NLP研究者可以执行的解决方案。

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [75] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: CAPE-KG是一个一致性感知的参数保留知识编辑框架，通过知识图谱在多跳问答任务中保持知识编辑的连贯性，解决现有方法中的知识污染和不稳定更新问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的参数保留知识编辑方法在多跳问答中存在一致性问题，导致知识污染、不稳定更新和检索行为与编辑意图不符，影响了多跳推理的可靠性。

Method: 提出CAPE-KG框架，确保知识图谱的构建、更新和检索过程始终与多跳问答任务要求对齐，在未编辑和已编辑知识上保持连贯推理。

Result: 在MQuAKE基准测试上的广泛实验显示，CAPE-KG在多跳问答的参数保留知识编辑性能上取得了准确度提升。

Conclusion: CAPE-KG通过解决一致性问题，有效提升了参数保留知识编辑在多跳问答中的可靠性和性能。

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [76] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: 本文提出了首个通过保形预测分析LLM作为评判者评估不确定性的框架，为LLM评分提供预测区间，并设计了针对离散评分任务的序数边界调整方法。


<details>
  <summary>Details</summary>
Motivation: LLM作为评判者的评估范式在自然语言生成评估中前景广阔，但其评估的不确定性尚未充分探索，这种可靠性不足可能限制其在实际应用中的部署。

Method: 使用保形预测方法构建连续预测区间，针对离散评分任务设计序数边界调整，提出区间中点评分作为原始模型评分和加权平均的低偏差替代方案。

Result: 大量实验表明，保形预测能够提供具有覆盖保证的有效预测区间，区间中点和评判者重新提示方法有助于改善判断质量。

Conclusion: 该框架为LLM评估提供了不确定性量化方法，增强了LLM作为评判者的可靠性，为实际应用部署提供了理论支持。

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [77] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: MemOrb是一个轻量级即插即用的语言强化记忆层，通过将多轮交互提炼为紧凑的策略反思来增强LLM智能体在客服场景中的长期可靠性


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在客服场景中跨会话遗忘、重复错误和缺乏持续自我改进机制的问题，提升在动态环境中的稳定性和一致性

Method: 提出MemOrb记忆层，将多轮交互提炼为策略反思存储在共享记忆库中，通过检索指导决策，无需微调

Result: 实验显示MemOrb显著提高任务成功率和稳定性，多轮成功率提升最高达63个百分点，在重复试验中表现更一致

Conclusion: 结构化反思是增强冻结LLM智能体在客服场景中长期可靠性的有效机制

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [78] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: LOTUSDIS是一个公开的泰语会议语料库，包含114小时自然对话，用于推进远场对话语音识别研究。该数据集通过多种麦克风在不同距离录制，包含重叠语音和真实声学效果。


<details>
  <summary>Details</summary>
Motivation: 解决远场语音识别中预训练数据与真实远场泰语语音不匹配的问题，提供距离多样性的训练数据以提高ASR系统的鲁棒性。

Method: 收集114小时自然泰语对话，使用9个独立单通道设备在0.12-10米距离录制，提供标准数据集划分和可复现的基线系统，对Whisper模型进行零样本和微调测试。

Result: 微调后泰语Whisper基线模型的整体WER从64.3降至38.3，远场WER从81.6降至49.5，在远距离麦克风上改进尤为显著。

Conclusion: 距离多样性的训练数据对鲁棒ASR至关重要，LOTUSDIS语料库为远场语音识别研究提供了有价值的资源。

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [79] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: DyGRASP是一种结合LLMs和时序GNNs的新方法，用于处理动态文本属性图（DyTAGs），有效捕捉近期和全局时间语义，在节点检索任务中性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态文本属性图，忽略了动态图中近期-全局时间语义（近期交互文本的语义依赖和节点全局语义演化），且LLMs处理动态文本存在效率问题。

Method: 提出DyGRASP方法：1）节点中心隐式推理+滑动窗口捕捉近期语义；2）定制提示词+类RNN链式结构进行显式推理捕捉全局语义；3）更新和合并层整合近期/全局语义与动态图结构信息。

Result: 在DyTAG基准测试中，DyGRASP在目标节点检索任务的Hit@10指标上最高提升34%，且在不同时序GNNs和LLMs上表现出强泛化能力。

Conclusion: DyGRASP有效解决了动态文本属性图的时间语义捕捉问题，在效率和性能上均优于现有方法，具有广泛适用性。

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [80] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: 本文研究了多语言模型中子词标记重叠对跨语言迁移的影响，通过控制实验发现标记重叠有利于跨语言语义关系的捕获，并能提升跨语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 多语言分词器自然会产生跨语言的重叠标记，但现有研究对标记重叠是促进跨语言迁移还是引入语言间干扰存在争议，且受多种混杂因素影响。

Method: 设计受控实验，在多种语言对上训练双语自回归模型，系统性地改变词汇重叠设置，并引入新的维度——跨语言标记的语义相似性分析。

Result: 任何类型的标记重叠都能创建捕获跨语言语义关系的嵌入空间，而词汇不相交的模型效果较弱。在XNLI和XQuAD任务上，有重叠的模型表现优于词汇不相交的模型，且随着重叠增加，迁移性能普遍提升。

Conclusion: 标记重叠在多语言模型中具有优势，保持大量共享词汇仍然是多语言分词器的有益设计选择。

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [81] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: 本研究系统分析了长上下文监督微调(SFT)对短上下文任务的影响，发现与长上下文预训练导致性能下降不同，长上下文SFT反而能提升短上下文性能。通过解构注意力机制和FFN组件，揭示了长上下文SFT促进上下文知识偏好，而混合训练可缓解这种偏差。


<details>
  <summary>Details</summary>
Motivation: 随着现实应用对长上下文窗口需求增加，长上下文数据的持续预训练和SFT成为常见方法。虽然预训练中数据长度的影响已被广泛研究，但SFT中数据长度的影响尚不明确，特别是对短上下文任务的影响。

Method: 系统研究SFT数据长度如何影响LLM在短上下文任务上的行为。解构分析多头注意力(MHA)和前馈网络(FFN)两个关键组件，研究它们的相互作用，并揭示知识偏好偏差。

Result: 发现长上下文SFT能提升短上下文性能，这与长上下文预训练导致性能下降的现象相反。长上下文SFT促进上下文知识偏好，短上下文SFT偏好参数知识，仅依赖长上下文SFT并非最优。混合训练可缓解这种偏差。

Conclusion: 长上下文SFT对短上下文任务有积极影响，但存在知识偏好偏差。混合训练策略能为LLM微调提供可解释的指导，平衡上下文知识和参数知识的使用。

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [82] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: 提出了一种基于10-K文件的无监督方法，用于提取企业间风险关系，替代传统的主观人工分析


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家判断和手动分析企业间风险关系的方法存在主观性强、劳动密集且难以扩展的问题，需要系统化解决方案

Method: 利用自然语言处理技术，基于10-K文件的时序和词汇模式进行无监督微调，开发领域特定的金融编码器，并引入可量化的风险关系评分

Result: 广泛实验表明该方法在多个评估设置下优于强基线模型

Conclusion: 该方法能够有效捕捉隐含和抽象的风险联系，为投资组合管理和投资策略等应用提供透明、可解释的分析工具

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [83] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: 本文建立了AECBench基准测试，用于评估大型语言模型在建筑、工程和施工领域的性能表现，发现模型在高级认知任务上存在显著性能缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在AEC领域的应用日益增多，需要评估这些模型在专业和安全关键领域的鲁棒性和可靠性。

Method: 构建包含23个代表性任务的五级认知评估框架，创建4800个问题的数据集，并采用LLM-as-a-Judge方法进行可扩展评估。

Result: 评估9个LLM显示，模型在知识记忆和理解层面表现良好，但在表格知识解释、复杂推理计算和领域文档生成方面存在显著性能下降。

Conclusion: 本研究为未来在安全关键工程实践中稳健可靠地集成LLM奠定了基础。

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [84] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: 本文使用模型差异分析（model diffing）方法，对比了Gemma-2-9b-it模型与其SimPO增强变体之间的具体能力差异，发现SimPO主要在安全机制、多语言能力和指令遵循方面有显著提升，同时在模型自引用和幻觉管理方面有所减弱。


<details>
  <summary>Details</summary>
Motivation: 随着微调成为改进大语言模型的主要方法，理解微调过程中的具体变化变得愈发重要。传统的基准测试往往无法解释为什么一个模型优于另一个模型。

Method: 采用模型差异分析（一种机制可解释性方法），使用crosscoders工具识别和分类两个模型之间的潜在表示差异。

Result: SimPO获得的潜在概念主要增强了安全机制（+32.8%）、多语言能力（+43.8%）和指令遵循（+151.7%），同时减少了模型自引用（-44.1%）和幻觉管理（-68.5%）的强调。

Conclusion: 模型差异分析能够提供超越排行榜指标的细粒度洞察，将性能差距归因于具体的机制能力，为比较LLMs提供了透明且有针对性的框架。

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [85] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: MAPEX是一个基于多智能体协作的关键词提取框架，通过动态适应文档长度的双路径策略，显著提升了LLM在关键词提取任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督提示方法通常采用单阶段推理管道和统一提示策略，无法充分利用LLM的推理和生成能力，特别是在处理不同长度和复杂度的文档时效果受限。

Method: MAPEX引入多智能体协作机制，包含专家招募、候选提取、主题引导、知识增强和后处理模块。采用双路径策略：对短文本使用知识驱动提取，对长文本使用主题引导提取。

Result: 在6个基准数据集和3种不同LLM上的实验表明，MAPEX在F1@5指标上平均优于最先进的无监督方法2.44%，优于标准LLM基线4.01%。

Conclusion: MAPEX通过多智能体协作和动态适应策略，有效提升了LLM在关键词提取任务上的泛化能力和普适性，为复杂场景下的关键词提取提供了新思路。

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [86] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: 该论文研究了开源大语言模型在生物医学问答任务中能否替代闭源模型的问题，通过BioASQ挑战赛比较了多种开源模型与GPT-4o、Claude等闭源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型的快速发展，特别是DeepSeek-V3等模型性能接近闭源模型，作者希望验证在生物医学问答这一专业领域中，小型开源模型是否能够有效替代大型闭源模型。

Method: 使用嵌入距离检索相关片段、上下文学习、结构化输出等技术增强问答能力，并对精确答案问题采用集成方法整合不同模型的输出。

Result: 研究结果显示开源大语言模型与闭源模型表现相当，在某些情况下，特别是应用集成策略时，开源模型甚至超越了闭源模型。

Conclusion: 开源大语言模型在生物医学问答领域具有与闭源模型竞争的能力，特别是在使用适当的技术增强后，证明了开源模型的实用价值。

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [87] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: 本文系统研究了多层级特征集成在AI文本检测中的效果，发现尽管理论上多特征方法应提供互补信号，但实际仅带来微小性能提升（0.4-0.5%）且计算成本显著增加（4.2倍开销）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型技术的快速发展，研究者对多特征方法能否显著超越单一神经模型在AI文本检测上的表现产生兴趣。虽然直觉认为结合语义、句法和统计特征应提供互补信号，但这一假设尚未在现代LLM生成文本上得到严格验证。

Method: 实现MHFD（多层级特征检测）方法，通过自适应融合集成基于DeBERTa的语义分析、句法解析和统计概率特征。

Result: 实验结果显示，MHFD方法在域内检测中达到89.7%的准确率，在跨域检测中保持84.2%的稳定性能，相比现有方法有0.4-2.6%的微小提升。

Conclusion: 尽管有理论预期，但多特征集成带来的收益极小而计算成本巨大，表明现代神经语言模型可能已经高效地捕捉了大部分相关检测信号。

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [88] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: DivEye是一个新颖的AI生成文本检测框架，通过捕捉文本中不可预测性的波动来区分人类和AI生成内容，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器依赖token级似然度或黑盒分类器，难以应对高质量生成文本且缺乏可解释性。人类写作在词汇和结构不可预测性方面比AI输出表现出更丰富的变异性。

Method: DivEye使用基于surprisal的特征来捕捉文本中不可预测性的波动模式，通过一组可解释的统计特征来识别人类写作特有的变异性。

Result: DivEye在零样本检测中比现有方法提升33.2%，与微调基线竞争，对改写和对抗攻击具有鲁棒性，跨领域和模型泛化能力强，作为辅助信号可将现有检测器性能提升18.7%。

Conclusion: DivEye不仅提供有效的AI文本检测，还提供可解释的检测依据，揭示了节奏不可预测性作为LLM检测的强大信号。

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [89] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: JEDI是一种仅使用编码器的架构，联合执行提取式原子事实分解和可解释推理，无需在推理时使用生成模型，在NLI任务中实现了竞争性准确性和更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖资源密集型的生成式大语言模型进行原子事实分解，需要更高效、可解释且鲁棒的解决方案。

Method: 提出JEDI编码器架构，联合执行提取式原子事实分解和可解释推理；使用合成原理语料库进行训练，覆盖多个NLI基准。

Result: JEDI在分布内达到竞争性准确性，在分布外和对抗性设置中显著提高了鲁棒性，优于仅基于提取式原理监督的模型。

Conclusion: 研究表明，使用仅编码器架构和合成原理可以在NLI中实现可解释性和鲁棒泛化。

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [90] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文提出了一种使用动态时间规整（DTW）对齐语音和文本嵌入的方法，用于端到端语音翻译（E2E-ST），解决了现有方法需要语言特定对齐工具或对齐不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译中语音和文本模态之间的表示差异（模态鸿沟）需要解决。现有方法依赖语言特定的对齐工具或使用最近邻搜索导致对齐不准确，这限制了方法的通用性和准确性。

Method: 在训练过程中采用动态时间规整（DTW）来对齐语音和文本嵌入，无需语言特定的对齐工具，能够更准确地建立语音和文本之间的对应关系。

Result: 该方法在E2E-ST任务中有效缩小了模态鸿沟，相比之前的工作产生了更准确的对齐结果，在性能相当的情况下显著更快，并在低资源设置下的6个语言方向中有5个表现更优。

Conclusion: DTW对齐方法为端到端语音翻译提供了一种更通用、准确且高效的对齐方案，特别是在低资源语言场景下表现出色。

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [91] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: 本文首次系统研究了机器翻译中的测试时缩放（TTS）方法，通过在推理时生成多个候选翻译并选择最佳结果来提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 传统通过增加模型参数规模来提升NLP系统性能的方法计算成本高昂，测试时缩放提供了一种替代方案，但在机器翻译领域尚未得到系统研究。

Method: 采用简单的best-of-N框架，在WMT24基准测试上对6个高资源语言对和1个低资源语言对进行实验，覆盖5种模型规模（3B-72B）和多种TTS计算预算（N最大1024）。

Result: 对于高资源语言，TTS能显著提升翻译质量；小模型配合大N值可以匹配或超越大模型N=1的性能；在固定计算预算下，大模型通常更高效，而TTS在低资源情况下可能因评估指标盲点而降低质量。

Conclusion: 测试时缩放是机器翻译中有效的性能提升策略，但在低资源语言对中需要谨慎使用，大模型在计算效率上更具优势。

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


### [92] [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
*Chiara Alzetta,Serena Auriemma,Alessandro Bondielli,Luca Dini,Chiara Fazzone,Alessio Miaschi,Martina Miliani,Marta Sartor*

Main category: cs.CL

TL;DR: 本文通过分析意大利CLiC-it会议10年来的论文，追踪了意大利计算语言学和自然语言处理领域的研究趋势演变，特别是从基于资源的方法向大语言模型和多模态研究的转变。


<details>
  <summary>Details</summary>
Motivation: 跟踪过去十年意大利计算语言学和自然语言处理领域的研究趋势演变，了解从基于资源的方法向大语言模型和多模态研究的转变过程。

Method: 收集CLiC-it会议2014-2024年10届会议的论文，构建CLiC-it语料库，分析论文元数据（作者来源、性别、所属机构等）和内容主题。

Result: 揭示了意大利CL/NLP研究从词汇语义资源向语言建模和多模态的显著转变，反映了该领域的技术演进趋势。

Conclusion: 为意大利和国际研究界提供了关于该领域新兴趋势和关键发展的宝贵见解，支持该领域的明智决策和未来方向。

Abstract: Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

</details>


### [93] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: PoT是一种推理阶段方法，通过动态选择认知操作来探索多种推理轨迹，生成多样化候选回答，并根据用户偏好进行聚合重加权，实现个性化问答。


<details>
  <summary>Details</summary>
Motivation: 个性化问答系统需要适应用户特定信息需求，但面临从长、嘈杂、隐式上下文中推断偏好以及生成同时正确、上下文适当且符合用户期望的回答等挑战。

Method: 提出Pathways of Thoughts (PoT)方法，将LLM推理建模为迭代决策过程，动态选择推理、修订、个性化和澄清等认知操作，探索多种推理轨迹生成候选回答，然后根据用户偏好进行聚合重加权。

Result: 在LaMP-QA基准测试中，PoT始终优于竞争基线，相对改进高达13.1%。人工评估显示66%的情况下标注者偏好PoT输出，仅15%为平局。

Conclusion: PoT无需任务特定微调即可应用于任何LLM，通过多样化推理路径的互补优势生成个性化回答，显著提升个性化问答性能。

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [94] [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
*Hiram Ring*

Main category: cs.CL

TL;DR: 本文通过分析不同语料库中的句子重复率，实证检验了语言学中关于大多数语言表达都是独特的说法。


<details>
  <summary>Details</summary>
Motivation: 检验语言学中关于大多数句子都是独特的普遍说法，利用大型语料库进行实证研究。

Method: 使用NLTK Python库解析不同体裁的语料库，统计每个语料库中的完全字符串匹配数量。

Result: 虽然完全独特的句子在语料库中通常占多数，但这高度受体裁限制，重复句子在任何单个语料库中都不是微不足道的部分。

Conclusion: 语言表达独特性的说法需要根据体裁进行限定，重复句子在语言使用中具有重要地位。

Abstract: A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.

</details>


### [95] [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
*Timur Turatali,Anton Alekseev,Gulira Jumalieva,Gulnara Kabaeva,Sergey Nikolenko*

Main category: cs.CL

TL;DR: KyrgyzNER是首个吉尔吉斯语命名实体识别数据集，包含1,499篇新闻文章、10,900个句子和39,075个实体提及，涵盖27个实体类别。研究评估了多种模型，发现多语言RoBERTa模型表现最佳，为资源有限语言的处理提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语作为资源有限的语言，缺乏高质量的命名实体识别数据集。本研究旨在填补这一空白，为吉尔吉斯语NLP研究提供基础资源。

Method: 构建了首个手动标注的吉尔吉斯语NER数据集，包含27个实体类别。评估了传统序列标注方法（如条件随机场）和基于多语言transformer的预训练模型（如RoBERTa）在该数据集上的性能。

Result: 所有模型在罕见实体类别上都表现困难，但多语言RoBERTa模型在精确率和召回率之间取得了最佳平衡。其他多语言模型也获得了可比的结果。

Conclusion: 多语言预训练模型在处理资源有限语言方面具有潜力，但需要更细粒度的标注方案来进一步提升吉尔吉斯语处理管道的评估效果。

Abstract: We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.

</details>


### [96] [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
*Kun Zhu,Lizi Liao,Yuxuan Gu,Lei Huang,Xiaocheng Feng,Bing Qin*

Main category: cs.CL

TL;DR: 提出了一种新颖的上下文感知层次化分类法生成框架，通过LLM引导的多方面编码和动态聚类来构建更连贯和细粒度的科学文献分类体系。


<details>
  <summary>Details</summary>
Motivation: 现有基于无监督聚类或直接提示LLM的分类法构建方法缺乏连贯性和细粒度，无法有效应对科学文献快速增长带来的组织和综合需求。

Method: 利用LLM识别论文的关键方面（如方法、数据集、评估等），生成特定方面的论文摘要，然后进行编码和聚类以形成连贯的层次结构。

Result: 实验结果表明该方法显著优于现有方法，在分类法连贯性、粒度和可解释性方面达到了最先进性能。

Conclusion: 该方法为科学文献组织提供了有效的解决方案，并通过引入包含11.6k篇论文的专家标注数据集为后续研究提供了基准。

Abstract: The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

</details>


### [97] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: 提出'anecdoctoring'方法，一种跨语言和文化的自动生成对抗性提示的红队评估方法，用于检测生成式AI的虚假信息风险


<details>
  <summary>Details</summary>
Motivation: 生成式AI的误用风险中，虚假信息是首要风险。当前红队评估数据集主要基于美国和英语，缺乏跨语言和文化的鲁棒性

Method: 从三个语言（英语、西班牙语、印地语）和两个地区（美国和印度）的事实核查网站收集虚假信息声明，将其聚类为更广泛的叙事，并用知识图谱表征聚类结果，用于增强攻击者LLM

Result: 该方法相比少样本提示具有更高的攻击成功率，并提供可解释性优势

Conclusion: 结果强调了需要基于真实世界对抗性误用的、可全球扩展的虚假信息缓解措施

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [98] [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
*Chantal Shaib,Tuhin Chakrabarty,Diego Garcia-Olano,Byron C. Wallace*

Main category: cs.CL

TL;DR: 本文提出了AI "slop"（低质量AI生成文本）的分类体系和评估框架，通过专家访谈和文本标注研究发现，虽然"slop"判断具有一定主观性，但与连贯性和相关性等潜在维度相关。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对AI生成低质量文本（"slop"）的统一定义和衡量方法，需要建立系统的评估框架。

Method: 通过访谈NLP、写作和哲学领域的专家，开发"slop"分类法，并进行文本片段级别的标注分析。

Result: 研究发现二元"slop"判断具有一定主观性，但这些判断与连贯性、相关性等维度相关。提出的框架可用于AI文本的检测和偏好评估。

Conclusion: 该框架为评估AI生成文本质量提供了新视角，有助于理解影响质量判断的语言和风格因素。

Abstract: AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

</details>


### [99] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习学习连续思维链（CoT）的可扩展方法，使用软令牌和输入嵌入噪声实现RL探索，在数学推理基准测试中表现优于离散令牌CoT。


<details>
  <summary>Details</summary>
Motivation: 连续令牌在推理LLMs中具有比离散令牌更强的表达能力，但实际应用受到训练困难的限制，现有方法要么仅在推理时使用连续令牌，要么需要从真实离散CoT中蒸馏且计算成本高。

Method: 使用强化学习学习连续CoT，采用软令牌（令牌混合）和输入嵌入噪声来提供RL探索，计算开销最小，可学习包含数百个令牌的连续CoT。

Result: 在Llama和Qwen模型（最大8B）的数学推理基准测试中，连续CoT训练在pass@1上匹配离散令牌CoT，在pass@32上超越，显示出更大的CoT多样性。最佳性能场景是训练时使用连续CoT令牌，推理时使用离散令牌。

Conclusion: 连续CoT RL训练能更好地保留基础模型在域外任务上的预测，为基座模型提供更温和的调整。

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [100] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: OPRL是一种用于智能体强化学习的通用信用分配策略，通过交替优化隐式过程奖励模型和策略，将轨迹偏好转化为隐式步骤奖励，解决了稀疏和不可验证奖励环境中的时间信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地使用强化学习训练作为自主智能体，但在交互环境中，稀疏且有时不可验证的奖励使得时间信用分配极具挑战性。现有方法存在标注偏差、奖励攻击、方差过大或状态重叠罕见时失败等问题。

Method: OPRL交替优化隐式过程奖励模型和智能体策略，通过基于轨迹的DPO目标将轨迹偏好转化为隐式步骤奖励。这些步骤奖励用于计算步骤级优势，并与结果奖励的回合级优势结合进行策略更新，形成自我强化循环。

Result: 在WebShop、VisualSokoban和SOTOPIA三个不同的智能体基准测试中，OPRL表现出优于前沿LLM和强RL基线的性能，实现了最先进的结果，具有更高的样本效率和更低的训练方差。

Conclusion: OPRL通过理论保证学习到的步骤奖励与轨迹偏好一致，并作为基于势能的塑造奖励提供有界梯度来稳定训练，展示了在现实世界场景中进行智能体学习的潜力。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [101] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: SafeCoDe是一个轻量级、模型无关的解码框架，通过对比解码和全局感知令牌调制策略，动态调整多模态大语言模型的安全决策能力，平衡过敏感和欠敏感问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在安全决策方面存在局限性，无法平衡过敏感（错误拒绝良性查询）和欠敏感（漏检视觉风险）的问题，需要一种能够根据多模态上下文动态调整安全决策的方法。

Method: SafeCoDe采用两阶段方法：1）对比解码机制，通过对比真实图像和高斯噪声图像来突出对视觉上下文敏感的令牌；2）全局感知令牌调制策略，将场景级推理与令牌级调整相结合，根据预测的安全判断自适应拒绝行为。

Result: 在多种MLLM架构和安全基准上的广泛实验表明，SafeCoDe能够持续改进上下文敏感的拒绝行为，同时保持模型的有用性。

Conclusion: SafeCoDe作为一种轻量级解码框架，有效解决了多模态大语言模型在安全对齐方面的持续差距，提升了模型在真实世界应用中的安全决策能力。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [102] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 本文比较了多种预训练注意力模型在电子健康记录信息提取任务上的性能，发现临床数据预训练的模型在药物和医疗事件检测方面更有效，而通用领域预训练的Bert Base在药物相关事件上下文分类方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的模型已成为临床笔记自然语言处理的主要方法，但需要比较不同预训练模型在EHR信息提取任务上的有效性，特别是针对药物事件上下文信息的提取。

Method: 使用Bert Base、BioBert、Bio+Clinical Bert变体、RoBerta和Clinical Longformer等预训练模型，在CMED数据集上进行微调，执行药物提取、医疗事件检测和多维药物事件上下文分类任务。

Result: 临床数据预训练的模型在检测药物和医疗事件方面更有效，但Bert Base在分类药物相关事件上下文方面表现最优。

Conclusion: 不同预训练模型在不同医疗NLP任务中各有所长，临床数据预训练有助于事件检测，而通用领域预训练在复杂上下文分类任务中可能更有优势。

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [103] [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
*Gabriele Berton,Jayakrishnan Unnikrishnan,Son Tran,Mubarak Shah*

Main category: cs.CL

TL;DR: CompLLM是一种针对长上下文处理的软压缩技术，通过将上下文分段独立压缩，实现线性复杂度、可扩展性和可重用性，在保持性能的同时显著提升处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理长上下文时面临二次复杂度的计算挑战，现有的软压缩方法通常将上下文作为整体压缩，导致压缩复杂度高且无法重用计算。

Method: CompLLM将上下文划分为多个段，对每个段进行独立压缩，这种设计实现了线性压缩复杂度、支持短序列训练模型泛化到长上下文，并允许压缩段的缓存重用。

Result: 在2倍压缩率下，CompLLM在高上下文长度时将首token生成时间加速最多4倍，KV缓存大小减少50%，且在长序列上性能甚至超过未压缩上下文。

Conclusion: CompLLM证明了分段独立压缩策略的有效性，为长上下文处理提供了实用且高效的解决方案，具有重要的实际应用价值。

Abstract: Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

</details>


### [104] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: RLPT是一种新的训练时扩展范式，通过强化学习在预训练数据上优化大语言模型，无需人工标注奖励信号，直接从预训练数据中获取奖励，提升模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 计算资源呈指数级增长，而高质量文本数据的增长有限，这限制了传统大语言模型的扩展方法。需要新的训练范式来解决这一挑战。

Method: 采用强化学习在预训练数据上的方法，使用下一段推理目标作为奖励信号，让模型自主探索有意义的轨迹来学习预训练数据。

Result: 在多个模型和基准测试上验证了RLPT的有效性，例如在Qwen3-4B-Base模型上，MMLU、MMLU-Pro、GPQA-Diamond等基准测试均有显著提升。

Conclusion: RLPT展示出良好的扩展行为，为继续提升性能提供了强有力潜力，同时扩展了大语言模型的推理边界并增强了RLVR性能。

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [105] [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
*Nitesh Kumar,Usashi Chatterjee,Steven Schockaert*

Main category: cs.CL

TL;DR: 本文提出了一种从大语言模型中提取概念空间的方法，通过使用原型描述来编码特征，并通过对LLM进行微调来对齐原型嵌入与概念空间维度。


<details>
  <summary>Details</summary>
Motivation: 概念空间在认知科学中被广泛使用，并有望成为可解释AI的基石，但目前缺乏从LLM中提取概念空间的实用方法。现有的嵌入提取方法无法编码底层特征。

Method: 提出一种策略：通过嵌入对应原型的描述来编码特征，然后对LLM进行微调，使原型嵌入与概念空间维度对齐。

Result: 实证分析表明该方法非常有效。

Conclusion: 该方法成功解决了从LLM中提取概念空间的难题，为可解释AI提供了实用的工具。

Abstract: Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.

</details>


### [106] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: 本文介绍了SloPalSpeech——一个包含2,806小时斯洛伐克议会语音的大规模ASR数据集，通过微调Whisper模型显著降低了斯洛伐克语的词错误率。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语等低资源语言的自动语音识别因训练数据稀缺而受到限制，需要构建大规模高质量数据集来提升ASR性能。

Method: 开发了稳健的处理流程，将长格式议会录音对齐和分割成30秒的音频-文本对，并使用该数据集微调多个OpenAI Whisper模型（small、medium、large-v3和large-v3-turbo）。

Result: 在Common Voice和FLEURS等标准斯洛伐克基准测试上实现了显著的词错误率降低，微调后的Whisper-small模型WER最多降低了70%，接近更大的Whisper-large-v3模型的基线性能。

Conclusion: 公开发布完整的SloPalSpeech数据集、分割后的转录文本（6000万字）以及所有微调模型，以促进低资源语音识别的未来研究。

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [107] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: 该论文发布了一个沃洛夫语意图分类数据集WolBanking77，填补了低资源语言意图分类研究的空白，包含9,791个银行领域文本句子和4小时以上的语音数据。


<details>
  <summary>Details</summary>
Motivation: 现有意图分类研究主要关注高资源语言，而像沃洛夫语这样的低资源语言（西非地区超过1000万人使用）缺乏相关数据集，特别是在文盲率较高的地区，口语比书面语更常用。

Method: 构建了沃洛夫语银行领域数据集WolBanking77，包含文本和语音数据，并在多种基线模型上进行了实验，包括文本和语音的state-of-the-art模型。

Result: 在该数据集上的实验结果非常有前景，报告了NLP模型的F1分数和ASR模型的词错误率等基线指标，并进行了模型间比较。

Conclusion: 该工作填补了低资源语言意图分类研究的空白，计划共享数据集并进行维护更新，同时发布开源代码。

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


### [108] [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha*

Main category: cs.CL

TL;DR: DRISHTIKON是一个专门针对印度文化的多模态多语言基准测试，用于评估生成式AI系统的文化理解能力，覆盖15种语言和64,000多个文本-图像对。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多为通用性或全球性范围，缺乏对特定文化（特别是印度文化）的深度覆盖，需要专门的文化理解评估工具。

Method: 构建包含印度各地区、语言和文化主题的多模态数据集，评估各类视觉语言模型在零样本和思维链设置下的表现。

Result: 当前模型在处理文化基础的多模态输入方面存在明显局限，特别是在低资源语言和较少记录的传统文化方面表现不佳。

Conclusion: DRISHTIKON填补了包容性AI研究的重要空白，为推进文化感知的多模态语言技术提供了强大的测试平台。

Abstract: We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [109] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出PolypSeg-GradCAM框架，结合U-Net和Grad-CAM实现可解释的息肉分割，在Kvasir-SEG数据集上达到0.9257 IoU，提升AI辅助结肠镜检查的可信度


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要癌症死因，胃肠道息肉是关键前兆。现有深度学习息肉分割方法缺乏可解释性，阻碍临床采用

Method: 集成U-Net架构和梯度加权类激活映射(Grad-CAM)，在1000张标注内镜图像的Kvasir-SEG数据集上训练评估

Result: 测试集平均IoU达0.9257，训练和验证集Dice系数均高于0.96，Grad-CAM可视化证实预测基于临床相关区域

Conclusion: PolypSeg-GradCAM将高分割精度与可解释性结合，是迈向可靠AI辅助结肠镜检查和改善结直肠癌早期预防的重要一步

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [110] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一个基于深度学习的远程眼科应用，用于通过视网膜图像自动检测糖尿病视网膜病变，准确率达到85.4%，特别适用于资源匮乏地区的筛查。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是成年人视力丧失的主要原因，特别是在医疗资源不足地区，需要高效、低成本的筛查解决方案。

Method: 使用多个卷积神经网络（ResNet-18、EfficientNet-B0和SqueezeNet）进行开发和评估，以平衡准确性和计算效率，最终模型用于实时筛查。

Result: 系统分类疾病严重程度的准确率为85.4%，具备云可扩展性、安全数据管理和多用户框架，支持临床和远程医疗环境。

Conclusion: AI驱动的远程医疗解决方案有望扩大糖尿病视网膜病变筛查的可及性，特别是在偏远和资源受限环境中。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [111] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了Self Identity Mapping (SIM)正则化框架，通过逆映射机制重建输入来减少前向传播中的信息损失，并开发了ρSIM变体降低计算复杂度。该方法是模型无关、任务无关的即插即用模块。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术依赖启发式方法，在不同设置下效果不稳定。需要一种更可靠有效的正则化方法来增强表示学习。

Method: SIM框架利用逆映射机制从变换后的输出重建输入，减少信息损失并促进梯度流动。ρSIM通过补丁级特征采样和基于投影的方法重建潜在特征来降低复杂度。

Result: 在图像分类、少样本提示学习和领域泛化等任务上实验显示，ρSIM相比基线方法有持续改进，能增强各种任务的表示学习效果，且与现有正则化方法正交，能提升其效果。

Conclusion: SIM是一种简单有效的正则化框架，能有效保留语义信息，在密集到密集任务和非视觉领域都有良好表现，具有广泛适用性。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [112] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反演攻击方法，能够在单轮平均梯度SAG机制下实现高保真度的多图像重建，特别是在大批量场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究在具有挑战性的单轮平均梯度SAG机制下的梯度反演问题，其中每个样本的线索都纠缠在单个批次平均梯度中，需要开发新的方法来感知潜在的每图像信号。

Method: MAGIA框架包含两个核心创新：1）封闭形式的组合重缩放，创建可证明更紧的优化边界；2）基于动量的整批和子集损失混合，确保重建鲁棒性。该方法通过探测随机数据子集来感知潜在图像信号，无需标签推断。

Result: 大量实验表明，MAGIA在大型批次场景下显著优于先进方法，实现了高保真度的多图像重建，且计算开销与标准求解器相当，无需任何辅助信息。

Conclusion: MAGIA在梯度反演攻击中取得了突破性进展，特别是在大批量设置下表现出色，为隐私保护机器学习提供了重要的安全评估工具。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [113] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer是一个专门针对阿拉伯语文档OCR的视觉语言模型，通过在大规模合成和真实文档数据集上进行微调，显著提升了阿拉伯语OCR性能，在WER指标上达到0.25，超越了现有开源和商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文档OCR面临草书字体、多样化字体、变音符号和从右到左书写等挑战，现有多模态大语言模型在阿拉伯语上的表现有限，需要专门针对该语言的解决方案。

Method: 采用解码器专用微调策略，在预训练MLLM基础上进行适配，同时保留通用视觉特征。利用大规模合成和真实文档数据集进行训练，并创建了高质量的专家验证基准Misraj-DocOCR。

Result: Baseer显著优于现有开源和商业解决方案，WER达到0.25，在阿拉伯语文档OCR领域建立了新的最先进水平。

Conclusion: 研究证明了针对特定领域适配通用MLLM的益处，为形态丰富的语言（如阿拉伯语）的高精度OCR建立了强有力的基准。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [114] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度学习框架，将稀疏的InSAR时间序列数据转换为密集的时空张量，首次实现了将先进计算机视觉架构直接应用于地面变形预测问题。


<details>
  <summary>Details</summary>
Motivation: 监测地面位移对城市基础设施稳定性和减轻地质灾害至关重要，但从稀疏的InSAR时间序列数据预测未来变形仍是一个重大挑战。

Method: 设计并实现了一种混合CNN-LSTM模型，专门用于从生成的时空张量中同时学习空间模式和时间依赖性。

Result: 与LightGBM和LASSO回归等基线模型相比，所提出的架构提供了显著更准确和空间一致的预测，为该任务建立了新的性能基准。

Conclusion: 研究结果证实了时空深度学习在高分辨率变形预测中的有效性和潜力，可解释性分析表明基线模型往往默认简单的持续性模式，凸显了集成时空方法的必要性。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [115] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架是一种生成大规模数据集的新方法，用于探究AI模型学习到的概念，重点关注物体识别、位置关系和属性识别等基础概念。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在基础概念理解上存在不足，需要系统性的评估方法来验证模型对基本元素的理解能力，为处理更复杂任务奠定基础。

Method: 通过生成包含大量关于单个概念的问题和广泛语言变体的数据集，系统性地测试模型对基础概念的理解能力。

Result: 实验显示当代模型在物体识别方面表现良好，但在位置信息理解和带约束条件的问题上存在困难，特别是MobileVLM-V2模型出现显著答案不一致和错误答案。

Conclusion: Scrapbook框架为生成多样化数据集提供了有价值的工具，可用于系统评估和提升AI模型性能，特别是在位置理解和一致性方面需要改进。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [116] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 本研究评估多模态大语言模型在零样本条件下检测和描述交通事故的能力，使用基础设施摄像头图像，通过集成视觉分析技术提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测对公共安全至关重要，但缺乏多样化的标注数据集限制了传统方法的可扩展性。研究旨在探索MLLMs在零样本条件下的潜力，减少对大量标注数据的依赖。

Method: 使用CARLA模拟的DeepAccident数据集评估Gemini 1.5/2.0、Gemma 3和Pixtral模型；集成YOLO、Deep SORT和SAM等视觉分析技术到提示中；比较模型在事故识别和描述方面的性能。

Result: Pixtral表现最佳（F1-score 0.71，召回率83%）；Gemini模型通过增强提示提升精度（如Gemini 1.5达90%），但F1和召回率下降；Gemma 3性能最平衡。

Conclusion: MLLMs与先进视觉分析技术结合在自动化交通监控系统中具有巨大应用潜力，能够有效提升事故检测的准确性和可解释性。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [117] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文通过实证分析量化了视觉-语言-视觉管道中描述-生成瓶颈造成的信息损失，发现99.3%的样本存在显著感知退化，91.5%存在结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的集成增加，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性变得重要，但目前通过文本中介传递视觉内容时的退化程度尚未得到充分量化。

Method: 生成了150对图像通过描述-生成管道，并应用现有指标（LPIPS、SSIM和颜色距离）来测量感知、结构和色彩维度上的信息保存情况。

Result: 评估显示99.3%的样本表现出显著的感知退化，91.5%的样本表现出显著的结构信息损失。

Conclusion: 描述-生成瓶颈代表了当代多模态系统中可测量且一致的限制，为系统改进提供了实证依据。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [118] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 本文提出了一种AI驱动的工作流程，用于从高分辨率卫星图像自动推断屋顶属性，以解决小岛屿发展中国家建筑结构信息缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 许多气候脆弱地区的小岛屿发展中国家缺乏详细的建筑结构信息，这些信息对于评估灾害风险和城市韧性规划至关重要。

Method: 比较了地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类中的效果，并评估了整合邻近岛屿训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度和屋顶材料分类上分别达到了0.88和0.83的F1分数。

Conclusion: 结合本地能力建设，该工作旨在为小岛屿发展中国家提供利用AI和地球观测数据进行高效、基于证据的城市治理的新能力。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [119] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 本文提出VLA-LPAF轻量模块，通过2D数据增强视觉-语言-动作模型的多视角适应性，显著提升任务成功率


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理不同视角的视觉观察时存在视角异质性问题，限制了模型的泛化能力

Method: 开发VLA-LPAF模块，使用单视角图像进行微调，在潜在空间中融合多视角观察，有效弥合视角不一致带来的差距

Result: 在CALVIN上平均提升8%任务成功率，LIBERO上提升15%，定制仿真基准上提升30%，并在真实任务中验证了视角自适应特性

Conclusion: VLA-LPAF能够有效提高VLA模型的多视角适应性，显著提升跨环境任务性能

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [120] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件相机的立体深度估计方法URNet，通过局部-全局精炼模块和KL散度不确定性建模，在DSEC数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟等优势，但基于事件相机的立体深度估计仍面临挑战，需要更有效的精炼方法和不确定性建模来提高预测可靠性。

Method: 提出了URNet不确定性感知精炼网络，包含局部-全局精炼模块来捕获细粒度局部细节和长距离全局上下文，并引入基于KL散度的不确定性建模方法。

Result: 在DSEC数据集上的大量实验表明，URNet在定性和定量评估中都一致优于最先进的方法。

Conclusion: URNet通过有效的精炼机制和不确定性建模，显著提升了基于事件相机的立体深度估计性能，证明了该方法在事件视觉任务中的有效性。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [121] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一种新型混合AI框架，用于从多梯度DWI和形态学MRI数据中识别周围神经系统，通过模糊空间关系编码解剖知识，无需手动选择ROI，显著提高了子宫内膜异位症患者腰骶丛神经识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但周围神经成像仍然具有挑战性。传统束路成像方法存在局限性，需要开发更精确的神经识别技术。

Method: 提出Visionerves混合AI框架，包含两个阶段：(A)使用深度学习模型自动分割解剖结构；(B)通过符号空间推理进行束路成像和神经识别。该方法利用模糊空间关系编码解剖知识，无需手动选择感兴趣区域。

Result: 在10名子宫内膜异位症患者的腰骶丛神经识别中，Visionerves相比标准束路成像方法有显著改进，Dice评分提高达25%，空间误差减少至小于5毫米。

Conclusion: 这种自动且可重复的方法能够进行详细的神经分析，为非侵入性诊断子宫内膜异位症相关神经病变以及其他涉及神经的疾病铺平了道路。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [122] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive是首个在巴基斯坦驾驶环境中收集的隐私保护多模态驾驶员行为数据集，结合智能手机惯性/GPS传感器数据和同步的路面视频，用于检测不安全驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要来自发达国家，缺乏新兴经济体驾驶行为的多样性表示，且驾驶员面部记录侵犯隐私。巴基斯坦等国家存在道路条件复杂、交通流混合等挑战，需要可靠的不安全驾驶行为检测来改善道路安全。

Method: 使用定制Android应用收集高频加速度计、陀螺仪和GPS数据，结合同步的路面视频，记录三种目标驾驶行为（正常、攻击性、危险）在多种道路类型上。数据分为原始层、处理层和语义层结构。

Result: 创建了V-SenseDrive数据集，填补了全球驾驶员行为数据集的空白，为驾驶员行为分类、交通安全分析和ADAS开发提供了基础。

Conclusion: V-SenseDrive代表了巴基斯坦真实世界驾驶情况，为情境感知智能交通解决方案奠定了基础，解决了现有数据集在多样性和隐私保护方面的局限性。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [123] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL是一个参数量从3B到70B的多模态大语言模型系列，通过创新的领域增强技术实现了最先进的性能。该模型在通用基准测试中表现优异，在OCR和文档理解等特定领域具有显著优势，并展示了在数学推理和逻辑推理任务上的强大能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在保持强大通用性能的同时，通过领域增强技术提升特定领域能力的多模态大语言模型，以满足企业部署的多样化需求。

Method: 采用多阶段渐进式训练和高精度数据合成流水线，结合长链思维能力，在百度昆仑P800芯片上进行大规模训练，实现了超过90%的扩展效率。

Result: 在CCBench、SEEDBench IMG、ScienceQA和MMStar等基准测试中达到最先进水平，OCRBench得分873，DocVQA准确率94.75%，MathVista得分78.6%。

Conclusion: 该工作建立了一个有效的领域增强多模态模型开发方法学，证明了大规模AI基础设施能够高效训练SOTA级别的多模态模型，适用于多样化的企业部署场景。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [124] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一个基于ODE的去雾框架，将大气散射模型重新表述为常微分方程，通过单步推理实现真实世界图像去雾，并利用马尔可夫链布朗运动生成非均匀雾霾数据增强训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏配对的真实世界训练数据，导致域间差距问题，而传统基于大气散射模型的物理方法难以处理真实世界的复杂雾霾模式。

Method: 提出HazeFlow框架，将大气散射模型重新表述为ODE，学习从雾霾图像到清晰图像的最优ODE轨迹；引入基于马尔可夫链布朗运动的非均匀雾霾生成方法来解决训练数据稀缺问题。

Result: 在多个真实世界去雾基准数据集上实现了最先进的性能表现。

Conclusion: HazeFlow通过ODE框架和物理启发的数据增强方法，有效解决了真实世界去雾的泛化问题，在多个基准测试中表现出色。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [125] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种压缩版的EcoWeedNet模型，通过结构化通道剪枝、量化感知训练和TensorRT加速，在Jetson Orin Nano边缘设备上实现高效杂草检测。


<details>
  <summary>Details</summary>
Motivation: 农业领域部署深度学习模型面临边缘设备资源有限的挑战，需要开发轻量化但性能优异的模型。

Method: 采用结构化通道剪枝处理复杂架构（残差连接、注意力机制、拼接和CSP块），结合量化感知训练和TensorRT加速技术。

Result: 模型大小减少68.5%，计算量减少3.2 GFLOPs，推理速度达到184 FPS（FP16），比基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝率的EcoWeedNet优于YOLO11n和YOLO12n，达到83.7%精确率、77.5%召回率和85.9% mAP50。

Conclusion: 该方法证明了对复杂架构进行有效压缩的可行性，为精准农业提供了高效实用的解决方案。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [126] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态学习框架，结合增强型模态dropout和对比学习，解决医学诊断中模态不平衡和缺失问题，在疾病检测和预测任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医学诊断越来越多地使用多模态数据，但现有模型难以有效融合异构信息且对模态缺失缺乏鲁棒性。需要解决模态不平衡和缺失等现实限制。

Method: 引入可学习的模态token来改进缺失感知的模态融合，将传统的单模态对比目标与融合的多模态表示相结合，使用增强型模态dropout和对比学习。

Result: 在大规模临床数据集上验证，在疾病检测和预测任务中达到最先进性能，特别是在只有单一模态可用的实际场景中表现优异。

Conclusion: 该方法具有高效性、可扩展性和通用性，为多模态学习提供了可扩展、低成本的解决方案，在现实临床应用中具有重要潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [127] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究系统评估了9种分割架构在肺栓塞(PE)分割任务上的性能，发现3D U-Net with ResNet encoder是最有效的架构，CNN模型优于ViT模型，分类预训练反而会降低分割性能。


<details>
  <summary>Details</summary>
Motivation: 肺栓塞分割对于临床诊断至关重要，但目前缺乏对不同分割架构在PE任务上的系统性评估。本研究旨在通过统一测试框架，比较CNN和ViT架构在PE分割中的表现。

Method: 使用490个CTPA扫描的标注数据集，评估9种分割架构（包括CNN和ViT），采用预训练和随机初始化权重，在统一测试框架下进行性能审计。

Result: 最佳模型达到平均Dice分数0.7131，在60个测试扫描中检测到181个栓子，有49个假阳性和28个假阴性。3D模型在PE分割中表现优异，CNN模型普遍优于ViT模型。

Conclusion: 3D U-Net with ResNet encoder是PE分割的有效架构，中央和大栓子分割准确，但远端栓子仍具挑战性。分类预训练对分割任务可能产生负面影响，表明分类和分割依赖不同的判别特征。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [128] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出一种新颖的图神经网络，将时间动态与静态手形配置分离，用于手语手形识别，在37个手形类别上达到46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中具有基础音系学作用，但计算方法很少显式建模手形，限制了识别准确性和语言分析。

Method: 结合解剖学启发的图结构和对比学习，解决手形识别中的关键挑战，包括细微的类间差异和时间变化。

Result: 在手语序列中建立了首个结构化手形识别基准，在37个手形类别上达到46%的准确率（基线方法为25%）。

Conclusion: 该方法显著提升了手形识别性能，为手语计算分析提供了新的技术途径。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [129] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文研究了分类任务选择对胎儿超声图像中分布外检测性能的影响，发现不同任务在应对图像特征偏移和解剖特征偏移时的表现差异显著，且OOD检测性能与弃权预测性能并不完全一致。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注不确定性量化方法，但忽略了分类任务本身对OOD检测的影响。在胎儿超声图像存在异质性和多样临床设置的背景下，需要研究任务选择如何影响OOD检测的可靠性。

Method: 在四个分类任务上测试了八种不确定性量化方法，比较了不同任务在应对图像特征偏移和解剖特征偏移时的OOD检测性能。

Result: OOD检测性能随任务不同而显著变化，最佳任务取决于ID-OOD标准（图像特征偏移vs解剖特征偏移）。更好的OOD检测性能不一定能保证更好的弃权预测性能。

Conclusion: 在医学图像分析中，需要根据具体下游应用来协调任务选择和不确定性策略，不能仅依赖OOD检测性能作为唯一指标。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [130] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了OrthoLoC数据集，这是首个大规模无人机图像与正射地理数据配对的数据集，用于解决资源受限环境下的高精度视觉定位问题，并提出了AdHoP优化技术提升特征匹配性能。


<details>
  <summary>Details</summary>
Motivation: 在无互联网连接或GPS支持的资源受限场景下，现有视觉定位系统依赖大型图像数据库或重型3D模型不实用。正射地理数据轻量且易得，但此前缺乏系统研究。

Method: 创建包含16,425张无人机图像的配对数据集，涵盖德国和美国多个模态；提出AdHoP优化技术，可与任何特征匹配器集成提升匹配效果。

Result: 数据集解决了无人机图像与地理数据的领域偏移问题；AdHoP技术将匹配性能提升达95%，平移误差降低达63%。

Conclusion: OrthoLoC为基于正射地理数据的视觉定位提供了首个公平基准，AdHoP技术显著提升了定位精度，推动了资源受限环境下的高精度定位研究。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [131] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: SSDnet是一种零样本单图像异常定位方法，无需训练数据，利用卷积神经网络的归纳偏置进行自重构，通过patch-based训练框架检测异常作为局部模式偏差。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中缺乏训练数据和参考样本的异常检测问题，专注于单图像零样本设置。

Method: 设计基于patch的训练框架，直接输入图像进行自重构（而非DIP的噪声到图像映射），采用掩码、patch打乱和高斯噪声防止恒等映射，使用基于内积相似度的感知损失。

Result: 在MVTec-AD数据集上达到0.99 AUROC和0.60 AUPRC，在fabric数据集上达到0.98 AUROC和0.67 AUPRC，优于现有方法。

Conclusion: SSDnet无需外部训练数据、标签或参考，在噪声或像素缺失情况下保持鲁棒性，为零样本异常定位提供了有效解决方案。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [132] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 该论文提出了一种针对低资源孟加拉语的视觉-语言模型，通过三重损失函数（PAL+InfoNCE+OT）来改善图像与文本的对齐效果，在Flickr30k和MSCOCO数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如孟加拉语）中视觉-语言模型存在的挑战，包括配对数据稀缺、翻译对齐问题以及英语中心预训练忽略目标语言语义的问题。

Method: 使用LaBSE验证的英-孟配对数据和11万双语提示合成图像训练计算感知的孟加拉语字幕生成管道。采用冻结的MaxViT提取视觉特征，孟加拉语原生mBART-50解码，轻量级桥接模块连接多模态。核心创新是三重损失函数：PAL损失对齐真实和合成补丁描述符，InfoNCE强制全局真实-合成分离，基于Sinkhorn的最优传输确保细粒度补丁对应。

Result: 在Flickr30k-1k上达到BLEU-4 12.29、METEOR 27.98、BERTScore-F1 71.20；在MSCOCO-1k上达到BLEU-4 12.00、METEOR 28.14、BERTScore-F1 75.40，优于强CE基线，并将真实-合成质心差距缩小41%。

Conclusion: 提出的PAL+InfoNCE+OT三重损失协同机制有效改善了低资源语言的视觉-语言对齐，减少了虚假匹配，在孟加拉语图像字幕任务上取得了显著性能提升。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [133] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个统一的纯摄像头鸟瞰图框架，通过知识蒸馏将大型规划导向教师模型的能力压缩到紧凑的实时学生模型中，支持完整的自动驾驶堆栈功能。


<details>
  <summary>Details</summary>
Motivation: 解决现有高效摄像头基线模型功能不完整的问题，将大规模多模态感知规划模型的智能保留在资源受限的部署环境中。

Method: 采用模型无关的多阶段蒸馏策略，结合特征级、输出级和自适应区域感知监督，将高容量多模态知识转移到轻量级BEV表示中。

Result: 在nuScenes数据集上，TinyBEV实现39.0 mAP检测精度、1.08 minADE运动预测精度和0.32碰撞率，运行速度提升5倍（11 FPS），参数减少78%。

Conclusion: 研究表明完整的驾驶智能可以在资源受限环境中保留，弥合了大规模多模态模型与部署就绪实时自动驾驶系统之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [134] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动模糊球体标注策略，将球体标注在模糊条纹的中心而非前缘，并显式标注模糊属性，从而提升检测性能并实现运动估计。


<details>
  <summary>Details</summary>
Motivation: 现有标注方法将球体标记在模糊条纹的前缘，导致不对称性并忽略了与速度相关的运动线索，这限制了检测系统的性能。

Method: 引入新的标注策略，将球体置于模糊条纹中心并标注模糊属性；提出BlurBall模型，通过多帧输入的Squeeze-and-Excitation注意力机制联合估计球体位置和运动模糊属性。

Result: 新标注方法在各种模型上一致提升了检测性能；BlurBall模型在球体检测上达到了最先进的结果，并实现了更可靠的轨迹预测。

Conclusion: 利用模糊信息不仅提高了检测精度，还增强了轨迹预测的可靠性，有利于实时体育分析应用。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [135] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频目标检测方法MVP，通过关键帧检测和压缩域运动向量传播来减少计算开销，保持开放词汇检测能力


<details>
  <summary>Details</summary>
Motivation: 解决在大规模开放词汇检测器中逐帧运行的高计算成本问题，寻求在保持准确性的同时降低检测频率

Method: 使用固定间隔的关键帧进行OWLv2检测，通过压缩域运动向量在中间帧传播检测结果，采用3x3网格聚合运动向量进行平移和尺度更新

Result: 在ILSVRC2015-VID数据集上达到mAP@0.5=0.609，在宽松IoU阈值下接近逐帧检测性能，优于基于跟踪器的传播方法

Conclusion: 压缩域传播是减少检测器调用次数的实用方法，能在视频中保持强大的零样本覆盖能力

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [136] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文研究了HDR光照估计方法的颜色鲁棒性，发现使用预训练的白平衡网络预处理输入图像可以显著提高现有模型的颜色准确性，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 高动态范围（HDR）光照估计技术对增强现实应用至关重要，但现有方法往往忽视颜色鲁棒性这一影响视觉真实感的关键因素。

Method: 通过使用包含多样化光照颜色的新型HDR数据集，系统评估了几种适应策略，重点测试了预训练白平衡网络预处理输入图像的方法。

Result: 实验结果表明，白平衡预处理策略在所有测试场景中都优于其他方法，显著提高了颜色准确性。该发现对三种最先进的光照估计方法都具有普适性。

Conclusion: 简单的预处理技术（如白平衡）可以有效增强现有光照估计模型的颜色鲁棒性，为AR应用提供更真实的光照效果，且无需复杂的模型重新训练。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [137] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型和多模态大语言模型的免训练支票字段检测框架，实现零样本检测支票关键字段，解决传统方法依赖大量标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 支票作为金融系统基础工具面临欺诈风险，传统检测方法依赖大量标注数据，但由于隐私和专有性问题，这类数据稀缺。需要一种无需训练即可检测支票关键字段的方法。

Method: 使用视觉语言模型(VLM)和多模态大语言模型(MLLM)构建免训练框架，通过零样本学习自动识别和定位支票中的签名、MICR码、金额、收款人等关键字段。

Result: 在包含110张不同格式支票的手工数据集上评估，模型表现出强大的性能和泛化能力，能够准确检测各种支票字段。

Conclusion: 该框架不仅降低了实际金融场景中的部署门槛，还可作为生成高质量标注数据的引导机制，为开发专门化的实时目标检测模型奠定基础。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [138] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 论文评估了当前先进视觉语言模型在图表理解中的表现，发现在图表损坏或遮挡情况下性能急剧下降，并出现幻觉问题。作者提出了CHART NOISe数据集来测试模型在噪声和遮挡条件下的表现，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准假设图表干净且查询基于事实，但现实世界图表常包含失真且需要超越简单匹配的推理能力。需要评估模型在退化条件下的鲁棒性。

Method: 评估ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro在图表损坏和遮挡条件下的表现，引入CHART NOISe数据集（包含图表损坏、遮挡和反向不一致性测试），并提出质量过滤和遮挡检测等基线缓解策略。

Result: 模型在损坏或遮挡条件下性能急剧下降，幻觉问题（如数值捏造、趋势误解、实体混淆）更频繁出现，模型在退化设置下仍过度自信。

Conclusion: 研究揭示了图表推理中的系统性漏洞，建立了评估图表理解鲁棒性和可靠性的严格测试平台，为提升模型在现实世界图表分析中的性能提供了基础。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [139] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于神经表示的4D-MRI重建方法，使用两个协同网络（SAN和TMN）来替代传统的离散排序方法，能够高效准确地重建呼吸运动过程中的3D图像。


<details>
  <summary>Details</summary>
Motivation: 传统的4D-MRI重建方法（如相位分箱或独立模板扫描）难以捕捉时间变异性，工作流程复杂且计算负担重。

Method: 将呼吸运动建模为由1D替代信号引导的平滑连续变形，使用空间解剖网络（SAN）编码连续3D解剖表示，时序运动网络（TMN）基于Transformer生成的呼吸信号产生时间一致的变形场。

Result: 在19名志愿者的自由呼吸数据集上评估，该方法能准确捕捉规则和不规则呼吸模式，保持血管和支气管连续性，处理时间从传统方法的5小时减少到15分钟训练时间，每个3D体积推断时间不到1秒。

Conclusion: 该方法显著提高了效率，能够准确重建任意呼吸状态下的3D图像，在4D放射治疗规划和实时自适应治疗中具有强大应用潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [140] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 本文评估了五种基于卡尔曼滤波的跟踪方法在快速移动微小物体（如壁球）跟踪中的性能，发现DeepOCSORT在跟踪误差方面表现最佳，而ByteTrack在推理速度方面最快，但所有方法都存在显著的跟踪漂移问题。


<details>
  <summary>Details</summary>
Motivation: 快速移动微小物体的不可预测运动模式和小视觉标记使得精确跟踪成为计算机视觉中的挑战性问题，特别是在体育机器人应用中需要轻量级且准确的跟踪系统来改善机器人感知和规划能力。

Method: 使用包含10,000个标注壁球帧的自定义数据集，评估了五种最先进的卡尔曼滤波跟踪方法（OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT），重点分析推理速度和每图像更新频率对跟踪精度的影响。

Result: DeepOCSORT实现了最低的跟踪误差（平均ADE为31.15像素），而ByteTrack展示了最快的处理速度（平均推理时间为26.6ms）。但所有跟踪器都表现出显著的跟踪漂移，空间误差范围为3-11cm（ADE值：31-114像素）。

Conclusion: 当前跟踪方法在处理快速移动微小物体的不可预测运动模式方面存在根本性限制，误差率比标准物体跟踪基准高3-4倍，需要专门的方法论来改进快速移动微小物体跟踪应用。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [141] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一个用于压缩域视频动作识别的高效运动感知自适应裁剪模块，利用H.264视频中的运动向量定位运动密集区域，无需训练即可提升识别精度或降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统视频动作识别方法计算成本高，而压缩域处理可以显著降低计算开销。MoCrop旨在利用视频压缩中已有的运动向量信息，实现高效的动作识别。

Method: MoCrop包含三个轻量级组件：去噪与合并(DM)、蒙特卡洛采样(MCS)和自适应裁剪(AC)。通过运动密度子矩阵搜索定位运动密集区域，生成单个剪辑级裁剪应用于所有I帧。

Result: 在UCF101数据集上，MoCrop在保持相同FLOPs时提升ResNet-50准确率3.5%，或在降低26.5%FLOPs时提升准确率2.4%。应用于CoViAR模型，在原始计算成本下达到89.2%准确率，或在降低计算量时达到88.5%准确率。

Conclusion: MoCrop具有强泛化能力，在多种骨干网络上均表现一致，无需训练、不增加参数，适合压缩域实时部署，为高效视频动作识别提供了实用解决方案。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [142] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于码本的自适应特征压缩框架CAFC-SE，通过向量量化将连续视觉特征映射为离散索引，在低比特率条件下保持更好的分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低比特率条件下性能不佳，因为它们保留了过多冗余细节或学习了过度集中的符号分布，需要一种更有效的特征压缩方法。

Method: 使用向量量化(VQ)将连续视觉特征映射到码本中的离散索引，选择性传输到云端，保留更多信息丰富的视觉模式。

Result: 大量实验证明该方法在码率和准确率方面具有优越性，特别是在低比特率条件下表现更稳定。

Conclusion: CAFC-SE框架通过码本和向量量化技术，有效解决了低比特率条件下的特征压缩问题，提升了边缘-云系统的分析性能。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [143] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级的多核U型CNN架构，专为医学图像分割设计，在极低计算成本下实现了优于现有方法的性能


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中现有方法计算复杂度高、参数量大，难以在资源受限环境中部署的问题

Method: 设计了多核深度卷积块（MKDC）处理多分辨率空间关系，结合通道、空间和分组门控注意力机制突出图像显著特征

Result: 仅需0.316M参数和0.314G FLOPs，在六个医学图像基准测试中DICE分数优于TransUNet、UNeXt等SOTA方法，参数减少333倍，FLOPs减少123倍

Conclusion: MK-UNet在极低计算成本下实现了卓越的分割性能，为资源受限环境中的实时高保真医学诊断提供了无与伦比的解决方案

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [144] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过将术中3D重建与术前CT数据耦合，在手术视频和体积患者数据之间建立桥梁。该方法将3D高斯点绑定到CT网格上，通过光度监督联合优化高斯参数和网格变形。


<details>
  <summary>Details</summary>
Motivation: 解决手术导航中手术视频与术前CT数据之间的差距问题，实现更准确的手术引导和变形跟踪。

Method: 将3D高斯点参数化相对于其父网格三角形，通过光度监督联合优化高斯参数和网格变形，确保高斯点与网格对齐，并将变形传播回CT更新。

Result: 在猪内脏手术和人类肝脏合成数据上验证了方法的有效性，能够在单目RGB数据上实现术前CT的合理变形。

Conclusion: BridgeSplat为可变形手术导航提供了一种有效的解决方案，能够准确地将术中观察与术前规划数据对齐。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [145] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 提出了一种名为DGLE的伪标签优化框架，用于源数据不可访问的语义分割领域自适应，通过扩散模型从少量高质量伪标签传播生成完整的高质量伪标签集


<details>
  <summary>Details</summary>
Motivation: 现有源自由领域自适应方法在优化包含大量噪声的完整伪标签集时面临挑战，限制了自训练方法的性能

Method: 首先基于置信度过滤和超分辨率增强获得少量高质量伪标签作为初始种子，然后利用扩散模型的去噪能力和复杂分布建模能力传播生成完整的高质量伪标签

Result: 该方法有效避免了直接优化完整伪标签集的困难，显著提高了伪标签质量

Conclusion: DGLE框架通过扩散引导的标签丰富化策略，显著提升了目标域模型的性能

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [146] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 本文提出在双曲空间中嵌入特征提取器，用于粗到细的少样本类增量学习任务，通过双曲对比损失和最大熵分布增强模型性能。


<details>
  <summary>Details</summary>
Motivation: 双曲空间相比欧几里得空间在层次数据表示方面具有优势，特别是在处理粗到细的类增量学习任务时，能够更好地捕捉层次结构信息。

Method: 使用庞加莱球模型将特征提取器嵌入双曲空间，引入双曲对比损失和双曲全连接层进行优化和分类，并利用双曲空间中的最大熵分布生成增强特征以缓解少样本过拟合。

Result: 在C2FSCIL基准测试中，该方法有效提高了粗类和细类的分类准确率。

Conclusion: 双曲空间嵌入为粗到细的少样本类增量学习提供了有效的表示学习框架，显著提升了模型性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [147] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种几何感知的两阶段框架，用于智能图像编辑中的物体移除，能够同时移除目标物体及其因果视觉伪影（如阴影和反射）。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么严格遵循掩码对齐训练但无法移除未明确掩码的因果效应，要么采用松散掩码对齐策略但缺乏可控性且可能过度擦除其他物体。这些限制源于忽略了物体几何存在与其视觉效应之间的因果关系。

Method: 两阶段框架：第一阶段从几何（如深度）中直接移除物体，使用严格掩码对齐监督；第二阶段基于更新后的几何条件渲染真实感RGB图像，因果视觉效应作为修改3D几何的结果被隐式考虑。引入基于正负样本对的偏好驱动目标来指导几何移除阶段的学习。

Result: 在两个流行基准测试上的广泛实验表明，该方法在移除物体及其相关伪影方面达到了最先进的性能。

Conclusion: 通过解耦几何移除和外观渲染，该方法能够有效处理物体移除中的因果视觉伪影问题，同时保持结构感知编辑的强几何约束。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [148] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了SEGA方法，通过高斯平滑和梯度集成来提高无参考图像质量评估模型在黑盒攻击中的可迁移性


<details>
  <summary>Details</summary>
Motivation: 现有的NR-IQA模型白盒攻击方法在更现实的黑盒场景中可迁移性较差，需要解决这一挑战

Method: SEGA方法通过应用高斯平滑到源模型并集成它们的平滑梯度来近似目标模型的梯度，并使用专门的扰动过滤掩码来确保对抗性扰动的不可感知性

Result: 在CLIVE数据集上的实验结果表明SEGA具有优越的可迁移性

Conclusion: SEGA方法能够成功实现对NR-IQA模型的基于迁移的黑盒攻击

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [149] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: 本文提出HadaSmileNet框架，通过哈达玛乘法融合transformer表示和生理D-Marker特征，在笑容情感识别任务中实现高效高性能


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法计算效率低，需要辅助任务监督和复杂的损失平衡，需要更高效的融合方法

Method: 使用参数自由的哈达玛乘法直接融合transformer特征和D-Marker特征，系统评估15种融合策略

Result: 在四个基准数据集上达到SOTA性能：UvA-NEMO(88.7%)、MMI(99.7%)、SPOS(98.5%)、BBC(100%)，参数减少26%，训练简化

Conclusion: 该框架计算高效且性能优越，特别适合需要实时情感计算能力的多媒体数据挖掘应用

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [150] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的单目视频动态人体与静态场景联合重建框架，通过事件引导的损失函数解决快速运动下的运动模糊问题。


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建动态人体和静态场景存在困难，特别是在快速运动时RGB帧会出现运动模糊。事件相机具有微秒级时间分辨率的优势，更适合动态人体重建。

Method: 使用统一的3D高斯集合，其中包含可学习的语义属性；只有被分类为人体的高斯会进行形变动画，场景高斯保持静态。提出事件引导损失函数，匹配连续渲染之间的模拟亮度变化与事件流，提高快速运动区域的局部保真度。

Result: 在两个基准数据集ZJU-MoCap-Blur和MMHPSD-Blur上实现了最先进的人体-场景重建效果，在PSNR/SSIM指标上显著优于强基线，LPIPS指标降低，尤其对高速运动对象效果明显。

Conclusion: 该方法无需外部人体掩码，简化了单独高斯集合的管理，有效解决了快速运动下的重建问题，展示了事件相机在动态场景重建中的优势。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [151] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T是一个实时威胁监控框架，通过结构化语义元组、在线事件去重和基于大语言模型的逻辑推理，同时实现高性能威胁检测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时性能和决策可解释性的要求，需要一种统一框架来解决这一挑战。

Method: 1) 将视频帧解构为结构化的人-物-交互-地点语义元组；2) 高效的在线事件去重和更新机制；3) 使用思维链策略微调大语言模型进行透明逻辑推理。

Result: 在XD-Violence和UCF-Crime基准数据集上的实验表明，Live-E2T在威胁检测精度、实时效率和可解释性方面显著优于最先进方法。

Conclusion: Live-E2T成功统一了实时性能和可解释性目标，为实时威胁监控提供了有效的解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [152] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 本文提出了PhotoCritique数据集、PhotoEye模型和PhotoBench基准，旨在提升多模态大语言模型在美学视觉理解方面的能力，特别是针对摄影专业知识的深度分析。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在美学视觉理解方面存在显著不足，无法区分一般视觉理解（如物体识别）和美学视觉理解（如色彩、构图等）。现有方法往往局限于基础美学常识，难以满足需要专业摄影知识的真实场景需求。

Method: 1）构建PhotoCritique数据集，基于专业摄影师和爱好者的广泛讨论；2）提出PhotoEye模型，采用语言引导的多视角视觉融合机制；3）建立PhotoBench基准，用于全面评估美学视觉理解能力。

Result: 在现有基准和PhotoBench上，PhotoEye模型相比现有模型展现出明显优势，能够提供更专业和详细的美学分析。

Conclusion: 通过专业数据集、创新模型和全面基准的结合，本文显著提升了多模态大语言模型在美学视觉理解方面的能力，为专业级图像美学分析提供了有效解决方案。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [153] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于XMem模型的实时MRI引导放疗肿瘤分割框架，用于TrackRAD2025挑战赛，能够在长序列cine-MRI中实时跟踪肿瘤运动。


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤跟踪的精度，这对于提升癌症治疗的准确性和安全性至关重要。

Method: 利用XMem模型（一种内存增强架构）来分割长序列cine-MRI中的肿瘤，通过高效集成内存机制实现实时肿瘤运动跟踪。

Result: 由于详细的实验记录丢失，无法报告精确的定量结果，但初步印象显示XMem框架表现出合理的分割性能并满足临床实时要求。

Conclusion: 该工作有助于改善MRI引导放疗中的肿瘤跟踪精度，但需要进一步实验验证定量性能。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [154] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型解决多对比度MRI超分辨率问题，通过动态空间扭曲、语义感知令牌聚合和空间频率融合模块，实现空间语义一致性重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法在空间语义一致性建模不足，且未充分利用频域信息，导致细粒度对齐差和高频细节恢复不足。

Method: SSCM模型包含三个核心模块：动态空间扭曲模块用于跨对比度空间对齐，语义感知令牌聚合块用于长程语义一致性，空间频率融合块用于精细结构恢复。

Result: 在公开和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能，确保了空间和语义一致的重建。

Conclusion: SSCM模型有效解决了多对比度MRI超分辨率中的空间语义一致性问题，提高了重建质量。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [155] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出Oracle-educated GRPO (OraPO)与FactScore-based奖励(FactS)方法，在有限预算下实现放射学报告生成，通过单阶段强化学习训练和轻量级oracle步骤，显著提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成方法依赖大规模数据和计算资源，本文旨在在有限预算下解决这一挑战。

Method: 使用OraPO将失败的GRPO探索转化为直接偏好监督，结合FactS提取临床事实并检查与真实标签的蕴含关系，提供密集可解释的句子级奖励。

Result: 在CheXpert Plus数据集上达到SOTA性能(F1=0.341)，训练数据减少2-3个数量级，使用小型基础VLM在普通硬件上实现。

Conclusion: OraPO和FactS构建了一个紧凑而强大的框架，显著提高了临床挑战性案例的学习效率。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [156] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个无需训练、基于参考的框架，能够在扩散模型中实现多参考风格的可控融合，通过语义标记分解和相似性感知重加权模块解决现有方法单风格限制和缺乏平衡机制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的方法只能接受单一风格图像，无法实现混合美学效果，且缺乏平衡多种风格影响的机制，限制了多风格生成的可扩展性和可控性。

Method: AMSF通过语义标记分解模块编码所有风格图像和文本提示，自适应注入到冻结扩散模型的交叉注意力层，然后使用相似性感知重加权模块在每个去噪步骤重新校准对各风格分量的注意力分配。

Result: 定性和定量评估表明，AMSF生成的多风格融合结果始终优于最先进方法，且其融合设计可无缝扩展到两种或更多风格。

Conclusion: AMSF为扩散模型中的表达性多风格生成提供了实用的解决方案，实现了无需微调或外部适配器的平衡可控融合。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [157] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 本文提出MLF-4DRCNet，一种用于4D雷达和相机图像多级融合的两阶段3D物体检测框架，通过点级、场景级和提案级融合解决雷达点云稀疏性问题，在VoD和TJ4DRadSet数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达虽然成本效益高且鲁棒性强，但其点云存在显著稀疏性和噪声，限制了在3D物体检测中的独立应用。现有雷达-相机融合方法大多采用为LiDAR设计的BEV融合范式，忽视了雷达点云的稀疏性和不完整几何特性。

Method: 提出三模块框架：增强雷达点编码器（ERPE）通过三重注意力体素特征编码器稠密化雷达点云；分层场景融合池化（HSFP）使用可变形注意力动态融合多尺度体素特征与2D图像特征；提案级融合增强（PLFE）通过融合图像特征精炼区域提案。

Result: 在View-of-Delft和TJ4DRadSet数据集上的实验结果表明，MLF-4DRCNet达到了最先进的性能，在VoD数据集上性能可与基于LiDAR的模型相媲美。

Conclusion: 该研究证明了多级融合策略在4D雷达-相机融合中的有效性，为解决雷达点云稀疏性问题提供了创新解决方案，为自动驾驶感知系统提供了成本效益高的替代方案。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [158] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS是一种基于Rectified Flow模型的训练免费框架，通过双流潜在空间引导解决图像反演中的语义漂移问题，在保持结构完整性的同时提升语义准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于单潜在向量的图像反演方法难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移问题，如细节模糊或属性错误。

Method: PDLS将反演过程分解为结构路径和语义路径，通过最优控制问题建模，利用线性二次调节器（LQR）推导闭式解，动态引导生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K数据集上的多种反演任务（高斯去模糊、运动去模糊、超分辨率和自由形式修复）实验表明，PDLS比单潜在基线方法产生更忠实于原图且语义对齐更好的重建结果。

Conclusion: PDLS框架通过双流潜在引导有效解决了扩散模型潜在空间反演中的语义漂移问题，无需昂贵的逐图像优化即可实现高质量重建。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [159] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: Prima是一种基于视觉语言模型的神经影像AI系统，利用大型学术健康系统的22万份MRI研究数据训练，在神经疾病诊断中表现优异，AUC达92.0%，并能提供可解释的诊断建议和工作优先级排序。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求持续增长给医疗系统带来巨大压力，特别是在资源匮乏地区。需要开发能够处理真实临床MRI数据的AI基础模型来缓解医生负担和缩短诊断时间。

Method: 使用分层视觉架构训练视觉语言模型，基于超过22万份MRI研究数据构建通用且可迁移的MRI特征表示。在包含3万份MRI研究的1年系统研究中测试模型性能。

Result: 在52种主要神经系统疾病的放射学诊断中，Prima的平均ROC曲线下面积达到92.0%，优于其他最先进的通用和医学AI模型，并展示了跨敏感群体的算法公平性。

Conclusion: Prima展示了健康系统规模视觉语言模型的变革潜力，能够推进AI驱动的医疗保健发展，帮助减轻医疗系统偏见，特别是在资源匮乏人群中的诊断延迟问题。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [160] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出了Understanding-in-Generation (UiG)框架，通过将理解能力融入生成过程来增强统一模型的文本到图像生成性能


<details>
  <summary>Details</summary>
Motivation: 现有的推理方法将理解和生成过程分离，限制了它们指导统一模型解决生成能力不足的能力

Method: 引入"图像编辑"作为桥梁，在推理过程中通过强大的理解能力来提供生成指导，逐步将理解融入生成过程

Result: 在TIIF基准测试的长提示设置上获得了3.92%的性能提升

Conclusion: UiG框架在文本到图像生成方面相比现有方法表现出显著的性能改进

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [161] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出了一个用于内窥镜图像深度估计的基准测试和合成数据集EndoSynth，通过微调深度基础模型显著提升了在真实内窥镜图像上的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像深度估计领域缺乏稳健的基准测试和高质量数据集，限制了深度估计模型在临床场景中的应用。

Method: 创建了包含真实内窥镜图像的基准测试，并开发了EndoSynth合成数据集（包含内窥镜手术器械、真实深度和分割掩码），通过微调深度基础模型来提升性能。

Result: 使用EndoSynth合成数据集微调深度基础模型后，在大多数未见过的真实数据上准确率显著提升。

Conclusion: 该工作通过提供基准测试和合成数据集，推动了内窥镜图像深度估计领域的发展，为未来研究提供了重要资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [162] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 本文提出LEAF-Mamba模型，通过局部强调状态空间模块和自适应融合模块，在RGB-D显著目标检测任务中实现性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著目标检测方法在CNN的局部感受野限制和Transformer的二次复杂度之间难以平衡，状态空间模型虽然具有线性复杂度但直接应用会导致局部语义缺失和跨模态融合不足。

Method: 提出LEAF-Mamba模型，包含局部强调状态空间模块(LE-SSM)捕获多尺度局部依赖，以及基于SSM的自适应融合模块(AFM)实现互补的跨模态交互和可靠融合。

Result: 在16个最先进的RGB-D SOD方法中，LEAF-Mamba在效果和效率上均表现最优，同时在RGB-T SOD任务上也展现出优异的泛化能力。

Conclusion: LEAF-Mamba模型成功解决了RGB-D SOD中局部语义捕获和跨模态融合的挑战，实现了性能与计算效率的良好平衡，具有较强的通用性。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [163] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出了一种结合窗口多头注意力机制和空间注意力机制的轻量级食品图像分类算法，在降低计算复杂度的同时保持高分类准确率


<details>
  <summary>Details</summary>
Motivation: 食品行业对生产质量和效率要求不断提高，但Vision Transformer模型参数量大、计算复杂度高，难以在资源受限环境中部署

Method: 集成窗口多头注意力机制(WMHAM)和空间注意力机制(SAM)，WMHAM通过窗口划分降低计算成本，SAM自适应强调关键空间区域

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，同时显著减少参数和FLOPs

Conclusion: 该方法在计算效率和分类性能之间实现了有效平衡，适合在资源受限环境中部署

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [164] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一个三阶段框架，用于遥感图像中的无标注开放集土地覆盖发现、分割和描述，结合像素级精度和高级语义理解来解决开放世界遥感解释的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 遥感中的开放集土地覆盖分析需要实现细粒度空间定位和语义开放分类，既要无类别监督地检测和分割新对象，又要通过多模态推理为它们分配可解释的语义标签。

Method: 三阶段管道：(1)使用可提示的微调分割模型(SAM)进行精确发现和掩码提取；(2)通过两阶段微调的多模态大语言模型(MLLM)进行语义归因和上下文描述；(3)使用LLM作为评判者和人工评分来评估MLLM。

Result: 该框架支持跨多样化卫星图像的稳健评估，无需人工标注，为动态土地覆盖监测提供了可扩展和可解释的解决方案。

Conclusion: OSDA在自动制图更新和大规模地球观测分析方面显示出强大潜力，为开放世界遥感解释提供了架构无关且无标签的解决方案。

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [165] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2021植物识别挑战旨在评估如何利用植物标本馆收藏来改进生物多样性丰富但数据贫乏地区的植物自动识别，特别是针对南美洲圭亚那地盾地区。


<details>
  <summary>Details</summary>
Motivation: 当前自动植物识别主要依赖北美和西欧的野外照片数据，而生物多样性最丰富的热带地区数据稀缺。但植物标本馆收藏了大量热带地区植物标本，可用于弥补数据不足。

Method: 采用跨域分类任务，训练集包含数十万份植物标本馆标本和数千张野外照片，测试集仅包含野外照片。数据集还包含5种形态和功能性状数据。

Result: 挑战评估了不同研究团队的方法和系统，分析了利用植物标本馆数据改进热带地区植物识别的效果。

Conclusion: 植物标本馆收藏可以作为热带地区植物自动识别的重要补充数据源，有助于解决数据贫乏地区的植物识别问题。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [166] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGSwap的新方法，用于解决文本到图像生成中跨类别对象融合的问题，并引入了COF数据集作为基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法在融合跨类别对象时存在偏差、视觉混乱和语义不一致的问题，且缺乏全面的基准数据集。

Method: AGSwap方法包含两个关键组件：组间嵌入交换（通过特征操作融合不同概念的语义属性）和自适应组更新（基于平衡评估分数的动态优化机制）。

Result: 广泛的实验表明，AGSwap在简单和复杂提示下均优于包括GPT-Image-1在内的最先进的组合式文本到图像方法。

Conclusion: AGSwap是一种简单而高效的跨类别对象融合方法，结合新构建的COF数据集，显著提升了文本到图像生成的质量和一致性。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [167] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2019植物识别挑战赛旨在评估在数据稀缺地区的植物自动识别能力，使用主要聚焦于圭亚那地盾和北亚马逊雨林的1万种植物数据集。


<details>
  <summary>Details</summary>
Motivation: 当前自动化植物识别技术主要针对少数几万种物种，而地球上有近36.9万种植物，需要解决数据稀缺地区的植物识别问题。

Method: 基于包含1万种植物的数据集，主要覆盖圭亚那地盾和北亚马逊雨林区域，这是世界上植物和动物多样性最丰富的地区之一。

Result: 挑战赛评估了参与研究团队的系统性能，并与最佳热带植物专家进行了比较。

Conclusion: 该挑战赛为数据稀缺地区的植物自动识别提供了重要资源和评估基准，推动了相关技术的发展。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [168] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的零样本开放词汇遥感视觉定位框架，利用冻结的通用基础模型实现开放场景下的目标定位


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感视觉定位方法受限于封闭词汇集，以及现有开放词汇方法依赖高质量数据集和耗时微调的问题

Method: 采用三阶段框架：1）使用视觉语言模型获取跨注意力图；2）利用扩散模型补充结构信息；3）通过注意力进化模块净化分割掩码

Result: 在广泛实验中，该框架持续优于现有的弱监督和零样本方法

Conclusion: RSVG-ZeroOV提供了一个无需任务特定训练的高效可扩展解决方案

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [169] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一个属性提示组合（APC）框架，利用文本语义联合增强目标重识别（ReID）的判别性和泛化能力，通过属性提示生成器和快慢训练策略在多个数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型局限于单域或跨域场景，单域模型容易过拟合域特定特征，跨域模型可能抑制身份特异性判别线索。需要同时提升判别性和泛化性。

Method: APC框架包含属性提示生成器（APG）和快慢训练策略（FSTS）。APG由语义属性字典（SAD）和提示组合模块（PCM）组成，FSTS通过快速更新流和慢速更新流平衡ReID特异性判别和通用表示学习。

Result: 在传统和域泛化ReID数据集上的广泛实验表明，该框架在判别性和泛化性方面均优于现有最先进方法。

Conclusion: APC框架通过文本语义和视觉语言模型的结合，有效解决了ReID任务中判别性与泛化性的平衡问题，为实际应用提供了更实用的解决方案。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [170] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于最优传输的框架OTCCLIP，用于重建图像-文本对，以防御CLIP模型的数据投毒和后门攻击。该方法通过细粒度视觉和文本特征之间的最优传输距离重新分配新标题，并采用最优传输目标函数促进模态间和模态内的细粒度对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法仅依赖图像和标题的全局表示来纠正被投毒的图像-文本对，忽视了视觉和文本特征的细粒度特征，可能引入错误的图像-文本对并损害CLIP预训练。

Method: 提出基于最优传输的距离度量方法，计算细粒度视觉和文本特征集之间的距离，并基于最优传输距离重新分配新标题。同时采用最优传输目标函数促进模态间和模态内的细粒度对齐。

Result: 实验表明OTCCLIP能成功降低投毒攻击的攻击成功率，与之前的方法相比，显著提高了在投毒数据集上训练的CLIP的零样本和线性探测性能。

Conclusion: OTCCLIP框架通过细粒度特征的最优传输对齐，有效防御CLIP模型的投毒攻击，同时提升模型性能。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [171] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出了LFI框架，通过建模视觉理解作为交互过程来解决视觉基础模型从视觉语言模型知识迁移的局限性，在多个基准测试中取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型主要采用结果导向范式，忽视了底层的交互过程，这种表示差异阻碍了有效的知识迁移并限制了在多样化视觉任务中的泛化能力

Method: LFI框架包含两个技术创新：交互查询（维护跨网络层的持久关系结构）和基于交互的监督（源自视觉语言模型的跨模态注意力机制）

Result: 在TinyImageNet分类上提升3.3mAP，COCO检测/分割提升1.6mAP/2.4AP，在PACS和VLCS的跨域设置中分别提升2.4和9.3的零样本性能，人类评估显示语义一致性指标比结果导向方法高2.7倍

Conclusion: LFI框架通过显式建模视觉理解作为交互过程，实现了更忠实和高效的知识迁移，在参数开销最小和更快收敛的情况下，在多个视觉任务中取得了显著改进

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [172] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出HyPSAM模型，利用SAM的零样本泛化能力进行RGB-热成像显著目标检测，通过动态融合网络和即插即用优化网络实现高质量显著图生成和精炼


<details>
  <summary>Details</summary>
Motivation: 解决RGB-热成像显著目标检测中特征融合不足和数据稀缺的问题，提升边界精度和完整目标识别能力

Method: 1) 动态融合网络(DFNet)生成初始显著图作为视觉提示；2) 即插即用优化网络(P2RNet)指导SAM使用混合提示精炼显著图；3) 结合文本、掩码和框提示实现可靠模态输入和精确定位

Result: 在三个公开数据集上实现最先进性能，具有显著通用性，可与不同RGB-T SOD方法集成获得性能提升

Conclusion: HyPSAM展示了提示工程在RGB-T SOD领域的潜力，为多模态显著目标检测提供了有效解决方案

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [173] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: 提出TriFusion-AE多模态交叉注意力自编码器，融合文本先验、单目深度图和LiDAR点云，提升点云去噪和重建的鲁棒性


<details>
  <summary>Details</summary>
Motivation: LiDAR点云易受噪声、遮挡和对抗性破坏影响，现有自编码器在真实场景下性能下降严重

Method: 通过多模态交叉注意力机制对齐文本语义线索、图像几何特征和LiDAR空间结构，实现联合表示学习

Result: 在强对抗攻击和重噪声条件下显著优于CNN自编码器，在nuScenes-mini数据集上验证了有效性

Conclusion: 多模态融合框架具有模型无关性，可与任何CNN点云自编码器集成，提升鲁棒性

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [174] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 本文提出了COLT方法，旨在增强开源视频大语言模型的持续工具使用能力，使其能够在不断演化的工具流中自动获取工具使用能力而不遗忘已学工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设工具库固定，难以适应现实世界中工具数据持续演化的环境，存在灾难性遗忘问题。

Method: COLT方法包含可学习的工具代码本作为工具特定记忆系统，基于用户指令与工具特征的相似度动态选择相关工具，并构建了VideoToolBench视频工具使用指令调优数据集。

Result: 在现有视频LLM基准和VideoToolBench数据集上的广泛实验表明，COLT方法达到了最先进的性能。

Conclusion: COLT方法成功解决了视频LLM在持续工具学习环境中的挑战，展现了优异的工具使用能力。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [175] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 本文提出结构化反思方法，将错误修复路径转化为明确、可控且可训练的动作，通过Reflect-Call-Final策略优化多轮工具调用的成功率和错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具增强的大语言模型通常使用监督模仿或粗粒度强化学习训练，自我反思依赖启发式提示或单向推理，在多轮交互中脆弱且容易重复错误。

Method: 结合DAPO和GSPO目标，采用针对工具使用优化的奖励方案，训练模型进行结构化反思：诊断失败、提出正确可执行的后续调用。

Result: 在BFCL v3和Tool-Reflection-Bench基准测试中，多轮工具调用成功率和错误恢复能力显著提升，冗余调用减少。

Conclusion: 将反思显式化并直接优化可提高工具交互的可靠性，为智能体从失败中学习提供可复现的路径。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [176] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用现有扩散模型增强稀疏视角3D高斯溅射重建，通过蒸馏方法提供准确且跨视图一致的扩散先验，实现有效的伪影去除和修复。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D重建由于视觉信息不足导致明显的伪影，现有生成先验方法难以保证多视图一致性，产生模糊结构和不合理细节。

Method: 提出蒸馏方法提供准确且跨视图一致的扩散先验，以及自适应渐进增强方案进一步细化欠约束区域的修复。

Result: 大量实验表明FixingGS在视觉质量和重建性能上优于现有最先进方法。

Conclusion: FixingGS通过充分利用扩散模型能力，有效解决了稀疏视角3DGS重建中的伪影和多视图一致性问题。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [177] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 本文提出了VIR-Bench基准测试，用于评估多模态大语言模型在长距离旅行视频理解方面的能力，发现现有模型在处理扩展时空尺度的视频时表现不佳，并通过旅行规划代理验证了该基准的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准主要关注室内场景或短距离户外活动，缺乏对长距离旅行挑战的探索。掌握扩展地理时空轨迹对于下一代MLLMs至关重要，支持现实世界任务如具身AI规划和导航。

Method: 提出VIR-Bench基准，包含200个旅行视频，将行程重建作为评估MLLMs地理时空智能的挑战性任务。通过实验评估最先进的MLLMs性能，并开发原型旅行规划代理进行案例研究。

Result: 实验结果显示，包括专有模型在内的最先进MLLMs在VIR-Bench上难以获得高分，表明处理扩展时空尺度视频的困难性。旅行规划代理的行程推荐质量显著提升，验证了评估协议的有效性。

Conclusion: VIR-Bench填补了长距离旅行视频理解基准的空白，不仅有效评估模型性能，还能转化为面向用户应用的具体性能提升，为下一代MLLMs的发展提供了重要参考。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [178] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM提出了一种基于高斯分位数的非均匀权重分离方法，通过显著性感知混合量化算法对视觉语言模型进行超低比特量化（≤2比特），显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的计算需求和内存要求很高，限制了其在硬件受限环境中的应用，需要开发高效的超低比特量化方法。

Method: 基于高斯分位数非均匀分离模型权重，将权重分为异常值（显著）和多个正常值（非显著）子集，提出显著性感知混合量化算法，根据显著性度量和压缩目标对缩放器和二进制矩阵施加不同约束。

Result: 在语言模型部分，Bi-VLM在视觉问答任务上比SOTA方法提升3%-47%；在整个VLM上提升4%-45%。量化模型存在90%-99%的图像令牌冗余，可进一步剪枝提升效率。

Conclusion: Bi-VLM通过创新的权重分离和量化方法，成功实现了视觉语言模型的高效超低比特量化，显著提升了模型效率同时保持了性能。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [179] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: ColorBlindnessEval是一个新颖的基准测试，用于评估视觉语言模型在受石原色盲测试启发的视觉对抗场景中的鲁棒性。该数据集包含500张类似石原测试的图像，挑战模型从复杂视觉模式中准确识别数字信息。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在视觉对抗场景中的鲁棒性，特别是在受色盲测试启发的复杂视觉环境下，因为现有模型在识别嵌入复杂模式中的数字信息方面存在局限性。

Method: 创建包含500张石原式图像的数据集，数字范围0-99，使用不同颜色组合。评估9个VLMs，采用是/否和开放式提示，并与人类参与者表现进行对比。

Result: 实验显示模型在对抗性环境中解释数字的能力存在局限，突出显示了普遍的幻觉问题。模型表现不如人类参与者。

Conclusion: 研究结果强调了在复杂视觉环境中提高VLMs鲁棒性的必要性。ColorBlindnessEval可作为基准测试工具，帮助提高VLMs在关键应用中的可靠性。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [180] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT是一个自监督学习框架，通过多尺度向量量化在医学图像中创建离散表征瓶颈，抑制捷径学习，提高跨任务和跨域的表征迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖复杂架构、解剖学先验或精心调优的数据增强，限制了可扩展性和泛化性，且容易在解剖相似度高、病理细微的医学图像（如胸片）中出现捷径学习问题。

Method: 将多尺度向量量化集成到自监督学习流程中，通过离散表征瓶颈约束模型学习可重复、结构感知的特征，同时抑制视图特定或低效用模式。

Result: DiSSECT在分类和分割任务上表现优异，需要极少或无需微调，在低标签场景下具有高标签效率，在多个公共医学影像数据集上验证了其鲁棒性和泛化性。

Conclusion: DiSSECT框架通过离散表征学习有效解决了医学图像自监督学习中的捷径学习问题，实现了高效的表征迁移和优异的性能表现。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [181] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个多模态医学基础模型，结合图像分析和文本推理，实现像素级病灶定位、结构化报告生成和类似医生的诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型过于专业化且泛化能力有限，而临床应用需要精确的视觉定位、多模态整合和链式推理能力。

Method: 提出新颖的多模态训练方法，整合检测、分割和多模态链式推理，并发布涵盖推理、检测、分割和文档理解任务的开放数据套件。

Result: Citrus-V在多个基准测试中优于现有开源医学模型和专家级影像系统，支持精确病灶量化、自动报告生成和可靠第二意见。

Conclusion: 该模型提供了从视觉定位到临床推理的统一流程，为临床诊断和治疗决策提供了强大的多模态支持。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [182] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合热成像、深度学习和车联网通信的实时检测系统，用于减少鹿车碰撞事故，在热成像数据集上达到98.84%的平均精度，并在实地测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 美国每年发生约210万起鹿车碰撞事故，造成440人死亡、5.9万人受伤和100亿美元经济损失，同时导致鹿群数量下降，亟需有效的预防措施。

Method: 系统整合热成像技术、深度学习算法和C-V2X通信，使用在北卡罗来纳州Mars Hill收集的12,000多张热成像鹿图像数据集进行训练和验证。

Result: 系统表现优异：平均精度98.84%，精确率95.44%，召回率95.96%。在恶劣天气条件下检测精度达88-92%，而传统可见光摄像头效果低于60%。端到端延迟始终低于100毫秒。

Conclusion: 该研究通过热成像和联网车辆技术，为减少鹿车碰撞事故建立了可行的技术路径，实地测试验证了系统在各种天气条件下的鲁棒性。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [183] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi是一个新的扩散模型框架，通过将扩散模型与下游模型偏好的样本对齐，解决手术数据稀缺问题，避免数据记忆化导致的样本不一致问题。


<details>
  <summary>Details</summary>
Motivation: 手术标注数据稀缺限制了计算机辅助干预中深度学习系统的发展，现有扩散模型存在数据记忆化问题，导致样本不一致且可能损害下游性能。

Method: 构建偏好和非偏好合成图像对，通过轻量级微调扩散模型，使图像生成过程与下游目标明确对齐。

Result: 在三个手术数据集上，分类任务提升7-9%，分割任务提升2-10%，对代表性不足的类别改进尤为显著。迭代细化进一步将性能提升4-10%。

Conclusion: SAADi方法克服了样本退化问题，确立了任务感知对齐作为缓解数据稀缺和推进手术视觉应用的关键原则。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [184] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出了一种基于模型神经网络的动态PET图像去噪方法，通过结合帧间空间相关性和帧内结构一致性建立KMDS模型，并用神经网络替代参数估计实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 动态PET中短时间帧的图像质量受限于统计噪声，深度学习在医学图像去噪中表现出色，但需要更好地利用动态PET的时空特性。

Method: 建立基于核空间的多维稀疏模型，将参数估计替换为神经网络，形成端到端的KMDS-Net网络架构。

Result: 在模拟和真实数据上的实验表明，该方法在动态PET去噪方面表现优异，超越了之前的基线方法。

Conclusion: 该方法能有效实现动态PET的高时空分辨率，代码已开源。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [185] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出了一种结合光流分割标签插值和多任务学习的新框架，用于解决机器人辅助手术中视觉数据理解面临的时空不平衡问题


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需要精确理解视觉数据，但现有方法多为单任务，无法处理复杂的手术场景动态。同时，像素级分割数据获取困难，长期标注（如阶段和步骤）覆盖所有帧，而短期标注（如器械分割和动作检测）仅覆盖关键帧，造成时空不平衡

Method: 使用光流从标注的关键帧估计并传播标签到相邻未标注帧，通过标签插值丰富稀疏的空间监督，结合多任务学习平衡时空信息

Result: 该集成方法提高了手术场景理解的准确性和效率

Conclusion: 提出的框架通过光流标签插值和多任务学习的结合，增强了机器人辅助手术的实用性

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [186] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一的多模态加速框架，通过推测解码和多阶段蒸馏技术，显著提升多模态理解和生成任务的效率。


<details>
  <summary>Details</summary>
Motivation: 随着多模态任务中交错标记数量的增加，扩散去噪和自回归解码的迭代过程带来了巨大的计算开销，需要高效的加速解决方案。

Method: 采用分治策略，使用推测解码进行下一标记预测，结合多阶段蒸馏过程加速扩散去噪。通过对抗蒸馏和人类反馈学习开发高效的1-NFE模型。

Result: 在多模态理解任务上实现2倍以上加速，文本到图像生成实现16.67倍加速，图像编辑实现22倍加速，同时保持原始模型的高质量输出。

Conclusion: Hyper-Bagel框架通过高效的加速技术，使复杂的多模态交互变得无缝和即时，实现了成本效益和响应性的最佳平衡。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [187] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 评估多模态大语言模型在基督教圣像学单标签分类任务中的表现，发现GPT-4o和Gemini-2.5 Pro优于ResNet50基线，支持将LLMs用作数字人文工作流中的元数据管理工具。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态大语言模型是否能够解释通常由监督分类器处理的基督教圣像学，并评估其性能。

Method: 使用ArtDL、ICONCLASS和Wikidata三个数据集，在三种条件下测试模型：(1)使用类别标签分类，(2)使用Iconclass描述分类，(3)五样本少样本学习。结果与在同一数据集上微调的ResNet50基线进行比较。

Result: Gemini-2.5 Pro和GPT-4o优于ResNet50基线。在Wikidata数据集上准确率显著下降，Siglip达到最高准确率。使用类别描述丰富提示通常改善零样本性能，而少样本学习产生较低结果。

Conclusion: 通用多模态大语言模型能够在视觉复杂的文化遗产领域进行分类，支持将LLMs用作数字人文工作流中的元数据管理工具。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [188] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的重参数化图构建方法（LRGC），用于视觉图神经网络，通过可学习的注意力机制和软阈值重参数化来构建图像图表示，避免了传统方法中的超参数搜索问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉图神经网络依赖非参数化的统计方法来构建图结构，这种方法无法为每个节点选择最佳邻域，且需要超参数搜索。本文旨在开发一种可学习的、无需超参数的图构建方法。

Method: LRGC方法在每对节点之间应用键值注意力机制，然后使用软阈值重参数化进行边选择，这使得可以使用可微数学模型进行训练。通过可学习参数选择邻域，消除了传统聚类或阈值方法引入的偏差。

Result: 在ImageNet-1k基准数据集上，提出的ViG-LRGC方法在相似模型规模下优于最先进的ViG模型。

Conclusion: LRGC提供了一种有效的可学习图构建方法，能够自动适应训练数据，在图像表示学习任务中取得了优异的性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [189] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出Point Prompt Defender框架，通过对抗性强化学习自动优化SAM的点提示，采用攻击-防御范式提升分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或手动设计的提示，限制了可扩展性和泛化能力，需要自动化的提示优化方案

Method: 构建任务无关的点提示环境，使用双空间图表示图像块，训练攻击者和防御者智能体通过DQN学习优化提示选择

Result: 实验表明该方法有效提升SAM的鲁棒性和泛化能力，无需重新训练即可增强分割性能

Conclusion: 建立了灵活、可解释、即插即用的提示优化框架，为基于提示的分割提供了新思路

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [190] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds是首个多模态野生动物监测数据集，包含无人机图像、相机陷阱照片视频和生物声学记录，支持多模态AI研究用于环境监测。


<details>
  <summary>Details</summary>
Motivation: 解决濒危物种研究、保护生态学和栖息地管理中的关键需求，为综合环境监测提供数据支持。

Method: 在220英亩牧场进行四天同步监测，收集三种模态数据，包括无人机图像、相机陷阱和生物声学记录。

Result: 提供了传感器模态性能的比较分析，展示了在土地利用模式、物种检测、行为分析和栖息地监测方面的互补优势。

Conclusion: 建立了可复制的多模态野生动物监测协议，为保护计算机视觉研究贡献开放数据集，未来将扩展GPS追踪数据和更长时间覆盖。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [191] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 本文提出了RS3DBench基准数据集，包含54,951对遥感图像与像素级对齐的深度图，旨在推动遥感图像3D视觉模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏全面的深度信息或深度数据与遥感图像的精确对齐，限制了3D视觉模型在遥感领域的发展。

Method: 构建包含54,951对遥感图像-深度图对的数据集，并基于稳定扩散模型开发遥感深度估计模型，利用其多模态融合能力。

Result: 在RS3DBench数据集上实现了最先进的性能，为遥感3D视觉感知模型提供了有效的训练和评估工具。

Conclusion: 该工作对遥感领域3D视觉感知模型的发展和地理人工智能的进步做出了重要贡献，数据集和代码已开源。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [192] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出首个无需SfM的事件相机去模糊3D高斯泼溅方法DeblurSplat，通过DUSt3R直接获取初始点云避免姿态误差传递，并利用事件流提供细粒度监督信号


<details>
  <summary>Details</summary>
Motivation: 解决运动模糊问题，避免传统SfM方法中相机姿态不准确导致的累积误差传递到初始点云位置

Method: 1) 利用预训练的DUSt3R密集立体模块直接从模糊图像获取准确初始点云；2) 引入事件流到去模糊流程，通过解码事件流和模糊图像的潜在清晰图像提供细粒度监督

Result: 在多种场景下的大量实验表明，DeblurSplat不仅能生成高保真度的新视角，而且在去模糊3D-GS方面比现有技术具有显著渲染效率优势

Conclusion: 该方法成功解决了运动去模糊问题，避免了相机姿态误差传递，利用事件流特性实现了高效高质量的场景重建

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [193] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: 提出MoiréNet，一种结合频域和空域特征的U-Net卷积神经网络框架，用于有效去除图像中的摩尔纹伪影。该网络通过定向频域-空域编码器和频域-空域自适应选择器实现高效的去摩尔纹效果。


<details>
  <summary>Details</summary>
Motivation: 摩尔纹是由显示像素阵列和相机传感器网格之间的频谱混叠产生的各向异性、多尺度伪影，对数字图像去摩尔纹处理构成重大挑战。

Method: MoiréNet采用U-Net架构，包含两个关键组件：定向频域-空域编码器（DFSE）通过定向差分卷积识别摩尔纹方向，以及频域-空域自适应选择器（FSAS）实现精确的特征自适应抑制。

Result: 在公开数据集上的实验表明，MoiréNet实现了最先进的性能，仅需5.513M参数，比ESDNet-L减少了48%的参数数量，同时保持优异的恢复质量。

Conclusion: MoiréNet结合了卓越的恢复质量和参数效率，使其特别适合资源受限的应用场景，如智能手机摄影、工业成像和增强现实。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [194] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的频率感知音频-视觉分割（FAVS）框架，通过频率域分解和重组来解决音频-视觉分割任务中的模态间频率域矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有AVS方法忽视了音频和视觉模态在频率域上的固有矛盾——音频高频信号普遍存在干扰噪声，而视觉高频信号包含丰富的结构细节。忽略这些差异会导致性能不佳。

Method: FAVS框架包含两个关键模块：频率域增强分解器（FDED）模块采用基于残差的迭代频率分解来区分模态特定语义和结构特征；协同跨模态一致性（SCMC）模块利用专家混合架构通过动态专家路由增强语义一致性和模态特定特征保留。

Result: 在三个基准数据集上的大量实验表明，FAVS框架实现了最先进的性能，丰富的定性可视化进一步验证了所提出模块的有效性。

Conclusion: 通过将AVS任务重新定义为频率域分解和重组问题，FAVS框架成功解决了音频-视觉模态间的频率域矛盾，显著提升了分割性能。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [195] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 本文综述了可解释人工智能（xAI）在视觉感知任务中的四种代表性方法：显著性图、概念瓶颈模型、基于原型的方法和混合方法，分析了它们的机制、优缺点及评估指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分析中表现出色但缺乏可解释性，xAI旨在解决这一"黑箱"问题，让人类能够理解AI模型的决策过程。

Method: 系统性地调查和分析四种xAI方法：显著性图、概念瓶颈模型、原型方法和混合方法，比较它们的机制和评估标准。

Result: 提供了对四种xAI方法的全面概述，揭示了各自的优势和局限性，为未来研究提供了指导。

Conclusion: xAI对于提高AI模型在关键应用中的可靠性至关重要，需要继续研究和发展更有效的可解释性方法。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [196] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于去噪扩散概率模型（DDPM）的方法，通过改进噪声调度和时间步嵌入技术来生成高质量合成LiDAR数据，以增强自动驾驶车辆的3D视觉系统性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖LiDAR数据进行环境感知，但真实LiDAR数据采集耗时且易受噪声和稀疏性影响。需要生成高质量合成数据来改善计算机视觉任务性能。

Method: 使用改进的DDPM模型，包含新颖的噪声调度和时间步嵌入技术，优化去噪过程和时间感知能力，生成更真实的点云数据。

Result: 在IAMCV和KITTI-360数据集上的评估显示，该方法在四项性能指标上优于现有最先进方法，能有效缓解噪声和稀疏数据的影响。

Conclusion: 所提出的方法能够生成具有丰富空间关系和结构细节的多样化点云，显著提升了自动驾驶感知系统的性能。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [197] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: AGSSP是一种新的预训练范式，通过异常先验指导表示学习，解决金属表面缺陷检测中预训练-微调范式的领域差距问题。该方法使用两阶段框架：首先通过异常图蒸馏知识预训练骨干网络，然后使用伪缺陷框预训练检测器。


<details>
  <summary>Details</summary>
Motivation: 解决金属表面缺陷检测中预训练-微调范式的关键困境：在自然图像数据集上预训练存在领域差距，而在工业数据上进行自监督预训练又因现有学习目标无法区分细微缺陷模式与复杂背景噪声而效果不佳。

Method: 提出异常引导的自监督预训练(AGSSP)方法，包含两阶段：1)通过异常图知识蒸馏预训练骨干网络以捕获缺陷显著特征；2)使用异常图生成的伪缺陷框预训练检测器。开发了知识增强方法生成高质量异常图，并收集了12万张图像的大规模工业数据集。

Result: 实验表明AGSSP在各种设置下持续提升性能，相比基于ImageNet的模型，mAP@0.5提升高达10%，mAP@0.5:0.95提升11.4%。

Conclusion: AGSSP通过异常先验指导表示学习，有效解决了金属表面缺陷检测中的预训练困境，显著提升了检测性能，为工业缺陷检测提供了新的预训练范式。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [198] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型和预训练视觉模型的自学习零样本图像分类框架，无需标注数据，仅需类别名称即可训练轻量级分类器


<details>
  <summary>Details</summary>
Motivation: 深度学习方法通常依赖大量标注数据，但在实际应用中标注数据往往稀缺。视觉语言模型和预训练模型为解决这一问题提供了可能

Method: 使用置信度伪标签策略，在测试数据上直接训练轻量级分类器。VLM识别高置信度样本，预训练视觉模型增强其视觉表示，通过迭代训练捕获互补的语义和视觉线索

Result: 在十个不同数据集上的实验评估表明，该方法优于基线零样本方法

Conclusion: 该方法无需VLM微调和大语言模型，仅依赖视觉模型减少对语义表示的依赖，实现了动态适应的零样本分类

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [199] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出了首个音频驱动的通用逼真头像合成方法，结合了与人物无关的语音模型和新型通用头部头像先验（UHAP），能够同时处理几何变形和外观变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将音频特征映射到几何变形，忽略了音频依赖的外观变化。本文旨在开发一个能够同时捕捉几何和外观变化的通用音频驱动头像模型。

Method: 使用跨身份多视角视频训练UHAP，通过中性扫描数据进行监督学习。构建通用语音模型将原始音频输入直接映射到UHAP潜在表达空间，该空间编码几何和外观变化。采用单目编码器进行个性化微调。

Result: 该方法能够生成高度逼真的头像，具有精确的唇部同步和细微表情细节（如眉毛运动、视线转移和真实的口腔外观）。在唇同步准确性、图像质量和感知真实性方面优于现有几何方法。

Conclusion: 这是首个能够进行详细外观建模和渲染的通用音频驱动头像模型，在多个评估指标上均优于现有方法。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [200] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 提出基于机器学习的模块化管道，用于自动检测、跟踪和特征提取树突棘在3D+时间显微镜数据中，解决大规模分析树突棘结构动力学的挑战。


<details>
  <summary>Details</summary>
Motivation: 树突棘是大脑兴奋性突触的关键结构组成部分，其大小可作为突触效能的代理指标。目前对3D+时间显微镜数据中树突棘结构动力学的大规模分析仍然具有挑战性且劳动密集。

Method: 采用模块化机器学习管道，包括基于transformer的检测模块、整合空间特征的深度跟踪组件、利用空间一致性的时间跟踪模块，以及量化生物学相关脊柱特征的特征提取单元。

Result: 在开源标记的脊柱数据和两个新发布的注释数据集上验证了方法，建立了可扩展的端到端树突棘动力学分析基线。

Conclusion: 发布数据、代码和预训练权重，为未来研究提供基础，促进树突棘动力学的大规模自动化分析。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [201] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决含镜面环境中的3D重建和新视角合成问题，通过利用镜面反射作为补充视角来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理镜面环境时性能下降，主要关注镜面表面的对称映射，但忽视了镜面反射所携带的丰富信息。这些反射可以提供补充视角，填补缺失细节并显著提升重建质量。

Method: 提出了ReflectiveGS方法，这是3D高斯泼溅的扩展，将镜面反射作为补充视角而非简单的对称伪影，从而增强场景几何并恢复缺失细节。

Result: 在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS和训练速度方面均优于现有方法。

Conclusion: 该方法为镜面丰富环境中的3D重建设定了新的基准，证明了利用镜面反射作为补充视角的有效性。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [202] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的胆道定位方法，通过Yolo检测算法和GAN生成合成训练数据，旨在降低腹腔镜胆囊切除术中的胆管损伤风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽然恢复快、美容效果好，但胆管损伤风险较高，严重影响患者生活质量和生存率。为提高术中胆管可视化，需要开发有效的胆道定位技术。

Method: 构建并标注图像数据库训练Yolo检测算法，采用传统数据增强技术和生成对抗网络(GAN)生成合成训练数据集。

Result: 实验结果表明该方法能够有效定位胆道结构，讨论了实验结果和伦理考量。

Conclusion: 深度学习技术结合GAN数据增强可以有效改善腹腔镜手术中的胆道可视化，有望降低胆管损伤风险。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [203] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prompt-DAS框架，通过提示驱动的多任务学习实现电子显微镜图像中细胞器的领域自适应分割，支持无监督、弱监督和交互式分割


<details>
  <summary>Details</summary>
Motivation: 解决大规模电子显微镜图像中多细胞器实例分割的标注效率问题，利用SAM的提示机制但克服其需要每个实例单独提示的限制

Method: 构建可提示的多任务框架，结合辅助中心点检测任务，支持全点、稀疏点或无点提示配置，并提出提示引导的对比学习增强特征判别性

Result: 在多个挑战性基准测试中，该方法在UDA、WDA和基于SAM的方法上都表现出优越性能

Conclusion: Prompt-DAS框架为电子显微镜图像分割提供了灵活高效的解决方案，显著降低了标注需求并提升了分割性能

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [204] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 本文提出了一种鲁棒的自监督立体匹配方法RoSe，通过引入视觉基础模型的先验知识和场景对应先验，解决了恶劣天气条件下自监督立体匹配性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在恶劣天气条件下性能显著下降，主要原因是CNN特征提取器在退化区域表现不佳，以及光度一致性假设在恶劣天气下失效。

Method: 1) 将视觉基础模型的鲁棒先验注入CNN特征提取器；2) 利用场景对应先验构建鲁棒监督信号；3) 创建包含清晰和恶劣天气图像对的合成数据集；4) 提出两阶段训练范式：鲁棒自监督场景对应学习和恶劣天气蒸馏。

Result: 大量实验证明该方法有效且通用，在恶劣天气条件下优于现有最先进的自监督方法。

Conclusion: 通过引入鲁棒先验和场景对应学习，提出的RoSe方法显著提升了自监督立体匹配在恶劣天气条件下的性能，为实际应用提供了可靠解决方案。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [205] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉语言模型的链式逐步推理框架，通过细粒度奖励评估和强化学习来提升推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言推理方法通常在粗粒度级别使用推理链，难以进行细粒度结构化推理，并且难以评估中间推理的质量和奖励。

Method: 提出了一个简单有效的透明框架，包括步骤级推理数据、过程奖励模型（PRM）和强化学习训练，实现细粒度奖励评估。

Result: 在具有挑战性的视觉语言基准测试中取得了显著的改进，建立了强大的基线模型。

Conclusion: 该工作为视觉语言模型提供了基准，并为更复杂的多模态推理提供了见解，相关数据和代码将开源。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [206] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出一种弱监督的食品图像语义分割方法，结合SAM的零样本能力和ViT的注意力机制，仅需图像级标注即可生成分割掩码


<details>
  <summary>Details</summary>
Motivation: 解决食品图像分割需要大量像素级标注的问题，利用SAM的零样本能力和ViT的注意力机制实现弱监督分割

Method: 使用Swin Transformer生成类激活图作为SAM的提示，结合图像预处理技术和单掩码/多掩码生成策略，仅需图像级标注训练

Result: 在FoodSeg103数据集上评估，平均每张图像生成2.4个掩码（不包括背景），多掩码场景下mIoU达到0.54

Conclusion: 该方法可作为加速食品图像标注的工具，或集成到食品营养追踪应用中

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [207] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 提出了HyKid数据集，这是一个包含48名儿童脑积水患者的开源数据集，包含3D MRI图像和手动校正的脑组织分割，特别是脉络丛分割，为脑积水评估提供了高质量基准。


<details>
  <summary>Details</summary>
Motivation: 儿童脑积水评估具有挑战性，且缺乏公开的专家标注数据集，特别是包含脉络丛分割的数据集。

Method: 从48名儿科患者收集3D MRI图像，使用切片到体积算法从常规低分辨率图像重建1mm各向同性分辨率图像，由经验丰富的神经学家手动校正脑组织分割，并使用检索增强生成框架从临床放射学报告中提取结构化数据。

Result: 脉络丛体积与总脑脊液体积之间存在强相关性，为脑积水评估提供了潜在生物标志物，预测模型表现出色（AUC = 0.87）。

Conclusion: HyKid数据集为神经影像算法开发提供了高质量基准，并揭示了脉络丛相关特征在脑积水评估中的重要性。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [208] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的时序一致性U-Net分割架构，用于实现时间稳定且精确的超声心动图分割，通过构建回声动态图(EDG)提取视频动态信息，并在跳跃连接中引入心脏相位动态注意力(CPDA)来增强分割的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图中的心脏解剖结构分割对心血管诊断和治疗至关重要，但超声心动图容易变形和出现散斑噪声，导致帧间分割抖动。即使单帧分割精度高，时间不稳定性也会削弱功能估计并影响临床可解释性。

Method: DyL-UNet框架通过动态学习构建回声动态图(EDG)从视频中提取动态信息。它包含多个基于Swin-Transformer的编码器-解码器分支处理单帧图像，并在跳跃连接中引入心脏相位动态注意力(CPDA)，利用EDG编码的动态特征和心脏相位线索在分割过程中强制执行时间一致性。

Result: 在CAMUS和EchoNet-Dynamic数据集上的广泛实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了更优的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠的解决方案，能够在保持分割精度的同时显著改善时间稳定性。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [209] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出了一种多尺度特征交互网络（MsFIN）用于从行车记录仪视频中早期预测事故，通过多尺度特征聚合、时序特征处理和多尺度特征后融合来解决交通参与者特征交互和复杂多时序行为建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着行车记录仪的广泛部署和计算机视觉技术的发展，从行车记录仪视角开发事故预测模型对于主动安全干预变得至关重要。但存在两个关键挑战：建模交通参与者之间的特征级交互（在行车记录仪视图中常被遮挡）和捕捉事故前复杂、异步的多时序行为线索。

Method: MsFIN包含三个层次：多尺度特征聚合层设计多尺度模块提取短期、中期和长期时间尺度的场景表示，利用Transformer架构促进全面特征交互；时序特征处理层在因果约束下捕捉场景和对象特征的序列演化；多尺度特征后融合阶段融合多个时间尺度的场景和对象特征生成综合风险表示。

Result: 在DAD和DADA数据集上的实验表明，MsFIN在预测准确性和及时性方面显著优于采用单尺度特征提取的最先进模型。消融研究验证了MsFIN中每个模块的有效性。

Conclusion: 多尺度特征融合和上下文交互建模使MsFIN实现了优越性能，为行车记录仪视角的事故早期预警提供了有效解决方案。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [210] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian是一个用于稀疏视图3D高斯对象重建的高效框架，通过将扩散过程转移到小波域，仅在低分辨率LL子带应用扩散，高频子带使用轻量网络优化，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 3D高斯飞溅(3DGS)在稀疏视图设置下性能急剧下降，现有方法使用扩散模型修复损坏的渲染图作为伪真实值，但计算成本高昂。

Method: 提出小波域扩散方法：1）仅在低分辨率LL子带应用扩散；2）高频子带使用轻量网络优化；3）采用高效的在线随机掩码策略替代低效的留一法。

Result: 在Mip-NeRF 360和OmniObject3D两个基准数据集上的实验表明，WaveletGaussian在保持竞争性渲染质量的同时大幅减少了训练时间。

Conclusion: WaveletGaussian通过小波域扩散和高效训练策略，实现了稀疏视图3D高斯重建的高效优化，在质量和效率之间取得了良好平衡。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [211] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i是对Sa2VA模型的改进版本，通过修正训练和推理过程中的不一致性问题，在多个视频分割基准上实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 发现Sa2VA模型在视频对象分割任务中未能发挥其全部潜力，主要原因是训练和推理过程存在不一致性。

Method: 提出Sa2VA-i模型，通过修正训练和推理过程中的不一致性问题来改进原始Sa2VA模型。

Result: Sa2VA-i在多个视频基准上取得显著提升：MeViS +11.6 J&F，Ref-YT-VOS +1.4，Ref-DAVIS +3.3，ReVOS +4.1。Sa2VA-i-1B模型在MeViS基准上甚至能与原始Sa2VA-26B模型相媲美。

Conclusion: 这项工作强调了看似微不足道的实现细节的重要性，为视频分割领域提供了有价值的见解。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [212] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 本文提出了一种基于对抗性精炼的VQ-GAN框架，通过密集运动标记化技术压缩时空热图，有效解决人体运动理解中的高维度和冗余问题。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动理解是计算机视觉的核心挑战，其高维度和内在冗余性要求高效的压缩和表示方法。现有方法存在运动模糊和时间不对齐等重建伪影问题。

Method: 结合密集运动标记化与对抗性精炼的VQ-GAN框架，通过对抗训练消除重建伪影，保持人体运动的细粒度轨迹。

Result: 在CMU Panoptic数据集上，方法比dVAE基线SSIM提升9.31%，时间不稳定性降低37.1%。研究发现2D运动可用128个标记词汇表最优表示，而3D运动需要1024个标记代码本。

Conclusion: 该方法为多样化运动分析应用提供了实际部署可行性，证明了密集标记化策略在运动复杂度分析中的有效性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [213] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，将多光谱数据作为零样本输入引入到仅训练过RGB数据的通用多模态模型中，使这些模型能够理解专业的多光谱信号。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像在遥感应用中很重要，但现有的机器学习模型需要专门训练且成本高。强大的通用多模态模型无法理解专业的多光谱信号，限制了其应用。

Method: 利用多模态模型对视觉空间的理解，将多光谱数据适配到该空间，并将领域特定信息作为指令注入模型。以Gemini2.5模型为例进行验证。

Result: 在土地覆盖和土地利用分类的流行遥感基准测试中，该方法实现了显著的零样本性能提升，展示了Gemini2.5对新输入的易适应性。

Conclusion: 该方法使地理空间专业人员能够轻松利用强大的多模态模型（如Gemini2.5）来加速工作，受益于其丰富的推理和上下文能力，同时基于专业传感器数据。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [214] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2是一个基于Transformer的在线长时点跟踪模型，通过架构改进、内存优化和合成训练策略提升性能和效率，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决长时点跟踪问题，需要在视频帧间实现一致的点识别，应对显著的外观变化、运动和遮挡，特别针对在线实时和流式应用场景。

Method: 扩展Track-On模型为Track-On2，采用因果处理框架，通过内存机制保持时间一致性，进行粗粒度补丁级分类后细化，系统研究合成训练对内存行为的影响。

Result: 在五个合成和真实世界基准测试中取得最先进结果，超越了先前的在线跟踪器甚至利用双向上下文的强离线方法。

Conclusion: 基于因果、内存架构并纯合成数据训练的模型是解决真实世界点跟踪问题的可扩展有效方案。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [215] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个用于多摄像头、多光谱同步和实时检测海豹与北极熊的综合系统，可将数据集处理时间减少80%


<details>
  <summary>Details</summary>
Motivation: 开发一个能够高效处理空中调查数据的系统，用于在阿拉斯加周边海域监测冰相关海豹和北极熊

Method: 采用严格的校准和硬件同步技术，利用多光谱进行目标检测，所有数据都带有元数据注释，并将图像和检测结果映射到世界平面

Result: 相比之前的方法，KAMERA能够将数据集处理时间减少高达80%，并实现准确的调查区域估计

Conclusion: KAMERA系统有望在科学界激发其他测绘和检测工作的发展，所有软件、模型和原理图都已完全开源

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [216] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX是一种神经形态协同推理架构，通过联合优化空间和时间冗余，显著降低边缘计算中的数据传输和能耗，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决脉冲神经网络在边缘设备上部署时面临的高延迟、高能耗问题，以及边缘-云协同推理系统中的高传输成本问题。

Method: 采用学习型脉冲驱动压缩模块减少数据传输，结合动态早退机制根据输出置信度自适应终止推理。在ResNet-18和VGG-16骨干网络上进行原型实现。

Result: 数据传输减少高达2048倍，边缘能耗降低超过90%，端到端延迟比纯边缘推理降低3倍，精度损失小于2%。

Conclusion: NeuCODEX能够在资源受限环境中实现实用、高性能的SNN部署。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [217] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLO的息肉检测方法YOLO-LAN，通过使用M2IoU损失函数、数据增强和负样本训练，在结肠镜息肉检测任务中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是一种致命疾病，始于结肠内壁息肉的形成。结肠镜检查是检测息肉的标准方法，但人工检测存在不一致性和漏检问题。基于深度学习的物体检测可以提供更准确、实时的诊断方案。

Method: 提出YOLO-LAN检测流程，采用YOLO架构，使用M2IoU损失函数、多种数据增强技术和负样本训练来模拟真实临床场景。

Result: 在Kvasir-seg和BKAI-IGH NeoPolyp数据集上优于现有方法，YOLOv12达到mAP50 0.9619、mAP50:95 0.8599，YOLOv8达到mAP50 0.9540、mAP50:95 0.8487。

Conclusion: 该方法在息肉大小和精确定位检测方面表现出鲁棒性，具有临床应用的潜力，可用于AI辅助结直肠癌筛查。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [218] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 本文分析了MOSEv2赛道中SeC框架的长时记忆和概念感知记忆机制，展示了这些机制在处理遮挡、重现和干扰物方面的优势，最终在LSVOS挑战赛中获得了39.89%的JF分数，排名第一。


<details>
  <summary>Details</summary>
Motivation: 探索复杂半监督视频对象分割中的挑战，特别是通过分析SeC框架的长时记忆和概念感知记忆机制来解决MOSEv2赛道中的核心问题。

Method: 通过分析和改进SeC（增强的SAM-2框架），研究其长时记忆机制（保持时间连续性）和概念感知记忆（提供语义先验抑制干扰物）。

Result: 在MOSEv2测试集上获得了39.89%的JF分数，在LSVOS挑战赛的MOSEv2赛道中排名第一。

Conclusion: 长时记忆和概念感知记忆的结合有效解决了半监督视频对象分割中的关键挑战，特别是遮挡、重现和干扰物问题。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [219] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 本文基于人类视觉的双流假说，将视觉语言模型（VLM）的视觉处理解构为物体识别和空间感知两个独立部分进行研究，提出了基于视觉解码器的token压缩算法和RoPE缩放技术来提升解码效率和空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM通过序列化图像处理视觉信息，这与人类视觉的并行特性存在显著差异，且其不透明的内部机制阻碍了深入理解和架构创新。受人类视觉'what'和'where'双通路的启发，需要解构VLM的视觉处理机制。

Method: 1）将图像转换为文本token映射图研究物体识别；2）理论推导和实证验证VLM中位置表示的几何结构；3）提出基于即插即用视觉解码器的指令无关token压缩算法；4）引入RoPE缩放技术增强空间推理。

Result: 研究发现VLM对图像内容的感知呈现从浅层到深层的两阶段过程：从属性识别开始，最终实现语义消歧。同时验证了VLM位置表示背后的几何结构。

Conclusion: 通过严格实验验证了分析结果，为深入理解VLM内部机制提供了新视角，并为设计更强大的未来架构提供了明确原则。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [220] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种无需视觉编码器的文本到文本检索方法，通过VLLM生成结构化图像描述来替代传统文本到图像检索，显著减少了模态差距并提高了组合性。


<details>
  <summary>Details</summary>
Motivation: 解决对比学习视觉语言模型（如CLIP）存在的浅层语言理解、模态差距问题，以及大规模网络数据训练带来的计算成本和隐私担忧。

Method: 采用无视觉的单编码器检索流程，将文本到图像检索范式迁移到文本到文本范式，利用VLLM生成结构化图像描述，仅需少量GPU时间进行校准。

Result: 无视觉检索器在多个检索和组合性基准测试中达到最先进的零样本性能，甚至超越传统多模态模型，且模型参数仅需0.3B。

Conclusion: 该方法提供了一种更隐私友好、计算效率高的检索替代方案，证明了在检索任务中视觉编码器并非必需，并发布了新的组合性基准测试subFlickr和subCOCO。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [221] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 本文研究了对比视觉语言模型（VLMs）中组合性与长标题理解之间的双向关系，发现两者可以相互促进，但需要高质量数据和适当的模型设计。


<details>
  <summary>Details</summary>
Motivation: 现有的对比视觉语言模型在绑定视觉和文本信息方面取得了显著进展，但理解长而密集的标题仍然是一个开放挑战。作者假设组合性（即推理对象-属性绑定和对象间关系的能力）是理解较长标题的关键。

Method: 训练和评估了一系列针对组合性和长标题理解能力的模型，研究了不同训练策略（如冻结位置嵌入）和数据质量对模型性能的影响。

Result: 研究发现组合性训练能提高长标题检索性能，而长标题训练也能促进组合性。但这些增益对数据质量和模型设计敏感，低质量数据或有限参数更新会限制泛化能力。

Conclusion: 组合性理解和长标题理解是相互交织的能力，可以通过在密集、有基础描述的文本上进行训练来共同学习。高质量的长标题数据训练可以使模型在两项任务上都取得强劲表现。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [222] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出了一种利用合成RGB图像、少量真实标注和GAN跨模态对齐来增强热图像语义分割的框架，显著提升了复杂田间环境中植物分割性能。


<details>
  <summary>Details</summary>
Motivation: 热图像中植物分割在户外高通量表型分析中面临挑战，主要是植物与杂草对比度低和频繁遮挡导致性能不佳。

Method: 使用1,128张合成图像训练模型生成作物和杂草分割掩码，结合少量真实标注图像，采用CycleGAN-turbo进行RGB到热图像的跨模态对齐。

Result: 结合所有合成图像和少量真实图像，杂草类和植物类的分割性能相比全真实数据基线分别提升了22%和17%。

Conclusion: 合成数据与有限手动标注结合，通过生成模型进行跨域翻译，能显著提升复杂田间环境中多模态图像的语义分割性能。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [223] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种持续学习框架来解决数字人脸伪造检测问题，通过发展性混合专家架构来适应不断演变的伪造技术，同时避免遗忘已学习的伪造类型。


<details>
  <summary>Details</summary>
Motivation: 随着数字人脸生成和篡改技术的快速发展，现有的检测模型难以跟上技术演变的步伐。需要一种能够快速适应新伪造类型、同时保留已有知识的检测方法。

Method: 采用发展性混合专家架构，使用LoRA模型作为专家，分为Real-LoRA学习真实人脸知识，以及多个Fake-LoRA捕获不同伪造类型的增量信息。通过正交梯度和正交损失防止梯度干扰和灾难性遗忘。

Result: 在数据集和篡改类型增量协议下的实验结果表明，该方法在持续学习场景下具有有效性。

Conclusion: 该方法成功地将人脸伪造检测构建为持续学习问题，能够有效应对不断演变的伪造技术挑战。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [224] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型，支持图像理解和生成任务，具备物体定位、图像编辑和高分辨率图像合成等新能力，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态扩散语言模型仅支持简单的图像级理解任务和低分辨率图像生成，缺乏高级功能如物体定位和高质量图像编辑。

Method: 采用弹性混合Transformer架构、通用文本条件化和分层采样等新技术，通过规划和迭代自反思利用理解能力提升图像生成和编辑效果。

Result: 在RefCOCO物体定位、GenEval文本到图像生成和ImgEdit图像编辑等基准测试中超越现有自回归和连续扩散模型，如Qwen2.5-VL和FluxKontext-dev，并在推理时显著加速。

Conclusion: Lavida-O是首个统一的掩码扩散模型，通过结合理解与生成能力，实现了高效的图像理解和高质量图像生成，为多模态AI提供了新的解决方案。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [225] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 本文提出了基于概念的视频相似性估计任务ConViS，通过预定义的关键语义概念计算可解释的相似性分数，使视频比较更接近人类思维方式。同时构建了ConViS-Bench基准数据集，并评估了现有模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视频相似性评估通常依赖全局相似性分数，无法像人类那样从多个不同维度（如动作、地点等）进行细粒度比较。大型多模态模型的出现为利用自然语言进行视频比较任务提供了新机遇。

Method: 1. 定义ConViS任务：基于预定义语义概念计算视频对的可解释相似性分数
2. 构建ConViS-Bench基准：包含精心标注的视频对，涵盖多个领域，每个视频对都有概念级相似性分数和文本描述
3. 评估现有SOTA模型在该任务上的表现

Result: 不同模型在ConViS任务上表现出显著性能差异，某些概念对视频相似性估计更具挑战性。研究揭示了模型与人类判断之间的对齐程度。

Conclusion: ConViS-Bench将成为语言驱动视频理解研究的重要资源，基于概念的相似性评估方法能够实现更接近人类推理的视频比较，并支持概念条件视频检索等新应用。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [226] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 本文提出了一种新的图放射组学学习（GrRAiL）描述符，用于在临床MRI扫描中表征病灶内异质性，通过图论方法量化空间关联，在区分肿瘤复发与放射性效应方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决实体肿瘤影像学中可靠区分恶性病变与混淆病理的挑战，传统放射组学方法常忽略复杂空间关系，需要更有效的方法来捕捉病灶内异质性。

Method: GrRAiL方法分两步：(1)使用基于体素的放射组学测量识别亚区域聚类，(2)计算图论指标来量化聚类间的空间关联，生成加权图编码高阶空间关系。

Result: 在947名受试者的多中心研究中，GrRAiL在三个应用场景中均显著优于现有方法：胶质母细胞瘤（测试准确率78%，提升>10%）、脑转移瘤（测试准确率74%，提升>13%）和胰腺IPMN风险分层（测试准确率75%，提升>10%）。

Conclusion: GrRAiL能够可靠捕捉病灶内异质性，有效区分恶性肿瘤与混淆病理，在临床MRI应用中表现出优越性能和可行性。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [227] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS是第一个仅使用自我中心视觉来感知环境和导航的人类化身系统，通过将低级运动技能学习与基于视觉输入的高级控制学习解耦，实现了人类化的人类运动生成。


<details>
  <summary>Details</summary>
Motivation: 当前的人类运动生成方法忽视了感知与运动之间的相互依赖关系，使用与人类感知方式截然不同的任务特定"感知"。作者认为生成类人化身行为需要类人感知。

Method: 首先在大规模运动捕捉数据集上训练运动先验模型，然后使用Q学习训练策略，将自我中心视觉输入映射到运动先验的高级控制命令。

Result: 实验证明自我中心视觉能够使化身产生类人运动特征，例如化身会避开其视野中的障碍物行走。

Conclusion: 为化身配备类人传感器（特别是自我中心视觉）有望训练出行为像人类的化身。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [228] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 论文分析了布局到图像生成中边界框重叠问题的挑战，提出了OverLayScore指标和OverLayBench基准，并开发了CreatiLayout-AM模型来改善重叠区域的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法在处理边界框显著重叠的布局时表现不佳，特别是在大重叠区域和语义区分度小的重叠实例上。现有基准存在偏向简单情况的偏差。

Method: 提出了OverLayScore指标量化重叠复杂度，构建了OverLayBench基准，并开发了基于细调模态掩码数据集的CreatiLayout-AM模型。

Result: 分析显示现有基准偏向低OverLayScore的简单情况，新基准提供了更平衡的评估分布，模型在复杂重叠场景下表现有所改善。

Conclusion: 该研究为在现实和挑战性场景下实现更鲁棒的布局到图像生成奠定了基础。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [229] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识提取到显式的3D高斯溅射表示中，无需多视图训练数据即可生成3D场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖真实世界的多视图数据，但这些数据并不总是容易获得。视频扩散模型具有强大的想象力，但其2D特性限制了在需要与环境交互的模拟应用中的使用。

Method: 在典型的RGB解码器基础上增加3D高斯溅射解码器，该解码器由RGB解码器的输出进行监督训练。框架可以使用视频扩散模型生成的合成数据进行纯训练，支持从文本提示或单张图像实时渲染3D场景，并可扩展到从单目输入视频生成动态3D场景。

Result: 实验结果表明，该框架在静态和动态3D场景生成方面达到了最先进的性能。

Conclusion: 提出的自蒸馏框架成功地将视频扩散模型的3D知识提取到3D高斯溅射表示中，为无需多视图数据的3D场景生成提供了有效解决方案。

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [230] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视图前馈3D高斯溅射方法，用体素对齐的高斯预测取代传统的像素对齐方法，解决了像素对齐的局限性，实现了更好的多视图一致性和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯溅射方法主要依赖像素对齐的高斯预测范式，但这种方法存在多个固有局限性：重建的3D模型严重依赖输入视图数量、产生视图偏置的密度分布、在遮挡或低纹理区域引入对齐误差。

Method: VolSplat采用体素对齐的高斯预测方法，直接从预测的3D体素网格预测高斯，避免了像素对齐对易出错的2D特征匹配的依赖，同时支持基于3D场景复杂度的自适应高斯密度控制。

Result: 在RealEstate10K和ScanNet等基准测试上，VolSplat实现了最先进的性能，产生了更合理和视图一致的高斯重建，具有更好的几何一致性和新颖视图渲染质量。

Conclusion: VolSplat不仅提供了优越的结果，还为前馈3D重建建立了一个更可扩展的框架，具有更密集和更鲁棒的表示，为更广泛社区的研究铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [231] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow是一种条件感知重参数化方法，通过轻量级学习偏移来缩短概率路径，提高条件生成模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散和流匹配方法需要模型同时学习质量传输和条件注入，这增加了模型的学习负担。为了减轻模型负担，作者提出了条件感知重参数化方法。

Method: CAR-Flow通过学习轻量级的偏移来重新定位源分布、目标分布或两者的位置，从而缩短模型需要学习的概率路径。该方法在Flow Matching框架中引入条件感知机制。

Result: 在低维合成数据上可视化和量化了CAR的效果。在ImageNet-256数据集上，将CAR-Flow应用于SiT-XL/2模型，FID从2.07降低到1.68，仅引入不到0.6%的额外参数。

Conclusion: CAR-Flow通过条件感知重参数化有效缩短了概率路径，显著提高了条件生成模型的训练效率和生成质量，同时保持了参数效率。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [232] [ExtGraph: A Fast Extraction Method of User-intended Graphs from a Relational Database](https://arxiv.org/abs/2509.18534)
*Jeongho Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: 提出了一种高效的图提取方法ExtGraph，通过外连接和物化视图的混合查询处理，能够高效提取用户意图的图结构


<details>
  <summary>Details</summary>
Motivation: 企业重要数据通常存储在关系数据库中，需要从关系数据库中提取图结构进行分析，但现有方法由于复杂的连接查询处理往往无法准确提取用户意图的图

Method: 采用外连接和物化视图的混合查询处理方法，设计ExtGraph算法来高效提取图结构

Result: 在TPC-DS、DBLP和IMDB数据集上的实验表明，ExtGraph在图提取时间上比现有最优方法快2.78倍

Conclusion: ExtGraph方法能够有效解决从关系数据库中提取用户意图图结构的问题，具有显著的性能优势

Abstract: Graph analytics is widely used in many fields to analyze various complex
patterns. However, in most cases, important data in companies is stored in
RDBMS's, and so, it is necessary to extract graphs from relational databases to
perform graph analysis. Most of the existing methods do not extract a
user-intended graph since it typically requires complex join query processing.
We propose an efficient graph extraction method, \textit{ExtGraph}, which can
extract user-intended graphs efficiently by hybrid query processing of outer
join and materialized view. Through experiments using the TPC-DS, DBLP, and
IMDB datasets, we have shown that \textit{ExtGraph} outperforms the
state-of-the-art methods up to by 2.78x in terms of graph extraction time.

</details>


### [233] [CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases](https://arxiv.org/abs/2509.18670)
*Yeonwoo Jeong,Hyunji Cho,Kyuri Park,Youngjae Kim,Sungyong Park*

Main category: cs.DB

TL;DR: CALL是一种上下文感知的查询分组机制，通过基于共享集群访问模式组织查询来减少缓存未命中惩罚，从而显著降低搜索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入模型将不同查询映射到向量空间的相似区域，导致现代基于磁盘的向量数据库中出现非均匀的集群访问模式。现有方法优化单个查询但忽略了集群访问模式的影响，未能考虑访问相似集群的查询的局部性效应，从而增加了缓存未命中惩罚。

Method: 提出CALL机制：1）上下文感知查询分组，基于共享集群访问模式组织查询；2）组感知预取方法，在查询组间转换时最小化缓存未命中；3）延迟感知集群加载。

Result: 实验结果显示，CALL将第99百分位尾部延迟降低了高达33%，同时持续保持更高的缓存命中率，显著减少了搜索延迟。

Conclusion: CALL通过考虑查询间的集群访问模式局部性，有效优化了向量数据库的查询性能，特别是在减少尾部延迟方面表现出色。

Abstract: Embedding models capture both semantic and syntactic structures of queries,
often mapping different queries to similar regions in vector space. This
results in non-uniform cluster access patterns in modern disk-based vector
databases. While existing approaches optimize individual queries, they overlook
the impact of cluster access patterns, failing to account for the locality
effects of queries that access similar clusters. This oversight increases cache
miss penalty. To minimize the cache miss penalty, we propose CALL, a
context-aware query grouping mechanism that organizes queries based on shared
cluster access patterns. Additionally, CALL incorporates a group-aware
prefetching method to minimize cache misses during transitions between query
groups and latency-aware cluster loading. Experimental results show that CALL
reduces the 99th percentile tail latency by up to 33% while consistently
maintaining a higher cache hit ratio, substantially reducing search latency.

</details>


### [234] [Teaching RDM in a smart advanced inorganic lab course and its provision in the DALIA platform](https://arxiv.org/abs/2509.18902)
*Alexander Hoffmann,Jochen Ortmeyer,Fabian Fink,Charles Tapley Hoyt,Jonathan D. Geiger,Paul Kehrein,Torsten Schrade,Sonja Herres-Pawlis*

Main category: cs.DB

TL;DR: 该论文介绍了一个在化学本科实验课程中引入电子实验室笔记本Chemotion来教授研究数据管理（RDM）和FAIR数据原则的教学实践。


<details>
  <summary>Details</summary>
Motivation: 传统的手写实验记录和PDF存储方式限制了数据的重用和机器学习应用，需要培养学生掌握研究数据管理这一关键数据素养技能。

Method: 在RWTH亚琛大学第五学期实验课程中使用开源电子实验室笔记本Chemotion，结合研讨会和在线培训视频，让学生数字化规划、记录和评估实验。

Result: 学生能够通过Chemotion的直观界面和存储库实现可持续的数据共享，确保元数据和分析结果的长期重用。

Conclusion: 电子实验室笔记本Chemotion是有效教授研究数据管理和FAIR原则的教学工具，DALIA平台可作为学生的发现工具。

Abstract: Research data management (RDM) is a key data literacy skill that chemistry
students must acquire. Concepts such as the FAIR data principles (Findable,
Accessible, Interoperable, Reusable) should be taught and applied in
undergraduate studies already. Traditionally, research data from labs, theses,
and internships were handwritten and stored in inaccessible formats such as
PDFs, limiting reuse and machine learning applications. At RWTH Aachen
University, a fifth-semester lab course introduces students to the electronic
laboratory notebook (ELN) Chemotion, an open-source DFG-funded tool linked to
the national NFDI4Chem initiative. Students plan, document, and evaluate
experiments digitally, ensuring metadata and analysis are captured for
long-term reuse. Chemotion's intuitive interface and repository enable
sustainable data sharing. To reinforce RDM, students receive a seminar and
access to online training videos with interactive Moodle elements. Herein we
highlight the use of the DALIA platform as a discovery tool for the students.

</details>


### [235] [A decentralized future for the open-science databases](https://arxiv.org/abs/2509.19206)
*Gaurav Sharma,Viorel Munteanu,Nika Mansouri Ghiasi,Jineta Banerjee,Susheel Varma,Luca Foschini,Kyle Ellrott,Onur Mutlu,Dumitru Ciorbă,Roel A. Ophoff,Viorel Bostan,Christopher E Mason,Jason H. Moore,Despoina Sousoni,Arunkumar Krishnan,Christopher E. Mason,Mihai Dimian,Gustavo Stolovitzky,Fabio G. Liberante,Taras K. Oleksyk,Serghei Mangul*

Main category: cs.DB

TL;DR: 本文分析了集中式生物数据存储库的脆弱性，提出了采用联邦和去中心化架构的混合框架来增强科学数据基础设施的韧性，确保数据的长期完整性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 集中式数据存储库存在单点故障风险，容易受到网络攻击、技术故障、自然灾害以及资金和政治不确定性的影响，这可能导致数据不可用、丢失或完整性受损，阻碍科学进步。

Method: 通过分析集中式存储库的结构局限性，评估联邦和去中心化模型，提出一个混合框架，结合联邦和去中心化架构，以实现韧性、FAIR（可查找、可访问、可互操作、可重用）和可持续的科学数据管理。

Result: 提出的混合框架能够显著减少对治理不稳定、基础设施脆弱性和资金波动的暴露，同时促进公平和全球可访问性。

Conclusion: 开放科学的未来依赖于整合联邦和去中心化方法，建立全球分布、经济可持续和制度稳健的基础设施，将科学数据作为公共产品加以保护，确保其长期可访问性、互操作性和保存。

Abstract: Continuous and reliable access to curated biological data repositories is
indispensable for accelerating rigorous scientific inquiry and fostering
reproducible research. Centralized repositories, though widely used, are
vulnerable to single points of failure arising from cyberattacks, technical
faults, natural disasters, or funding and political uncertainties. This can
lead to widespread data unavailability, data loss, integrity compromises, and
substantial delays in critical research, ultimately impeding scientific
progress. Centralizing essential scientific resources in a single geopolitical
or institutional hub is inherently dangerous, as any disruption can paralyze
diverse ongoing research. The rapid acceleration of data generation, combined
with an increasingly volatile global landscape, necessitates a critical
re-evaluation of the sustainability of centralized models. Implementing
federated and decentralized architectures presents a compelling and
future-oriented pathway to substantially strengthen the resilience of
scientific data infrastructures, thereby mitigating vulnerabilities and
ensuring the long-term integrity of data. Here, we examine the structural
limitations of centralized repositories, evaluate federated and decentralized
models, and propose a hybrid framework for resilient, FAIR, and sustainable
scientific data stewardship. Such an approach offers a significant reduction in
exposure to governance instability, infrastructural fragility, and funding
volatility, and also fosters fairness and global accessibility. The future of
open science depends on integrating these complementary approaches to establish
a globally distributed, economically sustainable, and institutionally robust
infrastructure that safeguards scientific data as a public good, further
ensuring continued accessibility, interoperability, and preservation for
generations to come.

</details>


### [236] [Gate-Based and Annealing-Based Quantum Algorithms for the Maximum K-Plex Problem](https://arxiv.org/abs/2509.19214)
*Xiaofan Li,Gao Cong,Rui Zhou*

Main category: cs.DB

TL;DR: 本文研究了最大k-plex问题，提出了两种量子算法模型：基于门的qTKP和qMKP算法实现O*(1.42^n)时间复杂度，以及首个基于退火的qaMKP近似算法。


<details>
  <summary>Details</summary>
Motivation: k-plex模型作为团模型的松弛，更适合分析现实世界中存在噪声和不完整数据的图结构。最大k-plex问题在社交网络分析、社区检测等领域有重要应用，但现有经典算法时间复杂度较高。

Method: 提出了两种量子算法：1）基于门的qTKP算法，结合量子搜索与图编码、度计算等技术；qMKP算法使用二分搜索逐步寻找最大解；2）基于退火的qaMKP算法，将问题重新表述为二次无约束二进制优化问题。

Result: qTKP和qMKP算法将时间复杂度从经典算法的O*(c_k^n)（c_k>1.94）降低到O*(1.42^n)。qaMKP算法在量子比特资源利用上比基于门的算法更高效。实验在IBM量子模拟器和D-Wave量子计算机上验证了性能。

Conclusion: 量子算法显著提升了最大k-plex问题的求解效率，该工作可推广到其他团松弛问题如n-clan和n-club，展示了量子计算在图分析问题中的潜力。

Abstract: The $ k $-plex model, which allows each vertex to miss connections with up to
$ k $ neighbors, serves as a relaxation of the clique. Its adaptability makes
it more suitable for analyzing real-world graphs where noise and imperfect data
are common and the ideal clique model is often impractical. The problem of
identifying the maximum $ k $-plex (MKP, which is NP-hard) is gaining attention
in fields such as social network analysis, community detection, terrorist
network identification, and graph clustering. Recent works have focused on
optimizing the time complexity of MKP algorithms. The state-of-the-art has
reduced the complexity from a trivial $ O^*(2^n) $ to $ O^*(c_k^n) $, with $
c_k > 1.94 $ for $ k \geq 3 $, where $ n $ denotes the vertex number. This
paper investigates the MKP using two quantum models: gate-based model and
annealing-based model. Two gate-based algorithms, qTKP and qMKP, are proposed
to achieve $ O^*(1.42^n) $ time complexity. qTKP integrates quantum search with
graph encoding, degree counting, degree comparison, and size determination to
find a $ k $-plex of a given size; qMKP uses binary search to progressively
identify the maximum solution. Furthermore, by reformulating MKP as a quadratic
unconstrained binary optimization problem, we propose qaMKP, the first
annealing-based approximation algorithm, which utilizes qubit resources more
efficiently than gate-based algorithms. To validate the practical performance,
proof-of-principle experiments were conducted using the latest IBM gate-based
quantum simulator and D-Wave adiabatic quantum computer. This work holds
potential to be applied to a wide range of clique relaxations, e.g., $ n $-clan
and $ n $-club.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [237] [Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation](https://arxiv.org/abs/2509.18472)
*Atanu Barai,Kamalavasan Kamalakkannan,Patrick Diehl,Maxim Moraru,Jered Dominguez-Trujillo,Howard Pritchard,Nandakishore Santhi,Farzad Fatollahi-Fard,Galen Shipman*

Main category: cs.DC

TL;DR: 本文评估了FireSim框架在RISC-V处理器上的性能预测准确性，通过比较模拟结果与真实硬件性能，发现虽然FireSim能提供有价值的架构趋势分析，但模拟与实测运行时间存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V处理器在高性能计算领域的应用潜力增加，需要评估开源FPGA加速模拟框架FireSim的性能预测准确性，但目前缺乏对FireSim与物理硬件比较的系统性研究。

Method: 研究通过FireSim模拟商业单板计算机和桌面级RISC-V CPU，首先测量基准测试在单核和四核配置下的性能，然后使用代表性迷你应用和LAMMPS分子动力学代码评估性能。

Result: 研究发现FireSim能提供架构性能趋势的有价值见解，但模拟运行时间与实测结果存在偏差，这些偏差源于模拟环境的固有局限性和CPU制造商详细性能规格的有限可用性。

Conclusion: FireSim是RISC-V架构探索的有用工具，但需要更精确的配置匹配和考虑模拟环境限制，以提高性能预测的准确性。

Abstract: RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.

</details>


### [238] [6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks](https://arxiv.org/abs/2509.18735)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 6G Twin是首个端到端AI原生无线接入网络设计，集成了神经高斯无线电场用于压缩信道状态信息获取、持续信道预测与切换持久性、以及能量最优非线性预编码器。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAN中高开销的信道估计、移动性下的信道预测不稳定性、以及能量效率低下的问题，为6G网络提供实时、高效、节能的解决方案。

Method: 采用神经高斯无线电场替代密集导频减少开销；使用重放驱动的持续学习器维持移动性和小区切换下的准确性；设计minPMAC预编码器通过凸优化实现全局最优能量最小化。

Result: 导频开销降低约100倍，推理时间1.1毫秒，现场训练少于2分钟；信道NMSE比冻结预测器改善超过10dB；能量消耗降低4-10倍，在相同功率下数据速率提升高达5倍。

Conclusion: 6G Twin提供了一个实用的GPU就绪框架，在3GPP标准设置下实现了实时CSI、动态网络中的鲁棒跟踪与高效切换，以及最先进的吞吐量-能量权衡。

Abstract: This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.

</details>


### [239] [On The Reproducibility Limitations of RAG Systems](https://arxiv.org/abs/2509.18869)
*Baiqiang Wang,Dongfang Zhao,Nathan R Tallent,Luanzheng Guo*

Main category: cs.DC

TL;DR: ReproRAG是一个用于系统测量和量化基于向量的检索系统可重现性的综合基准测试框架，旨在解决RAG系统中检索组件非确定性导致的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: RAG在生成式AI驱动的科学工作流中应用日益广泛，但其可靠性经常因检索组件的非确定性而受到损害，需要系统评估可重现性。

Method: 开发ReproRAG框架，研究整个流程中的不确定性来源，包括嵌入模型、精度、检索算法、硬件配置和分布式执行环境，使用精确匹配率、Jaccard相似度和Kendall's Tau等指标。

Result: 大规模实证研究显示，不同嵌入模型对RAG可重现性有显著影响，框架有效揭示了可重现性与性能之间的权衡关系。

Conclusion: 开源的ReproRAG框架为研究人员和工程师提供了验证部署、基准测试可重现性和做出明智设计决策的工具，从而促进更值得信赖的科学AI。

Abstract: Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.

</details>


### [240] [TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning](https://arxiv.org/abs/2509.18957)
*Shengye Song,Minxian Xu,Kan Hu,Wenxia Guo,Kejiang Ye*

Main category: cs.DC

TL;DR: TD3-Sched是一个基于TD3强化学习的分布式调度器，用于云边系统中的资源调度，在动态工作负载下实现优化的CPU和内存分配决策。


<details>
  <summary>Details</summary>
Motivation: 云边系统中的资源调度具有挑战性，边缘节点运行延迟敏感的工作负载且资源受限，而现有的集中式调度器存在性能瓶颈和用户体验下降的问题。

Method: 提出了TD3-Sched，一个基于Twin Delayed Deep Deterministic Policy Gradient（TD3）的分布式强化学习调度器，用于连续控制CPU和内存分配。

Result: 在真实的云边测试平台上，TD3-Sched相比其他强化学习和基于规则的基线方法，在相同负载下延迟降低17.9%至38.6%，在高负载下降低16%至31.6%，SLO违规率仅为0.47%。

Conclusion: TD3-Sched在容器化云边环境中表现出更快的收敛速度、更低的延迟和更稳定的性能，同时保持了服务质量。

Abstract: Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.

</details>


### [241] [Scheduler-Driven Job Atomization](https://arxiv.org/abs/2509.19086)
*Michal Konopa,Jan Fesl,Ladislav Beránek*

Main category: cs.DC

TL;DR: 提出Scheduler-Driven Job Atomization (SJA)新范式，通过调度器与作业的双向交互，将作业分解为适合GPU执行间隙的子作业，提高GPU集群利用率


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群（特别是基于NVIDIA MIG架构）存在效率低下问题，作业被视为刚性不可分割块，依赖静态峰值内存估计导致碎片化、利用率低和作业拒绝

Method: SJA建立调度器与作业的双向交互：调度器公布可用执行间隙，作业响应表示能否生成适合该时间-容量窗口的子作业，调度器基于分配策略选择作业并生成安全自包含的子作业

Result: 该方法避免迁移或抢占的成本，在执前主动调整工作负载，提高GPU利用率，减少等待时间，最小化迁移开销

Conclusion: 本文作为概念论文介绍了SJA范式，定义了其构建模块并概述了未来研究方向，而非提供完整的实验评估

Abstract: Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.

</details>


### [242] [In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns](https://arxiv.org/abs/2509.19150)
*Harikrishna Tummalapalli,Riccardo Balin,Christine M. Simpson,Andrew Park,Aymen Alsaadi,Andrew E. Shao,Wesley Brewer,Shantenu Jha*

Main category: cs.DC

TL;DR: SimAI-Bench是一个用于原型设计和评估耦合AI-仿真工作流的工具，在Aurora超级计算机上测试了两种常见模式的数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 随着耦合AI-仿真工作流在HPC设施中成为主要工作负载，其复杂性不断增加，需要新的性能分析和原型设计工具。

Method: 使用SimAI-Bench工具在Aurora超级计算机上对两种模式进行基准测试：一对一工作流（仿真与AI训练实例共置）和多对一工作流（从仿真集合训练单个AI模型）。

Result: 对于一对一模式，节点本地和DragonHPC数据暂存策略比Redis和Lustre文件系统性能更好；对于多对一模式，随着集合规模增大，数据传输成为主要瓶颈，文件系统是最佳解决方案。

Conclusion: SimAI-Bench能够有效评估不同耦合工作流模式的数据传输性能，为HPC设施中的AI-仿真工作流优化提供了重要工具支持。

Abstract: Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.

</details>


### [243] [Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings](https://arxiv.org/abs/2509.19187)
*Jérémie Chalopin,Yi-Jun Chang,Lyuting Chen,Giuseppe A. Di Luna,Haoran Zhou*

Main category: cs.DC

TL;DR: 本文研究了面向环网络中在内容不可知异步消息传递系统下的领导者选举问题，分析了消息复杂度，并提出了在不同设置下的优化算法。


<details>
  <summary>Details</summary>
Motivation: 研究在消息内容可能被对手任意篡改的内容不可知模型下，环网络中领导者选举的消息复杂度问题，特别是探索在进程只能发送常数数量消息的限制下是否可行。

Method: 通过理论分析证明在均匀设置下无法解决领导者选举问题，然后提出非均匀设置下的算法，包括基于最小标识符的算法和基于对数复杂度的算法，以及匿名设置下的随机算法。

Result: 证明了均匀算法在常数消息限制下不可行，提出了非均匀算法实现O(n·U·ID_min)的消息复杂度，以及匿名随机算法实现O(log²U)的消息复杂度。

Conclusion: 领导者选举的消息复杂度高度依赖于系统设置，非均匀和匿名设置可以显著降低消息复杂度，对数依赖是最优的。

Abstract: We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.

</details>


### [244] [Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole](https://arxiv.org/abs/2509.19294)
*Jenny Lynn Almerol,Elisabetta Boella,Mario Spera,Daniele Gregori*

Main category: cs.DC

TL;DR: RISC-V加速器在科学计算中表现出色，天体物理N体代码在Wormhole n300卡上实现2倍加速和2倍节能


<details>
  <summary>Details</summary>
Motivation: 虽然RISC-V最初为AI工作负载设计，但其在高性能科学计算领域也展现出潜力，需要验证其在具体科学应用中的表现

Method: 在Tenstorrent开发的RISC-V架构Wormhole n300卡上加速天体物理N体代码，并与高度优化的CPU实现进行对比

Result: 该平台在天体物理模拟中具有高度竞争力，相比优化CPU实现提供超过2倍的加速和约2倍的节能效果

Conclusion: RISC-V架构的加速器平台对于采用此类算法的天体物理模拟具有显著优势，展示了其在科学计算领域的应用前景

Abstract: Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [245] [LIFY: IoT System for Monitoring Vital Signs of Elderly People](https://arxiv.org/abs/2509.18411)
*Sara Gonzalez,Martin Vasquez,Wilder Castellanos*

Main category: cs.NI

TL;DR: 本文描述了一个为老年护理机构开发的生理信号监测系统，该系统通过智能设备采集体温、心率和血氧数据，并通过云平台和Telegram实现实时监控和警报功能。


<details>
  <summary>Details</summary>
Motivation: 改善老年护理机构中老年人生理信号的记录和监测，提供更有效的健康管理解决方案。

Method: 开发配备体温、心率和血氧传感器的智能设备，建立互联网连接将数据传输到云平台，创建可视化仪表板并集成Telegram即时通讯系统进行警报通知。

Result: 成功实现了实时生理信号监测系统，具备用户管理、数据可视化和个性化警报功能。

Conclusion: 该技术解决方案能够有效改善老年护理机构的生理信号监测效率，为老年人健康管理提供了实用的技术支撑。

Abstract: This article describes the implementation of a technological solution aimed
at improving the recording of physiological signals in the elderly population
residing in geriatric facilities. The developed system consists of a smart
device equipped with sensors for body temperature, heart rate, and blood oxygen
levels. This device establishes an Internet connection to transmit data to a
cloud-based platform for storage. Within this platform, a dashboard has been
created to visualize real-time values captured by the sensors, along with
additional functionalities such as user management and the configuration of
personalized alerts, which are transmitted to the solution's users through the
instant messaging system called Telegram.

</details>


### [246] [5GC-Bench: A Framework for Stress-Testing and Benchmarking 5G Core VNFs](https://arxiv.org/abs/2509.18443)
*Ioannis Panitsas,Tolga O. Atalay,Dragoslav Stojadinovic,Angelos Stavrou,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 5GC-Bench是一个模块化框架，用于在真实工作负载下对5G核心网进行压力测试，能够联合仿真信令和服务流量，支持VNF性能分析和端到端服务链分析。


<details>
  <summary>Details</summary>
Motivation: 5G核心网的云原生解耦设计带来了灵活性和可扩展性，但也引入了显著挑战。控制面过程涉及多个VNF的复杂交互，而用户面需要维持多样化和资源密集型的流量。现有工具往往孤立地评估这些维度，依赖合成工作负载，或缺乏细粒度资源使用可见性。

Method: 开发了5GC-Bench框架，集成了OpenAirInterface 5GC，并在真实5G测试平台上部署。该框架能够联合仿真信令和服务流量，支持VNF性能分析和端到端服务链分析。

Result: 5GC-Bench能够发现资源约束并揭示在模拟实际5G部署场景下的跨VNF依赖关系，为容量规划和性能优化提供可操作的见解。

Conclusion: 该框架为5G核心网的压力测试提供了有效的解决方案，所有相关组件已公开发布以促进可重复性和进一步研究。

Abstract: The disaggregated, cloud-native design of the 5G Core (5GC) enables
flexibility and scalability but introduces significant challenges.
Control-plane procedures involve complex interactions across multiple Virtual
Network Functions (VNFs), while the user plane must sustain diverse and
resource-intensive traffic. Existing tools often benchmark these dimensions in
isolation, rely on synthetic workloads, or lack visibility into fine-grained
resource usage. This paper presents 5GC-Bench, a modular framework for
stress-testing the 5GC under realistic workloads. 5GC-Bench jointly emulates
signaling and service traffic, supporting both VNF profiling and end-to-end
service-chain analysis. By characterizing bottlenecks and resource demands, it
provides actionable insights for capacity planning and performance
optimization. We integrated 5GC-Bench with the OpenAirInterface (OAI) 5GC and
deployed it on a real 5G testbed, demonstrating its ability to uncover resource
constraints and expose cross-VNF dependencies under scenarios that mirror
operational 5G deployments. To foster reproducibility and further research, we
release publicly all the artifacts.

</details>


### [247] [Using Age of Information for Throughput Optimal Spectrum Sharing](https://arxiv.org/abs/2509.18465)
*Hongjae Nam,Vishrant Tripathi,David J. Love*

Main category: cs.NI

TL;DR: 该论文研究频谱共享问题，提出基于Whittle索引的调度策略，利用信息年龄(AoI)来优化次要用户(SU)的传输调度，在最大化吞吐量的同时最小化与主要用户(PU)的碰撞。


<details>
  <summary>Details</summary>
Motivation: 解决多信道频谱共享中次要用户需要同时最大化自身吞吐量和最小化与主要用户碰撞的挑战性问题，特别是在主要用户占用状态随时间变化的动态环境中。

Method: 1. 将多信道问题解耦为N个单信道问题；2. 证明存在最优阈值策略；3. 分析阈值策略结构建立索引性；4. 推导基于Whittle索引的调度策略，利用信道访问的信息年龄(AoI)；5. 扩展到跨信道相关的PU占用模型和未知马尔可夫转移矩阵的学习。

Result: 通过详细数值仿真验证了所提方法的性能提升，证明了基于AoI的Whittle索引策略在频谱共享场景中的有效性。

Conclusion: 提出的基于信息年龄的Whittle索引调度策略能够有效解决多信道频谱共享问题，在保证次要用户吞吐量的同时最小化碰撞，并且能够扩展到更复杂的信道相关性和未知参数场景。

Abstract: We consider a spectrum sharing problem where two users attempt to communicate
over N channels. The Primary User (PU) has prioritized transmissions and its
occupancy on each channel over time can be modeled as a Markov chain. The
Secondary User (SU) needs to determine which channels are free at each
time-slot and attempt opportunistic transmissions. The goal of the SU is to
maximize its own throughput, while simultaneously minimizing collisions with
the PU, and satisfying spectrum access constraints. To solve this problem, we
first decouple the multiple-channel problem into N single-channel problems. For
each decoupled problem, we prove that there exists an optimal threshold policy
that depends on the last observed PU occupancy and the freshness of this
occupancy information. Second, we establish the indexability of the decoupled
problems by analyzing the structure of the optimal threshold policy. Using this
structure, we derive a Whittle index-based scheduling policy that allocates SU
transmissions using the Age of Information (AoI) of accessed channels. We also
extend our insights to PU occupancy models that are correlated across channels
and incorporate learning of unknown Markov transition matrices into our
policies. Finally, we provide detailed numerical simulations that demonstrate
the performance gains of our approach.

</details>


### [248] [Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network Paths](https://arxiv.org/abs/2509.18519)
*Michael Luby,John Byers*

Main category: cs.NI

TL;DR: Whack-a-Mole是一种确定性数据包分发算法，用于在多条网络路径上分布数据包，具有可证明的紧致差异界限，特别适用于大规模分布式AI/ML训练和推理工作负载。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式AI/ML训练和推理工作负载对尾部延迟和传输不平衡高度敏感，集体完成时间（CCT）和有效训练时间比（ETTR）是关键性能指标。

Method: 算法将路径配置文件表示为m个选择单元在n条路径上的离散分配，使用位反转计数器为每个数据包选择路径。通过减少对降级路径的分配并将负载重新分配到更健康的路径来快速响应拥塞反馈。

Result: 证明在任何连续数据包序列中，每条路径的预期与实际数据包计数之间的差异界限为O(log m)。

Conclusion: Whack-a-Mole结合了确定性分布、低每包开销以及与擦除编码传输的兼容性，是构建旨在最小化CCT和最大化GPU利用率的多路径传输协议的有效基础模块。

Abstract: We present Whack-a-Mole, a deterministic packet spraying algorithm for
distributing packets across multiple network paths with provably tight
discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML
training and inference workloads, where collective completion time (CCT) and
effective training time ratio (ETTR) are highly sensitive to tail latency and
transport imbalance. Whack-a-Mole represents the path profile as a discrete
allocation of $m$ selection units across $n$ paths and uses a bit-reversal
counter to choose a path for each packet. We prove that the discrepancy between
expected and actual packet counts per path is bounded by $O(\log m)$ over any
contiguous packet sequence. The algorithm responds quickly to congestion
feedback by reducing allocations to degraded paths and redistributing load to
healthier ones. This combination of deterministic distribution, low per-packet
overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an
effective building block for multipath transport protocols that aim to minimize
CCT and maximize GPU utilization.

</details>


### [249] [Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.18545)
*Ioannis Panitsas,Tolga O. Atalay,Dragoslav Stojadinovic,Angelos Stavrou,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 本文提出了一种基于多智能体强化学习（MARL）的模块化框架，用于在异构多云环境中实现自主且接近最优的VNF放置，以解决5G网络切片部署中的资源分配挑战。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络采用云原生基础设施，网络切片在异构多云环境中的部署面临新的挑战，包括可变资源能力和切片特定需求。传统的组合优化方法计算复杂度高，难以满足实时决策需求。

Method: 开发了一个基于分解式多智能体强化学习（MARL）的模块化框架，结合真实流量配置文件来估计切片资源需求，使用MARL调度器在满足QoS约束的同时最小化部署成本。

Result: 在多云测试平台上的实验评估显示，与组合优化相比实现了19倍的加速，部署成本在最优解的7.8%以内。虽然在高负载下会产生最多2.42倍的QoS违规，但权衡后获得了显著更快的决策速度和降低的计算复杂度。

Conclusion: 基于MARL的方法为异构基础设施中的实时网络切片放置提供了可扩展且成本效益高的解决方案。

Abstract: Cellular networks are increasingly realized through software-based entities,
with core functions deployed as Virtual Network Functions (VNFs) on
Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key
enabler of 5G by providing logically isolated Quality of Service (QoS)
guarantees for diverse applications. With the adoption of cloud-native
infrastructures, the placement of network slices across heterogeneous
multi-cloud environments poses new challenges due to variable resource
capabilities and slice-specific requirements. This paper introduces a modular
framework for autonomous and near-optimal VNF placement based on a
disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework
incorporates real traffic profiles to estimate slice resource demands and
employs a MARL-based scheduler to minimize deployment cost while meeting QoS
constraints. Experimental evaluation on a multi-cloud testbed shows a 19x
speed-up compared to combinatorial optimization, with deployment costs within
7.8% of the optimal. While the method incurs up to 2.42x more QoS violations
under high load, the trade-off provides significantly faster decision-making
and reduced computational complexity. These results suggest that MARL-based
approaches offer a scalable and cost-efficient solution for real-time network
slice placement in heterogeneous infrastructures.

</details>


### [250] [Online Learning for Optimizing AoI-Energy Tradeoff under Unknown Channel Statistics](https://arxiv.org/abs/2509.18654)
*Mohamed A. Abd-Elmagid,Ming Shi,Eylem Ekici,Ness B. Shroff*

Main category: cs.NI

TL;DR: 本文研究了一个实时监控系统中源节点在信道统计信息未知的情况下，通过在线学习算法优化能量消耗与信息新鲜度（AoI）之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在已知信道统计信息的假设下优化了能量消耗与AoI性能的权衡，但实际应用中信道统计信息往往是未知的，需要开发能够在未知环境下有效工作的调度算法。

Method: 首先证明了在已知信道统计信息时最优调度策略具有基于AoI值的阈值结构，然后利用这一关键洞察开发了在线学习算法，该算法能够实现相对于时间范围长度的阶数最优遗憾（O(1)）。

Result: 提出的学习算法在信道统计信息未知的情况下，能够达到阶数最优的遗憾性能，即遗憾随时间的增长为常数级别。

Conclusion: 本文成功地将阈值结构的最优策略洞察应用于在线学习算法设计，在未知信道环境下实现了高效的实时监控调度。

Abstract: We consider a real-time monitoring system where a source node (with energy
limitations) aims to keep the information status at a destination node as fresh
as possible by scheduling status update transmissions over a set of channels.
The freshness of information at the destination node is measured in terms of
the Age of Information (AoI) metric. In this setting, a natural tradeoff exists
between the transmission cost (or equivalently, energy consumption) of the
source and the achievable AoI performance at the destination. This tradeoff has
been optimized in the existing literature under the assumption of having a
complete knowledge of the channel statistics. In this work, we develop online
learning-based algorithms with finite-time guarantees that optimize this
tradeoff in the practical scenario where the channel statistics are unknown to
the scheduler. In particular, when the channel statistics are known, the
optimal scheduling policy is first proven to have a threshold-based structure
with respect to the value of AoI (i.e., it is optimal to drop updates when the
AoI value is below some threshold). This key insight was then utilized to
develop the proposed learning algorithms that surprisingly achieve an
order-optimal regret (i.e., $O(1)$) with respect to the time horizon length.

</details>


### [251] [Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning](https://arxiv.org/abs/2509.18933)
*Gabriele Formis,Gianluca Cena,Lukasz Wisniewski,Stefano Scanzio*

Main category: cs.NI

TL;DR: 本文分析了基于机器学习技术的Wi-Fi链路质量预测模型，特别关注低复杂度实现的数据驱动模型，通过真实Wi-Fi测试床验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 无线通信的不确定性给通信质量一致性带来挑战，需要开发准确高效的预测模型来提升Wi-Fi在工业环境中的可靠性。

Method: 采用基于指数移动平均线性组合的数据驱动模型，设计用于低复杂度实现，适合处理资源有限的硬件平台。使用真实Wi-Fi测试床的实验数据评估性能，考虑信道相关和信道无关的训练数据。

Result: 信道无关模型表现出有竞争力的性能，允许设备制造商进行通用化训练。

Conclusion: 本研究为在工业环境中实际部署基于机器学习的预测模型以增强Wi-Fi可靠性提供了重要见解。

Abstract: Wireless communications are characterized by their unpredictability, posing
challenges for maintaining consistent communication quality. This paper
presents a comprehensive analysis of various prediction models, with a focus on
achieving accurate and efficient Wi-Fi link quality forecasts using machine
learning techniques. Specifically, the paper evaluates the performance of
data-driven models based on the linear combination of exponential moving
averages, which are designed for low-complexity implementations and are then
suitable for hardware platforms with limited processing resources. Accuracy of
the proposed approaches was assessed using experimental data from a real-world
Wi-Fi testbed, considering both channel-dependent and channel-independent
training data. Remarkably, channel-independent models, which allow for
generalized training by equipment manufacturers, demonstrated competitive
performance. Overall, this study provides insights into the practical
deployment of machine learning-based prediction models for enhancing Wi-Fi
dependability in industrial environments.

</details>


### [252] [Poster: The Internet Quality Barometer Framework](https://arxiv.org/abs/2509.19034)
*Lai Yi Ohlsen,Pavlos Sermpezis,Melissa Newcomb*

Main category: cs.NI

TL;DR: 提出了Internet Quality Barometer (IQB)框架，重新定义超越"速度"的互联网质量评估标准


<details>
  <summary>Details</summary>
Motivation: 传统互联网质量评估过于关注速度指标，需要更用户中心化的综合评估方法

Method: IQB通过三个步骤：(i)基于流行用例定义用户中心的质量标准；(ii)通过权重和质量阈值将网络需求映射到用例；(iii)利用公开互联网性能数据集计算综合IQB得分

Result: 开发出IQB评分系统，能够更全面反映互联网体验质量

Conclusion: IQB框架为互联网质量评估提供了更全面、用户导向的新方法

Abstract: In this paper, we introduce the Internet Quality Barometer (IQB), a framework
aiming to redefine Internet quality beyond ``speed''. IQB (i) defines Internet
quality in a user-centric way by considering popular use cases, (ii) maps
network requirements to use cases through a set of weights and quality
thresholds, and (iii) leverages publicly available Internet performance
datasets, to calculate the IQB score, a composite metric that reflects the
quality of Internet experience.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [253] [A Verified Compiler for Quantum Simulation](https://arxiv.org/abs/2509.18583)
*Liyi Li,Fenfen An,Federico Zahariev,Zhi Xiang Chong,Amr Sabry,Mark Gordon*

Main category: cs.PL

TL;DR: QBlue是一个用于哈密顿模拟的高层次、形式化验证的编译框架，基于二次量子化形式，提供类型系统确保正确性，并支持编译到数字和模拟量子电路。


<details>
  <summary>Details</summary>
Motivation: 现有的哈密顿模拟编译器通常基于低层次的泡利算符表示，限制了可编程性且缺乏编译管道的正确性保证。

Method: 基于二次量子化形式，使用产生和湮灭算符描述量子粒子系统；包含类型系统跟踪粒子类型并强制厄米结构；在Rocq证明框架中完全机械化验证。

Result: 开发了QBlue框架，支持编译到数字和模拟量子电路，捕获从静态约束到动态演化的多层语义。

Conclusion: QBlue是第一个端到端验证的二次量子化哈密顿模拟编译器，为量子计算中的哈密顿模拟提供了安全可靠的编译解决方案。

Abstract: Hamiltonian simulation is a central application of quantum computing, with
significant potential in modeling physical systems and solving complex
optimization problems. Existing compilers for such simulations typically focus
on low-level representations based on Pauli operators, limiting programmability
and offering no formal guarantees of correctness across the compilation
pipeline. We introduce QBlue, a high-level, formally verified framework for
compiling Hamiltonian simulations. QBlue is based on the formalism of second
quantization, which provides a natural and expressive way to describe quantum
particle systems using creation and annihilation operators. To ensure safety
and correctness, QBlue includes a type system that tracks particle types and
enforces Hermitian structure. The framework supports compilation to both
digital and analog quantum circuits and captures multiple layers of semantics,
from static constraints to dynamic evolution. All components of QBlue,
including its language design, type system, and compilation correctness, are
fully mechanized in the Rocq proof framework, making it the first end-to-end
verified compiler for second-quantized Hamiltonian simulation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [254] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: CoRaCMG是一个基于检索增强的提交消息生成框架，通过检索相似的diff-message对来指导LLMs生成更精确、信息量更大的提交消息，显著提升了多种LLM模型的性能。


<details>
  <summary>Details</summary>
Motivation: 提交消息在记录代码变更意图方面至关重要，但现有提交消息往往质量低下、模糊或不完整。虽然LLMs在自动生成提交消息方面显示出潜力，但其性能仍有局限。

Method: 提出CoRaCMG框架，包含三个阶段：(1)检索相似的diff-message对；(2)将检索结果与查询diff结合成结构化提示；(3)通过LLMs生成对应的提交消息。

Result: 实验结果表明，CoRaCMG显著提升了LLM在BLEU、Rouge-L、METEOR和CIDEr四个指标上的性能。DeepSeek-R1在BLEU和CIDEr上分别提升了76%和71%，GPT-4o在BLEU上提升了89%。当使用超过三个示例时，性能提升趋于平稳。

Conclusion: CoRaCMG通过让LLMs从检索到的示例对中学习项目特定术语和写作风格，能够生成更高质量的提交消息，且改进主要归因于模型捕捉人类编写提交消息的术语和风格的能力。

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [255] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: 提出一种通过分析开发者提示的情感来评估对话AI助手满意度的新方法，该方法比传统反馈渠道更高效。


<details>
  <summary>Details</summary>
Motivation: 评估开发者对对话AI助手的满意度很重要但具有挑战性，用户研究难以扩展，而大规模定量信号又过于浅层或稀疏。

Method: 使用情感分析技术分析开发者提示中的隐含满意度信号，基于372名专业开发者的工业使用日志进行分析。

Result: 该方法能在约8%的交互中识别出信号，比显式用户反馈高出13倍以上，即使使用现成的情感分析方法也能达到合理准确度。

Conclusion: 这种新方法为大规模理解开发者体验开辟了新方向，可以补充现有反馈渠道。

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [256] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: SC2Tools是一个用于处理StarCraft 2游戏数据的工具集，旨在简化数据收集和预处理工作，为游戏和电竞研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 游戏和电竞领域需要大规模AI和机器学习解决方案，但数据收集和预处理工作繁重，阻碍了技术能力较弱的研究人员参与该领域的研究。

Method: 开发了包含多个子模块的SC2Tools工具集，采用模块化结构设计，支持处理StarCraft 2比赛数据并创建大型数据集，同时提供PyTorch和PyTorch Lightning API接口。

Result: 成功创建了迄今为止最大的StarCraft 2比赛数据集之一，工具集不仅适用于StarCraft 2，还可用于其他类型的数据集创建。

Conclusion: 减轻数据收集、预处理和自定义代码开发的负担对于促进游戏和电竞研究至关重要，该工具集为StarCraft 2实验工作流程的标准化提供了基础工作。

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [257] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: 探索Landlock在科学网关安全中的应用，通过修改三个成熟科学代码（Einstein Toolkit、Octo-Tiger、FUKA）并实现基于Landlock的FUKA科学网关


<details>
  <summary>Details</summary>
Motivation: Linux内核新特性Landlock可为运行进程放弃资源访问权限，科学网关应用在启动MPI时需要网络访问，但为了安全应在读取用户参数文件前移除网络访问权限

Method: 修改三个科学代码（Einstein Toolkit、Octo-Tiger、FUKA），利用Landlock实现安全锁定，并开发基于Landlock的FUKA科学网关

Result: 成功实现了三个科学代码的Landlock安全锁定，并构建了功能完整的FUKA科学网关，使用Landlock替代用户认证提供安全保障

Conclusion: Landlock是科学网关安全的有用工具，能够有效限制应用在运行过程中的资源访问权限，提升系统安全性

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [258] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval是一个专门评估LLMs在逐步需求细化下迭代代码生成能力的基准测试，涵盖函数级和仓库级任务，结果显示当前模型在此任务上表现仍然很有挑战性


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要将代码生成任务形式化为静态、单轮问题，忽略了现实软件开发中的逐步需求变化和迭代工作流程，这种不匹配限制了理解LLMs支持真实开发工作流程的能力

Method: SR-Eval采用多智能体需求生成方法模拟开发过程，从最终需求中恢复多轮交互过程，并使用语义感知判别性测试用例生成组件确保每轮评估的一致性和判别性

Result: 评估11个代表性LLMs的结果显示，迭代代码生成在逐步需求细化下仍然极具挑战性：最佳模型在函数级任务上仅达到22.67%完成率，仓库级任务上为20.00%

Conclusion: 提示策略对性能有显著影响，表明需要开发更先进的方法来提升LLMs在迭代代码生成任务上的表现

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [259] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: 本文研究了使用大型语言模型（LLM）代理直接执行自然语言（NL）测试用例的可行性，重点关注NL测试用例的不健全性和执行一致性问题，并提出了带防护机制的算法和专门代理来改善测试可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的手写可执行测试脚本开发成本高且难以维护，而使用自然语言测试用例验证图形用户界面（GUI）应用是一个有前景的方向。LLM的发展为直接执行NL测试用例提供了可能性。

Method: 提出了一种带防护机制的算法，使用专门代理动态验证每个测试步骤的正确执行。引入了评估LLM测试执行能力的指标和一个量化执行一致性的指标，并提出了弱不健全性定义来表征NL测试用例执行可接受的上下文。

Result: 实验评估了8个公开可用的LLM（参数规模从3B到70B），结果显示Meta Llama 3.1 70B在NL测试用例执行中表现出可接受的能力，具有高执行一致性（超过3-sigma水平）。

Conclusion: 当前LLM代理在GUI测试中既有潜力也存在局限性，Meta Llama 3.1 70B在NL测试用例执行方面表现最佳，为GUI测试提供了新的可能性。

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [260] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文首次对基于基础模型的AI代理测试实践进行大规模实证研究，发现测试工作存在严重不平衡：确定性组件占70%以上测试投入，而核心的FM计划和触发组件测试不足5%，揭示了测试盲点。


<details>
  <summary>Details</summary>
Motivation: 基础模型AI代理的广泛应用面临非确定性和不可重现性挑战，但缺乏对开发者如何验证代理内部正确性的理解，需要填补这一研究空白。

Method: 分析39个开源代理框架和439个代理应用，识别10种测试模式，并将其映射到代理架构的规范组件上。

Result: 发现传统测试模式被广泛采用，而新型代理特定方法如DeepEval使用率仅1%；测试投入严重偏向确定性组件，FM计划体测试不足5%，触发组件测试仅1%。

Conclusion: 框架开发者需改进对新型测试方法的支持，应用开发者应采纳提示回归测试，研究者需探索采用障碍，以构建更稳健可靠的AI代理。

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [261] [Optimal estimation for regression discontinuity design with binary outcomes](https://arxiv.org/abs/2509.18857)
*Takuya Ishihara,Masayuki Sawada,Kohei Yata*

Main category: econ.EM

TL;DR: 本文开发了一种针对有界结果（包括二元结果）的断点回归设计的有限样本最优估计器，该估计器在回归函数属于Lipschitz类时，在线性收缩估计器中实现了精确的极小极大均方误差。


<details>
  <summary>Details</summary>
Motivation: 传统的大样本方法在小样本情况下可能不可靠，特别是在有效样本量较小时，需要开发有限样本最优估计器来提高估计精度和推断可靠性。

Method: 通过解决凸优化问题获得有限样本最优估计器，仅需Lipschitz常数作为调优参数，并提出了无需大样本近似的统一有效推断程序。

Result: 在小样本模拟中，该估计器比传统大样本技术具有更小的均方误差和更短的置信区间；在实证应用中，相比主流大样本方法能产生更有信息的置信区间。

Conclusion: 该方法为小样本断点回归设计提供了可靠的估计和推断工具，特别适用于样本量较小的多断点设计应用场景。

Abstract: We develop a finite-sample optimal estimator for regression discontinuity
designs when the outcomes are bounded, including binary outcomes as the leading
case. Our finite-sample optimal estimator achieves the exact minimax mean
squared error among linear shrinkage estimators with nonnegative weights when
the regression function of a bounded outcome lies in a Lipschitz class.
Although the original minimax problem involves an iterating (n+1)-dimensional
non-convex optimization problem where n is the sample size, we show that our
estimator is obtained by solving a convex optimization problem. A key advantage
of our estimator is that the Lipschitz constant is the only tuning parameter.
We also propose a uniformly valid inference procedure without a large-sample
approximation. In a simulation exercise for small samples, our estimator
exhibits smaller mean squared errors and shorter confidence intervals than
conventional large-sample techniques which may be unreliable when the effective
sample size is small. We apply our method to an empirical multi-cutoff design
where the sample size for each cutoff is small. In the application, our method
yields informative confidence intervals, in contrast to the leading
large-sample approach.

</details>


### [262] [Driver Identification and PCA Augmented Selection Shrinkage Framework for Nordic System Price Forecasting](https://arxiv.org/abs/2509.18887)
*Yousef Adeli Sadabad,Mohammad Reza Hesamzadeh,Gyorgy Dan,Matin Bagherpour,Darryl R. Biggar*

Main category: econ.EM

TL;DR: 本文提出了一个结合可解释驱动因素分析和稳健预测方法的系统框架，用于北欧电力市场系统价格的预测。该方法通过特征工程识别主要驱动因素，应用PCA处理多重共线性，并开发多预测选择-收缩算法来优化预测组合。


<details>
  <summary>Details</summary>
Motivation: 北欧电力市场系统价格是金融对冲合约的关键参考指标，准确识别其驱动因素并进行可靠预测对市场参与者设计有效对冲策略至关重要。

Method: 提出可解释特征工程算法（K-means聚类、MSTD分解、SARIMA模型），应用PCA处理数据，开发多预测选择-收缩算法进行模型选择和权重优化。

Result: 使用北欧电力市场历史数据验证，该方法在保持可比较计算成本的同时，显著优于单个输入模型和先进的Temporal Fusion Transformer，也优于多个成熟的实用预测组合方法。

Conclusion: 所提出的系统框架能够产生优越的预测结果，使用简单的输入模型即可超越现有最先进方法，为电力市场价格预测提供了有效的解决方案。

Abstract: The System Price (SP) of the Nordic electricity market serves as a key
reference for financial hedge contracts such as Electricity Price Area
Differentials (EPADs) and other risk management instruments. Therefore, the
identification of drivers and the accurate forecasting of SP are essential for
market participants to design effective hedging strategies. This paper develops
a systematic framework that combines interpretable drivers analysis with robust
forecasting methods. It proposes an interpretable feature engineering algorithm
to identify the main drivers of the Nordic SP based on a novel combination of
K-means clustering, Multiple Seasonal-Trend Decomposition (MSTD), and Seasonal
Autoregressive Integrated Moving Average (SARIMA) model. Then, it applies
principal component analysis (PCA) to the identified data matrix, which is
adapted to the downstream task of price forecasting to mitigate the issue of
imperfect multicollinearity in the data. Finally, we propose a multi-forecast
selection-shrinkage algorithm for Nordic SP forecasting, which selects a subset
of complementary forecast models based on their bias-variance tradeoff at the
ensemble level and then computes the optimal weights for the retained forecast
models to minimize the error variance of the combined forecast. Using
historical data from the Nordic electricity market, we demonstrate that the
proposed approach outperforms individual input models uniformly, robustly, and
significantly, while maintaining a comparable computational cost. Notably, our
systematic framework produces superior results using simple input models,
outperforming the state-of-the-art Temporal Fusion Transformer (TFT).
Furthermore, we show that our approach also exceeds the performance of several
well-established practical forecast combination methods.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [263] [The Role of Informal Care in Cognitive Outcome and Healthcare Utilization Among Older Adults with Dementia](https://arxiv.org/abs/2509.18468)
*Mohammad Abdullah Al Faisal*

Main category: econ.GN

TL;DR: 本文研究非正式照护与认知功能和医疗资源利用之间的关系，使用工具变量法解决内生性问题，发现非正式照护对认知功能无显著因果影响，但能显著降低养老院使用概率。


<details>
  <summary>Details</summary>
Motivation: 探讨非正式照护对痴呆症老年患者认知功能和医疗资源利用的因果影响，为长期照护政策提供依据。

Method: 使用健康与退休研究（HRS）2010-2022年数据，采用普通最小二乘法（OLS）和工具变量法（IV），以子女数量作为非正式照护强度的工具变量。

Result: IV估计显示非正式照护对认知功能无显著因果影响，但显著降低养老院使用概率、机构夜数和机构化概率；对医院使用、医生就诊或门诊手术无稳健因果效应。

Conclusion: 非正式照护在替代机构照护方面发挥重要作用，对痴呆症患者的长期照护政策具有重要意义。

Abstract: This paper examines the relationship between informal caregiving and both
cognitive functioning and healthcare utilization among older adults with
dementia. Using data from the RAND version of the Health and Retirement Study
(HRS), a nationally representative longitudinal panel of U.S. adults over age
50, covering the years 2010 to 2022, I estimate Ordinary Least Squares (OLS)
and Instrumental Variables (IV) models to address potential endogeneity in
caregiving decisions. The number of children is employed as an instrument for
informal care intensity. While OLS estimates suggest a negative association
between informal caregiving and cognition, IV estimates show no significant
causal effect after controlling for demographic, socioeconomic, and lagged
cognition variables. In contrast, IV results indicate that informal care
significantly reduces the likelihood of nursing home use, the number of
institutional nights, and the probability of institutionalization. No robust
causal effects are found for hospital use, doctor visits, or outpatient
surgery, although there is some suggestive evidence of a complementary
relationship between informal care and home health services. These findings
highlight the role of informal caregiving in substituting for institutional
care and underscore its importance in long-term care policy for dementia
patients. Keywords: Informal Caregiving; Cognitive Decline; Instrumental
Variables; Healthcare Utilization: Dementia Patients.

</details>


### [264] [Predicting Credit Spreads and Ratings with Machine Learning: The Role of Non-Financial Data](https://arxiv.org/abs/2509.19042)
*Yanran Wu,Xinlei Zhang,Quanyi Xu,Qianxin Yang,Chao Zhang*

Main category: econ.GN

TL;DR: 本文构建了包含167个指标的信用风险指标体系，首次整合了30个大规模企业非财务指标，使用7种机器学习模型预测债券信用利差，发现非财务指标的重要性远超传统指标，并开发了基于预测利差的信用评级模型。


<details>
  <summary>Details</summary>
Motivation: 传统信用风险模型主要依赖财务指标，忽略了非财务因素的重要性。本文旨在通过整合全面的非财务指标，提升债券信用利差预测和信用评级的准确性。

Method: 构建167个指标的信用风险指标体系（包括宏观、企业财务、债券特征和30个非财务指标），使用7种机器学习模型进行信用利差预测，并通过机制分析评估各指标重要性。

Result: 模型在解释信用利差方面优于中国信用评级机构，非财务指标的加入使样本外预测性能提升一倍以上。非财务指标在重要性排名中占据主导地位（前10名中有7个），模型能有效识别高风险特征。基于预测利差的信用评级模型准确率超过75%。

Conclusion: 非财务指标对信用风险预测具有关键作用，本文提出的方法为债券违约预警、信用评级和金融稳定提供了有价值的指导。

Abstract: We build a 167-indicator comprehensive credit risk indicator set, integrating
macro, corporate financial, bond-specific indicators, and for the first time,
30 large-scale corporate non-financial indicators. We use seven machine
learning models to construct a bond credit spread prediction model, test their
spread predictive power and economic mechanisms, and verify their credit rating
prediction effectiveness. Results show these models outperform Chinese credit
rating agencies in explaining credit spreads. Specially, adding non-financial
indicators more than doubles their out-of-sample performance vs. traditional
feature-driven models. Mechanism analysis finds non-financial indicators far
more important than traditional ones (macro-level, financial, bond
features)-seven of the top 10 are non-financial (e.g., corporate governance,
property rights nature, information disclosure evaluation), the most stable
predictors. Models identify high-risk traits (deteriorating operations,
short-term debt, higher financing constraints) via these indicators for spread
prediction and risk identification. Finally, we pioneer a credit rating model
using predicted spreads (predicted implied rating model), with
full/sub-industry models achieving over 75% accuracy, recall, F1. This paper
provides valuable guidance for bond default early warning, credit rating, and
financial stability.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [265] [An Advection-Difusion Model Incorporating Investor Inertia for the Dynamics of Financial Asset Prices](https://arxiv.org/abs/2509.18488)
*Diego,Gustavo*

Main category: econ.TH

TL;DR: 本文提出了一种新的股票价格动态模型，将投资者惯性纳入考虑，基于三状态离散随机游走框架，推导出对流-扩散偏微分方程，并证明对数价格服从正态分布。


<details>
  <summary>Details</summary>
Motivation: 传统的资产价格动态模型（如几何布朗运动）没有正式纳入投资者惯性概念，而实际投资决策中存在显著的惯性行为。

Method: 使用三状态离散随机游走模型（上涨、下跌、中性），将投资者惯性概念融入价格动态建模，推导出对流-扩散偏微分方程。

Result: 模型表明对数价格服从正态分布，具有很好的解析可处理性。通过模拟和巴西PETR4.SA数据的实证应用验证了模型的适用性。

Conclusion: 该框架成功将投资者惯性纳入价格动态模型，为资产定价提供了新的理论工具，具有重要的实践意义。

Abstract: Standard models of asset price dynamics, such as geometric Brownian motion
(Osborne, 1959, Samuelson, 2016), do not formally incorporate investor inertia.
This paper introduces a novel framework for modelling stock price dynamics that
incorporates the concept of investor inertia, inspired by diffusion with
retention models (Bevilacqua, 2011). The asset's log-price is modelled as a
three-state discrete random walk, allowing for movements in any of three
directions: up, down, or neutral. We demonstrate that this framework naturally
leads to an advection-diffusion partial differential equation, in which the
advection (drift) term arises directly from the asymmetry between buying,
selling, and holding decisions. Remarkably, the model implies that log-prices
follow a normal distribution a finding of great practical interest due to its
analytical tractability. The applicability of the model is confirmed through
simulation and an empirical application using Brazilian PETR4.SA data.

</details>


### [266] [Existence and Calculation of Optimal Equilibria on Overlapping Generations Economies](https://arxiv.org/abs/2509.19019)
*Leandro Lyra Braga Dognini*

Main category: econ.TH

TL;DR: 本文开发了一种基于连续逼近的算法，用于寻找具有异质有限寿命代理人的非平稳消费贷款重叠世代经济中的均衡集合，这些均衡满足Cass(1972)效率准则，并用于推导有效均衡的存在性结果。


<details>
  <summary>Details</summary>
Motivation: 重叠世代经济中第一福利定理失效，均衡可能无效。Cass(1972)准则提供了效率的必要充分条件，但未解决有效均衡存在性问题，且存在不存在有效均衡的例子。

Method: 开发基于嵌套紧集极限的算法，通过均衡方程的后向计算，从表现良好的尾部经济的帕累托最优均衡集合出发，寻找均衡集合的元素。

Result: 通过该算法计算的均衡满足Cass(1972)效率准则，并用于推导有效均衡的存在性结果。

Conclusion: 提出的算法能够找到满足效率准则的均衡，为重叠世代经济中有效均衡的存在性问题提供了解决方案。

Abstract: A well-known feature of overlapping generations economies is that the First
Welfare Theorem fails and equilibrium may be inefficient. The Cass (1972)
criterion furnishes a necessary and sufficient condition for efficiency, but
does not address the matter of existence of efficient equilibria, and Cass,
Okuno, and Zilcha (1979) provide nonexistence examples. I develop an algorithm
based on successive approximations of a nonstationary, consumption-loan, prone
to savings, overlapping generations economy with finite-lived heterogeneous
agents to find elements of its set of equilibria as the limit of nested compact
sets. These compact sets are the result of a backward calculation through
equilibrium equations that departs from the set of Pareto optimal equilibria of
well-behaved tail economies. The equilibria calculated through this algorithm
satisfy the Cass (1972) criterion and are used to derive the existence results
on efficient equilibria.

</details>
