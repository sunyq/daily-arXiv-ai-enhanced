<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 85]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 9]
- [econ.GN](#econ.GN) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: MEWI是一个三维多人模拟工具，用于在课堂环境中模拟医疗后送网络，提升决策能力和学习效果。


<details>
  <summary>Details</summary>
Motivation: 缺乏模拟医疗后送网络的工具，无法评估规划和实时决策表现。

Method: 开发MEWI模拟工具，模拟战场约束和不确定性，并测试两个作战场景。

Result: MEWI显著提升了学员对医疗后送知识的学习效果和协作决策能力。

Conclusion: MEWI是医疗教育高保真培训工具的重要进步，为联合部队提供了改进医疗后送教育和操作的关键见解。

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [2] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: 提出了一种名为PDL的新语言，用于解决LLM提示工程的复杂性，通过声明式表示提升程序员效率，并在实际案例中展示了4倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有框架要么隐藏复杂性，要么提供不灵活的固定模式，难以支持复杂的代理编程。

Method: 设计了Prompt Declaration Language (PDL)，将提示置于核心位置，支持手动和自动调整，并结合规则代码和外部工具。

Result: 通过实际案例（合规代理）验证，PDL的提示调整相比固定模式实现了4倍的性能提升。

Conclusion: PDL通过声明式表示和优化潜力，显著提升了LLM提示工程的效率和灵活性。

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [3] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: 论文研究了Jolting Technologies假说，提出AI能力可能呈现超指数增长（加速增长或正三阶导数），并通过理论框架和蒙特卡洛模拟验证检测方法。


<details>
  <summary>Details</summary>
Motivation: 探讨AI能力是否呈现超指数增长模式，为未来实证研究提供工具，并分析其潜在影响。

Method: 开发理论框架，通过蒙特卡洛模拟验证检测方法，分析缩短的创意到行动间隔和AI迭代改进的复合效应。

Result: 建立了检测超指数增长的方法，为理解AI发展轨迹及其对AGI出现的影响提供了数学基础。

Conclusion: 研究为AI发展路径和政策制定提供了理论支持，但需未来实证数据进一步验证。

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [4] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文证明了q-辩证系统比p-辩证系统更强大，后者又比d-辩证系统更强，强调了反例和矛盾在自动信念修正中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究辩证系统的目的是为了建模代理如何更新知识库以寻求一致性，并统一动态信念管理的计算框架。

Method: 通过比较d-、p-和q-辩证系统的能力，证明q-辩证系统在功能上优于其他两类系统。

Result: q-辩证系统严格强于p-辩证系统，而p-辩证系统又严格强于d-辩证系统。

Conclusion: 反例和矛盾在自动信念修正及数学家和研究社区的推理过程中具有互补作用。

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [5] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文探讨了如何将SCC递归语义扩展到无限论证框架（AFs），提出了两种方法，并评估了其在无限和有限框架中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的SCC递归语义在有限AFs中有效，但在无限AFs中因缺乏良基性而失效，因此需要扩展方法。

Method: 提出了两种扩展SCC递归语义到无限AFs的方法，并基于Baroni和Giacomin的标准进行系统评估。

Result: 研究发现方向性在无限框架中普遍失效，但在有限框架中部分语义满足方向性。

Conclusion: 这些结果推动了无限论证理论的发展，为处理无界或演化领域的推理系统奠定了基础。

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [6] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: 论文提出了一种系统化的指令数据构建框架，通过分层标注、种子选择算法、进化数据合成和模型缺陷诊断等方法，提升指令数据的覆盖范围和深度，构建了高质量数据集InfinityInstruct-Subject。


<details>
  <summary>Details</summary>
Motivation: 当前指令数据集虽规模庞大，但在复杂指令和罕见领域任务上表现不佳，主要因覆盖范围和深度不足。

Method: 提出分层标注系统、种子选择算法、进化数据合成和模型缺陷诊断的闭环框架。

Result: 构建了包含约150万指令的高质量数据集，实验证明其在提升指令跟随能力上的有效性。

Conclusion: 该工作为指令数据集从数量扩张到质量提升提供了理论和实践基础。

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [7] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种动态旅行规划系统，通过三个协作代理解决传统系统的不足，显著提升了查询解析、导航准确性和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划系统静态且碎片化，无法应对现实世界的复杂性和突发变化，导致用户体验不佳。

Method: 提出三个协作代理：旅行规划代理（基于网格空间和地图分析）、目的地助手代理（精细导航）、本地发现代理（利用图像嵌入和RAG应对干扰）。

Result: 系统在查询解析、导航准确性和抗干扰能力方面有显著提升。

Conclusion: 该系统在从城市探索到应急响应等多个领域具有应用潜力。

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [8] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: FR3E是一种结构化探索框架，通过识别推理轨迹中的高不确定性决策点并提供针对性反馈，提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR方法在提升LLM推理能力时存在探索不稳定的问题，需要更结构化的探索框架。

Method: 提出FR3E框架，通过高不确定性决策点识别和针对性rollout构建语义基础的中期反馈。

Result: 在数学推理基准测试中，FR3E提升了训练稳定性、生成长且连贯的响应，并增加完全正确轨迹的比例。

Conclusion: FR3E通过更稳健和结构化的探索，有效提升了LLM的推理能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
*Gheorghe Comanici,Eric Bieber,Mike Schaekermann,Ice Pasupat,Noveen Sachdeva,Inderjit Dhillon,Marcel Blistein,Ori Ram,Dan Zhang,Evan Rosen,Luke Marris,Sam Petulla,Colin Gaffney,Asaf Aharoni,Nathan Lintz,Tiago Cardal Pais,Henrik Jacobsson,Idan Szpektor,Nan-Jiang Jiang,Krishna Haridasan,Ahmed Omran,Nikunj Saunshi,Dara Bahri,Gaurav Mishra,Eric Chu,Toby Boyd,Brad Hekman,Aaron Parisi,Chaoyi Zhang,Kornraphop Kawintiranon,Tania Bedrax-Weiss,Oliver Wang,Ya Xu,Ollie Purkiss,Uri Mendlovic,Ilaï Deutel,Nam Nguyen,Adam Langley,Flip Korn,Lucia Rossazza,Alexandre Ramé,Sagar Waghmare,Helen Miller,Vaishakh Keshava,Ying Jian,Xiaofan Zhang,Raluca Ada Popa,Kedar Dhamdhere,Blaž Bratanič,Kyuyeun Kim,Terry Koo,Ferran Alet,Yi-ting Chen,Arsha Nagrani,Hannah Muckenhirn,Zhiyuan Zhang,Corbin Quick,Filip Pavetić,Duc Dung Nguyen,Joao Carreira,Michael Elabd,Haroon Qureshi,Fabian Mentzer,Yao-Yuan Yang,Danielle Eisenbud,Anmol Gulati,Ellie Talius,Eric Ni,Sahra Ghalebikesabi,Edouard Yvinec,Alaa Saade,Thatcher Ulrich,Lorenzo Blanco,Dan A. Calian,Muhuan Huang,Aäron van den Oord,Naman Goyal,Terry Chen,Praynaa Rawlani,Christian Schallhart,Swachhand Lokhande,Xianghong Luo,Jyn Shan,Ceslee Montgomery,Victoria Krakovna,Federico Piccinini,Omer Barak,Jingyu Cui,Yiling Jia,Mikhail Dektiarev,Alexey Kolganov,Shiyu Huang,Zhe Chen,Xingyu Wang,Jessica Austin,Peter de Boursac,Evgeny Sluzhaev,Frank Ding,Huijian Li,Surya Bhupatiraju,Mohit Agarwal,Sławek Kwasiborski,Paramjit Sandhu,Patrick Siegler,Ahmet Iscen,Eyal Ben-David,Shiraz Butt,Miltos Allamanis,Seth Benjamin,Robert Busa-Fekete,Felix Hernandez-Campos,Sasha Goldshtein,Matt Dibb,Weiyang Zhang,Annie Marsden,Carey Radebaugh,Stephen Roller,Abhishek Nayyar,Jacob Austin,Tayfun Terzi,Bhargav Kanagal Shamanna,Pete Shaw,Aayush Singh,Florian Luisier,Artur Mendonça,Vaibhav Aggarwal,Larisa Markeeva,Claudio Fantacci,Sergey Brin,HyunJeong Choe,Guanyu Wang,Hartwig Adam,Avigail Dabush,Tatsuya Kiyono,Eyal Marcus,Jeremy Cole,Theophane Weber,Hongrae Lee,Ronny Huang,Alex Muzio,Leandro Kieliger,Maigo Le,Courtney Biles,Long Le,Archit Sharma,Chengrun Yang,Avery Lamp,Dave Dopson,Nate Hurley,Katrina,Xu,Zhihao Shan,Shuang Song,Jiewen Tan,Alexandre Senges,George Zhang,Chong You,Yennie Jun,David Raposo,Susanna Ricco,Xuan Yang,Weijie Chen,Prakhar Gupta,Arthur Szlam,Kevin Villela,Chun-Sung Ferng,Daniel Kasenberg,Chen Liang,Rui Zhu,Arunachalam Narayanaswamy,Florence Perot,Paul Pucciarelli,Anna Shekhawat,Alexey Stern,Rishikesh Ingale,Stefani Karp,Sanaz Bahargam,Adrian Goedeckemeyer,Jie Han,Sicheng Li,Andrea Tacchetti,Dian Yu,Abhishek Chakladar,Zhiying Zhang,Mona El Mahdy,Xu Gao,Dale Johnson,Samrat Phatale,AJ Piergiovanni,Hyeontaek Lim,Clement Farabet,Carl Lebsack,Theo Guidroz,John Blitzer,Nico Duduta,David Madras,Steve Li,Daniel von Dincklage,Xin Li,Mahdis Mahdieh,George Tucker,Ganesh Jawahar,Owen Xiao,Danny Tarlow,Robert Geirhos,Noam Velan,Daniel Vlasic,Kalesha Bullard,SK Park,Nishesh Gupta,Kellie Webster,Ayal Hitron,Jieming Mao,Julian Eisenschlos,Laurel Prince,Nina D'Souza,Kelvin Zheng,Sara Nasso,Gabriela Botea,Carl Doersch,Caglar Unlu,Chris Alberti,Alexey Svyatkovskiy,Ankita Goel,Krzysztof Choromanski,Pan-Pan Jiang,Richard Nguyen,Four Flynn,Daria Ćurko,Peter Chen,Nicholas Roth,Kieran Milan,Caleb Habtegebriel,Shashi Narayan,Michael Moffitt,Jake Marcus,Thomas Anthony,Brendan McMahan,Gowoon Cheon,Ruibo Liu,Megan Barnes,Lukasz Lew,Rebeca Santamaria-Fernandez,Mayank Upadhyay,Arjun Akula,Arnar Mar Hrafnkelsson,Alvaro Caceres,Andrew Bunner,Michal Sokolik,Subha Puttagunta,Lawrence Moore,Berivan Isik,Weilun Chen,Jay Hartford,Lawrence Chan,Pradeep Shenoy,Dan Holtmann-Rice,Jane Park,Fabio Viola,Alex Salcianu,Sujeevan Rajayogam,Ian Stewart-Binks,Zelin Wu,Richard Everett,Xi Xiong,Pierre-Antoine Manzagol,Gary Leung,Carl Saroufim,Bo Pang,Dawid Wegner,George Papamakarios,Jennimaria Palomaki,Helena Pankov,Guangda Lai,Guilherme Tubone,Shubin Zhao,Theofilos Strinopoulos,Seth Neel,Mingqiu Wang,Joe Kelley,Li Li,Pingmei Xu,Anitha Vijayakumar,Andrea D'olimpio,Omer Levy,Massimo Nicosia,Grigory Rozhdestvenskiy,Ni Lao,Sirui Xie,Yash Katariya,Jon Simon,Sanjiv Kumar,Florian Hartmann,Michael Kilgore,Jinhyuk Lee,Aroma Mahendru,Roman Ring,Tom Hennigan,Fiona Lang,Colin Cherry,David Steiner,Dawsen Hwang,Ray Smith,Pidong Wang,Jeremy Chen,Ming-Hsuan Yang,Sam Kwei,Philippe Schlattner,Donnie Kim,Ganesh Poomal Girirajan,Nikola Momchev,Ayushi Agarwal,Xingyi Zhou,Ilkin Safarli,Zachary Garrett,AJ Pierigiovanni,Sarthak Jauhari,Alif Raditya Rochman,Shikhar Vashishth,Quan Yuan,Christof Angermueller,Jon Blanton,Xinying Song,Nitesh Bharadwaj Gundavarapu,Thi Avrahami,Maxine Deines,Subhrajit Roy,Manish Gupta,Christopher Semturs,Shobha Vasudevan,Aditya Srikanth Veerubhotla,Shriya Sharma,Josh Jacob,Zhen Yang,Andreas Terzis,Dan Karliner,Auriel Wright,Tania Rojas-Esponda,Ashley Brown,Abhijit Guha Roy,Pawan Dogra,Andrei Kapishnikov,Peter Young,Wendy Kan,Vinodh Kumar Rajendran,Maria Ivanova,Salil Deshmukh,Chia-Hua Ho,Mike Kwong,Stav Ginzburg,Annie Louis,KP Sawhney,Slav Petrov,Jing Xie,Yunfei Bai,Georgi Stoyanov,Alex Fabrikant,Rajesh Jayaram,Yuqi Li,Joe Heyward,Justin Gilmer,Yaqing Wang,Radu Soricut,Luyang Liu,Qingnan Duan,Jamie Hayes,Maura O'Brien,Gaurav Singh Tomar,Sivan Eiger,Bahar Fatemi,Jeffrey Hui,Catarina Barros,Adaeze Chukwuka,Alena Butryna,Saksham Thakur,Austin Huang,Zhufeng Pan,Haotian Tang,Serkan Cabi,Tulsee Doshi,Michiel Bakker,Sumit Bagri,Ruy Ley-Wild,Adam Lelkes,Jennie Lees,Patrick Kane,David Greene,Shimu Wu,Jörg Bornschein,Gabriela Surita,Sarah Hodkinson,Fangtao Li,Chris Hidey,Sébastien Pereira,Sean Ammirati,Phillip Lippe,Adam Kraft,Pu Han,Sebastian Gerlach,Zifeng Wang,Liviu Panait,Feng Han,Brian Farris,Yingying Bi,Hannah DeBalsi,Miaosen Wang,Gladys Tyen,James Cohan,Susan Zhang,Jarred Barber,Da-Woon Chung,Jaeyoun Kim,Markus Kunesch,Steven Pecht,Nami Akazawa,Abe Friesen,James Lyon,Ali Eslami,Junru Wu,Jie Tan,Yue Song,Ravi Kumar,Chris Welty,Ilia Akolzin,Gena Gibson,Sean Augenstein,Arjun Pillai,Nancy Yuen,Du Phan,Xin Wang,Iain Barr,Heiga Zen,Nan Hua,Casper Liu,Jilei,Wang,Tanuj Bhatia,Hao Xu,Oded Elyada,Pushmeet Kohli,Mirek Olšák,Ke Chen,Azalia Mirhoseini,Noam Shazeer,Shoshana Jakobovits,Maggie Tran,Nolan Ramsden,Tarun Bharti,Fred Alcober,Yunjie Li,Shilpa Shetty,Jing Chen,Dmitry Kalashnikov,Megha Nawhal,Sercan Arik,Hanwen Chen,Michiel Blokzijl,Shubham Gupta,James Rubin,Rigel Swavely,Sophie Bridgers,Ian Gemp,Chen Su,Arun Suggala,Juliette Pluto,Mary Cassin,Alain Vaucher,Kaiyang Ji,Jiahao Cai,Andrew Audibert,Animesh Sinha,David Tian,Efrat Farkash,Amy Hua,Jilin Chen,Duc-Hieu Tran,Edward Loper,Nicole Brichtova,Lara McConnaughey,Ballie Sandhu,Robert Leland,Doug DeCarlo,Andrew Over,James Huang,Xing Wu,Connie Fan,Eric Li,Yun Lei,Deepak Sharma,Cosmin Paduraru,Luo Yu,Matko Bošnjak,Phuong Dao,Min Choi,Sneha Kudugunta,Jakub Adamek,Carlos Guía,Ali Khodaei,Jie Feng,Wenjun Zeng,David Welling,Sandeep Tata,Christina Butterfield,Andrey Vlasov,Seliem El-Sayed,Swaroop Mishra,Tara Sainath,Shentao Yang,RJ Skerry-Ryan,Jeremy Shar,Robert Berry,Arunkumar Rajendran,Arun Kandoor,Andrea Burns,Deepali Jain,Tom Stone,Wonpyo Park,Shibo Wang,Albin Cassirer,Guohui Wang,Hayato Kobayashi,Sergey Rogulenko,Vineetha Govindaraj,Mikołaj Rybiński,Nadav Olmert,Colin Evans,Po-Sen Huang,Kelvin Xu,Premal Shah,Terry Thurk,Caitlin Sikora,Mu Cai,Jin Xie,Elahe Dabir,Saloni Shah,Norbert Kalb,Carrie Zhang,Shruthi Prabhakara,Amit Sabne,Artiom Myaskovsky,Vikas Raunak,Blanca Huergo,Behnam Neyshabur,Jon Clark,Ye Zhang,Shankar Krishnan,Eden Cohen,Dinesh Tewari,James Lottes,Yumeya Yamamori,Hui,Li,Mohamed Elhawaty,Ada Maksutaj Oflazer,Adrià Recasens,Sheryl Luo,Duy Nguyen,Taylor Bos,Kalyan Andra,Ana Salazar,Ed Chi,Jeongwoo Ko,Matt Ginsberg,Anders Andreassen,Anian Ruoss,Todor Davchev,Elnaz Davoodi,Chenxi Liu,Min Kim,Santiago Ontanon,Chi Ming To,Dawei Jia,Rosemary Ke,Jing Wang,Anna Korsun,Moran Ambar,Ilya Kornakov,Irene Giannoumis,Toni Creswell,Denny Zhou,Yi Su,Ishaan Watts,Aleksandr Zaks,Evgenii Eltyshev,Ziqiang Feng,Sidharth Mudgal,Alex Kaskasoli,Juliette Love,Kingshuk Dasgupta,Sam Shleifer,Richard Green,Sungyong Seo,Chansoo Lee,Dale Webster,Prakash Shroff,Ganna Raboshchuk,Isabel Leal,James Manyika,Sofia Erell,Daniel Murphy,Zhisheng Xiao,Anton Bulyenov,Julian Walker,Mark Collier,Matej Kastelic,Nelson George,Sushant Prakash,Sailesh Sidhwani,Alexey Frolov,Steven Hansen,Petko Georgiev,Tiberiu Sosea,Chris Apps,Aishwarya Kamath,David Reid,Emma Cooney,Charlotte Magister,Oriana Riva,Alec Go,Pu-Chin Chen,Sebastian Krause,Nir Levine,Marco Fornoni,Ilya Figotin,Nick Roy,Parsa Mahmoudieh,Vladimir Magay,Mukundan Madhavan,Jin Miao,Jianmo Ni,Yasuhisa Fujii,Ian Chou,George Scrivener,Zak Tsai,Siobhan Mcloughlin,Jeremy Selier,Sandra Lefdal,Jeffrey Zhao,Abhijit Karmarkar,Kushal Chauhan,Shivanker Goel,Zhaoyi Zhang,Vihan Jain,Parisa Haghani,Mostafa Dehghani,Jacob Scott,Erin Farnese,Anastasija Ilić,Steven Baker,Julia Pawar,Li Zhong,Josh Camp,Yoel Zeldes,Shravya Shetty,Anand Iyer,Vít Listík,Jiaxian Guo,Luming Tang,Mark Geller,Simon Bucher,Yifan Ding,Hongzhi Shi,Carrie Muir,Dominik Grewe,Ramy Eskander,Octavio Ponce,Boqing Gong,Derek Gasaway,Samira Khan,Umang Gupta,Angelos Filos,Weicheng Kuo,Klemen Kloboves,Jennifer Beattie,Christian Wright,Leon Li,Alicia Jin,Sandeep Mariserla,Miteyan Patel,Jens Heitkaemper,Dilip Krishnan,Vivek Sharma,David Bieber,Christian Frank,John Lambert,Paul Caron,Martin Polacek,Mai Giménez,Himadri Choudhury,Xing Yu,Sasan Tavakkol,Arun Ahuja,Franz Och,Rodolphe Jenatton,Wojtek Skut,Bryan Richter,David Gaddy,Andy Ly,Misha Bilenko,Megh Umekar,Ethan Liang,Martin Sevenich,Mandar Joshi,Hassan Mansoor,Rebecca Lin,Sumit Sanghai,Abhimanyu Singh,Xiaowei Li,Sudheendra Vijayanarasimhan,Zaheer Abbas,Yonatan Bitton,Hansa Srinivasan,Manish Reddy Vuyyuru,Alexander Frömmgen,Yanhua Sun,Ralph Leith,Alfonso Castaño,DJ Strouse,Le Yan,Austin Kyker,Satish Kambala,Mary Jasarevic,Thibault Sellam,Chao Jia,Alexander Pritzel,Raghavender R,Huizhong Chen,Natalie Clay,Sudeep Gandhe,Sean Kirmani,Sayna Ebrahimi,Hannah Kirkwood,Jonathan Mallinson,Chao Wang,Adnan Ozturel,Kuo Lin,Shyam Upadhyay,Vincent Cohen-Addad,Sean Purser-haskell,Yichong Xu,Ebrahim Songhori,Babi Seal,Alberto Magni,Almog Gueta,Tingting Zou,Guru Guruganesh,Thais Kagohara,Hung Nguyen,Khalid Salama,Alejandro Cruzado Ruiz,Justin Frye,Zhenkai Zhu,Matthias Lochbrunner,Simon Osindero,Wentao Yuan,Lisa Lee,Aman Prasad,Lam Nguyen Thiet,Daniele Calandriello,Victor Stone,Qixuan Feng,Han Ke,Maria Voitovich,Geta Sampemane,Lewis Chiang,Ling Wu,Alexander Bykovsky,Matt Young,Luke Vilnis,Ishita Dasgupta,Aditya Chawla,Qin Cao,Bowen Liang,Daniel Toyama,Szabolcs Payrits,Anca Stefanoiu,Dimitrios Vytiniotis,Ankesh Anand,Tianxiao Shen,Blagoj Mitrevski,Michael Tschannen,Sreenivas Gollapudi,Aishwarya P S,José Leal,Zhe Shen,Han Fu,Wei Wang,Arvind Kannan,Doron Kukliansky,Sergey Yaroshenko,Svetlana Grant,Umesh Telang,David Wood,Alexandra Chronopoulou,Alexandru Ţifrea,Tao Zhou,Tony,Nguy\~ên,Muge Ersoy,Anima Singh,Meiyan Xie,Emanuel Taropa,Woohyun Han,Eirikur Agustsson,Andrei Sozanschi,Hui Peng,Alex Chen,Yoel Drori,Efren Robles,Yang Gao,Xerxes Dotiwalla,Ying Chen,Anudhyan Boral,Alexei Bendebury,John Nham,Chris Tar,Luis Castro,Jiepu Jiang,Canoee Liu,Felix Halim,Jinoo Baek,Andy Wan,Jeremiah Liu,Yuan Cao,Shengyang Dai,Trilok Acharya,Ruoxi Sun,Fuzhao Xue,Saket Joshi,Morgane Lustman,Yongqin Xian,Rishabh Joshi,Deep Karkhanis,Nora Kassner,Jamie Hall,Xiangzhuo Ding,Gan Song,Gang Li,Chen Zhu,Yana Kulizhskaya,Bin Ni,Alexey Vlaskin,Solomon Demmessie,Lucio Dery,Salah Zaiem,Yanping Huang,Cindy Fan,Felix Gimeno,Ananth Balashankar,Koji Kojima,Hagai Taitelbaum,Maya Meng,Dero Gharibian,Sahil Singla,Wei Chen,Ambrose Slone,Guanjie Chen,Sujee Rajayogam,Max Schumacher,Suyog Kotecha,Rory Blevins,Qifei Wang,Mor Hazan Taege,Alex Morris,Xin Liu,Fayaz Jamil,Richard Zhang,Pratik Joshi,Ben Ingram,Tyler Liechty,Ahmed Eleryan,Scott Baird,Alex Grills,Gagan Bansal,Shan Han,Kiran Yalasangi,Shawn Xu,Majd Al Merey,Isabel Gao,Felix Weissenberger,Igor Karpov,Robert Riachi,Ankit Anand,Gautam Prasad,Kay Lamerigts,Reid Hayes,Jamie Rogers,Mandy Guo,Ashish Shenoy,Qiong,Hu,Kyle He,Yuchen Liu,Polina Zablotskaia,Sagar Gubbi,Yifan Chang,Jay Pavagadhi,Kristian Kjems,Archita Vadali,Diego Machado,Yeqing Li,Renshen Wang,Dipankar Ghosh,Aahil Mehta,Dana Alon,George Polovets,Alessio Tonioni,Nate Kushman,Joel D'sa,Lin Zhuo,Allen Wu,Rohin Shah,John Youssef,Jiayu Ye,Justin Snyder,Karel Lenc,Senaka Buthpitiya,Matthew Tung,Jichuan Chang,Tao Chen,David Saxton,Jenny Lee,Lydia Lihui Zhang,James Qin,Prabakar Radhakrishnan,Maxwell Chen,Piotr Ambroszczyk,Metin Toksoz-Exley,Yan Zhong,Nitzan Katz,Brendan O'Donoghue,Tamara von Glehn,Adi Gerzi Rosenthal,Aga Świetlik,Xiaokai Zhao,Nick Fernando,Jinliang Wei,Jieru Mei,Sergei Vassilvitskii,Diego Cedillo,Pranjal Awasthi,Hui Zheng,Koray Kavukcuoglu,Itay Laish,Joseph Pagadora,Marc Brockschmidt,Christopher A. Choquette-Choo,Arunkumar Byravan,Yifeng Lu,Xu Chen,Mia Chen,Kenton Lee,Rama Pasumarthi,Sijal Bhatnagar,Aditya Shah,Qiyin Wu,Zhuoyuan Chen,Zack Nado,Bartek Perz,Zixuan Jiang,David Kao,Ganesh Mallya,Nino Vieillard,Lantao Mei,Sertan Girgin,Mandy Jordan,Yeongil Ko,Alekh Agarwal,Yaxin Liu,Yasemin Altun,Raoul de Liedekerke,Anastasios Kementsietsidis,Daiyi Peng,Dangyi Liu,Utku Evci,Peter Humphreys,Austin Tarango,Xiang Deng,Yoad Lewenberg,Kevin Aydin,Chengda Wu,Bhavishya Mittal,Tsendsuren Munkhdalai,Kleopatra Chatziprimou,Rodrigo Benenson,Uri First,Xiao Ma,Jinning Li,Armand Joulin,Hamish Tomlinson,Tingnan Zhang,Milad Nasr,Zhi Hong,Michaël Sander,Lisa Anne Hendricks,Anuj Sharma,Andrew Bolt,Eszter Vértes,Jiri Simsa,Tomer Levinboim,Olcan Sercinoglu,Divyansh Shukla,Austin Wu,Craig Swanson,Danny Vainstein,Fan Bu,Bo Wang,Ryan Julian,Charles Yoon,Sergei Lebedev,Antonious Girgis,Bernd Bandemer,David Du,Todd Wang,Xi Chen,Ying Xiao,Peggy Lu,Natalie Ha,Vlad Ionescu,Simon Rowe,Josip Matak,Federico Lebron,Andreas Steiner,Lalit Jain,Manaal Faruqui,Nicolas Lacasse,Georgie Evans,Neesha Subramaniam,Dean Reich,Giulia Vezzani,Aditya Pandey,Joe Stanton,Tianhao Zhou,Liam McCafferty,Henry Griffiths,Verena Rieser,Soheil Hassas Yeganeh,Eleftheria Briakou,Lu Huang,Zichuan Wei,Liangchen Luo,Erik Jue,Gabby Wang,Victor Cotruta,Myriam Khan,Jongbin Park,Qiuchen Guo,Peiran Li,Rong Rong,Diego Antognini,Anastasia Petrushkina,Chetan Tekur,Eli Collins,Parul Bhatia,Chester Kwak,Wenhu Chen,Arvind Neelakantan,Immanuel Odisho,Sheng Peng,Vincent Nallatamby,Vaibhav Tulsyan,Fabian Pedregosa,Peng Xu,Raymond Lin,Yulong Wang,Emma Wang,Sholto Douglas,Reut Tsarfaty,Elena Gribovskaya,Renga Aravamudhan,Manu Agarwal,Mara Finkelstein,Qiao Zhang,Elizabeth Cole,Phil Crone,Sarmishta Velury,Anil Das,Chris Sauer,Luyao Xu,Danfeng Qin,Chenjie Gu,Dror Marcus,CJ Zheng,Wouter Van Gansbeke,Sobhan Miryoosefi,Haitian Sun,YaGuang Li,Charlie Chen,Jae Yoo,Pavel Dubov,Alex Tomala,Adams Yu,Paweł Wesołowski,Alok Gunjan,Eddie Cao,Jiaming Luo,Nikhil Sethi,Arkadiusz Socala,Laura Graesser,Tomas Kocisky,Arturo BC,Minmin Chen,Edward Lee,Sophie Wang,Weize Kong,Qiantong Xu,Nilesh Tripuraneni,Yiming Li,Xinxin Yu,Allen Porter,Paul Voigtlaender,Biao Zhang,Arpi Vezer,Sarah York,Qing Wei,Geoffrey Cideron,Mark Kurzeja,Seungyeon Kim,Benny Li,Angéline Pouget,Hyo Lee,Kaspar Daugaard,Yang Li,Dave Uthus,Aditya Siddhant,Paul Cavallaro,Sriram Ganapathy,Maulik Shah,Rolf Jagerman,Jeff Stanway,Piermaria Mendolicchio,Li Xiao,Kayi Lee,Tara Thompson,Shubham Milind Phal,Jason Chase,Sun Jae Lee,Adrian N Reyes,Disha Shrivastava,Zhen Qin,Roykrong Sukkerd,Seth Odoom,Lior Madmoni,John Aslanides,Jonathan Herzig,Elena Pochernina,Sheng Zhang,Parker Barnes,Daisuke Ikeda,Qiujia Li,Shuo-yiin Chang,Shakir Mohamed,Jim Sproch,Richard Powell,Bidisha Samanta,Domagoj Ćevid,Anton Kovsharov,Shrestha Basu Mallick,Srinivas Tadepalli,Anne Zheng,Kareem Ayoub,Andreas Noever,Christian Reisswig,Zhuo Xu,Junhyuk Oh,Martin Matysiak,Tim Blyth,Shereen Ashraf,Julien Amelot,Boone Severson,Michele Bevilacqua,Motoki Sano,Ethan Dyer,Ofir Roval,Anu Sinha,Yin Zhong,Sagi Perel,Tea Sabolić,Johannes Mauerer,Willi Gierke,Mauro Verzetti,Rodrigo Cabrera,Alvin Abdagic,Steven Hemingray,Austin Stone,Jong Lee,Farooq Ahmad,Karthik Raman,Lior Shani,Jonathan Lai,Orhan Firat,Nathan Waters,Eric Ge,Mo Shomrat,Himanshu Gupta,Rajeev Aggarwal,Tom Hudson,Bill Jia,Simon Baumgartner,Palak Jain,Joe Kovac,Junehyuk Jung,Ante Žužul,Will Truong,Morteza Zadimoghaddam,Songyou Peng,Marco Liang,Rachel Sterneck,Balaji Lakshminarayanan,Machel Reid,Oliver Woodman,Tong Zhou,Jianling Wang,Vincent Coriou,Arjun Narayanan,Jay Hoover,Yenai Ma,Apoorv Jindal,Clayton Sanford,Doug Reid,Swaroop Ramaswamy,Alex Kurakin,Roland Zimmermann,Yana Lunts,Dragos Dena,Zalán Borsos,Vered Cohen,Shujian Zhang,Will Grathwohl,Robert Dadashi,Morgan Redshaw,Joshua Kessinger,Julian Odell,Silvano Bonacina,Zihang Dai,Grace Chen,Ayush Dubey,Pablo Sprechmann,Mantas Pajarskas,Wenxuan Zhou,Niharika Ahuja,Tara Thomas,Martin Nikoltchev,Matija Kecman,Bharath Mankalale,Andrey Ryabtsev,Jennifer She,Christian Walder,Jiaming Shen,Lu Li,Carolina Parada,Sheena Panthaplackel,Okwan Kwon,Matt Lawlor,Utsav Prabhu,Yannick Schroecker,Marc'aurelio Ranzato,Pete Blois,Iurii Kemaev,Ting Yu,Dmitry,Lepikhin,Hao Xiong,Sahand Sharifzadeh,Oleaser Johnson,Jeremiah Willcock,Rui Yao,Greg Farquhar,Sujoy Basu,Hidetoshi Shimokawa,Nina Anderson,Haiguang Li,Khiem Pham,Yizhong Liang,Sebastian Borgeaud,Alexandre Moufarek,Hideto Kazawa,Blair Kutzman,Marcin Sieniek,Sara Smoot,Ruth Wang,Natalie Axelsson,Nova Fallen,Prasha Sundaram,Yuexiang Zhai,Varun Godbole,Petros Maniatis,Alek Wang,Ilia Shumailov,Santhosh Thangaraj,Remi Crocker,Nikita Gupta,Gang Wu,Phil Chen,Gellért Weisz,Celine Smith,Mojtaba Seyedhosseini,Boya Fang,Xiyang Luo,Roey Yogev,Zeynep Cankara,Andrew Hard,Helen Ran,Rahul Sukthankar,George Necula,Gaël Liu,Honglong Cai,Praseem Banzal,Daniel Keysers,Sanjay Ghemawat,Connie Tao,Emma Dunleavy,Aditi Chaudhary,Wei Li,Maciej Mikuła,Chen-Yu Lee,Tiziana Refice,Krishna Somandepalli,Alexandre Fréchette,Dan Bahir,John Karro,Keith Rush,Sarah Perrin,Bill Rosgen,Xiaomeng Yang,Clara Huiyi Hu,Mahmoud Alnahlawi,Justin Mao-Jones,Roopal Garg,Hoang Nguyen,Bat-Orgil Batsaikhan,Iñaki Iturrate,Anselm Levskaya,Avi Singh,Ashyana Kachra,Tony Lu,Denis Petek,Zheng Xu,Mark Graham,Lukas Zilka,Yael Karov,Marija Kostelac,Fangyu Liu,Yaohui Guo,Weiyue Wang,Bernd Bohnet,Emily Pitler,Tony Bruguier,Keisuke Kinoshita,Chrysovalantis Anastasiou,Nilpa Jha,Ting Liu,Jerome Connor,Phil Wallis,Philip Pham,Eric Bailey,Shixin Li,Heng-Tze Cheng,Sally Ma,Haiqiong Li,Akanksha Maurya,Kate Olszewska,Manfred Warmuth,Christy Koh,Dominik Paulus,Siddhartha Reddy Jonnalagadda,Enrique Piqueras,Ali Elqursh,Geoff Brown,Hadar Shemtov,Loren Maggiore,Fei Xia,Ryan Foley,Beka Westberg,George van den Driessche,Livio Baldini Soares,Arjun Kar,Michael Quinn,Siqi Zuo,Jialin Wu,Kyle Kastner,Anna Bortsova,Aijun Bai,Ales Mikhalap,Luowei Zhou,Jennifer Brennan,Vinay Ramasesh,Honglei Zhuang,John Maggs,Johan Schalkwyk,Yuntao Xu,Hui Huang,Andrew Howard,Sasha Brown,Linting Xue,Gloria Shen,Brian Albert,Neha Jha,Daniel Zheng,Varvara Krayvanova,Spurthi Amba Hombaiah,Olivier Lacombe,Gautam Vasudevan,Dan Graur,Tian Xie,Meet Gandhi,Bangju Wang,Dustin Zelle,Harman Singh,Dahun Kim,Sébastien Cevey,Victor Ungureanu,Natasha Noy,Fei Liu,Annie Xie,Fangxiaoyu Feng,Katerina Tsihlas,Daniel Formoso,Neera Vats,Quentin Wellens,Yinan Wang,Niket Kumar Bhumihar,Samrat Ghosh,Matt Hoffman,Tom Lieber,Oran Lang,Kush Bhatia,Tom Paine,Aroonalok Pyne,Ronny Votel,Madeleine Clare Elish,Benoit Schillings,Alex Panagopoulos,Haichuan Yang,Adam Raveret,Zohar Yahav,Shuang Liu,Warren Chen,Dalia El Badawy,Nishant Agrawal,Mohammed Badawi,Mahdi Mirzazadeh,Carla Bromberg,Fan Ye,Chang Liu,Tatiana Sholokhova,George-Cristian Muraru,Gargi Balasubramaniam,Jonathan Malmaud,Alen Carin,Danilo Martins,Irina Jurenka,Pankil Botadra,Dave Lacey,Richa Singh,Mariano Schain,Dan Zheng,Isabelle Guyon,Victor Lavrenko,Seungji Lee,Xiang Zhou,Demis Hassabis,Jeshwanth Challagundla,Derek Cheng,Nikhil Mehta,Matthew Mauger,Michela Paganini,Pushkar Mishra,Kate Lee,Zhang Li,Lexi Baugher,Ondrej Skopek,Max Chang,Amir Zait,Gaurav Menghani,Lizzetth Bellot,Guangxing Han,Jean-Michel Sarr,Sharat Chikkerur,Himanshu Sahni,Rohan Anil,Arun Narayanan,Chandu Thekkath,Daniele Pighin,Hana Strejček,Marko Velic,Fred Bertsch,Manuel Tragut,Keran Rong,Alicia Parrish,Kai Bailey,Jiho Park,Isabela Albuquerque,Abhishek Bapna,Rajesh Venkataraman,Alec Kosik,Johannes Griesser,Zhiwei Deng,Alek Andreev,Qingyun Dou,Kevin Hui,Fanny Wei,Xiaobin Yu,Lei Shu,Avia Aharon,David Barker,Badih Ghazi,Sebastian Flennerhag,Chris Breaux,Yuchuan Liu,Matthew Bilotti,Josh Woodward,Uri Alon,Stephanie Winkler,Tzu-Kuo Huang,Kostas Andriopoulos,João Gabriel Oliveira,Penporn Koanantakool,Berkin Akin,Michael Wunder,Cicero Nogueira dos Santos,Mohammad Hossein Bateni,Lin Yang,Dan Horgan,Beer Changpinyo,Keyvan Amiri,Min Ma,Dayeong Lee,Lihao Liang,Anirudh Baddepudi,Tejasi Latkar,Raia Hadsell,Jun Xu,Hairong Mu,Michael Han,Aedan Pope,Snchit Grover,Frank Kim,Ankit Bhagatwala,Guan Sun,Yamini Bansal,Amir Globerson,Alireza Nazari,Samira Daruki,Hagen Soltau,Jane Labanowski,Laurent El Shafey,Matt Harvey,Yanif Ahmad,Elan Rosenfeld,William Kong,Etienne Pot,Yi-Xuan Tan,Aurora Wei,Victoria Langston,Marcel Prasetya,Petar Veličković,Richard Killam,Robin Strudel,Darren Ni,Zhenhai Zhu,Aaron Archer,Kavya Kopparapu,Lynn Nguyen,Emilio Parisotto,Hussain Masoom,Sravanti Addepalli,Jordan Grimstad,Hexiang Hu,Joss Moore,Avinatan Hassidim,Le Hou,Mukund Raghavachari,Jared Lichtarge,Adam R. Brown,Hilal Dib,Natalia Ponomareva,Justin Fu,Yujing Zhang,Altaf Rahman,Joana Iljazi,Edouard Leurent,Gabriel Dulac-Arnold,Cosmo Du,Chulayuth Asawaroengchai,Larry Jin,Ela Gruzewska,Ziwei Ji,Benigno Uria,Daniel De Freitas,Paul Barham,Lauren Beltrone,Víctor Campos,Jun Yan,Neel Kovelamudi,Arthur Nguyen,Elinor Davies,Zhichun Wu,Zoltan Egyed,Kristina Toutanova,Nithya Attaluri,Hongliang Fei,Peter Stys,Siddhartha Brahma,Martin Izzard,Siva Velusamy,Scott Lundberg,Vincent Zhuang,Kevin Sequeira,Adam Santoro,Ehsan Amid,Ophir Aharoni,Shuai Ye,Mukund Sundararajan,Lijun Yu,Yu-Cheng Ling,Stephen Spencer,Hugo Song,Josip Djolonga,Christo Kirov,Sonal Gupta,Alessandro Bissacco,Clemens Meyer,Mukul Bhutani,Andrew Dai,Weiyi Wang,Siqi Liu,Ashwin Sreevatsa,Qijun Tan,Maria Wang,Lucy Kim,Yicheng Wang,Alex Irpan,Yang Xiao,Stanislav Fort,Yifan He,Alex Gurney,Bryan Gale,Yue Ma,Monica Roy,Viorica Patraucean,Taylan Bilal,Golnaz Ghiasi,Anahita Hosseini,Melvin Johnson,Zhuowan Li,Yi Tay,Benjamin Beyret,Katie Millican,Josef Broder,Mayank Lunayach,Danny Swisher,Eugen Vušak,David Parkinson,MH Tessler,Adi Mayrav Gilady,Richard Song,Allan Dafoe,Yves Raimond,Masa Yamaguchi,Itay Karo,Elizabeth Nielsen,Kevin Kilgour,Mike Dusenberry,Rajiv Mathews,Jiho Choi,Siyuan Qiao,Harsh Mehta,Sahitya Potluri,Chris Knutsen,Jialu Liu,Tat Tan,Kuntal Sengupta,Keerthana Gopalakrishnan,Abodunrinwa Toki,Mencher Chiang,Mike Burrows,Grace Vesom,Zafarali Ahmed,Ilia Labzovsky,Siddharth Vashishtha,Preeti Singh,Ankur Sharma,Ada Ma,Jinyu Xie,Pranav Talluri,Hannah Forbes-Pollard,Aarush Selvan,Joel Wee,Loic Matthey,Tom Funkhouser,Parthasarathy Gopavarapu,Lev Proleev,Cheng Li,Matt Thomas,Kashyap Kolipaka,Zhipeng Jia,Ashwin Kakarla,Srinivas Sunkara,Joan Puigcerver,Suraj Satishkumar Sheth,Emily Graves,Chen Wang,Sadh MNM Khan,Kai Kang,Shyamal Buch,Fred Zhang,Omkar Savant,David Soergel,Kevin Lee,Linda Friso,Xuanyi Dong,Rahul Arya,Shreyas Chandrakaladharan,Connor Schenck,Greg Billock,Tejas Iyer,Anton Bakalov,Leslie Baker,Alex Ruiz,Angad Chandorkar,Trieu Trinh,Matt Miecnikowski,Yanqi Zhou,Yangsibo Huang,Jiazhong Nie,Ali Shah,Ashish Thapliyal,Sam Haves,Lun Wang,Uri Shaham,Patrick Morris-Suzuki,Soroush Radpour,Leonard Berrada,Thomas Strohmann,Chaochao Yan,Jingwei Shen,Sonam Goenka,Tris Warkentin,Petar Dević,Dan Belov,Albert Webson,Madhavi Yenugula,Puranjay Datta,Jerry Chang,Nimesh Ghelani,Aviral Kumar,Vincent Perot,Jessica Lo,Yang Song,Herman Schmit,Jianmin Chen,Vasilisa Bashlovkina,Xiaoyue Pan,Diana Mincu,Paul Roit,Isabel Edkins,Andy Davis,Yujia Li,Ben Horn,Xinjian Li,Pradeep Kumar S,Eric Doi,Wanzheng Zhu,Sri Gayatri Sundara Padmanabhan,Siddharth Verma,Jasmine Liu,Heng Chen,Mihajlo Velimirović,Malcolm Reynolds,Priyanka Agrawal,Nick Sukhanov,Abhinit Modi,Siddharth Goyal,John Palowitch,Nima Khajehnouri,Wing Lowe,David Klinghoffer,Sharon Silver,Vinh Tran,Candice Schumann,Francesco Piccinno,Xi Liu,Mario Lučić,Xiaochen Yang,Sandeep Kumar,Ajay Kannan,Ragha Kotikalapudi,Mudit Bansal,Fabian Fuchs,Javad Hosseini,Abdelrahman Abdelhamed,Dawn Bloxwich,Tianhe Yu,Ruoxin Sang,Gregory Thornton,Karan Gill,Yuchi Liu,Virat Shejwalkar,Jason Lin,Zhipeng Yan,Kehang Han,Thomas Buschmann,Michael Pliskin,Zhi Xing,Susheel Tatineni,Junlin Zhang,Sissie Hsiao,Gavin Buttimore,Marcus Wu,Zefei Li,Geza Kovacs,Legg Yeung,Tao Huang,Aaron Cohen,Bethanie Brownfield,Averi Nowak,Mikel Rodriguez,Tianze Shi,Hado van Hasselt,Kevin Cen,Deepanway Ghoshal,Kushal Majmundar,Weiren Yu,Warren,Chen,Danila Sinopalnikov,Hao Zhang,Vlado Galić,Di Lu,Zeyu Zheng,Maggie Song,Gary Wang,Gui Citovsky,Swapnil Gawde,Isaac Galatzer-Levy,David Silver,Ivana Balazevic,Dipanjan Das,Kingshuk Majumder,Yale Cong,Praneet Dutta,Dustin Tran,Hui Wan,Junwei Yuan,Daniel Eppens,Alanna Walton,Been Kim,Harry Ragan,James Cobon-Kerr,Lu Liu,Weijun Wang,Bryce Petrini,Jack Rae,Rakesh Shivanna,Yan Xiong,Chace Lee,Pauline Coquinot,Yiming Gu,Lisa Patel,Blake Hechtman,Aviel Boag,Orion Jankowski,Alex Wertheim,Alex Lee,Paul Covington,Hila Noga,Sam Sobell,Shanthal Vasanth,William Bono,Chirag Nagpal,Wei Fan,Xavier Garcia,Kedar Soparkar,Aybuke Turker,Nathan Howard,Sachit Menon,Yuankai Chen,Vikas Verma,Vladimir Pchelin,Harish Rajamani,Valentin Dalibard,Ana Ramalho,Yang Guo,Kartikeya Badola,Seojin Bang,Nathalie Rauschmayr,Julia Proskurnia,Sudeep Dasari,Xinyun Chen,Mikhail Sushkov,Anja Hauth,Pauline Sho,Abhinav Singh,Bilva Chandra,Allie Culp,Max Dylla,Olivier Bachem,James Besley,Heri Zhao,Timothy Lillicrap,Wei Wei,Wael Al Jishi,Ning Niu,Alban Rrustemi,Raphaël Lopez Kaufman,Ryan Poplin,Jewel Zhao,Minh Truong,Shikhar Bharadwaj,Ester Hlavnova,Eli Stickgold,Cordelia Schmid,Georgi Stephanov,Zhaoqi Leng,Frederick Liu,Léonard Hussenot,Shenil Dodhia,Juliana Vicente Franco,Lesley Katzen,Abhanshu Sharma,Sarah Cogan,Zuguang Yang,Aniket Ray,Sergi Caelles,Shen Yan,Ravin Kumar,Daniel Gillick,Renee Wong,Joshua Ainslie,Jonathan Hoech,Séb Arnold,Dan Abolafia,Anca Dragan,Ben Hora,Grace Hu,Alexey Guseynov,Yang Lu,Chas Leichner,Jinmeng Rao,Abhimanyu Goyal,Nagabhushan Baddi,Daniel Hernandez Diaz,Tim McConnell,Max Bain,Jake Abernethy,Qiqi Yan,Rylan Schaeffer,Paul Vicol,Will Thompson,Montse Gonzalez Arenas,Mathias Bellaiche,Pablo Barrio,Stefan Zinke,Riccardo Patana,Pulkit Mehta,JK Kearns,Avraham Ruderman,Scott Pollom,David D'Ambrosio,Cath Hope,Yang Yu,Andrea Gesmundo,Kuang-Huei Lee,Aviv Rosenberg,Yiqian Zhou,Yaoyiran Li,Drew Garmon,Yonghui Wu,Safeen Huda,Gil Fidel,Martin Baeuml,Jian Li,Phoebe Kirk,Rhys May,Tao Tu,Sara Mc Carthy,Toshiyuki Fukuzawa,Miranda Aperghis,Chih-Kuan Yeh,Toshihiro Yoshino,Bo Li,Austin Myers,Kaisheng Yao,Ben Limonchik,Changwan Ryu,Rohun Saxena,Alex Goldin,Ruizhe Zhao,Rocky Rhodes,Tao Zhu,Divya Tyam,Heidi Howard,Nathan Byrd,Hongxu Ma,Yan Wu,Ryan Mullins,Qingze Wang,Aida Amini,Sebastien Baur,Yiran Mao,Subhashini Venugopalan,Will Song,Wen Ding,Paul Collins,Sashank Reddi,Megan Shum,Andrei Rusu,Luisa Zintgraf,Kelvin Chan,Sheela Goenka,Mathieu Blondel,Michael Collins,Renke Pan,Marissa Giustina,Nikolai Chinaev,Christian Schuler,Ce Zheng,Jonas Valfridsson,Alyssa Loo,Alex Yakubovich,Jamie Smith,Tao Jiang,Rich Munoz,Gabriel Barcik,Rishabh Bansal,Mingyao Yang,Yilun Du,Pablo Duque,Mary Phuong,Alexandra Belias,Kunal Lad,Zeyu Liu,Tal Schuster,Karthik Duddu,Jieru Hu,Paige Kunkle,Matthew Watson,Jackson Tolins,Josh Smith,Denis Teplyashin,Garrett Bingham,Marvin Ritter,Marco Andreetto,Divya Pitta,Mohak Patel,Shashank Viswanadha,Trevor Strohman,Catalin Ionescu,Jincheng Luo,Yogesh Kalley,Jeremy Wiesner,Dan Deutsch,Derek Lockhart,Peter Choy,Rumen Dangovski,Chawin Sitawarin,Cat Graves,Tanya Lando,Joost van Amersfoort,Ndidi Elue,Zhouyuan Huo,Pooya Moradi,Jean Tarbouriech,Henryk Michalewski,Wenting Ye,Eunyoung Kim,Alex Druinsky,Florent Altché,Xinyi Chen,Artur Dwornik,Da-Cheng Juan,Rivka Moroshko,Horia Toma,Jarrod Kahn,Hai Qian,Maximilian Sieb,Irene Cai,Roman Goldenberg,Praneeth Netrapalli,Sindhu Raghuram,Yuan Gong,Lijie Fan,Evan Palmer,Yossi Matias,Valentin Gabeur,Shreya Pathak,Tom Ouyang,Don Metzler,Geoff Bacon,Srinivasan Venkatachary,Sridhar Thiagarajan,Alex Cullum,Eran Ofek,Vytenis Sakenas,Mohamed Hammad,Cesar Magalhaes,Mayank Daswani,Oscar Chang,Ashok Popat,Ruichao Li,Komal Jalan,Yanhan Hou,Josh Lipschultz,Antoine He,Wenhao Jia,Pier Giuseppe Sessa,Prateek Kolhar,William Wong,Sumeet Singh,Lukas Haas,Jay Whang,Hanna Klimczak-Plucińska,Georges Rotival,Grace Chung,Yiqing Hua,Anfal Siddiqui,Nicolas Serrano,Dongkai Chen,Billy Porter,Libin Bai,Keshav Shivam,Sho Arora,Partha Talukdar,Tom Cobley,Sangnie Bhardwaj,Evgeny Gladchenko,Simon Green,Kelvin Guu,Felix Fischer,Xiao Wu,Eric Wang,Achintya Singhal,Tatiana Matejovicova,James Martens,Hongji Li,Roma Patel,Elizabeth Kemp,Jiaqi Pan,Lily Wang,Blake JianHang Chen,Jean-Baptiste Alayrac,Navneet Potti,Erika Gemzer,Eugene Ie,Kay McKinney,Takaaki Saeki,Edward Chou,Pascal Lamblin,SQ Mah,Zach Fisher,Martin Chadwick,Jon Stritar,Obaid Sarvana,Andrew Hogue,Artem Shtefan,Hadi Hashemi,Yang Xu,Jindong Gu,Sharad Vikram,Chung-Ching Chang,Sabela Ramos,Logan Kilpatrick,Weijuan Xi,Jenny Brennan,Yinghao Sun,Abhishek Jindal,Ionel Gog,Dawn Chen,Felix Wu,Jason Lee,Sudhindra Kopalle,Srinadh Bhojanapalli,Oriol Vinyals,Natan Potikha,Burcu Karagol Ayan,Yuan Yuan,Michael Riley,Piotr Stanczyk,Sergey Kishchenko,Bing Wang,Dan Garrette,Antoine Yang,Vlad Feinberg,CJ Carey,Javad Azizi,Viral Shah,Erica Moreira,Chongyang Shi,Josh Feldman,Elizabeth Salesky,Thomas Lampe,Aneesh Pappu,Duhyeon Kim,Jonas Adler,Avi Caciularu,Brian Walker,Yunhan Xu,Yochai Blau,Dylan Scandinaro,Terry Huang,Sam El-Husseini,Abhishek Sinha,Lijie Ren,Taylor Tobin,Patrik Sundberg,Tim Sohn,Vikas Yadav,Mimi Ly,Emily Xue,Jing Xiong,Afzal Shama Soudagar,Sneha Mondal,Nikhil Khadke,Qingchun Ren,Ben Vargas,Stan Bileschi,Sarah Chakera,Cindy Wang,Boyu Wang,Yoni Halpern,Joe Jiang,Vikas Sindhwani,Petre Petrov,Pranavaraj Ponnuramu,Sanket Vaibhav Mehta,Yu Watanabe,Betty Chan,Matheus Wisniewski,Trang Pham,Jingwei Zhang,Conglong Li,Dario de Cesare,Art Khurshudov,Alex Vasiloff,Melissa Tan,Zoe Ashwood,Bobak Shahriari,Maryam Majzoubi,Garrett Tanzer,Olga Kozlova,Robin Alazard,James Lee-Thorp,Nguyet Minh Phu,Isaac Tian,Junwhan Ahn,Andy Crawford,Lauren Lax,Yuan,Shangguan,Iftekhar Naim,David Ross,Oleksandr Ferludin,Tongfei Guo,Andrea Banino,Hubert Soyer,Xiaoen Ju,Dominika Rogozińska,Ishaan Malhi,Marcella Valentine,Daniel Balle,Apoorv Kulshreshtha,Maciej Kula,Yiwen Song,Sophia Austin,John Schultz,Roy Hirsch,Arthur Douillard,Apoorv Reddy,Michael Fink,Summer Yue,Khyatti Gupta,Adam Zhang,Norman Rink,Daniel McDuff,Lei Meng,András György,Yasaman Razeghi,Ricky Liang,Kazuki Osawa,Aviel Atias,Matan Eyal,Tyrone Hill,Nikolai Grigorev,Zhengdong Wang,Nitish Kulkarni,Rachel Soh,Ivan Lobov,Zachary Charles,Sid Lall,Kazuma Hashimoto,Ido Kessler,Victor Gomes,Zelda Mariet,Danny Driess,Alessandro Agostini,Canfer Akbulut,Jingcao Hu,Marissa Ikonomidis,Emily Caveness,Kartik Audhkhasi,Saurabh Agrawal,Ioana Bica,Evan Senter,Jayaram Mudigonda,Kelly Chen,Jingchen Ye,Xuanhui Wang,James Svensson,Philipp Fränken,Josh Newlan,Li Lao,Eva Schnider,Sami Alabed,Joseph Kready,Jesse Emond,Afief Halumi,Tim Zaman,Chengxi Ye,Naina Raisinghani,Vilobh Meshram,Bo Chang,Ankit Singh Rawat,Axel Stjerngren,Sergey Levi,Rui Wang,Xiangzhu Long,Mitchelle Rasquinha,Steven Hand,Aditi Mavalankar,Lauren Agubuzu,Sudeshna Roy,Junquan Chen,Jarek Wilkiewicz,Hao Zhou,Michal Jastrzebski,Qiong Hu,Agustin Dal Lago,Ramya Sree Boppana,Wei-Jen Ko,Jennifer Prendki,Yao Su,Zhi Li,Eliza Rutherford,Girish Ramchandra Rao,Ramona Comanescu,Adrià Puigdomènech,Qihang Chen,Dessie Petrova,Christine Chan,Vedrana Milutinovic,Felipe Tiengo Ferreira,Chin-Yi Cheng,Ming Zhang,Tapomay Dey,Sherry Yang,Ramesh Sampath,Quoc Le,Howard Zhou,Chu-Cheng Lin,Hoi Lam,Christine Kaeser-Chen,Kai Hui,Dean Hirsch,Tom Eccles,Basil Mustafa,Shruti Rijhwani,Morgane Rivière,Yuanzhong Xu,Junjie Wang,Xinyang Geng,Xiance Si,Arjun Khare,Cheolmin Kim,Vahab Mirrokni,Kamyu Lee,Khuslen Baatarsukh,Nathaniel Braun,Lisa Wang,Pallavi LV,Richard Tanburn,Yuvein,Zhu,Fangda Li,Setareh Ariafar,Dan Goldberg,Ken Burke,Daniil Mirylenka,Meiqi Guo,Olaf Ronneberger,Hadas Natalie Vogel,Liqun Cheng,Nishita Shetty,Johnson Jia,Thomas Jimma,Corey Fry,Ted Xiao,Martin Sundermeyer,Ryan Burnell,Yannis Assael,Mario Pinto,JD Chen,Rohit Sathyanarayana,Donghyun Cho,Jing Lu,Rishabh Agarwal,Sugato Basu,Lucas Gonzalez,Dhruv Shah,Meng Wei,Dre Mahaarachchi,Rohan Agrawal,Tero Rissa,Yani Donchev,Ramiro Leal-Cavazos,Adrian Hutter,Markus Mircea,Alon Jacovi,Faruk Ahmed,Jiageng Zhang,Shuguang Hu,Bo-Juen Chen,Jonni Kanerva,Guillaume Desjardins,Andrew Lee,Nikos Parotsidis,Asier Mujika,Tobias Weyand,Jasper Snoek,Jo Chick,Kai Chen,Paul Chang,Ethan Mahintorabi,Zi Wang,Tolly Powell,Orgad Keller,Abhirut Gupta,Claire Sha,Kanav Garg,Nicolas Heess,Ágoston Weisz,Cassidy Hardin,Bartek Wydrowski,Ben Coleman,Karina Zainullina,Pankaj Joshi,Alessandro Epasto,Terry Spitz,Binbin Xiong,Kai Zhao,Arseniy Klimovskiy,Ivy Zheng,Johan Ferret,Itay Yona,Waleed Khawaja,Jean-Baptiste Lespiau,Maxim Krikun,Siamak Shakeri,Timothee Cour,Bonnie Li,Igor Krivokon,Dan Suh,Alex Hofer,Jad Al Abdallah,Nikita Putikhin,Oscar Akerlund,Silvio Lattanzi,Anurag Kumar,Shane Settle,Himanshu Srivastava,Folawiyo Campbell-Ajala,Edouard Rosseel,Mihai Dorin Istin,Nishanth Dikkala,Anand Rao,Nick Young,Kate Lin,Dhruva Bhaswar,Yiming Wang,Jaume Sanchez Elias,Kritika Muralidharan,James Keeling,Dayou Du,Siddharth Gopal,Gregory Dibb,Charles Blundell,Manolis Delakis,Jacky Liang,Marco Tulio Ribeiro,Georgi Karadzhov,Guillermo Garrido,Ankur Bapna,Jiawei Cao,Adam Sadovsky,Pouya Tafti,Arthur Guez,Coline Devin,Yixian Di,Jinwei Xing,Chuqiao,Xu,Hanzhao Lin,Chun-Te Chu,Sameera Ponda,Wesley Helmholz,Fan Yang,Yue Gao,Sara Javanmardi,Wael Farhan,Alex Ramirez,Ricardo Figueira,Khe Chai Sim,Yuval Bahat,Ashwin Vaswani,Liangzhe Yuan,Gufeng Zhang,Leland Rechis,Hanjun Dai,Tayo Oguntebi,Alexandra Cordell,Eugénie Rives,Kaan Tekelioglu,Naveen Kumar,Bing Zhang,Aurick Zhou,Nikolay Savinov,Andrew Leach,Alex Tudor,Sanjay Ganapathy,Yanyan Zheng,Mirko Rossini,Vera Axelrod,Arnaud Autef,Yukun Zhu,Zheng Zheng,Mingda Zhang,Baochen Sun,Jie Ren,Nenad Tomasev,Nithish Kannan,Amer Sinha,Charles Chen,Louis O'Bryan,Alex Pak,Aditya Kusupati,Weel Yang,Deepak Ramachandran,Patrick Griffin,Seokhwan Kim,Philipp Neubeck,Craig Schiff,Tammo Spalink,Mingyang Ling,Arun Nair,Ga-Young Joung,Linda Deng,Avishkar Bhoopchand,Lora Aroyo,Tom Duerig,Jordan Griffith,Gabe Barth-Maron,Jake Ades,Alex Haig,Ankur Taly,Yunting Song,Paul Michel,Dave Orr,Dean Weesner,Corentin Tallec,Carrie Grimes Bostock,Paul Niemczyk,Andy Twigg,Mudit Verma,Rohith Vallu,Henry Wang,Marco Gelmi,Kiranbir Sodhia,Aleksandr Chuklin,Omer Goldman,Jasmine George,Liang Bai,Kelvin Zhang,Petar Sirkovic,Efrat Nehoran,Golan Pundak,Jiaqi Mu,Alice Chen,Alex Greve,Paulo Zacchello,David Amos,Heming Ge,Eric Noland,Colton Bishop,Jeffrey Dudek,Youhei Namiki,Elena Buchatskaya,Jing Li,Dorsa Sadigh,Masha Samsikova,Dan Malkin,Damien Vincent,Robert David,Rob Willoughby,Phoenix Meadowlark,Shawn Gao,Yan Li,Raj Apte,Amit Jhindal,Stein Xudong Lin,Alex Polozov,Zhicheng Wang,Tomas Mery,Anirudh GP,Varun Yerram,Sage Stevens,Tianqi Liu,Noah Fiedel,Charles Sutton,Matthew Johnson,Xiaodan Song,Kate Baumli,Nir Shabat,Muqthar Mohammad,Hao Liu,Marco Selvi,Yichao Zhou,Mehdi Hafezi Manshadi,Chu-ling Ko,Anthony Chen,Michael Bendersky,Jorge Gonzalez Mendez,Nisarg Kothari,Amir Zandieh,Yiling Huang,Daniel Andor,Ellie Pavlick,Idan Brusilovsky,Jitendra Harlalka,Sally Goldman,Andrew Lampinen,Guowang Li,Asahi Ushio,Somit Gupta,Lei Zhang,Chuyuan Kelly Fu,Madhavi Sewak,Timo Denk,Jed Borovik,Brendan Jou,Avital Zipori,Prateek Jain,Junwen Bai,Thang Luong,Jonathan Tompson,Alice Li,Li Liu,George Powell,Jiajun Shen,Alex Feng,Grishma Chole,Da Yu,Yinlam Chow,Tongxin Yin,Eric Malmi,Kefan Xiao,Yash Pande,Shachi Paul,Niccolò Dal Santo,Adil Dostmohamed,Sergio Guadarrama,Aaron Phillips,Thanumalayan Sankaranarayana Pillai,Gal Yona,Amin Ghafouri,Preethi Lahoti,Benjamin Lee,Dhruv Madeka,Eren Sezener,Simon Tokumine,Adrian Collister,Nicola De Cao,Richard Shin,Uday Kalra,Parker Beak,Emily Nottage,Ryo Nakashima,Ivan Jurin,Vikash Sehwag,Meenu Gaba,Junhao Zeng,Kevin R. McKee,Fernando Pereira,Tamar Yakar,Amayika Panda,Arka Dhar,Peilin Zhong,Daniel Sohn,Mark Brand,Lars Lowe Sjoesund,Viral Carpenter,Sharon Lin,Shantanu Thakoor,Marcus Wainwright,Ashwin Chaugule,Pranesh Srinivasan,Muye Zhu,Bernett Orlando,Jack Weber,Ayzaan Wahid,Gilles Baechler,Apurv Suman,Jovana Mitrović,Gabe Taubman,Honglin Yu,Helen King,Josh Dillon,Cathy Yip,Dhriti Varma,Tomas Izo,Levent Bolelli,Borja De Balle Pigem,Julia Di Trapani,Fotis Iliopoulos,Adam Paszke,Nishant Ranka,Joe Zou,Francesco Pongetti,Jed McGiffin,Alex Siegman,Rich Galt,Ross Hemsley,Goran Žužić,Victor Carbune,Tao Li,Myle Ott,Félix de Chaumont Quitry,David Vilar Torres,Yuri Chervonyi,Tomy Tsai,Prem Eruvbetine,Samuel Yang,Matthew Denton,Jake Walker,Slavica Andačić,Idan Heimlich Shtacher,Vittal Premachandran,Harshal Tushar Lehri,Cip Baetu,Damion Yates,Lampros Lamprou,Mariko Iinuma,Ioana Mihailescu,Ben Albrecht,Shachi Dave,Susie Sargsyan,Bryan Perozzi,Lucas Manning,Chiyuan Zhang,Denis Vnukov,Igor Mordatch,Raia Hadsell Wolfgang Macherey,Ryan Kappedal,Jim Stephan,Aditya Tripathi,Klaus Macherey,Jun Qian,Abhishek Bhowmick,Shekoofeh Azizi,Rémi Leblond,Shiva Mohan Reddy Garlapati,Timothy Knight,Matthew Wiethoff,Wei-Chih Hung,Anelia Angelova,Georgios Evangelopoulos,Pawel Janus,Dimitris Paparas,Matthew Rahtz,Ken Caluwaerts,Vivek Sampathkumar,Daniel Jarrett,Shadi Noghabi,Antoine Miech,Chak Yeung,Geoff Clark,Henry Prior,Fei Zheng,Jean Pouget-Abadie,Indro Bhattacharya,Kalpesh Krishna,Will Bishop,Zhe Yuan,Yunxiao Deng,Ashutosh Sathe,Kacper Krasowiak,Ciprian Chelba,Cho-Jui Hsieh,Kiran Vodrahalli,Buhuang Liu,Thomas Köppe,Amr Khalifa,Lubo Litchev,Pichi Charoenpanit,Reed Roberts,Sachin Yadav,Yasumasa Onoe,Desi Ivanov,Megha Mohabey,Vighnesh Birodkar,Nemanja Rakićević,Pierre Sermanet,Vaibhav Mehta,Krishan Subudhi,Travis Choma,Will Ng,Luheng He,Kathie Wang,Tasos Kementsietsidis,Shane Gu,Mansi Gupta,Andrew Nystrom,Mehran Kazemi,Timothy Chung,Nacho Cano,Nikhil Dhawan,Yufei Wang,Jiawei Xia,Trevor Yacovone,Eric Jia,Mingqing Chen,Simeon Ivanov,Ashrith Sheshan,Sid Dalmia,Paweł Stradomski,Pengcheng Yin,Salem Haykal,Congchao Wang,Dennis Duan,Neslihan Bulut,Greg Kochanski,Liam MacDermed,Namrata Godbole,Shitao Weng,Jingjing Chen,Rachana Fellinger,Ramin Mehran,Daniel Suo,Hisham Husain,Tong He,Kaushal Patel,Joshua Howland,Randall Parker,Kelvin Nguyen,Sharath Maddineni,Chris Rawles,Mina Khan,Shlomi Cohen-Ganor,Amol Mandhane,Xinyi Wu,Chenkai Kuang,Iulia Comşa,Ramya Ganeshan,Hanie Sedghi,Adam Bloniarz,Nuo Wang Pierse,Anton Briukhov,Petr Mitrichev,Anita Gergely,Serena Zhan,Allan Zhou,Nikita Saxena,Eva Lu,Josef Dean,Ashish Gupta,Nicolas Perez-Nieves,Renjie Wu,Cory McLean,Wei Liang,Disha Jindal,Anton Tsitsulin,Wenhao Yu,Kaiz Alarakyia,Tom Schaul,Piyush Patil,Peter Sung,Elijah Peake,Hongkun Yu,Feryal Behbahani,JD Co-Reyes,Alan Ansell,Sean Sun,Clara Barbu,Jonathan Lee,Seb Noury,James Allingham,Bilal Piot,Mohit Sharma,Christopher Yew,Ivan Korotkov,Bibo Xu,Demetra Brady,Goran Petrovic,Shibl Mourad,Claire Cui,Aditya Gupta,Parker Schuh,Saarthak Khanna,Anna Goldie,Abhinav Arora,Vadim Zubov,Amy Stuart,Mark Epstein,Yun Zhu,Jianqiao Liu,Yury Stuken,Ziyue Wang,Karolis Misiunas,Dee Guo,Ashleah Gill,Ale Hartman,Zaid Nabulsi,Aurko Roy,Aleksandra Faust,Jason Riesa,Ben Withbroe,Mengchao Wang,Marco Tagliasacchi,Andreea Marzoca,James Noraky,Serge Toropov,Malika Mehrotra,Bahram Raad,Sanja Deur,Steve Xu,Marianne Monteiro,Zhongru Wu,Yi Luan,Sam Ritter,Nick Li,Håvard Garnes,Yanzhang He,Martin Zlocha,Jifan Zhu,Matteo Hessel,Will Wu,Spandana Raj Babbula,Chizu Kawamoto,Yuanzhen Li,Mehadi Hassen,Yan Wang,Brian Wieder,James Freedman,Yin Zhang,Xinyi Bai,Tianli Yu,David Reitter,XiangHai Sheng,Mateo Wirth,Aditya Kini,Dima Damen,Mingcen Gao,Rachel Hornung,Michael Voznesensky,Brian Roark,Adhi Kuncoro,Yuxiang Zhou,Rushin Shah,Anthony Brohan,Kuangyuan Chen,James Wendt,David Rim,Paul Kishan Rubenstein,Jonathan Halcrow,Michelle Liu,Ty Geri,Yunhsuan Sung,Jane Shapiro,Shaan Bijwadia,Chris Duvarney,Christina Sorokin,Paul Natsev,Reeve Ingle,Pramod Gupta,Young Maeng,Ndaba Ndebele,Kexin Zhu,Valentin Anklin,Katherine Lee,Yuan Liu,Yaroslav Akulov,Shaleen Gupta,Guolong Su,Flavien Prost,Tianlin Liu,Vitaly Kovalev,Pol Moreno,Martin Scholz,Sam Redmond,Zongwei Zhou,Alex Castro-Ros,André Susano Pinto,Dia Kharrat,Michal Yarom,Rachel Saputro,Jannis Bulian,Ben Caine,Ji Liu,Abbas Abdolmaleki,Shariq Iqbal,Tautvydas Misiunas,Mikhail Sirotenko,Shefali Garg,Guy Bensky,Huan Gui,Xuezhi Wang,Raphael Koster,Mike Bernico,Da Huang,Romal Thoppilan,Trevor Cohn,Ben Golan,Wenlei Zhou,Andrew Rosenberg,Markus Freitag,Tynan Gangwani,Vincent Tsang,Anand Shukla,Xiaoqi Ren,Minh Giang,Chi Zou,Andre Elisseeff,Charline Le Lan,Dheeru Dua,Shuba Lall,Pranav Shyam,Frankie Garcia,Sarah Nguyen,Michael Guzman,AJ Maschinot,Marcello Maggioni,Ming-Wei Chang,Karol Gregor,Lotte Weerts,Kumaran Venkatesan,Bogdan Damoc,Leon Liu,Jan Wassenberg,Lewis Ho,Becca Roelofs,Majid Hadian,François-Xavier Aubet,Yu Liang,Sami Lachgar,Danny Karmon,Yong Cheng,Amelio Vázquez-Reina,Angie Chen,Zhuyun Dai,Andy Brock,Shubham Agrawal,Chenxi Pang,Peter Garst,Mariella Sanchez-Vargas,Ivor Rendulic,Aditya Ayyar,Andrija Ražnatović,Olivia Ma,Roopali Vij,Neha Sharma,Ashwin Balakrishna,Bingyuan Liu,Ian Mackinnon,Sorin Baltateanu,Petra Poklukar,Gabriel Ibagon,Colin Ji,Hongyang Jiao,Isaac Noble,Wojciech Stokowiec,Zhihao Li,Jeff Dean,David Lindner,Mark Omernick,Kristen Chiafullo,Mason Dimarco,Vitor Rodrigues,Vittorio Selo,Garrett Honke,Xintian,Wu,Wei He,Adam Hillier,Anhad Mohananey,Vihari Piratla,Chang Ye,Chase Malik,Sebastian Riedel,Samuel Albanie,Zi Yang,Kenny Vassigh,Maria Bauza,Sheng Li,Yiqing Tao,Nevan Wichers,Andrii Maksai,Abe Ittycheriah,Ross Mcilroy,Bryan Seybold,Noah Goodman,Romina Datta,Steven M. Hernandez,Tian Shi,Yony Kochinski,Anna Bulanova,Ken Franko,Mikita Sazanovich,Nicholas FitzGerald,Praneeth Kacham,Shubha Srinivas Raghvendra,Vincent Hellendoorn,Alexander Grushetsky,Julian Salazar,Angeliki Lazaridou,Jason Chang,Jan-Thorsten Peter,Sushant Kafle,Yann Dauphin,Abhishek Rao,Filippo Graziano,Izhak Shafran,Yuguo Liao,Tianli Ding,Geng Yan,Grace Chu,Zhao Fu,Vincent Roulet,Gabriel Rasskin,Duncan Williams,Shahar Drath,Alex Mossin,Raphael Hoffmann,Jordi Orbay,Francesco Bertolini,Hila Sheftel,Justin Chiu,Siyang Xue,Yuheng Kuang,Ferjad Naeem,Swaroop Nath,Nana Nti,Phil Culliton,Kashyap Krishnakumar,Michael Isard,Pei Sun,Ayan Chakrabarti,Nathan Clement,Regev Cohen,Arissa Wongpanich,GS Oh,Ashwin Murthy,Hao Zheng,Jessica Hamrick,Oskar Bunyan,Suhas Ganesh,Nitish Gupta,Roy Frostig,John Wieting,Yury Malkov,Pierre Marcenac,Zhixin,Lai,Xiaodan Tang,Mohammad Saleh,Fedir Zubach,Chinmay Kulkarni,Huanjie Zhou,Vicky Zayats,Nan Ding,Anshuman Tripathi,Arijit Pramanik,Patrik Zochbauer,Harish Ganapathy,Vedant Misra,Zach Behrman,Hugo Vallet,Mingyang Zhang,Mukund Sridhar,Ye Jin,Mohammad Babaeizadeh,Siim Põder,Megha Goel,Divya Jain,Tajwar Nasir,Shubham Mittal,Tim Dozat,Diego Ardila,Aliaksei Severyn,Fabio Pardo,Sammy Jerome,Siyang Qin,Louis Rouillard,Amir Yazdanbakhsh,Zizhao Zhang,Shivani Agrawal,Kaushik Shivakumar,Caden Lu,Praveen Kallakuri,Rachita Chhaparia,Kanishka Rao,Charles Kwong,Asya Fadeeva,Shitij Nigam,Yan Virin,Yuan Zhang,Balaji Venkatraman,Beliz Gunel,Marc Wilson,Huiyu Wang,Abhinav Gupta,Xiaowei Xu,Adrien Ali Taïga,Kareem Mohamed,Doug Fritz,Daniel Rodriguez,Zoubin Ghahramani,Harry Askham,Lior Belenki,James Zhao,Rahul Gupta,Krzysztof Jastrzębski,Takahiro Kosakai,Kaan Katircioglu,Jon Schneider,Rina Panigrahy,Konstantinos Bousmalis,Peter Grabowski,Prajit Ramachandran,Chaitra Hegde,Mihaela Rosca,Angelo Scorza Scarpati,Kyriakos Axiotis,Ying Xu,Zach Gleicher,Assaf Hurwitz Michaely,Mandar Sharma,Sanil Jain,Christoph Hirnschall,Tal Marian,Xuhui Jia,Kevin Mather,Kilol Gupta,Linhai Qiu,Nigamaa Nayakanti,Lucian Ionita,Steven Zheng,Lucia Loher,Kurt Shuster,Igor Petrovski,Roshan Sharma,Rahma Chaabouni,Angel Yeh,James An,Arushi Gupta,Steven Schwarcz,Seher Ellis,Sam Conway-Rahman,Javier Snaider,Alex Zhai,James Atwood,Daniel Golovin,Liqian Peng,Te I,Vivian Xia,Salvatore Scellato,Mahan Malihi,Arthur Bražinskas,Vlad-Doru Ion,Younghoon Jun,James Swirhun,Soroosh Mariooryad,Jiao Sun,Steve Chien,Rey Coaguila,Ariel Brand,Yi Gao,Tom Kwiatkowski,Roee Aharoni,Cheng-Chun Lee,Mislav Žanić,Yichi Zhang,Dan Ethier,Vitaly Nikolaev,Pranav Nair,Yoav Ben Shalom,Hen Fitoussi,Jai Gupta,Hongbin Liu,Dee Cattle,Tolga Bolukbasi,Ben Murdoch,Fantine Huot,Yin Li,Chris Hahn*

Main category: cs.CL

TL;DR: Gemini 2.X模型家族包括Gemini 2.5 Pro和2.5 Flash，以及早期的2.0 Flash和Flash-Lite。2.5 Pro在编码和推理任务上表现最佳，支持多模态理解和长视频处理；2.5 Flash则以较低计算需求提供优秀推理能力。


<details>
  <summary>Details</summary>
Motivation: 开发一系列模型以满足不同需求，覆盖模型能力与成本的全范围。

Method: 推出Gemini 2.X模型家族，包括高性能的2.5 Pro和低成本的2.5 Flash等。

Result: Gemini 2.5 Pro在多模态理解和长视频处理上表现突出，2.5 Flash在低计算需求下仍保持优秀推理能力。

Conclusion: Gemini 2.X模型家族为用户提供了从高性能到低成本的全方位选择，支持复杂代理工作流。

Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and
Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite
models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA
performance on frontier coding and reasoning benchmarks. In addition to its
incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that
excels at multimodal understanding and it is now able to process up to 3 hours
of video content. Its unique combination of long context, multimodal and
reasoning capabilities can be combined to unlock new agentic workflows. Gemini
2.5 Flash provides excellent reasoning abilities at a fraction of the compute
and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high
performance at low latency and cost. Taken together, the Gemini 2.X model
generation spans the full Pareto frontier of model capability vs cost, allowing
users to explore the boundaries of what is possible with complex agentic
problem solving.

</details>


### [10] [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
*Neil Rathi,Dan Jurafsky,Kaitlyn Zhou*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在多语言环境中存在过度自信和依赖风险，不同语言间的表达差异显著影响用户依赖行为。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在多语言环境中的安全性，尤其是其表达不确定性和局限性的准确性，以避免用户过度依赖自信的生成内容。

Method: 分析五种语言中LLM生成的认识标记分布，并测量用户在不同语言中对自信生成内容的依赖率。

Result: LLMs在所有语言中均表现出过度自信，但不同语言间表达差异显著（如日语中不确定性标记最多，德语和普通话中确定性标记最多）。用户在所有语言中均高度依赖自信生成内容，但依赖行为因语言而异（如日语用户更依赖不确定性表达）。

Conclusion: 多语言环境中的语言校准具有挑战性，需结合文化和语言背景进行模型安全性评估。

Abstract: As large language models (LLMs) are deployed globally, it is crucial that
their responses are calibrated across languages to accurately convey
uncertainty and limitations. Previous work has shown that LLMs are
linguistically overconfident in English, leading users to overrely on confident
generations. However, the usage and interpretation of epistemic markers (e.g.,
'It's definitely,' 'I think') can differ sharply across languages. Here, we
study the risks of multilingual linguistic (mis)calibration, overconfidence,
and overreliance across five languages to evaluate the safety of LLMs in a
global context.
  We find that overreliance risks are high across all languages. We first
analyze the distribution of LLM-generated epistemic markers, and observe that
while LLMs are cross-linguistically overconfident, they are also sensitive to
documented linguistic variation. For example, models generate the most markers
of uncertainty in Japanese and the most markers of certainty in German and
Mandarin. We then measure human reliance rates across languages, finding that
while users strongly rely on confident LLM generations in all languages,
reliance behaviors differ cross-linguistically: for example, users rely
significantly more on expressions of uncertainty in Japanese than in English.
Taken together, these results indicate high risk of reliance on overconfident
model generations across languages. Our findings highlight the challenges of
multilingual linguistic calibration and stress the importance of culturally and
linguistically contextualized model safety evaluations.

</details>


### [11] [ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time](https://arxiv.org/abs/2507.06313)
*Kiarash Zahirnia,Zahra Golpayegani,Walid Ahmad,Yang Liu*

Main category: cs.CL

TL;DR: 提出了一种名为ETT的方法，用于扩展短上下文Transformer模型的上下文长度，具有恒定内存需求和线性计算开销，显著提升了模型在长序列任务中的表现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的计算和内存开销随序列长度呈二次增长，限制了其在长序列处理中的应用。

Method: ETT通过在测试时对输入上下文进行高效微调，将其分块为重叠的小子序列，从而扩展上下文长度。

Result: 在LongBench上，ETT将GPT-Large和Phi-2的上下文长度从1k扩展到32k，准确率提升高达30%。

Conclusion: 研究发现仅微调FFN的第二层比全微调更有效，进一步提升了模型准确率。

Abstract: Transformer-based Language Models' computation and memory overhead increase
quadratically as a function of sequence length. The quadratic cost poses
challenges when employing LLMs for processing long sequences. In this work, we
introduce \ourmodelacronym~(Extend at Test-Time), method for extending the
context length of short context Transformer-based LLMs, with constant memory
requirement and linear computation overhead. ETT enable the extension of the
context length at test-time by efficient fine-tuning the model's parameters on
the input context, chunked into overlapping small subsequences. We evaluate ETT
on LongBench by extending the context length of GPT-Large and Phi-2 up to 32
times, increasing from 1k to 32k tokens. This results in up to a 30 percent
improvement in the model's accuracy. We also study how context can be stored in
LLM's weights effectively and efficiently. Through a detailed ablation study,
we examine which Transformer modules are most beneficial to fine-tune at
test-time. Interestingly, we find that fine-tuning the second layer of the FFNs
is more effective than full fine-tuning, leading to a further improvement in
the models' accuracy.

</details>


### [12] [Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?](https://arxiv.org/abs/2507.06335)
*Casey Kennington,David Schlangen*

Main category: cs.CL

TL;DR: 本文提出通过“词作为分类器”模型统一形式、分布和接地语义理论，结合认知科学和实验支持。


<details>
  <summary>Details</summary>
Motivation: 结合形式、分布和接地语义理论的优点，解决各自局限性。

Method: 回顾文献，结合认知科学，进行小规模实验，提出统一模型。

Result: “词作为分类器”模型在对话环境中表现良好，具有潜力。

Conclusion: “词作为分类器”模型是统一三种语义理论的有前景的路径。

Abstract: Formal, Distributional, and Grounded theories of computational semantics each
have their uses and their drawbacks. There has been a shift to ground models of
language by adding visual knowledge, and there has been a call to enrich models
of language with symbolic methods to gain the benefits from formal,
distributional, and grounded theories. In this paper, we attempt to make the
case that one potential path forward in unifying all three semantic fields is
paved with the words-as-classifier model, a model of word-level grounded
semantics that has been incorporated into formalisms and distributional
language models in the literature, and it has been well-tested within
interactive dialogue settings. We review that literature, motivate the
words-as-classifiers model with an appeal to recent work in cognitive science,
and describe a small experiment. Finally, we sketch a model of semantics
unified through words-as-classifiers.

</details>


### [13] [Evaluating Morphological Alignment of Tokenizers in 70 Languages](https://arxiv.org/abs/2507.06378)
*Catherine Arnett,Marisa Hudspeth,Brendan O'Connor*

Main category: cs.CL

TL;DR: 论文扩展了MorphScore以支持70种语言，并探讨了分词器质量评估中形态对齐与模型性能的关系，发现形态对齐对模型性能影响有限。


<details>
  <summary>Details</summary>
Motivation: 评估分词器质量时，形态对齐是否与模型性能相关尚不明确，因此需要扩展MorphScore并验证其相关性。

Method: 扩展MorphScore至70种语言，计算形态对齐分数，并与五种预训练语言模型在七项任务中的性能进行相关性分析。

Result: 形态对齐分数对模型性能的方差解释较少，表明形态对齐本身不足以衡量分词器质量对模型性能的影响。

Conclusion: 形态对齐并非衡量分词器质量对模型性能的关键维度，需探索其他评估方法。

Abstract: While tokenization is a key step in language modeling, with effects on model
training and performance, it remains unclear how to effectively evaluate
tokenizer quality. One proposed dimension of tokenizer quality is the extent to
which tokenizers preserve linguistically meaningful subwords, aligning token
boundaries with morphological boundaries within a word. We expand MorphScore
(Arnett & Bergen, 2025), which previously covered 22 languages, to support a
total of 70 languages. The updated MorphScore offers more flexibility in
evaluation and addresses some of the limitations of the original version. We
then correlate our alignment scores with downstream task performance for five
pre-trained languages models on seven tasks, with at least one task in each of
the languages in our sample. We find that morphological alignment does not
explain very much variance in model performance, suggesting that morphological
alignment alone does not measure dimensions of tokenization quality relevant to
model performance.

</details>


### [14] [Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles](https://arxiv.org/abs/2507.06393)
*Matilde Marcolli,Riny Huijbregts,Richard K. Larson*

Main category: cs.CL

TL;DR: 论文展示了句法对象的头函数将岩浆结构扩展为超岩浆结构，并通过彩色操作ad的形式描述句法生成系统，将句法规则统一为操作ad生成器的形式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过数学结构（如超岩浆和彩色操作ad）统一描述句法生成和移动规则，简化理论框架。

Method: 使用超岩浆结构和彩色操作ad理论，将句法对象的生成和过滤过程形式化，并与句法规则（如移动、相位）关联。

Result: 成功将句法规则（如内部合并、相位不可穿透性）统一为彩色操作ad生成器的形式，并展示了移动与相位结构的兼容性。

Conclusion: 通过数学结构（超岩浆和彩色操作ad）可以统一描述句法生成和规则，为句法理论提供了新的形式化工具。

Abstract: We show that head functions on syntactic objects extend the magma structure
to a hypermagma, with the c-command relation compatible with the magma
operation and the m-command relation with the hypermagma. We then show that the
structure of head and complement and specifier, additional modifier positions,
and the structure of phases in the Extended Projection can be formulated as a
bud generating system of a colored operad, in a form similar to the structure
of theta roles. We also show that, due to the special form of the colored
operad generators, the filtering of freely generated syntactic objects by these
coloring rules can be equivalently formulated as a filtering in the course of
structure formation via a colored Merge, which can in turn be related to the
hypermagma structure. The rules on movement by Internal Merge with respect to
phases, the Extended Projection Principle, Empty Category Principle, and Phase
Impenetrability Condition are all subsumed into the form of the colored operad
generators. Movement compatibilities between the phase structure and the theta
roles assignments can then be formulated in terms of the respective colored
operads and a transduction of colored operads.

</details>


### [15] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: PERK提出了一种参数高效的方法，通过梯度更新轻量级适配器在测试时编码长上下文，显著提升了长上下文推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文推理中传统方法内存消耗大、难以扩展的问题。

Method: 采用双嵌套优化循环的元训练方法，内循环快速编码上下文到低秩适配器，外循环学习利用适配器进行推理。

Result: 在多个长上下文任务中，PERK显著优于基线方法，性能提升高达90%（小模型）和27%（大模型）。

Conclusion: PERK在训练时内存消耗大，但在推理时更高效，且对推理复杂性、上下文长度和信息位置更鲁棒。

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [16] [Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling](https://arxiv.org/abs/2507.06419)
*Pankayaraj Pathmanathan,Furong Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为REFORM的自改进奖励建模框架，通过奖励引导的解码发现奖励模型的失败模式，并利用生成的对抗样本增强训练数据，提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于人类偏好的复杂性和数据集的有限覆盖，奖励模型在分布偏移或对抗扰动下容易失效，现有方法依赖先验知识，实用性受限。

Method: 提出了一种不依赖偏好分布先验的方法，通过奖励引导的解码发现失败模式，并引入REFORM框架，利用对抗样本增强训练数据。

Result: 在Anthropic HH和PKU Beavertails数据集上，REFORM显著提升了鲁棒性，同时保持了奖励质量和对齐性能。

Conclusion: REFORM通过自改进机制有效提升了奖励模型的鲁棒性和对齐质量，解决了现有方法的局限性。

Abstract: Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.

</details>


### [17] [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
*Shun Wang,Tyler Loakman,Youbo Lei,Yi Liu,Bohao Yang,Yuting Zhao,Dong Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 论文提出了一种基于字典学习和稀疏自编码器的大语言模型分解方法，提取单义特征并改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型被视为黑箱算法，缺乏透明性且难以优化下游任务性能。

Method: 采用字典学习和稀疏自编码器分解LLM，提取单义特征并自动优化提示。

Result: 方法显著提升了数学推理和隐喻检测等下游任务的性能。

Conclusion: 该分解方法提高了LLM的可解释性和性能，为模型优化提供了新途径。

Abstract: Large Language Models (LLMs) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective LLM decomposition method using a dictionary-learning
approach with sparse autoencoders. This helps extract monosemantic features
from polysemantic LLM neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by LLMs. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.

</details>


### [18] [Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling](https://arxiv.org/abs/2507.06435)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.CL

TL;DR: 论文提出了一种基于动态嵌入主题模型（DETM）的新方法，用于分析全球气候政策话语的演变，揭示了从早期关注温室气体到近期聚焦实施与合作的转变。


<details>
  <summary>Details</summary>
Motivation: 理解政策语言的演变对评估全球应对复杂挑战（如气候变化）至关重要，传统方法效率低且难以捕捉复杂性。

Method: 应用DETM模型分析UNFCCC政策决策（1995-2023年），包括预处理、模型训练和可视化。

Result: DETM有效捕捉了政策主题的演变，如从温室气体转向实施、合作与资金等。

Conclusion: DETM是分析政策话语演变的可扩展工具，未来可扩展至其他政策领域。

Abstract: Understanding how policy language evolves over time is critical for assessing
global responses to complex challenges such as climate change. Temporal
analysis helps stakeholders, including policymakers and researchers, to
evaluate past priorities, identify emerging themes, design governance
strategies, and develop mitigation measures. Traditional approaches, such as
manual thematic coding, are time-consuming and limited in capturing the
complex, interconnected nature of global policy discourse. With the increasing
relevance of unsupervised machine learning, these limitations can be addressed,
particularly under high-volume, complex, and high-dimensional data conditions.
In this work, we explore a novel approach that applies the dynamic embedded
topic model (DETM) to analyze the evolution of global climate policy discourse.
A probabilistic model designed to capture the temporal dynamics of topics over
time. We collected a corpus of United Nations Framework Convention on Climate
Change (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the
postponement of COP26 as a result of the COVID-19 pandemic. The model reveals
shifts from early emphases on greenhouse gases and international conventions to
recent focuses on implementation, technical collaboration, capacity building,
finance, and global agreements. Section 3 presents the modeling pipeline,
including preprocessing, model training, and visualization of temporal word
distributions. Our results show that DETM is a scalable and effective tool for
analyzing the evolution of global policy discourse. Section 4 discusses the
implications of these findings and we concluded with future directions and
refinements to extend this approach to other policy domains.

</details>


### [19] [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
*Zhenhailong Wang,Xuehang Guo,Sofia Stoica,Haiyang Xu,Hongru Wang,Hyeonjeong Ha,Xiusi Chen,Yangyi Chen,Ming Yan,Fei Huang,Heng Ji*

Main category: cs.CL

TL;DR: PAPO扩展了GRPO，通过隐式感知损失提升多模态推理任务中的视觉感知能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR在多模态任务中表现不佳，主要源于视觉输入感知的误差。

Method: 提出PAPO，通过KL散度项引入隐式感知损失，无需额外数据或外部奖励模型。

Result: 多模态基准上整体提升4.4%，高视觉依赖任务提升8.0%，感知错误减少30.5%。

Conclusion: PAPO为RL框架提供了视觉感知与推理的深度融合，奠定新基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.

</details>


### [20] [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
*Xin Su,Sungduk Yu,Phillip Howard,Steven Bethard*

Main category: cs.CL

TL;DR: 论文提出了一种基于SCATE框架的时间规范化方法，通过代码生成任务实现，利用大语言模型生成可执行代码，并通过数据增强提升小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于ISO-TimeML的系统表达能力有限，难以处理复杂时间表达式，因此需要一种更灵活且可扩展的方法。

Method: 将时间规范化任务转化为代码生成任务，基于SCATE框架定义时间语义，利用大语言模型生成可执行代码，并通过数据增强生成大规模标注数据。

Result: 实验表明，小模型在增强数据上训练后性能优于大语言模型，实现了高效、准确且可解释的时间规范化。

Conclusion: 该方法为时间规范化提供了新的解决方案，兼具实用性和可扩展性。

Abstract: Time normalization is the task of converting natural language temporal
expressions into machine-readable representations. It underpins many downstream
applications in information retrieval, question answering, and clinical
decision-making. Traditional systems based on the ISO-TimeML schema limit
expressivity and struggle with complex constructs such as compositional,
event-relative, and multi-span time expressions. In this work, we introduce a
novel formulation of time normalization as a code generation task grounded in
the SCATE framework, which defines temporal semantics through symbolic and
compositional operators. We implement a fully executable SCATE Python library
and demonstrate that large language models (LLMs) can generate executable SCATE
code. Leveraging this capability, we develop an automatic data augmentation
pipeline using LLMs to synthesize large-scale annotated data with code-level
validation. Our experiments show that small, locally deployable models trained
on this augmented data can achieve strong performance, outperforming even their
LLM parents and enabling practical, accurate, and interpretable time
normalization.

</details>


### [21] [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
*Dustin Wang,Rui-Jie Zhu,Steven Abreu,Yong Shan,Taylor Kergan,Yuqi Pan,Yuhong Chou,Zheng Li,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 论文系统评估了多种线性注意力模型在混合架构中的表现，发现语言建模稳定但召回性能随全注意力层增加而提升，推荐HGRN-2或GatedDeltaNet等架构。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列中的二次复杂度和内存问题，同时探索线性注意力模型在混合架构中的选择。

Method: 训练并开源72个模型，涵盖六种线性注意力变体和五种混合比例，评估其在语言建模和召回任务中的表现。

Result: 语言建模性能稳定，召回性能在全注意力层增加时显著提升，特别是在3:1比例以下。

Conclusion: 选择性门控、分层递归和可控遗忘对混合模型至关重要，推荐HGRN-2或GatedDeltaNet架构，混合比例在3:1至6:1之间。

Abstract: Transformers face quadratic complexity and memory issues with long sequences,
prompting the adoption of linear attention mechanisms using fixed-size hidden
states. However, linear models often suffer from limited recall performance,
leading to hybrid architectures that combine linear and full attention layers.
Despite extensive hybrid architecture research, the choice of linear attention
component has not been deeply explored. We systematically evaluate various
linear attention models across generations - vector recurrences to advanced
gating mechanisms - both standalone and hybridized. To enable this
comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M
parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six
linear attention variants across five hybridization ratios. Benchmarking on
standard language modeling and recall tasks reveals that superior standalone
linear models do not necessarily excel in hybrids. While language modeling
remains stable across linear-to-full attention ratios, recall significantly
improves with increased full attention layers, particularly below a 3:1 ratio.
Our study highlights selective gating, hierarchical recurrence, and controlled
forgetting as critical for effective hybrid models. We recommend architectures
such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1
to achieve Transformer-level recall efficiently. Our models are open-sourced at
https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.

</details>


### [22] [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489)
*Stephen Obadinma,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）的言语置信度在对抗攻击下表现脆弱，现有防御方法效果不佳，需设计更鲁棒的置信度表达机制。


<details>
  <summary>Details</summary>
Motivation: 确保LLMs在人类-AI交互中的透明性、信任和安全性，特别是在高风险应用中。

Method: 提出一种新框架，通过扰动和越狱方法攻击言语置信度，评估不同提示策略、模型规模和应用领域。

Result: 攻击显著影响置信度估计并导致频繁答案变化，现有置信度诱导方法和防御技术效果有限。

Conclusion: 需设计更鲁棒的LLMs置信度表达机制，以应对语义保留修改带来的误导性置信度。

Abstract: Robust verbal confidence generated by large language models (LLMs) is crucial
for the deployment of LLMs to ensure transparency, trust, and safety in
human-AI interactions across many high-stakes applications. In this paper, we
present the first comprehensive study on the robustness of verbal confidence
under adversarial attacks. We introduce a novel framework for attacking verbal
confidence scores through both perturbation and jailbreak-based methods, and
show that these attacks can significantly jeopardize verbal confidence
estimates and lead to frequent answer changes. We examine a variety of
prompting strategies, model sizes, and application domains, revealing that
current confidence elicitation methods are vulnerable and that commonly used
defence techniques are largely ineffective or counterproductive. Our findings
underscore the urgent need to design more robust mechanisms for confidence
expression in LLMs, as even subtle semantic-preserving modifications can lead
to misleading confidence in responses.

</details>


### [23] [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
*Russell Taylor,Benjamin Herbert,Michael Sana*

Main category: cs.CL

TL;DR: 提出了一种结合大型语言模型和专门技术的新方法，用于将英语双关语翻译成法语，并在竞赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决双关语翻译中语义和语音复杂性的挑战，填补翻译研究与计算语言学之间的空白。

Method: 采用三阶段方法：基线模型、引导思维链管道和多代理生成-判别框架。

Result: 在CLEF JOKER 2025竞赛中获得第一和第二名。

Conclusion: 通过语言学技术提升语言模型处理双关语的能力，为幽默翻译提供新思路。

Abstract: Translating wordplay across languages presents unique challenges that have
long confounded both professional human translators and machine translation
systems. This research proposes a novel approach for translating puns from
English to French by combining state-of-the-art large language models with
specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a
baseline using multiple frontier large language models with feedback based on a
new contrastive learning dataset. Second, we implement a guided
chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we
implement a multi-agent generator-discriminator framework for evaluating and
regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's
primary objective is to capture the linguistic creativity and humor of the
source text wordplay, rather than simply duplicating its vocabulary. Our best
runs earned first and second place in the CLEF JOKER 2025 Task 2 competition
where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational
linguistics by implementing linguistically-informed techniques for wordplay
translation, advancing our understanding of how language models can be
leveraged to handle the complex interplay between semantic ambiguity, phonetic
similarity, and the implicit cultural and linguistic awareness needed for
successful humor.

</details>


### [24] [SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](https://arxiv.org/abs/2507.06517)
*Zicong Tang,Shi Luohe,Zuchao Li,Baoyuan Qi,Guoming Liu,Lefei Zhang,Ping Wang*

Main category: cs.CL

TL;DR: SpindleKV是一种新型的KV缓存减少方法，通过注意力权重和代码本替换策略平衡浅层和深层，解决了GQA问题，实验表明其效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的KV缓存内存消耗问题日益严重，现有方法在浅层缓存减少上效果不足，且KV缓存存在高度相似性。

Method: 提出SpindleKV方法：深层采用基于注意力权重的淘汰策略，浅层采用基于代码本的替换策略，学习相似性和合并策略，并解决GQA问题。

Result: 在三种不同LLM的两个常见基准测试中，SpindleKV在KV缓存减少效果上优于基线方法，同时保持或提升模型性能。

Conclusion: SpindleKV通过平衡浅层和深层策略，有效减少KV缓存，同时解决GQA问题，为LLM推理系统提供了高效解决方案。

Abstract: Large Language Models (LLMs) have achieved impressive accomplishments in
recent years. However, the increasing memory consumption of KV cache has
possessed a significant challenge to the inference system. Eviction methods
have revealed the inherent redundancy within the KV cache, demonstrating its
potential for reduction, particularly in deeper layers. However, KV cache
reduction for shallower layers has been found to be insufficient. Based on our
observation that, the KV cache exhibits a high degree of similarity. Based on
this observation, we proposed a novel KV cache reduction method, SpindleKV,
which balances both shallow and deep layers. For deep layers, we employ an
attention weight based eviction method, while for shallow layers, we apply a
codebook based replacement approach which is learnt by similarity and merging
policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma
faced by other attention based eviction methods. Experiments on two common
benchmarks with three different LLMs shown that SpindleKV obtained better KV
cache reduction effect compared to baseline methods, while preserving similar
or even better model performance.

</details>


### [25] [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
*Huisheng Wang,Zhuoshi Pan,Hangjing Zhang,Mingxiao Liu,Hanqing Gao,H. Vicky Zhao*

Main category: cs.CL

TL;DR: 提出InvestAlign框架，通过理论解构建高质量SFT数据集，提升LLM在投资决策中的对齐效率。


<details>
  <summary>Details</summary>
Motivation: 解决行为金融中LLM与投资者决策对齐的挑战，避免真实用户数据的高成本和隐私风险。

Method: 利用理论解构建SFT数据集，训练LLM（InvestAgent），并验证其性能。

Result: InvestAlign生成的数据使LLM参数收敛更快，InvestAgent在简单和复杂问题中更接近真实用户数据。

Conclusion: InvestAlign是解决复杂投资问题和LLM对齐的有效方法。

Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes
under herd behavior is a critical challenge in behavioral finance, which
grapples with a fundamental limitation: the scarcity of real-user data needed
for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM
outputs and human behavioral patterns, its reliance on massive authentic data
imposes substantial collection costs and privacy risks. We propose InvestAlign,
a novel framework that constructs high-quality SFT datasets by leveraging
theoretical solutions to similar and simple optimal investment problems rather
than complex scenarios. Our theoretical analysis demonstrates that training
LLMs with InvestAlign-generated data achieves faster parameter convergence than
using real-user data, suggesting superior learning efficiency. Furthermore, we
develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which
demonstrates significantly closer alignment to real-user data than pre-SFT
models in both simple and complex investment problems. This highlights our
proposed InvestAlign as a promising approach with the potential to address
complex optimal investment problems and align LLMs with investor
decision-making processes under herd behavior. Our code is publicly available
at https://github.com/thu-social-network-research-group/InvestAlign.

</details>


### [26] [Large Language Model for Extracting Complex Contract Information in Industrial Scenes](https://arxiv.org/abs/2507.06539)
*Yunyang Cao,Yanjun Li,Silong Dai*

Main category: cs.CL

TL;DR: 提出了一种工业场景下复杂合同信息抽取的高质量数据集构建方法，并基于此微调大语言模型，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决工业合同信息抽取任务中高质量数据稀缺和模型鲁棒性不足的问题。

Method: 通过聚类分析、GPT-4/GPT-3.5标注关键信息、数据增强（生成新文本）构建高质量数据集，并微调大语言模型。

Result: 模型在保证高召回率和精度的同时提升了解析效率，LoRA、数据平衡和数据增强显著提升了准确性和鲁棒性。

Conclusion: 该方法为工业合同信息抽取任务提供了一种新颖高效的解决方案。

Abstract: This paper proposes a high-quality dataset construction method for complex
contract information extraction tasks in industrial scenarios and fine-tunes a
large language model based on this dataset. Firstly, cluster analysis is
performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to
extract key information from the original contract data, obtaining high-quality
data annotations. Secondly, data augmentation is achieved by constructing new
texts, and GPT-3.5 generates unstructured contract texts from randomly combined
keywords, improving model robustness. Finally, the large language model is
fine-tuned based on the high-quality dataset. Experimental results show that
the model achieves excellent overall performance while ensuring high field
recall and precision and considering parsing efficiency. LoRA, data balancing,
and data augmentation effectively enhance model accuracy and robustness. The
proposed method provides a novel and efficient solution for industrial contract
information extraction tasks.

</details>


### [27] [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
*Juan B. Gutiérrez*

Main category: cs.CL

TL;DR: 论文提出了一种将人类与大型语言模型（LLMs）平等视为节点的网络模型，分析了四种危害（偏离真相、自我修复、新虚构和外部检测），并通过数学模型证明，引入同行评审可显著提升系统真实性。


<details>
  <summary>Details</summary>
Motivation: 研究人类与LLMs交互中的信息失真问题，探索如何通过网络模型提升信息可靠性。

Method: 提出了一种基于节点平等和语句流通的讨论网络模型，定义了四种危害，并开发了数学模型和开源算法FOO。

Result: 模型显示，仅依赖自我修复的网络会稳定在中等错误率，而引入同行评审可显著降低错误率。

Conclusion: 信息可靠性依赖于将不完美模型整合为相互监督的网络，而非单一模型的完美化。

Abstract: Large-language models turn writing into a live exchange between humans and
software. We capture this new medium with a discursive-network model that
treats people and LLMs as equal nodes and tracks how their statements
circulate. Broadening the focus from isolated hallucinations, we define
invalidation (any factual, logical, or structural breach) and show it follows
four hazards: drift from truth, self-repair, fresh fabrication, and external
detection. A general mathematical model of discursive networks is developed to
provide valuable insights: A network governed only by drift and self-repair
stabilizes at a modest error rate; adding fabrication reproduces the high rates
seen in current LLMs. Giving each false claim even a small chance of peer
review shifts the system to a truth-dominant state. We operationalize peer
review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a
configurable loop in which any set of agents critique one another while a
harmoniser merges their verdicts. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
wiring imperfect ones into networks that keep each other honest.

</details>


### [28] [Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis](https://arxiv.org/abs/2507.06571)
*Srihari K B,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出了一种结合多模态知识图谱（MMKG）与生成式AI的食品领域QA框架，显著提升了性能与多样性。


<details>
  <summary>Details</summary>
Motivation: 通过结合结构化知识与多模态生成，提升食品领域问答系统的可靠性与多样性。

Method: 构建大规模MMKG，生成QA对，联合微调Meta LLaMA 3.1-8B和Stable Diffusion 3.5-Large，并采用混合检索生成策略。

Result: BERTScore提升16.2%，FID降低37.8%，CLIP对齐提升31.1%，图像重用准确率94.1%，合成充分性85%。

Conclusion: 结构化知识与多模态生成结合显著提升了食品QA的可靠性与多样性。

Abstract: We propose a unified food-domain QA framework that combines a large-scale
multimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000
recipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate
40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint
fine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves
BERTScore by 16.2\%, reduces FID by 37.8\%, and boosts CLIP alignment by
31.1\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\% to 7.3\%) and
LLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid
retrieval-generation strategy achieves 94.1\% accurate image reuse and 85\%
adequacy in synthesis. Our results demonstrate that structured knowledge and
multimodal generation together enhance reliability and diversity in food QA.

</details>


### [29] [Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review](https://arxiv.org/abs/2507.06623)
*James Stewart-Evans,Emma Wilson,Tessa Langley,Andrew Prayle,Angela Hands,Karen Exley,Jo Leonardi-Bee*

Main category: cs.CL

TL;DR: 论文探讨了使用Claude 3.5 Sonnet和审查协议进行数据提取的两种方法，发现其在简单数据上表现良好，但在复杂数据上准确率较低。建议进一步评估LLM性能。


<details>
  <summary>Details</summary>
Motivation: 数据提取阶段资源密集，研究者希望通过LLM和审查协议加速这一过程。

Method: 使用Claude 3.5 Sonnet和审查协议对10个证据源进行数据提取，并评估其性能。

Result: 简单数据提取准确率高（83.3%和100%），复杂数据准确率低（9.6%和15.8%）。整体精度>90%，但召回率和F1分数较低。

Conclusion: LLM在数据提取中表现不一，需进一步评估和比较传统方法。建议研究者报告LLM性能并优化协议。

Abstract: The data extraction stages of reviews are resource-intensive, and researchers
may seek to expediate data extraction using online (large language models) LLMs
and review protocols. Claude 3.5 Sonnet was used to trial two approaches that
used a review protocol to prompt data extraction from 10 evidence sources
included in a case study scoping review. A protocol-based approach was also
used to review extracted data. Limited performance evaluation was undertaken
which found high accuracy for the two extraction approaches (83.3% and 100%)
when extracting simple, well-defined citation details; accuracy was lower (9.6%
and 15.8%) when extracting more complex, subjective data items. Considering all
data items, both approaches had precision >90% but low recall (<25%) and F1
scores (<40%). The context of a complex scoping review, open response types and
methodological approach likely impacted performance due to missed and
misattributed data. LLM feedback considered the baseline extraction accurate
and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of
38 (21.1%) to key findings data items were considered to potentially add value.
However, when repeating the process with a dataset featuring deliberate errors,
only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for
expediency require more robust performance evaluation across a range of LLMs
and review contexts with comparison to conventional prompt engineering
approaches. We recommend researchers evaluate and report LLM performance if
using them similarly to conduct data extraction or review extracted data. LLM
feedback contributed to protocol adaptation and may assist future review
protocol drafting.

</details>


### [30] [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
*Liliang Ren,Congcong Chen,Haoran Xu,Young Jin Kim,Adam Atkinson,Zheng Zhan,Jiankai Sun,Baolin Peng,Liyuan Liu,Shuohang Wang,Hao Cheng,Jianfeng Gao,Weizhu Chen,Yelong Shen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Gated Memory Unit (GMU)的机制，用于在State Space Models (SSMs)中高效共享层间表示，并构建了SambaY架构，显著提升了解码效率和长上下文性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合架构如Samba和YOCO虽表现优异，但未充分利用SSM层间的表示共享潜力，因此研究如何通过GMU提升效率。

Method: 引入GMU机制，构建SambaY架构，结合Samba自解码器和跨解码器，共享内存读取状态。

Result: SambaY显著提升解码效率，保持线性预填充复杂度，无需显式位置编码，并在大规模实验中表现优于YOCO基线。

Conclusion: GMU和SambaY架构在高效序列建模中具有显著优势，尤其在长上下文和大规模计算场景下表现突出。

Abstract: Recent advances in language modeling have demonstrated the effectiveness of
State Space Models (SSMs) for efficient sequence modeling. While hybrid
architectures such as Samba and the decoder-decoder architecture, YOCO, have
shown promising performance gains over Transformers, prior works have not
investigated the efficiency potential of representation sharing between SSM
layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet
effective mechanism for efficient memory sharing across layers. We apply it to
create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in
the cross-decoder to share memory readout states from a Samba-based
self-decoder. SambaY significantly enhances decoding efficiency, preserves
linear pre-filling time complexity, and boosts long-context performance, all
while eliminating the need for explicit positional encoding. Through extensive
scaling experiments, we demonstrate that our model exhibits a significantly
lower irreducible loss compared to a strong YOCO baseline, indicating superior
performance scalability under large-scale compute regimes. Our largest model
enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves
significantly better performance than Phi4-mini-Reasoning on reasoning tasks
such as Math500, AIME24/25, and GPQA Diamond without any reinforcement
learning, while delivering up to 10x higher decoding throughput on 2K-length
prompts with 32K generation length under the vLLM inference framework. We
release our training codebase on open-source data at
https://github.com/microsoft/ArchScale.

</details>


### [31] [FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation](https://arxiv.org/abs/2507.06622)
*Boshko Koloski,Senja Pollak,Roberto Navigli,Blaž Škrlj*

Main category: cs.CL

TL;DR: FuDoBa是一种基于贝叶斯优化的方法，结合LLM嵌入与领域特定知识，生成低维、任务相关的表示，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的高维嵌入在领域特定应用中过于通用或低效的问题。

Method: 通过贝叶斯优化融合LLM嵌入与领域知识（本地和WikiData），生成低维表示。

Result: 在六个数据集上表现优于或与专有LLM嵌入基线相当。

Conclusion: FuDoBa能有效生成高效、可解释的表示，适用于领域特定任务。

Abstract: Building on the success of Large Language Models (LLMs), LLM-based
representations have dominated the document representation landscape, achieving
great performance on the document embedding benchmarks. However, the
high-dimensional, computationally expensive embeddings from LLMs tend to be
either too generic or inefficient for domain-specific applications. To address
these limitations, we introduce FuDoBa a Bayesian optimisation-based method
that integrates LLM-based embeddings with domain-specific structured knowledge,
sourced both locally and from external repositories like WikiData. This fusion
produces low-dimensional, task-relevant representations while reducing training
complexity and yielding interpretable early-fusion weights for enhanced
classification performance. We demonstrate the effectiveness of our approach on
six datasets in two domains, showing that when paired with robust AutoML-based
classifiers, our proposed representation learning approach performs on par
with, or surpasses, those produced solely by the proprietary LLM-based
embedding baselines.

</details>


### [32] [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
*Gennadii Iakovlev*

Main category: cs.CL

TL;DR: 该研究提出了一种通过人工智能检测政治人物互动的新方法，用于衡量精英极化，并分析了英国、匈牙利和意大利的数据，展示了该指数的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析政治人物在议会演讲中的互动，量化精英之间的极化程度，为欧盟范围内的长期研究奠定基础。

Method: 利用人工智能识别演讲中的发言者和被提及者，并评估情感温度，构建相互敌意指数。

Result: 生成的指数对选举活动、危机事件和权力更迭等事件反应良好，显示出较高的表面效度。

Conclusion: 该方法为未来欧盟范围内的精英极化研究提供了可行工具，且结果可按政党和季度汇总。

Abstract: This project introduces a new measure of elite polarization via actor and
subject detection using artificial intelligence. I identify when politicians
mention one another in parliamentary speeches, note who is speaking and who is
being addressed, and assess the emotional temperature behind these evaluations.
This maps how elites evaluate their various out-parties, allowing us to create
an index of mutual out-party hostility, that is, elite polarization. While I
analyzed polarization data over the past four decades for the UK, and two
decades for Hungary and Italy, my approach lays the groundwork for a
twenty-year, EU-wide time-series dataset on elite polarization. I obtain the
results that can be aggregated by party and quarter. The resulting index
demonstrates a good face validity: it reacts to events such as electoral
campaigns, country- and party-level crises, and to parties losing and assuming
power.

</details>


### [33] [CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs](https://arxiv.org/abs/2507.06715)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: CLI-RAG框架通过分层分块和双阶段检索机制，解决了临床文本生成中数据分散和语义密度高的挑战，显著提升了生成文本的对齐分数和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决临床文本生成中数据非结构化、分散以及语义密度高的问题，提升生成文本的临床相关性和实用性。

Method: 提出CLI-RAG框架，采用分层分块策略和双阶段检索机制（全局阶段识别相关笔记类型，局部阶段提取高价值内容）。

Result: 在MIMIC-III数据集上，生成的结构化进展笔记对齐分数达87.7%，优于临床医生撰写的80.7%，且在不同LLMs间表现一致。

Conclusion: CLI-RAG框架有效提升了临床文本生成的质量和可靠性，具备临床应用的潜力。

Abstract: Large language models (LLMs), including zero-shot and few-shot paradigms,
have shown promising capabilities in clinical text generation. However,
real-world applications face two key challenges: (1) patient data is highly
unstructured, heterogeneous, and scattered across multiple note types and (2)
clinical notes are often long and semantically dense, making naive prompting
infeasible due to context length constraints and the risk of omitting
clinically relevant information.
  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a
domain-specific framework for structured and clinically grounded text
generation using LLMs. It incorporates a novel hierarchical chunking strategy
that respects clinical document structure and introduces a task-specific
dual-stage retrieval mechanism. The global stage identifies relevant note types
using evidence-based queries, while the local stage extracts high-value content
within those notes creating relevance at both document and section levels.
  We apply the system to generate structured progress notes for individual
hospital visits using 15 clinical note types from the MIMIC-III dataset.
Experiments show that it preserves temporal and semantic alignment across
visits, achieving an average alignment score of 87.7%, surpassing the 80.7%
baseline from real clinician-authored notes. The generated outputs also
demonstrate high consistency across LLMs, reinforcing deterministic behavior
essential for reproducibility, reliability, and clinical trust.

</details>


### [34] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）在处理确定和不确定预测时，其隐藏状态的动态变化基本一致，不确定性并未显著影响推理动态。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何内部表示和处理预测，以检测不确定性和防止幻觉，填补现有研究中不确定性对隐藏状态处理影响的空白。

Method: 使用Tuned Lens（Logit Lens的变体）分析11个数据集和5个模型的层间概率轨迹，以错误预测作为高认知不确定性的代表。

Result: 确定和不确定预测的轨迹一致，均在相似层出现置信度骤增；但更优秀的模型可能学会不同方式处理不确定性。

Conclusion: 研究挑战了简单方法检测不确定性的可行性，并展示了可解释性方法在研究不确定性影响推理中的潜力。

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


### [35] [KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution](https://arxiv.org/abs/2507.06753)
*Ye Kyaw Thu,Thura Aung,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 首次将Kolmogorov-Arnold卷积（KAConvText）应用于句子分类，涵盖仇恨言论检测、新闻分类和民族语言识别任务，结果显示KAConvText-MLP结合微调fastText嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索KAConvText在句子分类任务中的应用，解决数据不平衡问题，并比较不同嵌入配置的效果。

Method: 使用随机和fastText嵌入（静态和微调），结合CBOW和Skip-gram模型，比较CNN和CNN-KAN基线，并测试MLP和KAN分类头。

Result: KAConvText-MLP在仇恨言论检测（91.23%准确率）、新闻分类（92.66%准确率）和语言识别（99.82%准确率）中表现最优。

Conclusion: KAConvText结合微调fastText嵌入和MLP分类头在多种句子分类任务中表现出色，KAN分类头增强了可解释性。

Abstract: This paper presents the first application of Kolmogorov-Arnold Convolution
for Text (KAConvText) in sentence classification, addressing three tasks:
imbalanced binary hate speech detection, balanced multiclass news
classification, and imbalanced multiclass ethnic language identification. We
investigate various embedding configurations, comparing random to fastText
embeddings in both static and fine-tuned settings, with embedding dimensions of
100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs
and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we
investigated KAConvText with different classification heads - MLP and KAN,
where using KAN head supports enhanced interpretability. Results show that
KAConvText-MLP with fine-tuned fastText embeddings achieves the best
performance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,
92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%
accuracy (F1-score = 0.9982) for language identification.

</details>


### [36] [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
*Mohammad Ghiasvand Mohammadkhani,Hamid Beigy*

Main category: cs.CL

TL;DR: 论文提出了一种无需训练的框架CE-Judge，利用开源模型和清单直觉进行多语言文本评估，性能优于基线模型并与GPT-4o相当。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法在多语言场景中依赖专有模型或大量训练数据，成本高且效率低，亟需一种无需训练的高效解决方案。

Method: 提出CE-Judge框架，基于清单直觉和开源模型，无需训练即可进行多语言评估。

Result: 在多语言和多个基准数据集上的实验表明，CE-Judge性能优于基线，与GPT-4o相当。

Conclusion: CE-Judge为多语言文本评估提供了一种高效、低成本的无训练解决方案。

Abstract: Automated text evaluation has long been a central issue in Natural Language
Processing (NLP). Recently, the field has shifted toward using Large Language
Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While
promising and easily adaptable across tasks, this approach has seen limited
exploration in multilingual contexts. Existing multilingual studies often rely
on proprietary models or require extensive training data for fine-tuning,
raising concerns about cost, time, and efficiency. In this paper, we propose
Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free
framework that uses checklist intuition for multilingual evaluation with an
open-source model. Experiments across multiple languages and three benchmark
datasets, under both pointwise and pairwise settings, show that our method
generally surpasses the baselines and performs on par with the GPT-4o model.

</details>


### [37] [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
*Seonwu Kim,Yohan Na,Kihun Kim,Hanhee Cho,Geun Lim,Mintae Kim,Seongik Park,Ki Hyun Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CL

TL;DR: 研究表明，DACP方法能显著提升小型LLMs在特定领域的性能，同时保持通用能力，为企业提供高效、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 开源大型语言模型（LLMs）的兴起为企业应用提供了机会，但许多组织缺乏部署和维护大规模模型的基础设施，因此小型LLMs（sLLMs）成为实用替代方案。

Method: 研究验证了领域自适应持续预训练（DACP）方法在不同基础模型和服务领域的有效性。

Result: 实验和实际评估表明，应用DACP的sLLMs在目标领域性能显著提升，同时保持通用能力。

Conclusion: DACP为小型LLMs提供了一种成本高效且可扩展的企业级部署解决方案。

Abstract: The emergence of open-source large language models (LLMs) has expanded
opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result,
small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has
been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the
effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations,
we demonstrate that DACP-applied sLLMs achieve substantial gains in target
domain performance while preserving general capabilities, offering a
cost-efficient and scalable solution for enterprise-level deployment.

</details>


### [38] [Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams](https://arxiv.org/abs/2507.06803)
*Matthew Anderson Hendricks,Alice Cicirello*

Main category: cs.CL

TL;DR: 提出一种利用领域和专家知识自动生成动态系统计算模型的策略，通过SysML图提取信息，结合NLP和LLM优化中间输出，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 加速工程动态系统的设计和部署，减少人工建模的复杂性。

Method: 分五步实现，包括使用SysML图提取依赖关系、属性和操作，结合NLP和LLM优化中间输出，并通过代码生成和计算模型生成步骤完成。

Result: 通过案例研究验证了方法的有效性，展示了从文本到模型的端到端示例，性能优于仅使用LLM的结果。

Conclusion: 该方法具有通用性，适用于不同系统和领域，显著提高了动态系统建模的效率和准确性。

Abstract: This paper contributes to speeding up the design and deployment of
engineering dynamical systems by proposing a strategy for exploiting domain and
expert knowledge for the automated generation of dynamical system computational
model starting from a corpus of document relevant to the dynamical system of
interest and an input document describing the specific system. This strategy is
implemented in five steps and, crucially, it uses system modeling language
diagrams (SysML) to extract accurate information about the dependencies,
attributes, and operations of components. Natural Language Processing (NLP)
strategies and Large Language Models (LLMs) are employed in specific tasks to
improve intermediate outputs of the SySML diagrams automated generation, such
as: list of key nouns; list of extracted relationships; list of key phrases and
key relationships; block attribute values; block relationships; and BDD diagram
generation. The applicability of automated SysML diagram generation is
illustrated with different case studies. The computational models of complex
dynamical systems from SysML diagrams are then obtained via code generation and
computational model generation steps. In the code generation step, NLP
strategies are used for summarization, while LLMs are used for validation only.
The proposed approach is not limited to a specific system, domain, or
computational software. The applicability of the proposed approach is shown via
an end-to-end example from text to model of a simple pendulum, showing improved
performance compared to results yielded by LLMs only.

</details>


### [39] [Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework](https://arxiv.org/abs/2507.06829)
*Zenan Xu,Zexuan Qiu,Guanhua Huang,Kun Li,Siheng Li,Chenchen Zhang,Kejiao Li,Qi Yi,Yuhao Jiang,Bo Zhou,Fengzong Lian,Zhanhui Kang*

Main category: cs.CL

TL;DR: 论文提出了一种结合顺序推理和并行推理优势的协作推理框架，并引入语义熵（SE）作为动态控制推理过程的质量指标。


<details>
  <summary>Details</summary>
Motivation: 当前顺序推理和并行推理方法存在效率低和缺乏协调的问题，需要一种更灵活的推理框架。

Method: 设计了一个协作推理框架，利用语义熵（SE）动态评估推理质量，实现高效控制。

Result: 语义熵（SE）能够准确反映推理质量，并与准确性呈强负相关。

Conclusion: 该框架结合了两种推理范式的优势，通过SE实现了高效的动态控制。

Abstract: Recent advances in large language models (LLMs) have accelerated progress
toward artificial general intelligence, with inference-time scaling emerging as
a key technique. Contemporary approaches leverage either sequential reasoning
(iteratively extending chains of thought) or parallel reasoning (generating
multiple solutions simultaneously) to scale inference. However, both paradigms
face fundamental limitations: sequential scaling typically relies on arbitrary
token budgets for termination, leading to inefficiency or premature cutoff;
while parallel scaling often lacks coordination among parallel branches and
requires intrusive fine-tuning to perform effectively. In light of these
challenges, we aim to design a flexible test-time collaborative inference
framework that exploits the complementary strengths of both sequential and
parallel reasoning paradigms. Towards this goal, the core challenge lies in
developing an efficient and accurate intrinsic quality metric to assess model
responses during collaborative inference, enabling dynamic control and early
termination of the reasoning trace. To address this challenge, we introduce
semantic entropy (SE), which quantifies the semantic diversity of parallel
model responses and serves as a robust indicator of reasoning quality due to
its strong negative correlation with accuracy...

</details>


### [40] [Shifting from Ranking to Set Selection for Retrieval Augmented Generation](https://arxiv.org/abs/2507.06838)
*Dahyun Lee,Yongrae Jo,Haeju Park,Moontae Lee*

Main category: cs.CL

TL;DR: SETR提出了一种基于集合的段落选择方法，通过Chain-of-Thought推理明确查询的信息需求，并选择最优段落集合以满足需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于段落个体相关性重排序，难以满足多跳问答中复杂查询的信息需求。

Method: 提出SETR方法，通过Chain-of-Thought推理识别查询需求，并选择最优段落集合。

Result: 在多跳RAG基准测试中，SETR在答案正确性和检索质量上优于现有方法。

Conclusion: SETR为RAG系统提供了一种高效且有效的传统重排序替代方案。

Abstract: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved
passages are not only individually relevant but also collectively form a
comprehensive set. Existing approaches primarily rerank top-k passages based on
their individual relevance, often failing to meet the information needs of
complex queries in multi-hop question answering. In this work, we propose a
set-wise passage selection approach and introduce SETR, which explicitly
identifies the information requirements of a query through Chain-of-Thought
reasoning and selects an optimal set of passages that collectively satisfy
those requirements. Experiments on multi-hop RAG benchmarks show that SETR
outperforms both proprietary LLM-based rerankers and open-source baselines in
terms of answer correctness and retrieval quality, providing an effective and
efficient alternative to traditional rerankers in RAG systems. The code is
available at https://github.com/LGAI-Research/SetR

</details>


### [41] [Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights](https://arxiv.org/abs/2507.06893)
*Alexandra Abbas,Celia Waggoner,Justin Olive*

Main category: cs.CL

TL;DR: 论文总结了八个月维护开源AI评估工具的经验，提出了解决社区贡献扩展、统计方法和质量控制的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的能力和安全性是AI发展的重要环节，但现有工具和方法面临挑战。

Method: 提出结构化队列管理框架、统计方法优化重采样和跨模型比较，以及系统质量控制流程。

Result: 分析表明，AI评估需要专门的基础设施、统计严谨性和社区协调。

Conclusion: AI评估需超越传统软件开发实践，结合专业基础设施和社区协作。

Abstract: AI evaluations have become critical tools for assessing large language model
capabilities and safety. This paper presents practical insights from eight
months of maintaining $inspect\_evals$, an open-source repository of 70+
community-contributed AI evaluations. We identify key challenges in
implementing and maintaining AI evaluations and develop solutions including:
(1) a structured cohort management framework for scaling community
contributions, (2) statistical methodologies for optimal resampling and
cross-model comparison with uncertainty quantification, and (3) systematic
quality control processes for reproducibility. Our analysis reveals that AI
evaluation requires specialized infrastructure, statistical rigor, and
community coordination beyond traditional software development practices.

</details>


### [42] [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
*Luca Mariotti,Veronica Guidetti,Federica Mandreoli*

Main category: cs.CL

TL;DR: SCoRE是一个模块化、低成本的句子级关系抽取系统，无需微调即可适配不同语料库和知识图谱，结合对比学习和贝叶斯kNN分类器，性能优越且节能。


<details>
  <summary>Details</summary>
Motivation: 解决低监督环境下关系抽取的需求，提供适应性强、抗噪声的解决方案，并与预训练大语言模型无缝集成。

Method: 采用监督对比学习和贝叶斯kNN分类器进行多标签分类，提出新评估指标CSD和P@R，并发布Wiki20d基准数据集。

Result: 在五个基准测试中，SCoRE性能匹配或超越现有方法，同时显著降低能耗。

Conclusion: SCoRE的高效性、模块化和可扩展性使其成为实际关系抽取应用的理想选择。

Abstract: The growing demand for efficient knowledge graph (KG) enrichment leveraging
external corpora has intensified interest in relation extraction (RE),
particularly under low-supervision settings. To address the need for adaptable
and noise-resilient RE solutions that integrate seamlessly with pre-trained
large language models (PLMs), we introduce SCoRE, a modular and cost-effective
sentence-level RE system. SCoRE enables easy PLM switching, requires no
finetuning, and adapts smoothly to diverse corpora and KGs. By combining
supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)
classifier for multi-label classification, it delivers robust performance
despite the noisy annotations of distantly supervised corpora. To improve RE
evaluation, we propose two novel metrics: Correlation Structure Distance (CSD),
measuring the alignment between learned relational patterns and KG structures,
and Precision at R (P@R), assessing utility as a recommender system. We also
release Wiki20d, a benchmark dataset replicating real-world RE conditions where
only KG-derived annotations are available. Experiments on five benchmarks show
that SCoRE matches or surpasses state-of-the-art methods while significantly
reducing energy consumption. Further analyses reveal that increasing model
complexity, as seen in prior work, degrades performance, highlighting the
advantages of SCoRE's minimal design. Combining efficiency, modularity, and
scalability, SCoRE stands as an optimal choice for real-world RE applications.

</details>


### [43] [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
*Ziang Ye,Yang Zhang,Wentao Shi,Xiaoyu You,Fuli Feng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: GUI代理基于大型视觉语言模型（LVLMs）实现自动化人机交互，但存在视觉定位漏洞，易受新型后门攻击。提出的VisualTrap方法能有效劫持视觉定位，攻击隐蔽且泛化性强。


<details>
  <summary>Details</summary>
Motivation: GUI代理在自动化任务中潜力巨大，但视觉定位漏洞可能被利用，导致后门攻击，威胁安全。

Method: 提出VisualTrap方法，通过注入毒化数据误导视觉定位，攻击隐蔽且泛化性强。

Result: 实验显示，仅需5%毒化数据即可有效攻击，攻击可跨任务和环境泛化。

Conclusion: GUI代理的后门攻击风险需进一步研究，以保障安全性。

Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models
(LVLMs) have emerged as a revolutionary approach to automating human-machine
interactions, capable of autonomously operating personal devices (e.g., mobile
phones) or applications within the device to perform complex real-world tasks
in a human-like manner. However, their close integration with personal devices
raises significant security concerns, with many threats, including backdoor
attacks, remaining largely unexplored. This work reveals that the visual
grounding of GUI agent-mapping textual plans to GUI elements-can introduce
vulnerabilities, enabling new types of backdoor attacks. With backdoor attack
targeting visual grounding, the agent's behavior can be compromised even when
given correct task-solving plans. To validate this vulnerability, we propose
VisualTrap, a method that can hijack the grounding by misleading the agent to
locate textual plans to trigger locations instead of the intended targets.
VisualTrap uses the common method of injecting poisoned data for attacks, and
does so during the pre-training of visual grounding to ensure practical
feasibility of attacking. Empirical results show that VisualTrap can
effectively hijack visual grounding with as little as 5% poisoned data and
highly stealthy visual triggers (invisible to the human eye); and the attack
can be generalized to downstream tasks, even after clean fine-tuning. Moreover,
the injected trigger can remain effective across different GUI environments,
e.g., being trained on mobile/web and generalizing to desktop environments.
These findings underscore the urgent need for further research on backdoor
attack risks in GUI agents.

</details>


### [44] [MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection](https://arxiv.org/abs/2507.06908)
*Ziyan Liu,Chunxiao Fan,Haoran Lou,Yuexin Wu,Kaiwei Deng*

Main category: cs.CL

TL;DR: 提出了一种名为MIND的多智能体框架，用于零样本有害模因检测，无需依赖标注数据。


<details>
  <summary>Details</summary>
Motivation: 由于模因的快速演变和缺乏最新标注数据，传统数据驱动方法难以检测新模因。

Method: 1) 从未标注参考集中检索相似模因；2) 提出双向洞察推导机制；3) 使用多智能体辩论机制进行决策。

Result: 在三个模因数据集上的实验表明，MIND优于现有零样本方法，且具有强泛化能力。

Conclusion: MIND为有害模因检测提供了可扩展的解决方案。

Abstract: The rapid expansion of memes on social media has highlighted the urgent need
for effective approaches to detect harmful content. However, traditional
data-driven approaches struggle to detect new memes due to their evolving
nature and the lack of up-to-date annotated data. To address this issue, we
propose MIND, a multi-agent framework for zero-shot harmful meme detection that
does not rely on annotated data. MIND implements three key strategies: 1) We
retrieve similar memes from an unannotated reference set to provide contextual
information. 2) We propose a bi-directional insight derivation mechanism to
extract a comprehensive understanding of similar memes. 3) We then employ a
multi-agent debate mechanism to ensure robust decision-making through reasoned
arbitration. Extensive experiments on three meme datasets demonstrate that our
proposed framework not only outperforms existing zero-shot approaches but also
shows strong generalization across different model architectures and parameter
scales, providing a scalable solution for harmful meme detection. The code is
available at https://github.com/destroy-lonely/MIND.

</details>


### [45] [MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction](https://arxiv.org/abs/2507.06909)
*Xiao Wang,Jiahuan Pei,Diancheng Shui,Zhiguang Han,Xin Sun,Dawei Zhu,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 论文研究了法律判决预测中多被告和多指控的处理方式，提出了MPMCP数据集，并评估了不同法律大语言模型在四种场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨多被告和多指控在法律判决预测中是否需要分开处理，填补研究空白。

Method: 引入MPMCP数据集，评估四种法律场景下LLMs的表现，包括指控预测和刑罚预测任务。

Result: 多被告多指控场景（S4）最具挑战性，不同模型表现差异显著，如InternLM2和Lawformer在S4中F1分数分别下降4.5%和19.7%。

Conclusion: 多被告多指控场景对法律判决预测模型提出了更高要求，需进一步优化模型性能。

Abstract: Legal judgment prediction offers a compelling method to aid legal
practitioners and researchers. However, the research question remains
relatively under-explored: Should multiple defendants and charges be treated
separately in LJP? To address this, we introduce a new dataset namely
multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating
the performance of several prevailing legal large language models (LLMs) on
four practical legal judgment scenarios: (S1) single defendant with a single
charge, (S2) single defendant with multiple charges, (S3) multiple defendants
with a single charge, and (S4) multiple defendants with multiple charges. We
evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty
term prediction. We have conducted extensive experiments and found that the
scenario involving multiple defendants and multiple charges (S4) poses the
greatest challenges, followed by S2, S3, and S1. The impact varies
significantly depending on the model. For example, in S4 compared to S1,
InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,
while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.
Our dataset and code are available at
https://github.com/lololo-xiao/MultiJustice-MPMCP.

</details>


### [46] [Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues](https://arxiv.org/abs/2507.06910)
*Fareya Ikram,Alexander Scarlatos,Andrew Lan*

Main category: cs.CL

TL;DR: 研究探讨了现代大语言模型（如Llama 3和GPT-4o）在预测辅导对话中导师策略和学生表现的能力，发现即使先进模型也难以准确预测导师策略，但导师策略对学生表现影响显著。


<details>
  <summary>Details</summary>
Motivation: 在线学习和AI辅导的兴起使得导师策略对学生表现的影响日益重要，但预测导师策略的研究较少。

Method: 使用两个数学辅导对话数据集，评估Llama 3和GPT-4o在预测导师策略和学生表现上的能力。

Result: 先进的大语言模型在预测导师策略上表现不佳，但导师策略对学生表现有显著影响。

Conclusion: 需要更强大的方法来预测导师策略，以优化AI辅导效果。

Abstract: Tutoring dialogues have gained significant attention in recent years, given
the prominence of online learning and the emerging tutoring abilities of
artificial intelligence (AI) agents powered by large language models (LLMs).
Recent studies have shown that the strategies used by tutors can have
significant effects on student outcomes, necessitating methods to predict how
tutors will behave and how their actions impact students. However, few works
have studied predicting tutor strategy in dialogues. Therefore, in this work we
investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to
predict both future tutor moves and student outcomes in dialogues, using two
math tutoring dialogue datasets. We find that even state-of-the-art LLMs
struggle to predict future tutor strategy while tutor strategy is highly
indicative of student outcomes, outlining a need for more powerful methods to
approach this task.

</details>


### [47] [Rethinking Verification for LLM Code Generation: From Generation to Testing](https://arxiv.org/abs/2507.06920)
*Zihan Ma,Taolin Zhang,Maosong Cao,Wenwei Zhang,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出了一种名为SAGA的人机协作方法，通过结合人类编程专长与LLM推理能力，显著提升测试用例的覆盖率和质量，并开发了TCGBench用于测试用例生成任务的研究。实验表明SAGA在检测率和验证器准确率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估套件（如HumanEval和LiveCodeBench）的测试用例数量有限且同质化，导致细微错误未被发现，影响了性能评估和强化学习框架中的奖励估计。

Method: 提出多维度指标量化测试套件的全面性，并引入人机协作方法SAGA，结合人类编程专长与LLM推理能力，提升测试用例质量。开发TCGBench用于研究。

Result: SAGA在TCGBench上的检测率为90.62%，验证器准确率为32.58%，其生成的代码评估基准的验证器准确率比LiveCodeBench-v6高10.78%。

Conclusion: SAGA方法有效提升了测试用例的质量和覆盖率，为可靠的LLM代码评估奠定了基础，并推动了代码生成中强化学习的进一步发展。

Abstract: Large language models (LLMs) have recently achieved notable success in
code-generation benchmarks such as HumanEval and LiveCodeBench. However, a
detailed examination reveals that these evaluation suites often comprise only a
limited number of homogeneous test cases, resulting in subtle faults going
undetected. This not only artificially inflates measured performance but also
compromises accurate reward estimation in reinforcement learning frameworks
utilizing verifiable rewards (RLVR). To address these critical shortcomings, we
systematically investigate the test-case generation (TCG) task by proposing
multi-dimensional metrics designed to rigorously quantify test-suite
thoroughness. Furthermore, we introduce a human-LLM collaborative method
(SAGA), leveraging human programming expertise with LLM reasoning capability,
aimed at significantly enhancing both the coverage and the quality of generated
test cases. In addition, we develop a TCGBench to facilitate the study of the
TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a
verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)
of the code generation evaluation benchmark synthesized by SAGA is 10.78%
higher than that of LiveCodeBench-v6. These results demonstrate the
effectiveness of our proposed method. We hope this work contributes to building
a scalable foundation for reliable LLM code evaluation, further advancing RLVR
in code generation, and paving the way for automated adversarial test synthesis
and adaptive benchmark integration.

</details>


### [48] [Investigating the Robustness of Retrieval-Augmented Generation at the Query Level](https://arxiv.org/abs/2507.06956)
*Sezen Perçin,Xin Su,Qutub Sha Syed,Phillip Howard,Aleksei Kuvshinov,Leo Schwinn,Kay-Ulrich Scholl*

Main category: cs.CL

TL;DR: 论文研究了检索增强生成（RAG）系统对查询扰动的敏感性，发现即使查询有微小变化，检索性能也会显著下降，并提出评估框架和改进建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）更新成本高且效率低，RAG虽能动态引入外部知识，但其性能高度依赖查询质量，因此需要研究其鲁棒性。

Method: 通过分析RAG流程中各组件对查询扰动的敏感性，并在端到端问答任务中测试通用和领域数据集，提出评估框架。

Result: 实验表明，常用检索器在查询微小变化下性能显著下降，基于1092次实验提出了改进建议。

Conclusion: RAG系统对查询扰动敏感，需系统性评估和改进，论文提供了实用建议。

Abstract: Large language models (LLMs) are very costly and inefficient to update with
new information. To address this limitation, retrieval-augmented generation
(RAG) has been proposed as a solution that dynamically incorporates external
knowledge during inference, improving factual consistency and reducing
hallucinations. Despite its promise, RAG systems face practical challenges-most
notably, a strong dependence on the quality of the input query for accurate
retrieval. In this paper, we investigate the sensitivity of different
components in the RAG pipeline to various types of query perturbations. Our
analysis reveals that the performance of commonly used retrievers can degrade
significantly even under minor query variations. We study each module in
isolation as well as their combined effect in an end-to-end question answering
setting, using both general-domain and domain-specific datasets. Additionally,
we propose an evaluation framework to systematically assess the query-level
robustness of RAG pipelines and offer actionable recommendations for
practitioners based on the results of more than 1092 experiments we performed.

</details>


### [49] [FRaN-X: FRaming and Narratives-eXplorer](https://arxiv.org/abs/2507.06974)
*Artur Muratov,Hana Fatima Shaikh,Vanshikaa Jani,Tarek Mahmoud,Zhuohan Xie,Daniil Orel,Aaryamonvikram Singh,Yuxia Wang,Aadi Joshi,Hasan Iqbal,Ming Shan Hee,Dhruv Sahnan,Nikolaos Nikolaidis,Purificação Silvano,Dimitar Dimitrov,Roman Yangarber,Ricardo Campos,Alípio Jorge,Nuno Guimarães,Elisa Sartori,Nicolas Stefanovitch,Giovanni Da San Martino,Jakub Piskorski,Preslav Nakov*

Main category: cs.CL

TL;DR: FRaN-X是一个自动检测实体提及并分类其叙事角色的工具，支持五种语言和两个领域，提供交互式界面和可视化分析。


<details>
  <summary>Details</summary>
Motivation: 解决自动检测和标记实体如何被框架化的挑战，帮助媒体分析师比较不同来源的叙事。

Method: 两阶段系统，结合序列标记和细粒度角色分类，使用22种细粒度角色分类。

Result: 支持多语言和多领域，提供交互式界面、搜索功能和可视化分析。

Conclusion: FRaN-X是一个强大的工具，可用于媒体分析，公开可用且功能丰富。

Abstract: We present FRaN-X, a Framing and Narratives Explorer that automatically
detects entity mentions and classifies their narrative roles directly from raw
text. FRaN-X comprises a two-stage system that combines sequence labeling with
fine-grained role classification to reveal how entities are portrayed as
protagonists, antagonists, or innocents, using a unique taxonomy of 22
fine-grained roles nested under these three main categories. The system
supports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)
and two domains (the Russia-Ukraine Conflict and Climate Change). It provides
an interactive web interface for media analysts to explore and compare framing
across different sources, tackling the challenge of automatically detecting and
labeling how entities are framed. Our system allows end users to focus on a
single article as well as analyze up to four articles simultaneously. We
provide aggregate level analysis including an intuitive graph visualization
that highlights the narrative a group of articles are pushing. Our system
includes a search feature for users to look up entities of interest, along with
a timeline view that allows analysts to track an entity's role transitions
across different contexts within the article. The FRaN-X system and the trained
models are licensed under an MIT License. FRaN-X is publicly accessible at
https://fran-x.streamlit.app/ and a video demonstration is available at
https://youtu.be/VZVi-1B6yYk.

</details>


### [50] [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
*Weijia Shi,Akshita Bhagia,Kevin Farhat,Niklas Muennighoff,Pete Walsh,Jacob Morrison,Dustin Schwenk,Shayne Longpre,Jake Poznanski,Allyson Ettinger,Daogao Liu,Margaret Li,Dirk Groeneveld,Mike Lewis,Wen-tau Yih,Luca Soldaini,Kyle Lo,Noah A. Smith,Luke Zettlemoyer,Pang Wei Koh,Hannaneh Hajishirzi,Ali Farhadi,Sewon Min*

Main category: cs.CL

TL;DR: FlexOlmo是一种支持分布式训练和数据灵活推理的语言模型，通过混合专家架构实现独立训练和灵活集成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在数据共享受限的情况下，如何有效利用封闭数据集进行模型训练和推理的问题。

Method: 采用混合专家（MoE）架构，每个专家独立训练于封闭数据集，通过领域感知路由集成，无需联合训练。

Result: 在31个任务上表现优异，相对提升41%，优于现有模型合并方法10.1%。

Conclusion: FlexOlmo为受监管行业提供了一种尊重数据所有者偏好的解决方案，支持数据本地化和细粒度访问控制。

Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1)
distributed training without data sharing, where different model parameters are
independently trained on closed datasets, and (2) data-flexible inference,
where these parameters along with their associated data can be flexibly
included or excluded from model inferences with no further training. FlexOlmo
employs a mixture-of-experts (MoE) architecture where each expert is trained
independently on closed datasets and later integrated through a new
domain-informed routing without any joint training. FlexOlmo is trained on
FlexMix, a corpus we curate comprising publicly available datasets alongside
seven domain-specific sets, representing realistic approximations of closed
sets. We evaluate models with up to 37 billion parameters (20 billion active)
on 31 diverse downstream tasks. We show that a general expert trained on public
data can be effectively combined with independently trained experts from other
data owners, leading to an average 41% relative improvement while allowing
users to opt out of certain data based on data licensing or permission
requirements. Our approach also outperforms prior model merging methods by
10.1% on average and surpasses the standard MoE trained without data
restrictions using the same training FLOPs. Altogether, this research presents
a solution for both data owners and researchers in regulated industries with
sensitive or protected data. FlexOlmo enables benefiting from closed data while
respecting data owners' preferences by keeping their data local and supporting
fine-grained control of data access during inference.

</details>


### [51] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 论文提出了一种统一密集检索和响应生成的方法，通过联合微调和机制设计提升对话搜索系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话搜索系统通常使用分离的模型，无法同时利用模型的内在知识，影响了检索和生成的效果。

Method: 采用联合微调不同目标的方法，并设计两种机制以减少不一致风险和数据差异。

Result: 在五个对话搜索数据集上的评估表明，统一模型能同时提升两项任务性能，并优于现有基线。

Conclusion: 统一密集检索和响应生成的方法能有效提升对话搜索系统的性能。

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


### [52] [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
*Ashen Weligalle*

Main category: cs.CL

TL;DR: 论文研究了离散扩散模型（D3PM）在自然语言生成中的可行性，与传统自回归模型（AR）进行了比较，发现D3PM在并行生成速度上有优势，但压缩性能略逊于AR模型。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在离散数据（如自然语言）上的应用潜力，解决传统自回归模型在生成顺序和并行性上的限制。

Method: 评估离散去噪扩散概率模型（D3PM）与传统自回归模型（AR），使用Bits Per Token（BPT）、负对数似然（NLL）、困惑度（PPL）和批处理速度作为指标。

Result: D3PM在生成速度上表现更优（3.97批次/秒），但压缩性能（BPT均值8.05）不及AR模型（BPT均值4.59）。

Conclusion: 扩散模型在离散数据生成中具有潜力，尤其在并行性方面，但仍需改进压缩性能，为非自回归语言生成提供了研究方向。

Abstract: Diffusion models have emerged as a powerful class of generative models,
achieving state-of-the-art results in continuous data domains such as image and
video generation. Their core mechanism involves a forward diffusion process
that gradually transforms structured data into a Gaussian-like distribution,
followed by a learned reverse process to reconstruct the data. While successful
in continuous modalities, applying this framework to discrete data-particularly
natural language-remains challenging due to token dependency complexities and
the lack of a defined generation order.This thesis investigates the feasibility
and performance of discrete diffusion models for natural language generation.
Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model
(D3PM) and compare it with traditional autoregressive (AR) language models. To
assess generative performance, we use Bits Per Token (BPT), Negative
Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.
  Results show the best-performing D3PM model achieves a BPT of 5.72, with a
mean of 8.05. The AR model outperforms in compression with a lower mean BPT of
4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches
per sec., indicating potential for parallel generation.All evaluations were
conducted under consistent conditions-generating 100,000 tokens per model with
a fixed batch size of four-for fair comparison. This research presents a
detailed analysis of diffusion-based vs. autoregressive models, highlighting
trade-offs in generative quality and efficiency. Findings emphasize both the
promise and limitations of diffusion models for discrete data, supporting
future work in non-autoregressive language generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种结合CLIP感知损失模块和课程对比正则化的水下图像增强方法，以提升感知质量和内容恢复。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水下图像增强方法忽视人类感知且缺乏解空间约束，导致增强图像感知质量下降或内容恢复不佳。

Method: 利用CLIP模型的视觉语义特征提取能力设计感知损失模块，并结合课程对比正则化优化增强网络。

Result: 实验表明，该方法在视觉质量和泛化能力上优于现有技术。

Conclusion: 结合CLIP感知和课程对比正则化能有效提升水下图像增强的感知质量和内容恢复效果。

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [54] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: SPARC框架通过全局TopK稀疏机制和跨重建损失，学习跨模型和模态的统一潜在空间，显著提升概念对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如稀疏自编码器）为每个模型生成独立的潜在概念，导致概念空间不兼容，限制了跨模型的可解释性。

Method: SPARC采用全局TopK稀疏机制和跨重建损失，确保不同模型对同一概念激活相同的潜在维度，并保持语义一致性。

Result: 在Open Images上，SPARC的Jaccard相似度达到0.80，比之前方法提高了三倍以上。

Conclusion: SPARC实现了跨模型和模态的共享稀疏潜在空间，支持直接比较不同架构对同一概念的表示，并具有实际应用价值。

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [55] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: BayesSDF提出了一种基于概率的框架，用于量化神经隐式SDF模型中的不确定性，解决了现有方法在几何一致性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 科学模拟应用（如森林中的流体模拟）需要精确的表面几何和不确定性量化，现有方法缺乏直接几何集成，导致不确定性校准不佳。

Method: BayesSDF利用Laplace近似和Hessian度量，实现高效、表面感知的不确定性估计。

Result: BayesSDF在合成和真实数据集上表现优于现有方法，提供了与重建几何紧密相关的不确定性预测。

Conclusion: BayesSDF为不确定性感知的3D场景重建、模拟和机器人决策奠定了坚实基础。

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [56] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: LIRA框架通过结合语义增强特征提取器和交错局部视觉耦合，解决了多模态模型在分割和理解任务中的不准确和幻觉问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在分割和理解任务中存在不准确分割和幻觉理解的问题，主要源于视觉理解能力弱和缺乏细粒度感知。

Method: 提出LIRA框架，包含语义增强特征提取器（SEFE）和交错局部视觉耦合（ILVC），分别提升分割准确性和减少幻觉理解。

Result: LIRA在分割和理解任务中达到最先进性能，并通过AttrEval数据集量化了分割精度与语义推断能力的关系。

Conclusion: LIRA通过结合视觉理解和分割的互补关系，有效解决了现有模型的局限性，为多模态任务提供了新思路。

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [57] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: 本文综述了离线手写文本识别（HTR）中的数据增强与生成技术，探讨了传统方法与深度学习方法（如GANs、扩散模型和基于Transformer的方法）的优缺点，并分析了生成多样且真实手写样本的挑战。


<details>
  <summary>Details</summary>
Motivation: 离线HTR系统在历史文档数字化等领域至关重要，但标注数据稀缺，尤其是低资源语言和复杂脚本，限制了其性能。本文旨在通过综述数据增强与生成技术，提升HTR系统的准确性和鲁棒性。

Method: 采用PRISMA方法，从1,302项研究中筛选出848项，系统分析了传统和深度学习的数据增强与生成技术，包括GANs、扩散模型和Transformer方法。

Result: 综述揭示了生成多样且真实手写样本的挑战，如保持脚本真实性和解决数据稀缺问题，并评估了现有数据集和评估指标。

Conclusion: 本文指出了当前研究的不足，并提出了未来方向，以推动手写文本生成技术在多样语言和风格中的应用。

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [58] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: 提出了一种名为CCPDA的数据增强方法，通过集中复制粘贴火源区域来提升多类分割模型的性能，尤其针对火源类。


<details>
  <summary>Details</summary>
Motivation: 由于标注成本高且公开数据集稀缺，特别是在野火科学领域，需要一种有效的数据增强方法来提升分割模型的训练效果。

Method: CCPDA包括三个步骤：识别火源簇、集中处理火源核心区域、将处理后的火源粘贴到目标图像上。

Result: CCPDA显著提升了火源类的分割性能，优于其他数据增强方法。

Conclusion: CCPDA有效缓解了小规模标注数据集的训练难题，尤其在火源类分割中表现突出。

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [59] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: AR2通过对齐干净和损坏图像的类激活图（CAMs），提升预训练CNN的鲁棒性，无需改变架构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在常见损坏（如噪声、模糊等）下性能下降，限制了其在实际应用中的可靠性。

Method: AR2采用CAM对齐策略，通过迭代修复（CAM引导的细化和标准微调）增强鲁棒性。

Result: AR2在标准损坏基准测试中表现优于现有方法，平衡了干净数据的准确性和鲁棒性。

Conclusion: AR2为提升模型在多样化损坏环境中的可靠性提供了有效且可扩展的解决方案。

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [60] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: 论文提出了首个水下多鱼追踪数据集MFT25和专用追踪框架SU-T，展示了其在水下场景中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 水下多目标追踪技术对海洋生态和水产养殖至关重要，但相关研究较少。

Method: 提出MFT25数据集和SU-T框架，后者采用UKF优化非线性鱼群运动模式，并引入FishIoU匹配方法。

Result: SU-T在MFT25上达到34.1 HOTA和44.6 IDF1，表现最优。

Conclusion: MFT25和SU-T为水下追踪研究提供了基础，对海洋生物学和水产养殖有重要应用。

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [61] [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116)
*Juyi Lin,Amir Taherin,Arash Akbari,Arman Akbari,Lei Lu,Guangyu Chen,Taskin Padir,Xiaomeng Yang,Weiwei Chen,Yiqian Li,Xue Lin,David Kaeli,Pu Zhao,Yanzhi Wang*

Main category: cs.CV

TL;DR: 提出了一种高效的VLA模型优化框架VOTE，通过无分词器微调和集成投票策略，显著提升推理速度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在新对象或陌生环境中的泛化能力有限，且附加组件增加计算开销。

Method: 采用无分词器微调方法和集成投票策略，减少计算开销并加速推理。

Result: 实验表明，方法实现了35倍推理加速和145 Hz吞吐量，性能达到SOTA。

Conclusion: VOTE框架高效且通用，显著提升VLA模型的性能和效率。

Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35$\times$ faster inference and 145 Hz
throughput. All the details and codes will be open-sourced.

</details>


### [62] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: SImpHAR框架通过模拟生物阻抗信号和两阶段训练策略，显著提升了基于生物阻抗的人体活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 生物阻抗传感在细粒度运动捕捉中具有独特优势，但缺乏标记数据限制了其应用。

Method: 提出模拟管道生成逼真的生物阻抗信号，并设计两阶段训练策略，无需标签对齐的合成数据。

Result: 在多个数据集上表现优于现有方法，准确率和宏F1分数分别提升22.3%和21.8%。

Conclusion: 模拟驱动增强和模块化训练为基于生物阻抗的人体活动识别提供了新思路。

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [63] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: PCL-Former是一种基于多阶段Transformer架构的时序动作定位方法，通过专用模块处理不同子任务，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受Transformer和多阶段架构在视频识别和目标检测领域的成功启发，探索其在时序动作定位任务中的应用。

Method: 提出PCL-Former，包含三个专用Transformer模块：Proposal-Former、Classification-Former和Localization-Former，分别处理候选段识别、动作分类和边界预测。

Result: 在THUMOS14、ActivityNet-1.3和HACS数据集上分别优于现有方法2.8%、1.2%和4.8%。

Conclusion: PCL-Former通过模块化设计和专用损失函数，显著提升了时序动作定位的性能。

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [64] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: THOR是一种实时自适应时空RGB帧采样方法，利用热传感技术捕捉手部活动，显著减少数据量和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决穿戴式相机连续处理RGB图像时的高能耗、大数据量、隐私问题和计算资源需求。

Method: 通过低分辨率热成像数据识别手部活动转换，动态调整RGB帧采样率，并利用热信号定位感兴趣区域。

Result: 仅使用3%的RGB数据，活动识别F1分数达95%，与使用全部数据（94%）相当。

Conclusion: THOR为穿戴式相机实时监测手部活动提供了一种更实用的解决方案。

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [65] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: 提出了一种基于事件自动编码器的高效事件数据压缩与重建方法，显著提升了事件相机的物体检测性能，适用于实时边缘计算。


<details>
  <summary>Details</summary>
Motivation: 传统帧式视觉系统存在运动模糊、高延迟和数据冗余问题，事件相机虽能解决这些问题，但其稀疏和噪声事件流对物体检测带来挑战。

Method: 采用卷积编码的事件自动编码器架构，结合自适应阈值选择和轻量级分类器，以降低计算复杂度并提高识别精度。

Result: 在SEFD数据集上，模型精度与YOLO-v4相当，但参数减少35.5倍；在嵌入式平台上实现8至44.8 FPS的高帧率。

Conclusion: 该方法显著提升了事件相机的性能，适合低功耗、高速的实时边缘计算应用。

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [66] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-RTS通过结合数据高效的强化学习和视频自适应测试时间缩放策略，显著提升了视频推理能力的数据效率，无需资源密集的监督微调步骤。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习和大型语言模型的视频推理方法依赖大规模监督微调和长链思维标注，成本高且难以扩展。

Method: 跳过监督微调，采用纯强化学习训练和输出奖励，结合稀疏到密集的视频测试时间缩放策略。

Result: 在多个视频推理基准测试中，Video-RTS仅用3.6%的训练样本，平均准确率提升2.4%。

Conclusion: Video-RTS通过纯强化学习和自适应视频缩放策略，显著提升了视频推理的性能和效率。

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [67] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: 提出了一种名为Mask6D的新型预训练策略，通过结合2D-3D对应图和可见掩码图，提升了在遮挡或杂乱场景下的6D物体姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于单目RGB图像的姿态估计网络在遮挡或杂乱场景下难以提取区分性姿态特征，限制了性能。

Method: 引入2D-3D对应图和可见掩码图作为额外模态信息，结合RGB图像进行重建预训练，设计目标聚焦的损失函数以减少背景干扰。

Result: 实验表明，该方法优于现有的端到端姿态估计方法。

Conclusion: Mask6D通过多模态预训练策略有效提升了姿态估计的鲁棒性。

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [68] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: 提出了一种双边协作框架（BC-HOI），通过注意力偏差引导（ABG）和基于大型语言模型的监督引导（LSG）解决开放词汇HOI检测中视觉特征过于粗糙的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖视觉语言模型（VLM）生成的整体特征，但这些特征过于粗糙，与检测任务需求不符。

Method: BC-HOI框架包括ABG（引导VLM生成细粒度实例级特征）和LSG（利用LLM提供细粒度监督）。

Result: 在HICO-DET和V-COCO基准测试中表现优异，开放和封闭设置下均优于现有方法。

Conclusion: BC-HOI通过双边协作有效提升了开放词汇HOI检测的性能。

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [69] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 该论文通过分类交通场景中的关键元素，分析视觉驱动任务和数据集，提出了一种统一的分类框架，并总结了现有弱点和潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 利用视觉传感器和计算机视觉算法的进步，提升道路安全性，系统化分类和分析交通场景中的关键元素。

Method: 提出一种分类法，将交通实体分为异常和正常但关键两类，涵盖10个类别和20个子类，并分析35个视觉驱动任务和73个数据集。

Result: 建立了统一的分类框架，提供了数据集和任务的综合分析，指出了现有基准的优缺点。

Conclusion: 该研究为快速发展的领域提供了全面概述，指导资源选择，并突出了关键研究空白。

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [70] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: 论文提出FIFA框架，用于评估和改进视频多模态大语言模型（VideoMLLMs）中的幻觉问题，并通过后校正提升生成内容的真实性。


<details>
  <summary>Details</summary>
Motivation: VideoMLLMs在视频到文本和文本到视频任务中表现优异，但存在幻觉问题，现有评估方法无法全面评估开放性问题中的幻觉。

Method: 提出FIFA框架，提取描述性事实并建模其时空语义依赖关系，通过VideoQA模型验证；引入后校正工具修正幻觉内容。

Result: FIFA比现有评估方法更接近人类判断，后校正显著提升了文本和视频生成的事实一致性。

Conclusion: FIFA和后校正有效解决了VideoMLLMs中的幻觉问题，提升了生成内容的真实性。

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [71] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为KSCU的新方法，通过针对扩散模型的关键步骤进行概念遗忘，平衡了遗忘效果与生成能力的保留。


<details>
  <summary>Details</summary>
Motivation: 现有的概念遗忘方法难以平衡遗忘效果与生成能力的保留，导致扩散模型的安全风险未能有效解决。

Method: KSCU方法利用扩散模型的逐步采样特性，专注于关键步骤进行微调，减少参数更新次数。

Result: 实验表明，KSCU能有效阻止不良图像生成，同时更好地保留模型的生成能力。

Conclusion: KSCU为扩散模型的安全使用提供了一种高效且实用的解决方案。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [72] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: 本文提出了一种完整的流水线系统，将英语语音转换为流畅的3D手语动画，填补了从语音到手语生成的研究空白。


<details>
  <summary>Details</summary>
Motivation: 帮助聋哑和听力障碍人群更轻松地交流，解决从语音到手语动画生成这一被忽视的问题。

Method: 系统结合Whisper语音转文本、MarianMT模型翻译为ASL gloss，利用Word2Vec和FastText优化翻译，并通过3D关键点运动系统生成动画。

Result: 模型在ASL gloss翻译上表现优异，BLEU分数达0.7714和0.8923，并创建了Sign3D-WLASL和BookGlossCorpus-CG数据集。

Conclusion: 该流水线系统整合了音频、文本和动作数据，实现了从英语语音到逼真3D手语动画的完整转换。

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [73] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: ILNet提出了一种基于逆学习注意力（IL Attention）和动态锚点选择（DAS）的多智能体轨迹预测方法，显著提升了复杂交互场景下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模交互时缺乏显式的时空协调能力，且固定锚点选择策略难以适应不同未来环境。受人类驾驶行为的启发，提出动态调整初始决策的方法。

Method: ILNet采用逆学习范式建模交互，引入动态锚点选择模块（DAS）提取关键点作为锚点，几乎不增加参数。

Result: 在INTERACTION和Argoverse数据集上达到最优性能，尤其在复杂交互场景中表现出更高的准确性和多模态分布。

Conclusion: ILNet通过动态建模交互和锚点选择，显著提升了轨迹预测的性能和适应性。

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [74] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种模型无关的主动学习方法，用于优化野生动物相机陷阱数据的标注，结合不确定性和多样性指标，仅需30%的数据即可达到全数据集训练的检测性能。


<details>
  <summary>Details</summary>
Motivation: 野生动物相机陷阱数据量大且标注成本高，现有主动学习方法需完全访问模型，限制了应用。

Method: 提出模型无关的主动学习方法，结合对象和图像层面的不确定性与多样性指标进行样本选择。

Result: 实验表明，仅使用30%的标注数据，动物检测器性能与全数据集训练相当或更好。

Conclusion: 该方法显著减少了数据标注需求，为自动化野生动物监测提供了高效解决方案。

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [75] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: EXAONE Path 2.0提出了一种基于slide-level监督的病理学基础模型，解决了patch-level自监督学习在生物标志物预测中的局限性，仅用37k WSIs即达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: patch-level自监督学习（SSL）在生物标志物预测中可能忽略复杂领域特征，且数据效率低。

Method: 提出EXAONE Path 2.0，通过slide-level监督学习patch-level表征。

Result: 仅用37k WSIs训练，在10个生物标志物预测任务中达到SOTA性能。

Conclusion: EXAONE Path 2.0显著提高了数据效率和性能，解决了SSL的局限性。

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [76] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: ToBo是一种自监督学习框架，通过压缩场景为瓶颈令牌并预测后续场景，学习动态场景的紧凑表示。


<details>
  <summary>Details</summary>
Motivation: 动态场景的紧凑表示对视觉跟踪和机器人操作等任务至关重要，但现有方法难以高效捕捉时间动态。

Method: ToBo通过压缩步骤将参考场景编码为瓶颈令牌，在扩展步骤中利用少量目标补丁预测目标场景，以学习时间依赖。

Result: 实验表明ToBo在视频标签传播和机器人操作等任务中优于基线方法，并在真实环境中验证了其鲁棒性。

Conclusion: ToBo能有效学习动态场景的时间依赖，适用于多种任务和模型规模。

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [77] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: 本文提出了一种名为CDR-CA的新任务，旨在根据应用上下文优化多属性的多样性。通过扩展DPP为多源形式（MS-DPP），并结合切线归一化，显著提升了结果多样性在文本到图像检索中的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注图像外观的多样性指标，但实际应用中多样性的需求和指标因场景而异，限制了其应用范围。因此，需要一种能根据上下文动态调整多属性多样性的方法。

Method: 提出MS-DPP，将DPP扩展为多源形式，基于流形表示构建统一的相似性矩阵，并引入切线归一化以反映上下文。

Result: 实验证明，该方法在优化多属性多样性方面效果显著。

Conclusion: CDR-CA任务及MS-DPP方法为结果多样性提供了更灵活的解决方案，适用于不同应用场景。

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [78] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Concept-TRAK的新方法，用于解决扩散模型在图像生成中的版权和透明度问题，通过概念级归因分析提供更细粒度的贡献识别。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其广泛使用引发了版权和透明度问题，现有方法无法识别特定元素（如风格或对象）的贡献。

Method: 提出Concept-TRAK方法，扩展影响函数，包括基于扩散后验采样的训练损失和概念感知奖励函数。

Result: 在AbC基准测试中，Concept-TRAK显著优于现有方法，并通过案例研究展示了其在版权保护、内容安全和提示工程中的实用性。

Conclusion: 概念级归因为生成式AI的负责任开发和治理提供了可操作的见解。

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [79] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: 提出了一种基于分布散度的相似性函数（DSF），通过将多视图表示为分布并测量分布间的散度来显式捕捉联合结构，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要捕获成对关系，未能建模多视图的联合结构，因此需要更有效的方法。

Method: 提出DSF，将多视图表示为分布并计算分布间的散度作为相似性度量。

Result: DSF在kNN分类和线性评估等任务中表现优于其他多视图方法，且效率更高。

Conclusion: DSF无需温度超参数即可有效工作，并在理论和实验上优于余弦相似性。

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [80] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: 提出了一种新的损失函数EBT，通过将像素分为边缘、边界和纹理三类，优化边缘检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统WBCE损失函数忽视非边缘像素的结构差异，导致预测模糊，EBT损失旨在解决这一问题。

Method: EBT损失将像素分为边缘、边界和纹理三类，并为每类分配不同的监督权重。

Result: 实验表明EBT损失在多个基准测试中表现优异，且超参数鲁棒性强。

Conclusion: EBT损失优于传统方法，易于部署且无需复杂调参。

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [81] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: 论文提出首个半导体晶圆切割过程的公开数据集CHDL，并设计了双路径预测架构DIFFUMA，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 高精度工业场景（如半导体制造）缺乏专用数据集，阻碍了复杂过程建模与预测的研究。

Method: 构建CHDL数据集，并提出DIFFUMA模型，结合Mamba模块和扩散模块，分别处理全局时序和局部空间细节。

Result: DIFFUMA在CHDL数据集上MSE降低39%，SSIM提升至0.988，性能优于现有方法。

Conclusion: 研究不仅提供了SOTA模型，还为工业AI领域贡献了宝贵的数据资源。

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [82] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST是一种新颖的运动扩散模型，通过时间片段Banzhaf交互解决从罕见语言提示生成人类运动的挑战。它利用细粒度片段关系改进文本-运动匹配，消除冗余，并在生成阶段利用检索到的运动片段生成语义一致的运动。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在从罕见语言提示生成人类运动时因粗粒度匹配和忽略语义线索而面临的挑战。

Method: 提出时间片段Banzhaf交互，量化文本-运动片段一致性，并在生成阶段利用检索到的运动片段生成运动。

Result: MOST在文本-运动检索和生成方面达到最先进性能，尤其在罕见提示下表现突出。

Conclusion: MOST通过细粒度片段关系和消除冗余，显著提升了文本到运动生成的性能。

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [83] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 提出了一种自适应边界对比学习方法AMContrast3D++，用于点云的3D语义分割，通过模糊度感知优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对模糊点采用均等惩罚，忽略了过渡区域的特征差异，导致模型性能受限。

Method: 设计AMContrast3D++，结合对比学习和模糊度预测模块，通过并行训练分支和掩码细化机制优化模糊点嵌入。

Result: 在S3DIS和ScanNet数据集上验证了方法的有效性，提升了分割性能和鲁棒性。

Conclusion: 自适应模糊度感知方法显著改善了3D语义分割的准确性和鲁棒性。

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [84] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: FOLC-Net框架通过轻量级架构和优化机制，显著提升了MRI多视角和单视角疾病诊断的性能，尤其在矢状面表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决现有SOTA模型在处理轴向、冠状和矢状面时的性能下降问题，提升医学图像分析的适应性和鲁棒性。

Method: 提出FOLC-Net，结合MRFO优化、全局模型克隆和ConvNeXt，构建轻量级联邦学习架构。

Result: FOLC-Net在矢状面准确率达92.44%，优于现有方法，且在所有视角下均表现更优。

Conclusion: FOLC-Net为分散式环境下的医学图像分析提供了更可靠和高效的解决方案。

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [85] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出了一种双摄像头系统（DCS）和曝光自适应融合网络（EAFNet）来解决HDR视频重建中的闪烁问题，通过选择性特征融合和多尺度架构实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 解决交替曝光方法中参考图像的曝光波动导致的闪烁问题。

Method: 使用双摄像头系统（DCS）分别捕获参考和非参考序列，并设计EAFNet进行特征融合和多尺度重建。

Result: 在多个数据集上实现了最先进的性能，验证了DCS在HDR视频重建中的潜力。

Conclusion: 提出的DCS和EAFNet方法有效解决了HDR视频重建中的闪烁问题，具有广泛应用前景。

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [86] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: CMDCL提出了一种跨模态双因果学习方法，通过文本和视觉因果干预解决长时动作识别中的跨模态偏差和视觉混淆问题。


<details>
  <summary>Details</summary>
Motivation: 长时动作识别（LTAR）因时间跨度长、动作关联复杂和视觉混淆问题而具有挑战性。现有方法缺乏跨模态因果建模，限制了其在基于视觉语言模型（VLM）的LTAR中的应用。

Method: CMDCL通过结构因果模型揭示视频与标签文本之间的因果关系，分别通过文本因果干预和视觉因果干预解决跨模态偏差和视觉混淆问题。

Result: 在Charades、Breakfast和COIN三个基准测试中，CMDCL表现出色，验证了其有效性。

Conclusion: CMDCL通过双因果干预实现了鲁棒的动作表示，为长时动作识别提供了有效的解决方案。

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [87] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Omni-Fuse的新型空间-光谱全融合网络，用于高光谱图像分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学高光谱成像（MHSI）在疾病诊断中具有潜力，但高维度和光谱冗余特性使得空间和光谱信息的融合具有挑战性。

Method: 设计了跨维度特征融合操作，包括双向注意力机制的跨维度增强模块、光谱引导的空间查询选择以及两阶段跨维度解码器。

Result: 在两个显微高光谱图像数据集上的实验表明，该方法在DSC指标上比现有方法提升了5.73%。

Conclusion: Omni-Fuse通过高效的跨维度特征融合，显著提升了高光谱图像的分割性能。

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [88] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 提出了一种轻量级框架，利用2D全身骨架作为辅助条件，通过扩散模型生成与音频同步的说话人视频，并发布了首个公开数据集CSG-405。


<details>
  <summary>Details</summary>
Motivation: 解决语音与视觉内容之间的一对多映射问题，以及缺乏大规模公开数据集和高计算需求的问题。

Method: 使用2D骨架作为辅助条件，结合细粒度音频片段和参考图像骨架，通过扩散模型预测骨骼运动，再生成高保真视频。

Result: 在视觉质量和同步性上超越现有方法，并能泛化到不同说话人和场景。

Conclusion: 提出的框架和数据集为语音-手势视频生成研究提供了高效且可扩展的解决方案。

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [89] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 提出了一种基于视点依赖投影（VDP）的点云分割方法，通过动态适应视点变化的3D到2D映射，解决了现有方法因视点独立性导致的投影多样性和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于投影的点云分割方法依赖视点独立的投影，导致投影多样性不足且计算冗余。

Method: 设计了VDP框架，通过数据驱动的投影生成自适应射线，并结合颜色正则化优化框架。

Result: 在S3DIS和ScanNet基准测试中表现优异，计算成本低。

Conclusion: PointVDP提供了一种资源高效的语义理解解决方案。

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [90] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: 论文提出了一种结合符号回归和轨迹引导的视频生成框架，以解决现有模型在物理对齐上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和自回归视频生成模型虽视觉逼真，但缺乏物理准确性，无法模拟真实世界物体运动。

Method: 通过提取输入视频的运动轨迹，利用检索式预训练增强符号回归，发现运动方程以预测物理准确的未来轨迹，并引导视频生成。

Result: 在经典力学场景（如弹簧-质量系统、摆锤、抛体运动）中，该方法成功恢复真实运动方程，并提升了生成视频的物理对齐性。

Conclusion: 该框架无需微调现有模型，即可显著提升视频生成的物理准确性。

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [91] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: IAP是一种新型对抗补丁攻击框架，通过感知感知定位和扰动优化方案生成高度不可见的对抗补丁。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标攻击场景中表现不佳或生成的补丁不够隐蔽，容易被人类或自动防御系统检测到。

Method: IAP结合类感知定位和敏感性地图选择补丁位置，并通过感知正则化对抗损失和颜色恒常性梯度更新规则优化扰动。

Result: IAP在多种图像基准和模型架构上均表现出高攻击成功率，同时显著提升补丁的不可见性。

Conclusion: IAP不仅对人类高度不可见，还能有效绕过多种先进补丁防御系统。

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [92] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 提出了一种新的损失函数（Crag and Tail loss），用于从稀疏标注中学习密集预测任务，特别是在医学图像中定位关键点。


<details>
  <summary>Details</summary>
Motivation: 医学领域中稀疏标注的挑战性，尤其是需要密集像素级标注时，标注成本高且难以实现。

Method: 将问题建模为稀疏热图回归，并提出Crag and Tail损失函数，有效利用稀疏标注并减少假阴性影响。

Result: 通过消融实验验证了方法的有效性，能够准确实现关键点的密集定位。

Conclusion: 该方法在标注难以获取的场景中具有潜力，推动了相关研究进展。

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [93] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: ClipGS是一种支持裁剪平面的高斯泼溅框架，用于医学体积数据的交互式电影化渲染，提高了渲染质量和效率。


<details>
  <summary>Details</summary>
Motivation: 医学体积数据的可视化对诊断和手术规划至关重要，但现有方法计算成本高且渲染速度慢，限制了交互性。

Method: 提出ClipGS框架，包括可学习的截断方案和自适应调整模型，动态调整高斯基元的可见性和变形。

Result: 在五种医学数据上验证，平均PSNR为36.635，帧率为156 FPS，模型大小为16.1 MB，优于现有方法。

Conclusion: ClipGS在渲染质量和效率上表现优异，适用于医学数据的交互式可视化。

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [94] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: Diff$^2$I2P提出了一种基于扩散先验的全可微分图像到点云配准框架，通过Control-Side Score Distillation和Deformable Correspondence Tuning模块，显著提升了跨模态特征对齐和配准效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过度量学习实现跨模态特征对齐，但忽略了图像与点云数据的模态差异，导致对应关系不准确。

Method: 提出Diff$^2$I2P框架，结合扩散先验，使用CSD技术优化变换预测，并通过DCT模块实现可微分的对应关系估计和PnP求解。

Result: 在7-Scenes基准测试中，Diff$^2$I2P显著优于现有方法，配准召回率提升超过7%。

Conclusion: Diff$^2$I2P通过扩散先验和可微分设计，有效解决了跨模态配准问题，性能显著提升。

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [95] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文分析了扩散模型中先验和似然梯度方向的不稳定性，提出了一种新的梯度管理技术SPGD，通过渐进式似然预热和自适应方向动量平滑来提升图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复中表现出色，但先验和似然梯度之间的相互作用及其不稳定性尚未充分研究，影响了生成过程和恢复效果。

Method: 提出SPGD技术，包括渐进式似然预热策略（减少梯度冲突）和自适应方向动量平滑（减少似然梯度波动）。

Result: SPGD显著提升了生成稳定性，在多种恢复任务中实现了最先进的定量指标和视觉上更优的结果。

Conclusion: SPGD通过梯度管理有效解决了扩散模型中的不稳定性问题，为图像恢复提供了更可靠的解决方案。

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [96] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: MK-Pose是一种多模态关键点学习框架，结合RGB图像、点云和类别文本描述，通过自监督关键点检测和图增强特征融合，显著提升了类别级物体姿态估计的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物体遮挡和跨实例、跨类别泛化方面表现不佳，MK-Pose旨在解决这些问题。

Method: 提出自监督关键点检测模块（含注意力查询生成、软热图匹配和图关系建模）和图增强特征融合模块。

Result: 在CAMERA25、REAL275和HouseCat6D数据集上表现优于现有方法，IoU和平均精度均有所提升。

Conclusion: MK-Pose无需形状先验即可实现高性能，为类别级姿态估计提供了新思路。

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [97] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian是一种无需重新训练的3D高斯压缩方法，通过混合精度量化和属性判别剪枝实现高效压缩，适用于移动设备。


<details>
  <summary>Details</summary>
Motivation: 大规模3D模型需要高效压缩以减少内存和计算成本，但现有方法缺乏灵活性且需要重新训练。

Method: 结合混合精度量化和属性判别剪枝，实现无需重新训练的高效压缩。

Result: 压缩率高达96.4%，渲染质量损失小（PSNR下降<1 dB），速度比现有方法快1.7-2.1倍。

Conclusion: FlexGaussian是一种灵活、高效的3D高斯压缩方法，适用于资源受限的设备。

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [98] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: CheXPO是一种针对胸部X光图像的偏好优化策略，通过结合置信度-相似性联合挖掘和反事实推理，解决了医学视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型（VLMs）存在幻觉问题，影响可靠性，而传统偏好优化方法面临数据分布不平衡、临床无关样本和高成本专家标注等挑战。

Method: CheXPO通过合成多任务胸部X光视觉指令数据集进行监督微调（SFT），利用置信度分析和相似性检索扩展困难样本，并通过反事实推理提供细粒度临床偏好。

Result: 实验显示，CheXPO仅用5%的SFT样本就实现了8.93%的性能提升，达到多种临床任务的最先进水平。

Conclusion: CheXPO为放射学应用提供了一种可扩展、可解释的解决方案。

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [99] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: QUANet提出了一种基于数量导向的文本提示和视觉-文本数量对齐损失的方法，以增强模型在计数任务中的数量感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本提示中仅包含对象类别信息，无法准确区分计数任务中的对象数量，因此需要改进。

Method: QUANet引入数量导向的文本提示和视觉-文本对齐损失，并设计了一个双流自适应计数解码器，包含Transformer流、CNN流和T2C适配器。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等标准基准测试中，QUANet表现出强大的零样本类无关计数泛化能力。

Conclusion: QUANet通过改进文本提示和解码器设计，显著提升了计数任务的性能。

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [100] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: MCA-RG是一个知识驱动的框架，通过将视觉特征与医学概念对齐来改进放射学报告生成，解决了病理和解剖特征与文本描述映射不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 临床应用中，LLMs在放射学报告生成中难以准确映射病理和解剖特征，且语义无关的特征提取影响了报告的准确性。

Method: MCA-RG利用病理库和解剖库对齐视觉特征，采用解剖对比学习和匹配损失优化特征，并通过特征门控机制过滤低质量特征。

Result: 在MIMIC-CXR和CheXpert Plus基准测试中，MCA-RG表现出色。

Conclusion: MCA-RG通过知识驱动的方法显著提升了放射学报告生成的准确性和临床适用性。

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [101] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: StixelNExT++是一种用于单目感知系统的新型场景表示方法，通过聚类3D Stixel单元增强对象分割，实现高压缩场景信息，并在Waymo数据集上表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 提升单目感知系统的场景表示能力，同时保持对点云和鸟瞰图表示的适应性。

Method: 基于Stixel表示，推断3D Stixels并聚类小单元；使用轻量级神经网络，训练数据来自LiDAR生成的真值。

Result: 在Waymo数据集上30米范围内表现优异，计算时间低至每帧10毫秒。

Conclusion: StixelNExT++在自动驾驶系统中具有集体感知的潜力。

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [102] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 提出了一种多模态特征融合框架，结合3D CT图像和临床数据，用于提升非小细胞肺癌患者免疫治疗的生存预测准确性。


<details>
  <summary>Details</summary>
Motivation: 个性化治疗需要准确的预后预测，但缺乏大规模数据集和有效的多模态特征融合方法。

Method: 使用跨模态掩码学习策略，结合Slice-Depth Transformer和基于图的Transformer，分别处理CT图像和临床数据。

Result: 方法在多模态整合中表现优异，超越了现有方法，为预后模型设定了新标准。

Conclusion: 提出的框架显著提升了生存预测的准确性，为个性化治疗提供了有力工具。

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [103] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: STG-Mamba是一种用于音乐引导舞蹈视频合成的空间-时间图模型，通过音乐到骨架和骨架到视频的两步映射实现，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决音乐到舞蹈视频的合成问题，捕捉关节在空间和时间上的依赖关系。

Method: 1. 音乐到骨架翻译：使用STGM块构建骨架序列；2. 骨架到视频翻译：采用自监督正则化网络生成视频。

Result: 实验表明STG-Mamba显著优于现有方法。

Conclusion: STG-Mamba为音乐到舞蹈视频合成提供了高效解决方案。

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [104] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 论文提出SpatialReasoner框架，通过LLM驱动的空间推理和分层特征场，提升开放词汇3D视觉定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言查询中空间关系推理不足，导致定位不准确，如“椅子上的书”这类场景。

Method: 结合LLM微调以捕捉空间关系，并构建视觉属性增强的分层特征场，通过CLIP特征和SAM掩码实现。

Result: 实验表明，该框架能无缝集成不同神经表示，显著提升3D视觉定位性能。

Conclusion: SpatialReasoner有效解决了空间关系推理问题，为3D视觉定位提供了新思路。

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [105] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: 设计并开发了一个基于OCR的流水线，用于从发票中高效提取表格数据，显著提升了数据提取的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决从非标准或噪声较多的发票中提取结构化表格数据的挑战，支持自动化财务流程和数字存档等实际应用。

Method: 利用Tesseract OCR进行文本识别，结合自定义后处理逻辑，包括动态预处理、表格边界检测和行列映射。

Result: 流水线显著提高了数据提取的准确性和一致性，适用于实际应用场景。

Conclusion: 该OCR驱动的流水线为发票表格提取提供了一种高效且可靠的解决方案。

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [106] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于伪注释和对比视频-语言对齐的分层预训练策略，用于提升手语翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决手语视频与文本表示之间的差异问题，同时避免依赖注释的负担。

Method: 采用分层特征提取（帧、片段、视频级别），结合伪注释和对比对齐策略。

Result: 实验表明，该方法提升了BLEU-4和ROUGE分数，同时保持高效性。

Conclusion: 分层预训练策略有效提升了手语翻译性能，为无注释方法提供了新思路。

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [107] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: 提出了一种基于几何深度学习的框架，用于高效预测CAD网格的Laplace-Beltrami（LB）谱，显著节省计算时间且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法（FEM）计算LB谱的复杂度高，不适用于频繁处理大型CAD网格的场景，如质量控制和数据库应用。

Method: 采用图神经网络（GNN）架构，结合高斯曲率、平均曲率和主曲率等丰富网格特征，预测LB谱。

Result: 实验表明，该方法比线性FEM快约5倍，且准确性相当。

Conclusion: LB谱可通过深度学习高效学习，为几何处理任务提供了新思路。

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [108] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: 提出了一种结合视觉适配器、提示学习和部分最优传输（POT）与对比学习（CL）的新方法，用于提升CLIP在医学图像异常检测中的适应性。


<details>
  <summary>Details</summary>
Motivation: 医学异常检测面临成像模态多样、解剖变异大和标记数据有限等挑战，现有方法难以捕捉细微异常。

Method: 采用多提示学习与局部特征对齐（通过POT），结合对比学习增强类内凝聚和类间分离。

Result: 在少样本、零样本和跨数据集场景中取得最优结果，无需合成数据或记忆库。

Conclusion: 该方法显著提升了医学图像异常检测的性能，代码已开源。

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [109] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: RPFNet通过双分支特征提取框架（RPM和FDFM）和交叉促进模块（CPM），结合频域卷积和残差先验，高效实现图像融合，提升高级视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像融合中长距离特征依赖计算成本高和缺乏真实标注的问题。

Method: 采用双分支框架（RPM提取模态差异，FDFM进行频域融合），结合CPM增强局部与全局特征协同，并使用辅助损失函数优化训练。

Result: RPFNet能有效整合特征、增强纹理细节和显著对象，提升高级视觉任务效果。

Conclusion: RPFNet通过频域建模和残差先验，显著提升图像融合性能，适用于高级视觉任务。

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [110] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: 提出了一种基于提示复杂度的自适应缓存方法（PCA），通过动态调整重用阈值和优化输入输出关系，显著提升了视频生成的推理速度，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 固定频率的缓存重用机制在复杂场景中会导致质量下降，手动调整阈值效率低且不鲁棒。

Method: 提出PCA缓存方法，利用输入提示的语义信息动态调整重用阈值；改进TeaCache的输入输出关系建模；引入动态CFGCache机制。

Result: 实验表明，该方法在Wan2.1模型上实现了2.79倍的加速，同时保持了高视觉保真度。

Conclusion: PCA缓存方法通过自适应和动态优化，显著提升了视频生成的效率和输出质量。

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [111] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: 提出了一种名为D2I的框架，通过训练时的规则奖励提升多模态LLM的理解和推理能力，无需额外标注或复杂奖励。


<details>
  <summary>Details</summary>
Motivation: 多模态推理研究中模态对齐和训练成本问题尚未充分解决，现有方法依赖额外标注和规则奖励，增加了成本和限制了扩展性。

Method: D2I框架在训练时通过规则奖励设置深思熟虑的推理策略，评估时转为直觉推理，隐式反映模型能力。

Result: D2I在领域内和领域外基准测试中均优于基线方法。

Conclusion: D2I展示了规则奖励在多模态LLM中培养可迁移推理能力的作用，为训练和测试时推理深度解耦提供了方向。

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [112] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种局部和全局双粒度身份关联机制，通过跨模态身份关系和动态调整机制提升文本到人物图像匹配的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂的一对多身份关系，限制了性能提升，因此需要一种更有效的方法。

Method: 局部层面显式建立跨模态身份关系，全局层面构建动态跨模态身份关联网络，并结合信息不对称样本对构建和一致性学习。

Result: 实验结果表明，该方法显著提升了跨模态匹配的准确性。

Conclusion: 该方法为文本到人物图像匹配提供了一种高效实用的解决方案。

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [113] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: 该研究开发了高效的OCR系统，用于濒危语言满语的识别，通过微调开源视觉语言模型，显著提升了真实历史文档的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 满语作为濒危语言对早期现代东亚历史研究至关重要，但缺乏有效的OCR系统处理真实历史文档。

Method: 研究通过参数高效训练，在6万张合成的满语单词图像上微调了三个开源视觉语言模型（LLaMA-3.2-11B、Qwen2.5-VL-7B、Qwen2.5-VL-3B）。

Result: LLaMA-3.2-11B在合成数据上表现优异（98.3%单词准确率），在真实手写文档上保持93.1%准确率，远超传统方法。

Conclusion: 该研究为濒危语言OCR提供了可迁移的框架，降低了技术和财务门槛，助力历史学家和语言学家处理历史档案。

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [114] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 论文提出了一种生成合成热成像数据的流程，用于增强无人机热成像数据集，提升深度学习模型在搜索救援等领域的性能。


<details>
  <summary>Details</summary>
Motivation: 无人机热成像在低光或遮挡条件下有广泛应用潜力，但数据稀缺限制了深度学习模型的进展。

Method: 通过将任意对象类别集成到现有热背景中，控制其位置、尺度和方向，生成合成热图像。

Result: 在HIT-UAV和MONET数据集中新增类别（无人机和动物），验证了合成数据在目标检测任务中的有效性。

Conclusion: 合成热成像数据能有效扩展应用场景，热成像检测器优于可见光检测器，且需重视空中视角的模拟。

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [115] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: GreenHyperSpectra数据集用于植物性状预测，通过半监督和自监督方法解决标签稀缺和领域转移问题，性能优于现有监督基线。


<details>
  <summary>Details</summary>
Motivation: 传统野外采样难以覆盖生态尺度上的性状变异，机器学习结合遥感高光谱数据提供解决方案，但面临标签稀缺和领域转移的挑战。

Method: 提出GreenHyperSpectra数据集，采用半监督和自监督方法预训练多输出回归模型，评估框架包括分布内和分布外场景。

Result: 模型性能优于现有监督基线，显著提升光谱表征学习能力。

Conclusion: GreenHyperSpectra为植物功能性状评估与表征学习的交叉研究提供了方法论框架和数据支持。

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [116] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的颜色空间HVI和网络HVI-CIDNet+，用于低光图像增强，解决了现有方法的颜色偏差和亮度伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于sRGB和HSV颜色空间的低光图像增强方法存在颜色偏差、亮度伪影和噪声问题。

Method: 提出HVI颜色空间和HVI-CIDNet+网络，利用HV颜色图和可学习强度去除噪声，并通过Prior-guided Attention Block和Region Refinement Block优化内容恢复和颜色校正。

Result: 在10个基准数据集上，HVI-CIDNet+优于现有最先进方法。

Conclusion: HVI颜色空间和HVI-CIDNet+有效解决了低光图像增强中的颜色失真和噪声问题，性能显著提升。

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [117] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: 提出一种基于Vision Transformer的端到端弱监督语义分割方法，利用注意力图生成伪分割掩码，减少对精细标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖外部模块（如CAM）生成伪分割掩码，而本研究旨在直接利用ViT的注意力图，提升效率和准确性。

Method: 训练稀疏ViT，使用多[CLS]标记和随机掩码策略，聚合自注意力图生成伪分割掩码。

Result: 在多个基准数据集上表现优异，生成的伪掩码可用于训练分割模型，效果接近全监督模型。

Conclusion: 该方法显著减少了对精细标注的需求，同时提升了分割性能，为弱监督语义分割提供了新思路。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [118] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Blümel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: 长期生物特征评估显示，个体间的日间分数波动比整个测量期间更显著，强调长期测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估长期生物特征数据的稳定性，为生物识别技术的改进提供基础。

Method: 使用多种生物识别工具和技术，对400多名参与者进行两年半的定期评估，并分析238,000多个数据集。

Result: 发现个体日间生物特征分数波动显著，但长期稳定性较高。

Conclusion: 长期控制在生物特征测试中至关重要，为未来生物识别分析奠定基础。

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [119] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 论文提出SemRaFiner方法，用于稀疏雷达点云的泛光分割，通过优化特征提取和训练过程，提升场景理解能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要语义场景理解，但相机和LiDAR在恶劣天气下受限，雷达虽能提供运动信息但数据稀疏且噪声多。

Method: 提出SemRaFiner方法，优化稀疏雷达点云的特征提取，改进训练过程，包括数据增强。

Result: 实验表明，该方法在雷达泛光分割任务中优于现有技术。

Conclusion: SemRaFiner通过优化雷达点云处理，提升了场景理解的准确性。

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [120] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: 论文提出了一种自适应部分发现和学习方法（APL），通过共享可学习部分查询和DINO部分先验，生成一致的对象部分及其对应关系，无需额外标注。该方法通过新颖的all-min对比损失学习区分性强且泛化性好的部分表示，显著提升了细粒度数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法仅依赖DINO CLS token的全局表示，导致区分性和泛化性之间的固有权衡。APL旨在通过自适应部分发现和学习解决这一问题。

Method: APL利用共享可学习部分查询和DINO部分先验生成一致的对象部分及其对应关系，提出all-min对比损失学习区分性和泛化性兼顾的部分表示。

Result: APL在细粒度数据集上表现出显著性能提升，并能轻松集成到不同GCD框架中。

Conclusion: APL通过自适应部分发现和学习，有效解决了GCD任务中区分性和泛化性的权衡问题，为细粒度数据集提供了显著改进。

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [121] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 论文提出了一个多属性中国书法字符数据集（MCCD），填补了现有数据集稀缺且缺乏属性信息的空白，为书法研究提供了丰富资源。


<details>
  <summary>Details</summary>
Motivation: 研究书法属性信息（如风格、朝代、书法家）具有重要文化历史价值，但现有数据集稀缺且缺乏属性信息，限制了深入研究。

Method: 构建了包含7,765类329,715个书法字符图像的MCCD数据集，并基于脚本风格、朝代和书法家提取了三个子集。

Result: 实验表明，书法字符的笔画结构复杂性和属性间的相互作用增加了准确识别的难度。

Conclusion: MCCD填补了详细书法数据集的空白，为书法研究和多领域进展提供了宝贵资源。

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [122] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,César Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hernández,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: 利用深度学习模型和卫星图像识别棕榈树分布，揭示古代人类管理对植被的长期影响。


<details>
  <summary>Details</summary>
Motivation: 研究古代人类管理对热带森林的长期影响，特别是在高分辨率尺度上。

Method: 结合深度学习模型（识别棕榈树）和聚类算法（识别棕榈集群），估计古代管理区域。

Result: 棕榈树在考古遗址附近显著更多，古代人类管理区域可能比考古证据显示的大两个数量级。

Conclusion: 古代人类通过影响植被促进了棕榈树的增殖，留下了持久的生态足迹，为研究人地关系提供了新方法。

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [123] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 该研究提出了一种深度学习方法ProRSeg，用于多领域MR-MR图像配准，并在前列腺癌患者的MR引导自适应放疗中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 精确的可变形图像配准（DIR）在MR引导自适应放疗（MRgART）中对轮廓传播和剂量累积至关重要。

Method: 研究使用262对3T MR模拟扫描训练了ProRSeg方法，通过加权分割一致性损失进行优化，并在不同领域数据集上测试其性能。

Result: ProRSeg在膀胱配准中表现出多领域泛化能力（DSC 0.88-0.86），而在直肠和CTV上表现依赖领域。剂量累积结果显示83.3%患者满足CTV覆盖和膀胱保护约束。

Conclusion: ProRSeg在多领域MR-MR配准中表现合理，初步验证了其用于评估治疗合规性的可行性。

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [124] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Percep360的全景生成方法，用于自动驾驶，通过局部场景扩散方法和概率提示方法实现高质量、可控的全景数据生成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要全面的360度全景感知，但现有数据采集和标注过程复杂且耗时。现有生成模型无法实现高质量、可控的全景生成。

Method: 提出局部场景扩散方法（LSDM）解决信息丢失问题，并通过概率提示方法（PPM）实现可控生成。

Result: 生成图像在无参考质量指标上优于原始拼接图像，并提升了下游感知模型的性能。

Conclusion: Percep360为自动驾驶提供了高质量、可控的全景数据生成方案，具有实际应用价值。

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [125] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttopää,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: MassID45数据集结合分子和成像数据，用于训练自动分类器，以处理批量昆虫样本，推动生态和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 昆虫多样性研究面临种群下降和分类效率低的挑战，需要高效方法支持大规模生态调查。

Method: 结合DNA条形码和高分辨率成像数据，利用AI辅助工具对批量样本进行分割和分类标注。

Result: 数据集包含17,000多个标本的分割掩码和分类标签，为昆虫群落快速表征提供潜力。

Conclusion: MassID45数据集在微小物体检测和实例分割方面具有创新性，促进生态和机器学习研究的进步。

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [126] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: FreeTTA是一种无需训练且通用的测试时适应方法，通过在线EM算法利用视觉语言模型的零样本预测作为先验，显著提升了跨域和分布外场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在测试数据分布与训练数据不一致时的性能下降问题，同时避免传统测试时适应方法的高成本和不现实假设。

Method: 提出FreeTTA方法，利用在线EM算法和零样本预测作为先验，迭代更新测试样本的后验概率和参数。

Result: 在15个数据集上，FreeTTA在跨域和分布外场景下均显著优于现有方法。

Conclusion: FreeTTA是一种灵活且高效的测试时适应方法，首次显式建模测试数据分布，为实际应用提供了新方向。

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [127] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: 论文提出了一种名为DenoiseCP-Net的新型多任务架构，用于在恶劣天气条件下基于LiDAR的集体感知，通过去噪和物体检测的集成，显著降低了带宽需求和延迟。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知系统在恶劣天气和遮挡环境下容易失效，集体感知虽能解决这一问题，但相关研究较少。

Method: 提出DenoiseCP-Net，将体素级去噪和物体检测集成到稀疏卷积骨干网络中，避免冗余计算。

Result: 在模拟的雨雪雾天气中，DenoiseCP-Net实现了近乎完美的去噪效果，带宽需求降低23.6%，同时保持检测精度和降低延迟。

Conclusion: DenoiseCP-Net为恶劣天气下的集体感知提供了高效解决方案，显著提升了系统性能。

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [128] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种名为GNN-ViTCap的新框架，用于从组织病理学显微图像中进行分类和字幕生成，解决了冗余补丁和未知补丁位置的问题，并在分类和字幕生成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 组织病理学显微图像的准确评估对癌症诊断和治疗至关重要，但现有方法面临冗余补丁和未知补丁位置的挑战，且自动生成病理字幕仍是一个难题。

Method: 结合视觉特征提取器、动态聚类、图神经网络和语言模型，通过GNN-ViTCap框架实现分类和字幕生成。

Result: 在BreakHis和PatchGastric数据集上，分类任务的F1得分为0.934，AUC为0.963；字幕生成任务的BLEU-4得分为0.811，METEOR得分为0.569。

Conclusion: GNN-ViTCap在显微图像分类和字幕生成任务上优于现有方法，为基于显微图像的诊断提供了可靠高效的解决方案。

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [129] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: 提出了一种轻量级且训练高效的方法，利用预训练的病理学基础模型的特征嵌入，直接从H&E染色组织学图像预测细胞组成，避免了昂贵的空间转录组学实验。


<details>
  <summary>Details</summary>
Motivation: 数字病理学和深度学习的快速发展为病理学基础模型的出现提供了条件，这些模型有望解决各种疾病条件下的通用病理学问题。同时，空间转录组学技术为从H&E染色图像中更精细地分析细胞水平提供了可能。

Method: 通过从预训练的病理学基础模型中提取信息丰富的特征嵌入，训练一个轻量级多层感知机（MLP）回归器，预测由cell2location生成的细胞类型丰度。

Result: 该方法能够准确预测细胞类型组成，性能与现有方法（如Hist2Cell）相当，同时显著降低了计算复杂度。

Conclusion: 该方法展示了从H&E染色图像中高效预测细胞组成的潜力，为病理学研究提供了一种低成本、高效的替代方案。

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [130] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: MST-Distill提出了一种新型跨模态知识蒸馏框架，通过混合专家教师模型和动态路由网络解决传统方法的局限性，显著提升了跨模态蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在跨模态场景中因数据和统计异质性难以有效利用跨模态教师模型的互补知识。

Method: 提出MST-Distill框架，结合跨模态和多模态教师模型，使用动态路由网络和掩码模块抑制模态差异。

Result: 在五个多模态数据集上的实验表明，MST-Distill显著优于现有方法。

Conclusion: MST-Distill通过动态教师模型和掩码模块有效解决了知识漂移问题，提升了跨模态知识蒸馏效果。

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [131] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: 研究探讨了如何通过整合上下文元数据（如GPS、时间戳和食物信息）提升大型多模态模型（LMMs）在营养分析中的表现，并引入了新数据集ACETADA。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估专有模型（如GPT-4），忽略了其他LLMs，且上下文元数据对模型性能的影响尚未充分探索。

Method: 通过整合GPS、时间戳和食物信息等元数据，结合多种推理修饰符（如Chain-of-Thought），评估了八种LMMs的性能。

Result: 实验表明，整合元数据能显著降低营养值预测的MAE和MAPE。

Conclusion: 上下文感知的LMMs在营养分析中具有显著潜力。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [132] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet是一个深度学习框架，通过将标尺读数统一为关键点检测问题，解决了计算机视觉中像素到实际尺寸转换的挑战。


<details>
  <summary>Details</summary>
Motivation: 准确将像素测量转换为实际尺寸是计算机视觉中的基础问题，影响生物医学、法医学、营养分析和电子商务等关键应用。

Method: RulerNet通过几何级数参数表示标尺，采用抗透视变换的标注和训练策略，结合合成数据增强训练多样性。

Result: RulerNet在多样化标尺和成像条件下表现出色，支持实时尺寸估计，适用于移动或边缘设备。

Conclusion: RulerNet作为一种通用测量工具，具有在高影响力领域与其他视觉组件集成的潜力。

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [133] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: 论文提出了一种新的自动评估方法L-VQAScore，用于解决文本到图像生成模型在复杂组合生成（如时尚领域）中的评估挑战，特别是属性混淆问题。该方法结合视觉定位和VQA探测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型评估方法在复杂组合生成（如时尚领域）中表现不佳，尤其是难以准确评估实体-属性语义的关联。

Method: 提出了一种基于视觉问答（VQA）定位策略的自动评估方法L-VQAScore，通过视觉定位和VQA探测来评估正确和错误的属性生成。

Result: 在新构建的数据集上，L-VQAScore在与人评估的相关性上优于现有方法，能够捕捉细粒度的实体-属性关联。

Conclusion: L-VQAScore是一种可靠且可扩展的评估方法，有望替代主观评估。

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [134] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本描述生成多样化自然人体运动序列的方法，通过构建大规模数据集和评估框架，实现了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法在零样本泛化能力上受限，且缺乏全面的评估框架，阻碍了任务进展。

Method: 开发高效标注流程并构建MotionMillion数据集（最大规模），提出MotionMillion-Eval评估基准，采用可扩展架构训练7B参数模型。

Result: 模型在零样本和复杂组合运动上表现出强泛化能力。

Conclusion: 该方法为零样本人体运动生成迈出重要一步，代码已开源。

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [135] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: 提出了一种名为VLV的自动编码器框架，通过利用预训练组件（视觉编码器、T2I扩散模型解码器和LLM）减少训练成本和数据需求，构建高性能视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 减少构建高性能视觉语言模型所需的昂贵训练成本和大规模配对数据。

Method: 利用预训练组件建立信息瓶颈，通过冻结T2I扩散解码器规范化语言表示空间，并使用LLM解码中间表示生成详细描述。

Result: 构建了与GPT-4o和Gemini 2.0 Flash相当的先进字幕生成器，训练成本低于1000美元。

Conclusion: VLV框架显著降低了训练成本和数据需求，同时保持了高性能。

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [136] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent是一个统一的超分辨率通用系统，能将任何图像提升至4K分辨率，甚至更高。它通过三个核心组件（分析、感知代理和修复代理）实现高质量图像修复，并在多个领域取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率图像修复的普遍性问题，尤其是在极端退化情况下，提供高质量的4K输出。

Method: 系统包含三个核心模块：分析模块定制流程，感知代理分析输入并制定修复计划，修复代理执行计划并优化输出。

Result: 在11个任务类别和26个基准测试中表现优异，涵盖自然图像、医学影像等多个领域。

Conclusion: 4KAgent为低层次视觉任务建立了新的代理范式，推动了视觉自主代理的创新。

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [137] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 该论文探讨了利用预训练的文本到图像扩散模型作为视觉编码器的潜力，以解决CLIP在捕捉细粒度细节上的不足，并提出了一种融合策略以提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: CLIP作为视觉编码器在捕捉全局信息时可能忽略细粒度细节，限制了多模态大语言模型在视觉问答中的表现。

Method: 研究扩散模型的内部表示，发现其语义丰富且能编码图像-文本对齐；提出融合CLIP和扩散特征的策略，并分析了信息泄漏问题及其缓解方法。

Result: 在通用视觉问答和专用MLLM基准测试中，融合策略显示出扩散模型在视觉理解中的潜力，尤其在需要空间和组合推理的任务中。

Conclusion: 扩散模型可作为视觉编码器提升多模态语言模型的性能，特别是在需要细粒度视觉理解的任务中。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [138] [Interactive Text-to-SQL via Expected Information Gain for Disambiguation](https://arxiv.org/abs/2507.06467)
*Luyu Qiu,Jianing Li,Chi Su,Lei Chen*

Main category: cs.DB

TL;DR: 论文提出了一种交互式Text-to-SQL框架，通过概率推理和多候选查询解决自然语言查询的歧义问题。


<details>
  <summary>Details</summary>
Motivation: 许多终端用户缺乏SQL技能，而现有Text-to-SQL系统在处理复杂数据库时因自然语言歧义易出错。

Method: 采用概率推理模型生成多个候选SQL查询，通过用户交互逐步消除歧义，基于预期信息增益选择澄清问题。

Result: 系统能够动态调整SQL查询分布，通过交互减少不确定性，提高准确性。

Conclusion: 交互式框架有效解决了自然语言歧义问题，提升了Text-to-SQL系统的实用性。

Abstract: Relational databases are foundational to numerous domains, including business
intelligence, scientific research, and enterprise systems. However, accessing
and analyzing structured data often requires proficiency in SQL, which is a
skill that many end users lack. With the development of Natural Language
Processing (NLP) technology, the Text-to-SQL systems attempt to bridge this gap
by translating natural language questions into executable SQL queries via an
automated algorithm. Yet, when operating on complex real-world databases, the
Text-to-SQL systems often suffer from ambiguity due to natural ambiguity in
natural language queries. These ambiguities pose a significant challenge for
existing Text-to-SQL translation systems, which tend to commit early to a
potentially incorrect interpretation. To address this, we propose an
interactive Text-to-SQL framework that models SQL generation as a probabilistic
reasoning process over multiple candidate queries. Rather than producing a
single deterministic output, our system maintains a distribution over possible
SQL outputs and seeks to resolve uncertainty through user interaction. At each
interaction step, the system selects a branching decision and formulates a
clarification question aimed at disambiguating that aspect of the query.
Crucially, we adopt a principled decision criterion based on Expected
Information Gain to identify the clarification that will, in expectation, most
reduce the uncertainty in the SQL distribution.

</details>


### [139] [QUEST: Query Optimization in Unstructured Document Analysis](https://arxiv.org/abs/2507.06515)
*Zhaoze Sun,Qiyan Deng,Chengliang Chai,Kaisen Jin,Xinyu Guo,Han Han,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.DB

TL;DR: QUEST提出了一种针对大型语言模型（LLM）驱动的数据系统的优化策略，通过索引、证据增强检索和实例优化查询执行，显著降低了LLM成本并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在优化LLM成本方面效果不佳，导致查询执行效率低下。QUEST旨在填补这一空白，提供更高效的优化策略。

Method: QUEST采用索引策略减少提取操作成本，证据增强检索减少遗漏，实例优化查询执行根据不同文档生成不同计划。

Result: 在3个真实数据集上，QUEST实现了30%-6倍的成本节省，F1分数提高了10%-27%。

Conclusion: QUEST通过创新优化策略，显著提升了LLM驱动的数据系统的效率和性能。

Abstract: Most recently, researchers have started building large language models (LLMs)
powered data systems that allow users to analyze unstructured text documents
like working with a database because LLMs are very effective in extracting
attributes from documents. In such systems, LLM-based extraction operations
constitute the performance bottleneck of query execution due to the high
monetary cost and slow LLM inference. Existing systems typically borrow the
query optimization principles popular in relational databases to produce query
execution plans, which unfortunately are ineffective in minimizing LLM cost. To
fill this gap, we propose QUEST, which features a bunch of novel optimization
strategies for unstructured document analysis. First, we introduce an
index-based strategy to minimize the cost of each extraction operation. With
this index, QUEST quickly retrieves the text segments relevant to the target
attributes and only feeds them to LLMs. Furthermore, we design an
evidence-augmented retrieval strategy to reduce the possibility of missing
relevant segments. Moreover, we develop an instance-optimized query execution
strategy: because the attribute extraction cost could vary significantly
document by document, QUEST produces different plans for different documents.
For each document, QUEST produces a plan to minimize the frequency of attribute
extraction. The innovations include LLM cost-aware operator ordering strategies
and an optimized join execution approach that transforms joins into filters.
Extensive experiments on 3 real-world datasets demonstrate the superiority of
QUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%
compared with state-of-the-art baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [140] [Designing Parallel Algorithms for Community Detection using Arachne](https://arxiv.org/abs/2507.06471)
*Fuhuan Li,Zhihui Du,David A. Bader*

Main category: cs.DC

TL;DR: 本文提出了基于Arachne框架的并行社区检测算法（Label Propagation和Louvain），显著提升了性能，速度比NetworkX快710倍，igraph快75倍，NetworKit快12倍。


<details>
  <summary>Details</summary>
Motivation: 随着图数据的广泛应用，需要高效且可扩展的社区检测算法。现有Python工具（如NetworkX和igraph）缺乏高效并行化能力。

Method: 在Arachne框架中并行实现Label Propagation和Louvain算法，并分析其性能。

Result: 实验结果显示，Arachne方法显著优于基线工具，最高速度提升710倍，并分析了并行Louvain算法的可扩展性。

Conclusion: Arachne框架及其社区检测实现是开源的，性能优越，适用于大规模图分析。

Abstract: The rise of graph data in various fields calls for efficient and scalable
community detection algorithms. In this paper, we present parallel
implementations of two widely used algorithms: Label Propagation and Louvain,
specifically designed to leverage the capabilities of Arachne which is a
Python-accessible, open-source framework for large-scale graph analysis. Our
implementations achieve substantial speedups over existing Python-based tools
like NetworkX and igraph, which lack efficient parallelization, and are
competitive with parallel frameworks such as NetworKit. Experimental results
show that Arachne-based methods outperform these baselines, achieving speedups
of up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit.
Additionally, we analyze the scalability of our implementation under varying
thread counts, demonstrating how different phases contribute to overall
performance gains of the parallel Louvain algorithm. Arachne, including our
community detection implementation, is open-source and available at
https://github.com/Bears-R-Us/arkouda-njit .

</details>


### [141] [Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing](https://arxiv.org/abs/2507.06608)
*Xiaoxiang Shi,Colin Cai,Junjia Du,Zhanda Zhu,Xingda Wei,Zhihao Jia*

Main category: cs.DC

TL;DR: 本文提出了一种在单个GPU内动态分配资源以解耦预填充和解码阶段的方法，显著提高了吞吐量和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有预填充-解码（PD）解耦方法需要更多硬件资源，而分块预填充方法虽提高了GPU利用率，但引入了阶段干扰。本文探索在单个GPU内实现解耦的可能性。

Method: 通过分析GPU资源的边际效益，动态分配资源给预填充和解码阶段，实现同一GPU内的解耦。

Result: 实验表明，Nexus系统在多种模型和工作负载下，吞吐量最高提升2.2倍，TTFT降低20倍，TBT降低2.5倍，且仅用一半GPU数量即可超越vLLM解耦方案。

Conclusion: 在单个GPU内动态分配资源解耦预填充和解码阶段是可行的，能显著提升性能并减少硬件需求。

Abstract: Current prefill-decode (PD) disaggregation is typically deployed at the level
of entire serving engines, assigning separate GPUs to handle prefill and decode
phases. While effective at reducing latency, this approach demands more
hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode
requests within the same batch, but introduces phase interference between
prefill and decode.
  While existing PD disaggregation solutions separate the phases across GPUs,
we ask: can the same decoupling be achieved within a single serving engine? The
key challenge lies in managing the conflicting resource requirements of prefill
and decode when they share the same hardware. In this paper, we first show that
chunked prefill requests cause interference with decode requests due to their
distinct requirements for GPU resources. Second, we find that GPU resources
exhibit diminishing returns. Beyond a saturation point, increasing GPU
allocation yields negligible latency improvements. This insight enables us to
split a single GPU's resources and dynamically allocate them to prefill and
decode on the fly, effectively disaggregating the two phases within the same
GPU.
  Across a range of models and workloads, our system Nexus achieves up to 2.2x
higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also
outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x
lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using
only half the number of GPUs.

</details>


### [142] [Towards Efficient and Scalable Distributed Vector Search with RDMA](https://arxiv.org/abs/2507.06653)
*Xiangyu Zhi,Meng Chen,Xiao Yan,Baotong Lu,Hui Li,Qianxi Zhang,Qi Chen,James Cheng*

Main category: cs.DC

TL;DR: CoTra是一个分布式向量搜索系统，通过算法与系统协同设计解决计算与通信效率的冲突，显著提升了查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 由于数据集庞大且数据读取密集，单机内存和带宽限制了基于相似性的向量搜索应用。

Method: 采用聚类数据分区、异步执行和任务推送等技术减少通信，结合任务调度、通信批处理和存储格式优化。

Result: 在16台机器上，CoTra的查询吞吐量比单机提升9.8-13.4倍，是基线最佳性能的2.12-3.58倍。

Conclusion: CoTra通过分布式协作搜索和系统优化，有效解决了向量搜索的扩展性问题。

Abstract: Similarity-based vector search facilitates many important applications such
as search and recommendation but is limited by the memory capacity and
bandwidth of a single machine due to large datasets and intensive data read. In
this paper, we present CoTra, a system that scales up vector search for
distributed execution. We observe a tension between computation and
communication efficiency, which is the main challenge for good scalability,
i.e., handling the local vectors on each machine independently blows up
computation as the pruning power of vector index is not fully utilized, while
running a global index over all machines introduces rich data dependencies and
thus extensive communication. To resolve such tension, we leverage the fact
that vector search is approximate in nature and robust to asynchronous
execution. In particular, we run collaborative vector search over the machines
with algorithm-system co-designs including clustering-based data partitioning
to reduce communication, asynchronous execution to avoid communication stall,
and task push to reduce network traffic. To make collaborative search
efficient, we introduce a suite of system optimizations including task
scheduling, communication batching, and storage format. We evaluate CoTra on
real datasets and compare with four baselines. The results show that when using
16 machines, the query throughput of CoTra scales to 9.8-13.4x over a single
machine and is 2.12-3.58x of the best-performing baseline at 0.95 recall@10.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [143] [One task to rule them all: A closer look at traffic classification generalizability](https://arxiv.org/abs/2507.06430)
*Elham Akbari,Zihao Zhou,Mohammad Ali Salahuddin,Noura Limam,Raouf Boutaba,Bertrand Mathieu,Stephanie Moteau,Stephane Tuffin*

Main category: cs.NI

TL;DR: 论文探讨现有网站指纹和流量分类解决方案在评估环境变化时表现不佳的问题，并提出一个考虑实际标签约束的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案的性能依赖于特定环境的假设，无法适应评估环境的变化。

Method: 通过将三种先前解决方案的模型应用于彼此的数据集，识别导致性能差异的数据集和模型特性，并设计一个模拟现实标签约束的评估框架。

Result: 在分布偏移的情况下，最佳解决方案的性能仅为30%-40%，且简单1-Nearest Neighbor分类器的表现接近。

Conclusion: 论文强调实际应用中流量模型的性能表现，并指出分布偏移对分类性能的显著影响。

Abstract: Existing website fingerprinting and traffic classification solutions do not
work well when the evaluation context changes, as their performances often
heavily rely on context-specific assumptions. To clarify this problem, we take
three prior solutions presented for different but similar traffic
classification and website fingerprinting tasks, and apply each solution's
model to another solution's dataset. We pinpoint dataset-specific and
model-specific properties that lead each of them to overperform in their
specific evaluation context.
  As a realistic evaluation context that takes practical labeling constraints
into account, we design an evaluation framework using two recent real-world TLS
traffic datasets from large-scale networks. The framework simulates a
futuristic scenario in which SNIs are hidden in some networks but not in
others, and the classifier's goal is to predict destination services in one
network's traffic, having been trained on a labelled dataset collected from a
different network. Our framework has the distinction of including real-world
distribution shift, while excluding concept drift. We show that, even when
abundant labeled data is available, the best solutions' performances under
distribution shift are between 30% and 40%, and a simple 1-Nearest Neighbor
classifier's performance is not far behind. We depict all performances measured
on different models, not just the best ones, for a fair representation of
traffic models in practice.

</details>


### [144] [Stacked Intelligent Metasurfaces-Aided eVTOL Delay Sensitive Communications](https://arxiv.org/abs/2507.06632)
*Liyuan Chen,Kai Xiong,Yujie Qin,Hanqing Yu,Supeng Leng,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于网络演算和优化算法的方法，以解决高级空中交通（AAM）系统中eVTOL通信延迟的数学建模问题。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，地面交通拥堵问题日益严重，AAM系统利用低空空域构建三维交通网络成为解决方案，但URLLC技术的不成熟对安全性提出了挑战。

Method: 采用网络演算工具首次推导AAM系统中通信延迟的概率上界，并通过BCD算法和SDR方法解决非凸优化问题。

Result: 提出了量化动态空中交通模式下延迟概率上界的数学框架，并分析了负载强度和总延迟对延迟概率上界的影响。

Conclusion: 该方法为AAM系统的安全运行提供了理论支持，并为未来研究奠定了基础。

Abstract: With rapid urbanization and increasing population density, urban traffic
congestion has become a critical issue, and traditional ground transportation
methods are no longer sufficient to address it effectively. To tackle this
challenge, the concept of Advanced Air Mobility (AAM) has emerged, aiming to
utilize low-altitude airspace to establish a three-dimensional transportation
system. Among various components of the AAM system, electric vertical take-off
and landing (eVTOL) aircraft plays a pivotal role due to their flexibility and
efficiency. However, the immaturity of Ultra Reliable Low Latency Communication
(URLLC) technologies poses significant challenges to safety-critical AAM
operations. Specifically, existing Stacked Intelligent Metasurfaces (SIM)-based
eVTOL systems lack rigorous mathematical frameworks to quantify probabilistic
delay bounds under dynamic air traffic patterns, a prerequisite for collision
avoidance and airspace management. To bridge this gap, we employ network
calculus tools to derive the probabilistic upper bound on communication delay
in the AAM system for the first time. Furthermore, we formulate a complex
non-convex optimization problem that jointly minimizes the probabilistic delay
bound and the propagation delay. To solve this problem efficiently, we propose
a solution based on the Block Coordinate Descent (BCD) algorithm and
Semidefinite Relaxation (SDR) method. In addition, we conduct a comprehensive
analysis of how various factors impact regret and transmission rate, and
explore the influence of varying load intensity and total delay on the
probabilistic delay bound.

</details>


### [145] [Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G](https://arxiv.org/abs/2507.06911)
*Michele Polese,Niloofar Mohamadi,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文提出了一种融合O-RAN和AI-RAN的新架构，支持在共享基础设施上统一编排和管理电信与AI工作负载，为网络运营商提供了边缘AI的盈利机会。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型AI应用在网络边缘的普及，需要从单纯利用AI优化网络转向支持分布式AI工作负载，从而利用现有基础设施实现盈利。

Method: 提出了融合O-RAN和AI-RAN的架构，包括AI-RAN Orchestrator和AI-RAN站点，支持异构AI部署和实时处理能力。

Result: 该架构支持灵活部署选项，满足不同时间尺度的异构工作负载编排需求，同时保持开放接口和多厂商互操作性。

Conclusion: 该架构为网络运营商提供了边缘AI的盈利机会，同时扩展了Open RAN的原则以支持分布式AI工作负载。

Abstract: The proliferation of data-intensive Artificial Intelligence (AI) applications
at the network edge demands a fundamental shift in RAN design, from merely
consuming AI for network optimization, to actively enabling distributed AI
workloads. This paradigm shift presents a significant opportunity for network
operators to monetize AI at the edge while leveraging existing infrastructure
investments. To realize this vision, this article presents a novel converged
O-RAN and AI-RAN architecture that unifies orchestration and management of both
telecommunications and AI workloads on shared infrastructure. The proposed
architecture extends the Open RAN principles of modularity, disaggregation, and
cloud-nativeness to support heterogeneous AI deployments. We introduce two key
architectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN
Service Management and Orchestration (SMO) to enable integrated resource and
allocation across RAN and AI workloads; and (ii) AI-RAN sites that provide
distributed edge AI platforms with real-time processing capabilities. The
proposed system supports flexible deployment options, allowing AI workloads to
be orchestrated with specific timing requirements (real-time or batch
processing) and geographic targeting. The proposed architecture addresses the
orchestration requirements for managing heterogeneous workloads at different
time scales while maintaining open, standardized interfaces and multi-vendor
interoperability.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [146] [Pyrosome: Verified Compilation for Modular Metatheory](https://arxiv.org/abs/2507.06360)
*Dustin Jamner,Gabriel Kammer,Ritam Nag,Adam Chlipala*

Main category: cs.PL

TL;DR: Pyrosome是一个模块化语言元理论的通用框架，支持可扩展的语义和编译，通过Coq实现。其核心创新是支持语言、目标语言和编译器的新规则添加，同时保持等价性。


<details>
  <summary>Details</summary>
Motivation: 解决传统语义推理技术通常与特定语言和编译器结构绑定的问题，提供一种可扩展且可验证的编译器框架。

Method: 基于依赖排序的等式理论定义编程语言的深层嵌入语义，通过类型检查和等式推理完成编译器正确性证明。支持垂直组合和特性扩展。

Result: 成功构建了一个从System F到CPS翻译和闭包转换的多遍编译器，展示了如何逐步扩展语言特性并重用原有定理。

Conclusion: Pyrosome为模块化语言设计和编译器验证提供了灵活且强大的工具，支持多种语言特性和编译策略。

Abstract: We present Pyrosome, a generic framework for modular language metatheory that
embodies a novel approach to extensible semantics and compilation, implemented
in Coq. Common techniques for semantic reasoning are often tied to the specific
structures of the languages and compilers that they support. In Pyrosome,
verified compilers are fully extensible, meaning that to extend a language
(even with a new kind of effect) simply requires defining and verifying the
compilation of the new feature, reusing the old correctness theorem for all
other cases. The novel enabling idea is an inductive formulation of equivalence
preservation that supports the addition of new rules to the source language,
target language, and compiler.
  Pyrosome defines a formal, deeply embedded notion of programming languages
with semantics given by dependently sorted equational theories, so all
compiler-correctness proofs boil down to type-checking and equational
reasoning. We support vertical composition of any compilers expressed in our
framework in addition to feature extension. As a case study, we present a
multipass compiler from System F with simple references, through CPS
translation and closure conversion. Specifically, we demonstrate how we can
build such a compiler incrementally by starting with a compiler for simply
typed lambda-calculus and adding natural numbers, the unit type, recursive
functions, and a global heap, then extending judgments with a type environment
and adding type abstraction, all while reusing the original theorems. We also
present a linear version of the simply typed CPS pass and compile a small
imperative language to the simply typed target to show how Pyrosome handles
substructural typing and imperative features.

</details>


### [147] [Fast Collection Operations from Indexed Stream Fusion](https://arxiv.org/abs/2507.06456)
*Scott Kovach,Praneeth Kolichala,Kyle A. Miller,David Broman,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: 提出了一种高效遍历和组合关联集合数据结构的系统，无需专用编译器或分阶段编译即可实现高效和可组合性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要专用编译器或分阶段编译的问题，提供更高效的集合操作方式。

Method: 基于索引流的表示方法，避免中间分配，支持复杂连接操作。

Result: 在Lean、Morphic和Rust中实现，并在Lean中提供了功能正确性的机械化证明。

Conclusion: 该系统提供了一种高效且可组合的集合操作方法，无需依赖专用编译器。

Abstract: We present a system of efficient methods for traversing and combining
associative collection data structures. A distinguishing feature of the system
is that, like traditional sequential iterator libraries, it does not require
specialized compiler infrastructure or staged compilation for efficiency and
composability. By using a representation based on indexed streams, the library
can express complex joins over input collections while using no intermediate
allocations. We implement the library for the Lean, Morphic, and Rust
programming languages and provide a mechanized proof of functional correctness
in Lean.

</details>


### [148] [Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing](https://arxiv.org/abs/2507.06584)
*Qiong Feng,Xiaotian Ma,Ziyuan Feng,Marat Akhin,Wei Song,Peng Liang*

Main category: cs.PL

TL;DR: CrossLangFuzzer是一个用于检测跨语言编译器错误的框架，通过生成和变异中间表示程序，发现了多个JVM语言编译器中的错误。


<details>
  <summary>Details</summary>
Motivation: 跨语言编译的正确性研究不足，需要填补这一空白。

Method: 提出CrossLangFuzzer框架，使用通用中间表示生成测试程序，并应用三种变异技术增强多样性。

Result: 发现24个编译器错误，其中TypeChanger变异技术最有效。

Conclusion: 研究首次专注于跨语言编译错误的诊断，有助于提升多语言环境下的编译器正确性。

Abstract: Compilers play a central role in translating high-level code into executable
programs, making their correctness essential for ensuring code safety and
reliability. While extensive research has focused on verifying the correctness
of compilers for single-language compilation, the correctness of cross-language
compilation - which involves the interaction between two languages and their
respective compilers - remains largely unexplored. To fill this research gap,
we propose CrossLangFuzzer, a novel framework that introduces a universal
intermediate representation (IR) for JVM-based languages and automatically
generates cross-language test programs with diverse type parameters and complex
inheritance structures. After generating the initial IR, CrossLangFuzzer
applies three mutation techniques - LangShuffler, FunctionRemoval, and
TypeChanger - to enhance program diversity. By evaluating both the original and
mutated programs across multiple compiler versions, CrossLangFuzzer
successfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed
bugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2
confirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java
compiler. Among all mutators, TypeChanger is the most effective, detecting 11
of the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes
of cross-compilation bugs, examining the respective responsibilities of
language compilers when incorrect behavior occurs during cross-language
compilation. To the best of our knowledge, this is the firstwork specifically
focused on identifying and diagnosing compiler bugs in cross-language
compilation scenarios. Our research helps to understand these challenges and
contributes to improving compiler correctness in multi-language environments.

</details>


### [149] [Sound Interval-Based Synthesis for Probabilistic Programs](https://arxiv.org/abs/2507.06939)
*Guilherme Espada,Alcides Fonseca*

Main category: cs.PL

TL;DR: 提出一种类型系统和类型导向的合成算法，自动选择概率编程模型，提升效率并减少对统计专业知识的需求。


<details>
  <summary>Details</summary>
Motivation: 概率编程需要统计专业知识选择合适模型，限制了非专家用户的使用。自动模型选择可解决这一问题。

Method: 设计类型系统静态拒绝无效程序，类型导向合成算法保证生成类型安全程序，启发式搜索处理大搜索空间。

Result: 方法在复杂程序上优于随机搜索和DaPPer，显著提升合成效率。

Conclusion: 该技术使遗传编程等复杂方法得以应用，扩展了概率编程的实用性。

Abstract: Probabilistic programming has become a standard practice to model stochastic
events and learn about the behavior of nature in different scientific contexts,
ranging from Genetics and Ecology to Linguistics and Psychology. However,
domain practitioners (such as biologists) also need to be experts in statistics
in order to select which probabilistic model is suitable for a given particular
problem, relying then on probabilistic inference engines such as Stan, Pyro or
Edward to fine-tune the parameters of that particular model. Probabilistic
Programming would be more useful if the model selection is made automatic,
without requiring statistics expertise from the end user. Automatically
selecting the model is challenging because of the large search space of
probabilistic programs needed to be explored, because the fact that most of
that search space contains invalid programs, and because invalid programs may
only be detected in some executions, due to its probabilistic nature. We
propose a type system to statically reject invalid probabilistic programs, a
type-directed synthesis algorithm that guarantees that generated programs are
type-safe by construction, and an heuristic search procedure to handle the vast
search space. We collect a number of probabilistic programs from the
literature, and use them to compare our method with both a type-agnostic random
search, and a data-guided method from the literature (DaPPer). Our results show
that our technique both outperforms random search and DaPPer, specially on more
complex programs. This drastic performance difference in synthesis allows for
fast sampling of programs and enables techniques that previously suffered from
the complexity of synthesis, such as Genetic Programming, to be applied.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [150] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: 研究探讨了测试用例和测试套件的质量属性在实践中的重要性及挑战，通过工业调查发现Fault Detection等属性最重要，并指出资源效率等属性意见分歧较大。


<details>
  <summary>Details</summary>
Motivation: 理解测试用例和测试套件质量属性的相对重要性及实践中的挑战，以提升软件测试信心。

Method: 基于文献综述设计问卷，通过LinkedIn进行工业调查，收集354份从业者反馈。

Result: Fault Detection、Usability等属性最重要，Resource Efficiency等属性意见分歧大；常见挑战包括定义不足、缺乏有效指标等。

Conclusion: 研究结果为学术界和公司提供了指导，支持从业者在不同测试环境下实现高质量测试用例和套件。

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [151] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: 论文改进了检测Eager Test smell的规则，提出了一种新的定义和启发式方法，并通过实验验证其优于现有规则。


<details>
  <summary>Details</summary>
Motivation: 现有检测Eager Test smell的规则不够准确，导致检测结果不一致，无法满足实践需求。

Method: 通过文献综述分析现有定义和规则，提出新的定义和启发式方法，并在300个Java单元测试案例中手动验证。

Result: 新启发式方法能更精确地检测Eager Test smell，发现现有规则遗漏的模式。

Conclusion: 新方法更准确地捕捉了Eager Test smell的本质，可能解决实践者对现有规则不足的担忧。

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [152] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: 研究评估了8种先进LLM在生成高效C语言图分析代码中的表现，发现Claude Sonnet 4 Extended在代码生成和效率上表现最佳，但LLM在创新算法方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM在功能正确性或高级语言（如Python）中的应用，缺乏对LLM在严格运行时和内存约束下生成高效C代码能力的系统评估。

Method: 采用两种方法评估LLM：1）生成优于现有基准算法的代码；2）生成可集成到基准中的图算法代码。

Result: Claude Sonnet 4 Extended在即用代码生成和效率上表现最佳，甚至超越人工编写的基线（如三角形计数）。

Conclusion: 当代LLM擅长优化和集成现有算法，但在创新算法方面仍有不足。研究提供了提示词、生成代码和测量脚本以促进可重复研究。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [153] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: 论文探讨了问题跟踪生态系统（ITE）的复杂性，提出了最佳实践本体论以解决研究和实践中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 问题跟踪系统（ITS）在软件工程中广泛应用，但其生态系统（ITE）的复杂性和多样性尚未充分研究，需要更深入的探索。

Method: 通过访谈实践者和对多种ITS进行档案分析，揭示了ITE问题的上下文依赖性，并提出了最佳实践本体论。

Result: 研究发现ITE问题具有高度上下文依赖性，现有解决方案缺乏一致性和可比性。

Conclusion: 论文提出最佳实践本体论，旨在促进研究和实践中更一致的解决方案。

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [154] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: 论文探讨了语义冲突检测问题，提出了一种基于Code Llama 70B的测试生成工具，以改进SMAT的性能，并在复杂场景中验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 传统合并工具无法检测语义冲突，现有工具SMAT虽有效但存在高假阴性率，因此探索利用大型语言模型（LLMs）改进测试生成。

Method: 提出并集成基于Code Llama 70B的测试生成工具到SMAT中，通过不同交互策略、提示内容和参数配置生成测试，并在简单和复杂系统中评估其效果。

Result: LLM生成的测试在复杂场景中仍具挑战性和计算成本高，但显示出改进语义冲突检测的潜力。

Conclusion: 尽管LLM在复杂场景中面临挑战，但其在提升语义冲突检测方面具有前景。

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [155] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: 本文扩展并简化了AADL的形式化语义，通过Kripke结构的模态逻辑显式引入时间，并扩展了AADL标准中的运行时服务以支持行为规范语言。


<details>
  <summary>Details</summary>
Motivation: AADL标准缺乏对时间的显式建模，本文旨在填补这一空白并简化形式化语义。

Method: 使用Kripke结构的模态逻辑扩展AADL的形式化语义，并扩展运行时服务以支持行为规范语言。

Result: 提出了一个包含时间的AADL运行时服务示例，并通过HAMR实现BLESS编写的状态转换机行为。

Conclusion: 本文成功扩展了AADL的形式化语义，并展示了其在行为规范语言中的应用。

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [156] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: 论文探讨了大语言模型（LLM）在代码生成中链式思维（CoT）提示技术的质量及其影响因素，发现外部和内部因素均影响CoT质量，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的CoT质量及其对代码生成的影响，以提升LLM在代码生成中的可靠性和正确性。

Method: 通过分析1,023个失败的代码样本和210个CoT-代码对，评估CoT质量及其对代码生成的影响，并通过提示LLM改进低质量CoT。

Result: 发现外部因素（53.60%）和内部因素（40.10%）影响CoT质量；即使CoT正确，18.5%的代码仍有错误；改进CoT可行。

Conclusion: 研究揭示了CoT在代码生成中的挑战，为提升LLM推理和可靠性提供了方向。

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [157] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文分析了62种开源公平性干预工具，发现32%在过去一年内活跃维护，50%提供偏差检测和缓解功能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的偏见在关键领域造成负面影响，但公平性干预工具的采用率低，部分原因是实践者不了解其存在。

Method: 系统识别并分析62种开源公平性干预工具，评估其活跃度和功能特性。

Result: 32%的工具在过去一年内活跃维护，50%提供偏差检测和缓解功能，主要在模型训练中实现。

Conclusion: 研究揭示了公平性工具的现状，为实践者选择工具提供了参考。

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [158] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: 论文提出了一种名为5C Prompt Contract的框架，通过五个直观组件（Character, Cause, Constraint, Contingency, Calibration）简化提示设计，提升LLM交互的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在关键任务中的应用增加，需要一种既系统又简洁的提示设计框架，以减少认知负担并保持模型的创造性。

Method: 提出5C Prompt Contract框架，包含五个组件，明确整合了备用指令和输出优化，适用于多种LLM架构。

Result: 实验表明，5C框架在输入标记效率上表现优异，同时保持输出的一致性和丰富性，适用于资源有限的中小企业。

Conclusion: 5C框架为LLM交互提供了一种可靠、可解释且灵活的设计方法，尤其适合资源有限的用户。

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [159] [Trial Length, Pricing, and Rationally Inattentive Customers](https://arxiv.org/abs/2507.06422)
*F. Nguyen*

Main category: econ.GN

TL;DR: 论文提出了一种基于理性疏忽框架的理论，解释了免费试用后自动续订的商业模型，分析了试用期长度、续订价格及消费者保护政策的影响。


<details>
  <summary>Details</summary>
Motivation: 传统的试用模型认为消费者通过试用学习产品价值，但忽略了消费者因认知成本而未能取消不需要的订阅。本文旨在补充这一理论。

Method: 采用基于香农熵的信息处理成本模型，分析消费者注意力随时间衰减对试用期长度和续订价格的影响。

Result: 研究发现，试用期长度和续订价格是互补的；消费者保护政策（如“点击取消”）会缩短试用期。

Conclusion: 该模型为订阅合同常见特征提供了微观基础解释，并为评估数字市场的消费者保护政策提供了新视角。

Abstract: The "free trial" followed by automatic renewal is a dominant business model
in the digital economy. Standard models explain trials as a mechanism for
consumers to learn their valuation for a product. We propose a complementary
theory based on the rational inattention framework. Consumers know their
valuation but face a cognitive cost to remember to cancel an unwanted
subscription. We model this using a Shannon entropy-based cost of information
processing, where a consumer's baseline attention level decays with the length
of the trial period. This creates a novel trade-off for a monopolist firm: a
longer trial increases "inattentive revenue" from consumers who fail to cancel,
but it also lowers ex-ante consumer utility, making the initial offer less
attractive. We show that this trade-off leads to an interior optimal trial
length, even for products where value-learning is instantaneous. Our model,
under standard assumptions about demand elasticity and the distribution of
consumer valuations, generates sharp, testable predictions about the
relationship between contract terms. We find that the optimal renewal price and
trial length are complements: firms offering longer trials will also set higher
post-trial prices. We analyze the impact of policies aimed at curbing consumer
exploitation, such as "click-to-cancel" regulations. We show that such
policies, by making attention effectively cheaper, lead firms to reduce trial
lengths. The effect on price depends directly on the elasticity of demand from
loyal subscribers. We also extend the model to include paid trials, showing
that introductory prices and trial lengths act as strategic substitutes. Our
framework provides a micro-founded explanation for common features of
subscription contracts and offers a new lens through which to evaluate consumer
protection policies in digital markets.

</details>


### [160] [The Post Science Paradigm of Scientific Discovery in the Era of Artificial Intelligence: Modelling the Collapse of Ideation Costs, Epistemic Inversion, and the End of Knowledge Scarcity](https://arxiv.org/abs/2507.07019)
*Christian William Callaghan*

Main category: econ.GN

TL;DR: 论文探讨了AI导致创意边际成本崩溃后，经济约束从创意生成转向创意与人类需求递归结构的对齐，提出了体验矩阵理论（EMT），并分析了政策与制度设计的转变。


<details>
  <summary>Details</summary>
Motivation: 挑战知识稀缺性的基础假设，研究AI时代下创意生成成本下降后，经济和社会价值如何转向对齐创意的角色。

Method: 发展体验矩阵理论（EMT），将其形式化并应用于AI下的创意崩溃和制度调整动态，使用经济模型验证。

Result: 在后稀缺范式中，经济和社会价值更多流向引导、解释和嵌入创意的角色，而非单纯生成创意者。

Conclusion: 提出从知识经济向对齐经济的转型，重新定义增长理论、政策设计和制度目的，强调认知能力与人类体验价值的对齐。

Abstract: This paper develops a theoretical and formal response to the collapse in the
marginal cost of ideation caused by artificial intelligence (AI). In
challenging the foundational assumption of knowledge scarcity, the paper argues
that the key economic constraint is no longer the generation of ideas, but the
alignment of ideation with the recursive structure of human needs. Building on
previous work, we further develop Experiential Matrix Theory (EMT), a framework
that models innovation as a recursive optimisation process in which alignment,
rather than ideation, becomes the binding constraint. Accordingly, we formalise
core mechanisms of EMT and apply it to the dynamics of ideation collapse and
institutional realignment under AI. Using a series of defensible economic
models, we show that in this post-scarcity paradigm, the creation of economic
and social value increasingly accrues to roles that guide, interpret, and
socially embed ideation, rather than to those that merely generate new ideas.
The paper theorises a transition from a knowledge economy to an alignment
economy, and derives policy implications for labor hierarchies, subsidy
structures, and institutional design. The university, in this context, must
invert its function from knowledge transmission to epistemic alignment. The
paper concludes by reframing growth not as a function of knowledge
accumulation, but of how well society aligns its expanding cognitive capacity
with the frontier of experiential human value. This redefinition of the
innovation constraint implies a transformation of growth theory, policy design,
and institutional purpose in the AI era.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [161] [Sustainability Transitions and Bending the Curve of Biodiversity Collapse in the Amazon Forest](https://arxiv.org/abs/2507.06663)
*Romero-Goyeneche Oscar Yandy,Ramirez Matias,Osorio-Garcia Ana Milena,Harman Canalle Ursula*

Main category: econ.TH

TL;DR: 分析亚马逊地区森林砍伐，提出三种可持续路径：优化、自然资本和再生变革，强调再生策略的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨森林砍伐问题，结合可持续转型和社会生态概念，寻找有效治理路径。

Method: 基于文献综述和秘鲁Puerto Maldonado的案例研究，分析基础设施、技术、叙事和制度的相互作用。

Result: 优化路径部分有效但强化剥削逻辑，自然资本框架依赖集中治理和市场工具，再生策略更具潜力。

Conclusion: 再生策略基于地方能动性和社区实验，对扭转生物多样性丧失至关重要。

Abstract: This paper undertakes an analysis of deforestation in the Amazon area using a
pathways-based approach to sustainability. We ground the analysis primarily in
the sustainability transitions literature but also draw a bridge with
socio-ecological concepts which helps us to understand the nature of
transitions in this context. The concept of a deforestation system is developed
by examining the interplay of infrastructure, technologies, narratives, and
institutions. Drawing on a literature review and an in-depth case study of
Puerto Maldonado in Madre de Dios, Peru, the paper identifies three pathways
for addressing deforestation: optimisation, natural capital, and regenerative
change. We suggest that while the optimisation pathway provides partial
solutions through mitigation and compensation strategies, it often reinforces
extractivist logics. The study also underscores the limitations of natural
capital frameworks, which tend to rely on centralised governance and
market-based instruments while lacking broader social engagement. In contrast,
our findings emphasise the potential of regenerative strategies rooted in local
agency, community-led experimentation, and context-sensitive institutional
arrangements. The paper contributes to ongoing debates on biodiversity
governance by illustrating how the spatial and long-term dynamics of
deforestation interact, and why inclusive, territorially grounded pathways are
crucial for bending the curve of biodiversity loss.

</details>
